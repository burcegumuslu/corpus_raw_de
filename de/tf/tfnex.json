{"title": null, "author": null, "url": null, "hostname": null, "description": null, "sitename": null, "date": "2023-10-26", "id": null, "license": null, "body": null, "comments": "", "commentsbody": null, "raw_text": null, "text": "2023-10-26T23:03:09Z\nhttps://opus4.kobv.de/opus4-fau/oai\noai:ub.uni-erlangen.de-opus:167\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:570\nmsc\nmsc:92C37\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nUntersuchungen zum Einfluss von Notch1 in der terminalen B-Zelldifferenzierung\nThe Influence of Notch1 during the terminal differentiation of B-cells\nRomer, Sven\nGen notch\nImmunglobuline\nImmunologie\nImmunsystem\nM\u00e4use <Unterfamilie>\nApoptosis\nB-Lymphozyt\nB-Lymphozyten-Rezeptor\nB-Lymphozyt-Tumor\nddc:570\nNotch1 wurde 1917 erstmals in der Fruchtfliege als strukturumbildende Mutation bei der Fl\u00fcgeldifferenzierung beschrieben. Im Verlauf der n\u00e4chsten 88 Jahre wurden auch au\u00dferhalb der klassischen Entwicklungsbiologie weitere essenzielle Funktionen des Transmembranrezeptors aufgedeckt. So ist Notch1 in der H\u00e4matopoese ein essenzieller Faktor f\u00fcr die Differenzierung von T-lymphoiden Vorl\u00e4uferzellen und ein allgemein antiapoptotischer Faktor f\u00fcr T Zellen. In unreifen B-Zellen dagegen wirkt Notch1 eher proapoptotisch und antiproliferativ, was bisher in H\u00fchner B-Zelllinien gezeigt wurde. Im ersten Teil dieser Arbeit wurde deshalb die Beteiligung von Notch1 an der Apoptose und der Zellzykluskontrolle in murinen B-Zelllinien n\u00e4her untersucht. Hier konnte in der reifen murinen B Zelllinie NYC 31.1 gezeigt werden, dass ein transduziertes, konstitutiv aktives Notch1 nach B Zellrezeptor-Kreuzvernetzung auf die Zelllinie stark Apoptose-induzierend wirkt, ohne aber Proliferation oder Zellzyklus zu beeinflussen. Im Gegensatz zu H\u00fchner-B-Zellen konnte der proapoptotische Effekt erst nach B-Zellaktivierung nachgewiesen werden. Da neuere Studien, unter anderem aus unserem Labor, zudem eine verst\u00e4rkte Synthese von Notch1 und seinem Zielgen Hes1 in in-vitro-aktivierten Milz-B-Zellen nachweisen konnten, lie\u00dfen sie eine Beteiligung von Notch1 bei der terminalen Differenzierung reifer B-Zellen in Plasmazellen vermuten. Im zweiten Teil dieser Arbeit sollte deshalb der Einfluss eines konstitutiv aktiven Notch1 auf die Aktivierung retroviral transduzierter muriner Milz-B-Zellen untersucht werden. Diese Analysen ergaben, dass Notch1 die f\u00fcr die terminale Plasmazelldifferenzierung wichtigen Transkriptionsfaktoren Blimp1 und XBP1 nicht reguliert. Hingegen aber zeigten Notch1-transduzierte Milz-B-Zellkulturen eine deutlich verminderte Frequenz IgM-sezernierender B-Zellen. Nach polyklonaler LPS-Aktivierung bewirkte Notch1 au\u00dferdem einen deutlich beschleunigten Zelltod. Diese Daten deuten darauf hin, dass Notch1 in Antigen-stimulierten reifen Milz-B-Zellen aktiviert wird und inhibitorisch sowohl auf die Immunglobulinsekretion, als auch auf die Lebensdauer der aktivierten Milz-B-Zellen einwirkt. Zusammengefasst sprechen diese Daten f\u00fcr eine generelle inhibitorische Funktion von aktiviertem Notch1 in reifen Antigen-stimulierten B-Lymphozyten\nNotch1 originally was identified 1917 as a wing-deforming mutation in fruit flies. During the next 88 years to come further essential functions of this transmembrane-receptor were described. In the hematopoietic system Notch1 is inevitable for the early T-cell differentation and further on it functions as a general anti-apoptotic factor for T-cells. In immature chicken B-cell lines however, Notch1 inhibits cell-proliferation and acts proapoptotic. In the first part, this study analysed the involvement of Notch1 in apoptotic processes and the cell-cycle control of the mature murine B-cell line NYC 31.1. The study revealed that active Notch1 acts proapoptotic only after cell death inducing B-cell-receptor crosslinking, without further influencing cell-cycle or proliferation of the cells. Since new studies showed an increased synthesis of Notch1 and its specific target gene Hes1 during induced plasmacytic differentiation of murine splenic B-cells and thus suggest an influence of Notch1 in this process, the second part of this study analysed the effect of active Notch1 in retroviral transduced murine splenic B-cells. It revealed that Notch1 does not regulate the transcription-factors Blimp1 and XBP1, but Notch1 transduced splenic B-cell cultures showed a reduced frequency of IgM-secreting cells. In addition, active Notch1 enforced and accelerated the cell death of polyclonal LPS-preactivated splenic B-cells. Taken together, these data point to a general inhibitory role of active Notch1 in mature activated B-lymphocytes.\n2005-08-29\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/167\nurn:nbn:de:bvb:29-opus-2049\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-2049\nhttps://opus4.kobv.de/opus4-fau/files/167/Promotion_Final_englSum.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:168\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:14H37\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nOvale und ebene algebraische Kurven mit unendlicher Kollineationsgruppe\nOvals and plane algebraic curves with an infinite collineation group\nWagner, Doris\nOval\nalgebraische Kurve\nebene algebraische Kurve\nKollineationsgruppe\ntransitive Automorphismengruppe\ntransitive Gruppe\ntotal unzusammenh\u00e4ngende\nddc:510\nIn der vorliegenden Arbeit werden Ovale und ihre Kollineationsgruppen in kompakten unzusammenh\u00e4ngenden Pappos-Ebenen untersucht. F\u00fcr eine gewisse Klasse von abgeschlossenen, stetig differenzierbaren Ovalen in p-adischen Ebenen \u2013 sog. Tillmann-Ovalen \u2013 wird gekl\u00e4rt, unter welchen Umst\u00e4nden diese eine transitive Kollineationsgruppe gestatten. Ovale mit transitiver Kollineationsgruppe nennt man auch homogen. Die Existenz von homogenen Ovalen in p-adischen Ebenen, die keine Kegelschnitte sind, wird nachgewiesen. Es werden weiterhin alle algebraischen Kurven in einer projektiven Ebene \u00fcber einem algebraisch abgeschlossenen K\u00f6rper klassifiziert, die eine unendliche Kollineationsgruppe gestatten. Au\u00dferdem werden Scharen bzw. Mengen verschiedener algebraischer Kurven mit unendlichem gemeinsamen Stabilisator betrachtet. Hierbei wird speziell auch auf Kegelschnitte bzw. Kegelschnittscharen eingegangen. Die Klassifikation aller algebraischen Kurven mit unendlicher Kollineationsgruppe \u00fcber algebraisch nicht abgeschlossenen K\u00f6rpern wird ebenfalls durchgef\u00fchrt.\nIn this paper we study ovals and their collineation groups in compact totally disconnected Pappian planes. For a certain class of closed, continuously differentiable ovals in p-adic planes \u2013 so called Tillmann-Ovals \u2013 we answer the question, under what circumstances they have a transitive collineation group. Ovals whith a transitive collineation group are also called homogeneous. The existence of homogeneous ovals in p-adic planes, which are not conics, is proved. In addition, we classify all algebraic curves in a projective plane over an algebraically closed field, which have an infinite collineation group. We also look at sets of different algebraic curves with an infinite common stabilizer. Especially sets of conics are considered. Also, the classification of all algebraic curves with an infinite collineation group over not algebraically closed fields is carried out.\n2005-09-02\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/168\nurn:nbn:de:bvb:29-opus-2057\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-2057\nhttps://opus4.kobv.de/opus4-fau/files/168/dissertation-doris-wagner.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:179\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:540\nmsc\nmsc:92Dxx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDetermination of single base mutations related to the gene specific diseases by using electrochemical DNA biosensors in the integrated system\nBestimmung von Einzelbasenmutationen hervorgerufen durch genetische Erkrankungen unter Benutzung von elektrochemischen Biosensoren in einem integrierten System\n\u00dclker, Burcu\nDNS\nBiosensor\nSNP\nddc:540\nIn this work, an electrochemical DNA biosensor for the detection of single base mutations was developed by using low cost screen-printed electrodes. It was shown that the DNA biosensors were able to detect single base mutations in the genes of FcV and FcII proteins which are involved in coagulation. The optimisation studies are performed in order to increase the electrochemical activity of the surface and enhanced the sensitivity of the detection methods. The screen printed carbon paste electrodes were selected for the development of DNA biosensors. The carbon paste material was applied to the surface of the chip by using screen printing technology, which is particularly suitable for the development of DNA chips. The DNA chip designed within the scope of this study consists of eight working electrodes (WEs), four reference electrodes (REs) and six counter electrodes (CEs). Label-free and enzyme -based electrochemical methods were used to detect single base mutations. It was even possible to discriminate genotypes. The procedures for label-free and enzyme-based electrochemical detection methods in each case consist of surface preparation, hybridization, washing and transduction steps. Concerning the label-free electrochemical detection, the guanine bases of the probe oligonuclotides were substituted by inosine. Inosine has similar base-pairing properties to guanine but its oxidation signal distinctly differs from that of guanine. Thus, the detection of the hybridization was accomplished with the appearance of the guanine oxidation signal, whereas no guanine signal was obtained from inosine substituted probes. In the enzyme-based electrochemical detection method, the hybridization event was detected indirectly by measuring the signal of the electrochemically active end product which occurred from the substrate in the presence of a specific enzyme, alkaline phosphatase. The label-free electrochemical detection system was preferred due to its characteristics of rapid and low-cost hybridization detection without the use of external redox indicators, but the use of this system is limited because of its low sensitivity. The enzyme-based electrochemical method provides increased sensitivity due to the huge amount of end product obtained from the substrate in the present of the specific enzyme. The electrode surface was activated chemically by using the ionic detergent SDS. Specific probe oligonucleotides were covalently immobilised on the working electrode surface using Silane mediated modification chemistry. The electrochemical detection methods were optimised by investigating different experimental parameters and the optimum conditions were chosen. The efficiency of the electrochemical surface activity is effected by the conditions of screen printing, pre-treatment, silane surface chemistry and probe immobilization. Different curing temperatures and curing times after screen printing were compared by measuring the guanine signal of the aminolinked probe in order to increase the electrochemical activity of the surface. The unavoidable adsorption of electrochemically active contaminations to the surface reduces the sensitivity and effects the reproducibility. For this reason, the activation of the electrode surface by chemical pre-treatment was necessary. Different detergents were analysed and the pre-treatment time was optimised. The optimum conditions for the silane chemistry were constituted according to the obtained results. The hybridization and wash stringency profoundly influence the determination of single base mutations. The stringency of the hybridization was mainly attained by optimising the hybridization temperature, also taking the respective theoretical melting temperature into consideration . In this regard, the hybridization was performed at different temperatures to determine the most suitable one for resolving one base mutations. It is well-known that DNA strongly adsorbs to most carbon surfaces. This imposes the necessity to reduce the adsorption of target on the transducer surface during the hybridization detection assay. Concerning this, the utilized ionic detergent content of the washing buffer was exposed to be inevitable to prevent adsorption and non-specific binding of the target. A high stringency of the washing buffer is essential for enhancing the selectivity to enable a reliable detection of single base mutations. The ionic detergent content and the stringency of washing buffer for label-free as well as enzyme-based detection methods were investigated. The detection of single base mutations with both of the detection systems were successfully accomplished by using optimum washing buffers. The label-free and enzyme-based determination of single base mutations related to FcV and FcII were also implemented successfully in a disposable integrated cartridge consisting of the fluidic platform. The blood sample was injected into this cartridge and the processes sample preparation, PCR and electrochemical detection were performed fully automatically for the first time. The eight WEs were measured simultaneously by a multi-potentiostat.\nIn dieser Arbeit wurde ein elektrochemischer DNA-Biosensor zur Detektion von Einzelbasenmutationen unter Verwendung von kosteng\u00fcnstigen Siebdruckelektroden entwickelt. Es wurde die Detektion von Einzelbasenmutationen in den Genen f\u00fcr das Faktor II- und Faktor V-Protein gezeigt. FcII und FcV sind Proteine aus der Blutgerinnungskaskade. Optimierungsarbeiten wurden durchgef\u00fchrt, um die elektrochemische Aktivit\u00e4t der Elektrodenoberfl\u00e4che zu steigern. Mit der Steigerung der Oberfl\u00e4chenaktivit\u00e4t konnte auch die Sensitivit\u00e4t der Detektion erh\u00f6ht werden. F\u00fcr die Entwicklung der DNA-Biosensoren wurden Siebdruckelektroden eingesetzt. Die Aufbringung der Kohlenstoffpaste auf die Oberfl\u00e4che der Chips erfolgte mit der Siebdruck-Technologie, welche besonders geeignet ist f\u00fcr die Entwicklung von DNA-Chips. Die in dieser Arbeit entwickelten DNA-Chips wiesen acht Arbeitselektroden, vier Referenzelektroden und sechs Gegenelektroden auf. Zur Detektion der Einzelbasenmutationen wurden markierungsfreie und enzymbasierte elektrochemische Methoden eingesetzt. Auch eine Genotypisierung konnte durchgef\u00fchrt werden. Die Schritte f\u00fcr die markierungsfreie und die enzymbasierte Methode bestanden jeweils aus Oberfl\u00e4chenvorbehandlung, Hybridisierung, Waschen und Auslesung. Bei der markierungsfreien Methode war bei dem F\u00e4nger-Oligonukleotid das Guanin ersetzt durch das Inosin. Inosin hat die gleichen Basenpaarungs-Eigenschaften wie Guanin. Jedoch unterscheidet sich das Oxidationssignal von Inosin von dem von Guanin. Damit war eine Korrelation zwischen dem Guanin-Oxidationssignal und der Hybridisierung geschaffen. Die Messung von einem Guanin-Oxidationssignal war damit ein Hinweis auf eine Hybridisierung, da das F\u00e4nger-Oligonukleotid kein Guanin-Signal generieren konnte. Bei der enzymbasierten Methode erfolgte die Detektion des Hybridisierungsereignisses \u00fcber die Messung des Signals von einem elektrochemisch aktiven Endprodukt, welches von einem spezifischen Enzym, der alkalischen Phosphatase, bei Vorhandensein des Substrats generiert wurde. Das markierungsfreie elektrochemische Detektions-System wurde bevorzugt, weil es schneller und kosteng\u00fcnstiger ist, da es ohne Redox-Indikatoren auskommt. Jedoch ist es wegen der geringen Sensitivit\u00e4t begrenzt einsetzbar. Die enzymbasierte Methode zeigt eine h\u00f6here Sensitivit\u00e4t in Abh\u00e4ngigkeit von der H\u00f6he des Endproduktes, welches bei Vorhandensein des spezifischen Enzyms generiert wird. Die Elektrodenoberfl\u00e4che wurde chemisch aktiviert durch das ionische Detergenz SDS. Unter Benutzung von Silan-Oberfl\u00e4chenchemie wurden spezifische F\u00e4nger-Oligonukleotide auf die Oberfl\u00e4che der Arbeitselektrode immobilisiert. \u00dcber die Untersuchung der verschiedenen experimentellen Parameter wurden die elektrochemischen Detektionsmethoden optimiert und die optimalen Parameter ausgew\u00e4hlt. Die Effektivit\u00e4t der elektrochemischen Oberfl\u00e4chenaktivit\u00e4t ist abh\u00e4ngig von den Bedingungen beim Siebdrucken, bei der Vorbehandlung, der Silan-Oberfl\u00e4chenchemie und der Immobilisierung. Um die elektrochemische Aktivit\u00e4t der Oberfl\u00e4che zu erh\u00f6hen wurden verschiedene Aush\u00e4rtetemperaturen und Aush\u00e4rtezeiten \u00fcber das Auslesen des Guanin-Signals von aminomarkierten F\u00e4nger-Oligonukleotiden untersucht. Die unvermeidbare Adsorption von elektrochemisch aktiven Verunreinigungen an die Oberfl\u00e4che reduzieren die Sensitivit\u00e4t und beeinflussen die Reproduzierbarkeit. Aus diesem Grunde war die Aktivierung der Elektrodenoberfl\u00e4che \u00fcber eine chemische Vorbehandlung erforderlich. Dazu wurden verschiedene Detergenzien getestet und die Dauer der Vorbehandlung optimiert. Von den erhaltenen Resultaten wurden die optimalen Bedingungen f\u00fcr die Silanchemie abgeleitet. Bei der Bestimmung von Einzelbasenmutationen ist die Stringenz bei der Hybridisierung und beim Waschen von gro\u00dfer Bedeutung. Die Stringenz bei der Hybridisierung wurde vor allem erreicht \u00fcber die Optimierung der Hybridisierungstemperatur. Die theoretische Schmelztemperatur diente dabei als Anhaltswert. Dazu wurde die Hybridisierung bei verschiedenen Temperaturen durchgef\u00fchrt und die mit den besten Resultaten im Hinblick auf die Einzelbasendiskriminierung ausgew\u00e4hlt. Es ist bekannt, dass DNA an die meisten Kohlenstoffoberfl\u00e4chen stark adsorbiert. Aus diesem Grunde war es wichtig, die Adsorption der Ziel-DNA an die Sensoroberfl\u00e4che w\u00e4hrend der Hybridisierung zu reduzieren. Es stellte sich heraus, dass der verwendete Anteil an einem ionischem Detergenz in dem Waschpuffer f\u00fcr eine Verhinderung der Adsorption und der unspezifischen Bindung der Ziel-DNA unerl\u00e4sslich ist. Eine hohe Stringenz des Waschpuffers ist essentiell f\u00fcr eine Steigerung der Selektivit\u00e4t, die wiederum notwendig ist f\u00fcr eine zuverl\u00e4ssige Detektion von Einzelbasenmutationen. Der Anteil an ionischem Detergenz und die Stringenz des Waschpuffers wurden sowohl f\u00fcr den markierungsfreie als auch f\u00fcr die enzymbasierte Detektionsmethode untersucht. Bei Verwendung von optimalen Waschpuffern konnte mit beiden Detektionssystemen die Bestimmung von Einzelbasenmutationen erfolgreich durchgef\u00fchrt werden. Die markierungsfreie und die enzymbasierte Bestimmung von Einzelbasenmutationen beim FcII- und FcV-Gen konnte erfolgreich in eine integrierte Cartridge zur Einmalverwendung, welche die fluidische Plattform enthielt, implementiert werden. Zum ersten Mal war es m\u00f6glich, die Prozesse der Probenaufbereitung, PCR und elektrochemische Detektion vollst\u00e4ndig automatisch in einer integrierten Cartridge durchzuf\u00fchren. Dabei wurden die acht Arbeitselektroden simultan \u00fcber ein Multipotentiostat ausgelesen.\n2005-09-07\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/179\nurn:nbn:de:bvb:29-opus-2228\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-2228\nhttps://opus4.kobv.de/opus4-fau/files/179/Dissertation_BurcuUelker.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:192\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:37B55\nmsc:37C70\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nExistence and structure of strange non-chaotic attractors\nExistenz und Struktur seltsamer nichtchaotischer Attraktoren\nJ\u00e4ger, Tobias\nErgodentheorie\nSeltsamer Attraktor\nQuasiperiodizit\u00e4t\nNichtlineare Dynamik\nddc:510\nThe existence of strange non-chaotic attractors (SNA) in quasiperiodically forced systems is remarkable in two ways, as it shows that strange attractors can occur both in systems which are not hyperbolic and in combination with nonchaotic dynamics. Further, SNA\u2019s seem to play an important role in the bifurcations of invariant tori, as they often occur in so-called \u2018non-smooth\u2019 saddle-node and pitchfork bifurcations. This is accompanied by a very distinctive behaviour, which can be described as \u2018exponential evolution of peaks\u2019. Based on this observation, the aim of this thesis is to develop a new method by which the existence of SNA can be proved and their structure can be analyzed. The introduction first provides some basic terminology. Further, it shows by means of numerical simulations that the described phenomenon is present in a number of different systems, some of which are directly motivated by physical models. Afterwards so-called \u2018pinched skew products\u2019 are studied in Chapter 2. For these very simple model systems the existence of SNA is already well-known, such that the aim here is to obtain further information about the properties and structure of these objects. This is achived by a quantitative description of the exponential evolution of peaks. The remainder of this thesis is then dedicated to the proof of the existence of SNA in certain parameter families of interval maps with additive quasiperiodic forcing. Chapter 3 first contains a description of saddle-node bifurcations in the considered families. Depending on whether these bifurcations involve the occurrence of SNA, they are called either \u2018smooth\u2019 (without SNA) or \u2018non-smooth\u2019 (with SNA). The characteristic behaviour during non-smooth bifurcations which can be observed numerically further suggests that there exists a particular type of very unusual orbits at the bifurcation point, which will be referred to as \u2018sink-source-orbits\u2019. The existence of such orbits implies the existence of SNA, which leads to a significant simplification of the problem. The sink-source-orbits are then constructed by means of approximation with finite trajectories. Chapter 4 first outlines the overall strategy and also contains the first step of the inductive construction. Before this is continued in Chapter 6, the necessary tools and auxiliary statements are provided in Chapter 5. Finally, the last chapter uses a slight modification of this construction to show the existence of SNA with a certain kind of symmetry, as they occur in non-smooth pitchfork bifurcations.\nDie Existenz seltsamer nicht-chaotischer Attraktoren (SNA) in quasiperiodisch getriebenen Systemen ist in zweierlei Hinsicht bemerkenswert, weil dies zeigt da\u00df seltsame Attraktoren sowohl in nicht-hyperbolischen Systemen als auch in Verbindung mit nicht-chaotischer Dynamik auftreten k\u00f6nnen. Dar\u00fcberhinaus spielen SNA auch eine besondere Rolle f\u00fcr die Verzweigungen invarianter Tori, da sie hsufig in sogenannten \u2018nicht-glatten\u2019 Sattel- und Heugabelverzweigungen auftreten. Dies ist meist von einem vorhergehenden sehr charakteristischen Verhalten der invarianten Tori begleitet, welches sich als \u2018Exponentielle Entwicklung von Peaks\u2019 beschreiben l\u00e4\u00dft. Ausgehend von dieser Beobachtung sollen in der vorliegenden Arbeit neue Methoden entwickelt werden, um sowohl die Existenz von SNA nachzuweisen als auch deren Struktur n\u00e4her zu untersuchen. Dabei werden in der Einleitung zun\u00e4chst die ben\u00f6tigten Grundbegriffe bereitgestellt und dann anhand numerischer Daten belegt, das sich das beschriebene Ph\u00e4nomen in einer ganzen Reihe verschiedener und auch physikalisch relevanter Systeme beobachten l\u00e4\u00dft. In Kapitel 2 werden dann zun\u00e4chst sogenannte \u2018Pinched skew products\u2019 untersucht. In diesen sehr einfachen Modellsystemen ist die Existenz von SNA bereits seit l\u00e4ngerem bekannt, so da\u00df es hier darum geht die Struktur dieser Objekte n\u00e4her zu beschreiben. Dies wird durch eine quantifizierte Beschreibung der exponentiellen Entwicklung von Peaks erm\u00f6glicht. Der gesamte restliche Teil der Arbeit ist dem Nachweis von SNA in bestimmten Parameterfamilien quasiperiodisch getriebener Intervallabbildungen gewidmet. Im 3. Kapitel werden dazu einige allgemeine Grundlagen zusammengestellt. Zun\u00e4chst werden allgemeine Sattelverzweigungen in den betrachteten Systemen beschrieben, die abh\u00e4ngig davon ob es am Verzweigungspunkt zum Auftreten von SNA\u2019s kommt als \u2018glatt\u2019 (ohne SNA) oder \u2018nicht-glatt\u2019 bezeichnet werden. Der in den numerischen Simulationen beobachtete Ablauf von nicht-glatten Verzweigungen legt zudem nahe, da\u00df es dabei zum Auftreten eines speziellen Typs sehr ungew\u00f6hnlicher Orbits kommt, die als \u2018Sink-source-orbits\u2019 bezeichnet werden. Die Existenz dieser Orbits impliziert wiederum die Existenz von SNA\u2019s. Daher gen\u00fcgt es in den folgenden Kapiteln die erstere nachzuweisen, was zu einer wesentlichen Vereinfachung des Problems f\u00fchrt. Die Konstruktion der Sink-source-orbits geschieht dann durch Approximation mit immer l\u00e4ngeren endlichen Trajektorien. In Kapitel 4 wird zun\u00a8achst die zugrundeliegende Strategie erl\u00e4utert und die erste Stufe der induktiven Konstruktion ausgef\u00fchrt. Zu Ende gef\u00fchrt wird diese dann in Kapitel 6, wobei die wesentlichen Hilfsmittel und Werkzeuge zuvor in Kapitel 5 bereitgestellt werden. Im letzten Kapitel wird schlie\u00dflich mit einer leichten Abwandlung der vorhergehenden Konstruktion die Existenz von SNA mit speziellen Symmetrieeigenschaften gezeigt, wie sie in nicht-glatten Heugabelverzweigungen entstehen.\n2005-10-28\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/192\nurn:nbn:de:bvb:29-opus-2420\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-2420\nhttps://opus4.kobv.de/opus4-fau/files/192/TobiasJaegerDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:224\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\nmsc\nmsc:65M60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nAdaptive Finite Element Method for the Numerical Simulation of Electric, Magnetic and Acoustic Fields\nAdaptive Finite-Elemente-Methode f\u00fcr die numerische Simulation elektrisher, magnetischer und akustischer Felder\nZhelezina, Elena\nFinite-Elemente-Methode\nFehlerabsch\u00e4tzung\nMagnetostatik\nElektrostatik\nddc:600\nThe subject of this thesis is an adaptive finite element method for the different kinds of partial differential equations. This topic is extremely popular nowadays since an accurate computation of the real-life engineering problems requires large number of unknowns. That is very costly with respect to computer time. Therefore, developing numerical methods which are more efficient is a desirable goal. Adaptive finite element methods in which a mesh (discretization of the computational domain) changes to improve a solution of the problem, can significantly reduce a demand for large number of unknowns. Changing of mesh is controlled and governed by an {\\it a posteriori} error estimation of the solution. At first, a post-processing error estimation and adaptive procedures based on it are studied. {\\it A posteriori} local and global error estimates based on a patch recovery technique are derived. A method for computation of an error distribution over the computational mesh and a remeshing strategy for construction of an optimal mesh based on the error distribution are presented. An adaptive automatic mesh generation procedure is discussed. It adjusts automatically the initial coarse computational mesh so, that the estimated errors are controlled within a specified tolerance. Algorithms for obtaining a regular refined grid, which means mesh without hanging nodes, in two and three dimensional spaces for different types of mesh are described. The presented adaptive procedures are tested on electrostatic problems in two and three dimensional spaces. The results are compared to non-adaptive numerical simulations. Then, adaptive procedures for a harmonic analysis of the acoustic wave equation are developed. Procedures for a local and global error estimation, a recovery and a remeshing technique are introduced. Numerical examples in two-dimensional space confirm the reliability of the developed adaptive finite element method for a frequency domain analysis of an acoustic equation. Next, a recovery technique for N\\'ed\\'elec's edge finite elements is developed. Local and global error indicators, which are based on energy errors for electromagnetic problems with discontinuity of magnetic potential across an interface of two different medium are introduced. The efficiency of the developed adaptive finite element method performance is proved on an example in 3D case. Finally, adaptive numerical simulations for industrial applications such as computation of the electric field of an voltage-driven bar, standing waves in a model of cleaning bath and a magnetic flux of problem TEAM 20 are conducted. By means of the adaptive finite element, a sufficiently accurate solution for these problems are received.\nDie vorliegende Arbeit besch\u00e4ftigt sich mit der Entwicklung von adaptiven Finite-Elemente-Methoden (FEM) f\u00fcr unterschiedliche partielle Differentialgleichungen (pDGL). Da f\u00fcr die pr\u00e4zise numerische Berechnung von physikalischen Problemstellungen die Anzahl der Unbekannten bei Standardverfahren sehr hoch ist, gilt es effiziente numerische Verfahren zu entwickeln, welche bei vorgegebner Genauigkeit die Anzahl der Unbekannten minimiert. Adaptive Finite-Elemente-Methoden, bei denen ein initiales FE-Gitter (eine Diskretisierung eines Simulationsgebiets) ver\u00e4ndert wird, um die L\u00f6sung f\u00fcr das Problem zu optimieren, k\u00f6nnen den Bedarf an einer gro\u00dfen Zahl von Unbekannten deutlich verringern. Die Verfeinerung des FE-Gitters wird von einer {\\it a posteriori} Fehlerabsch\u00e4tzung der L\u00f6sung kontrolliert und gesteuert. Zuerst werden {\\it a posteriori} Fehlerabsch\u00e4tzverfahren und darauf basierende adaptive Methoden studiert. Eine nachfolgende lokale und globale Fehlerabsch\u00e4tzung, die auf einer Patch-Methode beruht, wird davon abgeleitet. Eine M\u00f6glich\\-keit f\u00fcr die Berechnung der Fehlerverteilung \u00fcber das berechnete FE-Gitter und die \u00c4nderung von diesem f\u00fcr die Bestimmung eines optimalen FE-Gitters in Bezug auf die Fehlerverteilung werden hier vorgestellt. Eine M\u00f6glich\\-keit zur automatischen Erzeugung eines solchen FE-Gitters wird untersucht. Sie passt das grob berechnete FE-Gitter so an, dass der abgesch\u00e4tzte Fehler innerhalb einer bestimmten Toleranz liegt. Es wurden Algorithmen f\u00fcr verschiedene Gitter implementiert, um ein regul\u00e4res feineres FE-Gitter zu erhalten, also ein Gitter ohne h\u00e4ngende Knoten im zwei- und dreidimensionalen Raum. Die vorgestellten adaptiven Algorithmen wurden auf elektrostatische Problemstellungen im zwei- und dreidimensionalen Raum angewandt. Die Ergebnisse wurden mit nicht adaptiven numerischen Simulationen verglichen. Anschlie\u00dfend wurden adaptive Methoden f\u00fcr die harmonische Analyse der akustischen Wellengleichung entwickelt. Es wurden M\u00f6glichkeiten der lokalen und globalen Fehlerabsch\u00e4tzung, eine Recovery-Technik und eine Vernetzungsmethode eingef\u00fchrt. Rechenbeispiele im zweidimensionalen Raum best\u00e4tigen die Zuverl\u00e4ssigkeit der entwickelten adaptiven Finite-Elemente-Methode f\u00fcr die Analyse einer akustischen Wellengleichung im Frequenzbereich. Folgend wurde eine Recovery-Technik f\u00fcr N\\'ed\\'elec's Kanten-Finite-Elemente entwickelt. Es werden lokale und globale Fehlerindikatoren vorgestellt, welche die Unstetigkeit des magnetischen Vektorpotenzials an der Schnittstelle zwischen zwei Medien mit unterschiedlichen magnetischen Permeabilit\u00e4ten und den sich daraus ergebenden Energiefehlern ber\u00fccksichtigen. Die Effizienz der entwickelten adaptiven Finite-Elemente-Methoden wurde an einem dreidimensionalen Beispiel nachgewiesen. Zum Schluss wurden adaptive numerische Simulationen f\u00fcr Industrieanwendungen wie die Berechnung des elektrischen Feldes eines spannungsangeregter, mikromechanischen Balkens, die Simulation von Ultraschallstehwellen in einem Reinigungsbad sowie die Berechnung des TEAM 20 Problems (Testing Electromagnetic Analysis Methods) durchgef\u00fchrt. Durch die adaptive Finite-Elemente-Methode konnte eine hinreichende Genauigkeit bei gleichzeitiger hoher Effizienz f\u00fcr diese Probleme erzielt werden.\n2005-12-23\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/224\nurn:nbn:de:bvb:29-opus-2895\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-2895\nhttps://opus4.kobv.de/opus4-fau/files/224/ElenaZhelezinaDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:269\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\nmsc\nmsc:74E10\nmsc:76F02\nmsc:76F05\nmsc:76F55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nThe Turbulence Closure Model Based on Linear Anisotropy Invariant Analysis\nTurbulenzschliessungsmodell auf der Grundlage der linearen Anisotropie-Invariantenanalyse\nTerentiev, Leonid\nTurbulenz\nTurbulenztheorie\nddc:600\nThe dissertation work focuses on the development of the complete second-order turbulence closure model, which is systematically derived from the Navier-Stokes equations. In the derivation of the closure model the author tries to avoid the large number of assumptions, which significantly decreases the theoretical reliability of the model. An alternative way for the representation of anisotropy characteristics of the Reynolds stress and other feasible turbulent second-order tensor quantities is proposed in this work. The major advantage of the introduced anisotropy eigenvalue space is a linear description of all limiting states of turbulence. The linear way of treatment of the anisotropy invariants also allows one to introduce a simple scheme for interpolation of all model parameters between the turbulence limiting states. This makes it possible to extend the class of turbulent flows, for which the second-order closure model can be successfully applied. On the one hand, it eliminates an empirical element and generalizes the modeling approach; on the other, it simplifies the algorithm description and reduces the model complexity. The numerical validation showed that the prediction performance of the proposed closure model is sufficiently high for all considered homogeneous flows.\nDer Schwerpunkt der Dissertationsarbeit liegt in der Entwicklung vom kompletten Zweiter-Ordnung-Turbulenzschliessungsmodells f\u00fcr inkompressible Str\u00f6mungen, das von den Navier-Stokes Gleichungen direkt abgeleitet ist. In der Ableitung der Schlie\u00dfungsmodelle versucht der Verfasser eine gro\u00dfe Anzahl der empirischen Annahmen zu vermeiden, welche einen bedeutenden Verlust der theoretischen Zuverl\u00e4ssigkeit des Modells nach sich ziehen k\u00f6nnen. Diese Arbeit stellt einen neuen alternativen Weg zur Darstellung der Reynolds-Spannungs-Anisotropie-Invarianten und der anderen m\u00f6glichen Zweiter-Ordnungs-Tensoren vor. Der Hauptvorteil des eingef\u00fchrten linearen Anisotropie-Eigenwertraums ist eine lineare Beschreibungsweise von allen Turbulenzzustandsgrenzen. Die Anwendung von der linearen Anisotropie-Invarianten erlaubt es, eine einfache Methode f\u00fcr die Interpolation aller Modellparameter zwischen den Turbulenzzustandsgrenzen einzuf\u00fchren. Es f\u00fchrt auch zu einer Erweiterung der turbulenten Str\u00f6mungstypen, f\u00fcr die das Zweite-Ordnung-Turbulenzschliessungsmodell mit gutem Erfolg angewandt werden kann. Zum einem beseitigt es den Empirismus und generalisiert das Modellierungsverfahren, zum anderen, vereinfacht es die Algorithmusbeschreibung und verringert die Modellkomplexit\u00e4t. Im Rahmen der Dissertationsarbeit wurden die Schlie\u00dfungsmodelle f\u00fcr Dissipation- und Druck-Scher-Korrelation-Tensor entwickelt, erprobt und ausf\u00fchrlich diskutiert. Die numerischen Validierungen ergaben, dass das vorgeschlagene Schlie\u00dfungsmodell in der Lage ist, alle untersuchten homogenen Str\u00f6mungen korrekt wiederzugeben.\n2006-05-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/269\nurn:nbn:de:bvb:29-opus-3521\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-3521\nhttps://opus4.kobv.de/opus4-fau/files/269/LeonidTerentievDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:273\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:47D08\nmsc:60J25\nmsc:60J80\nmsc:60K35\nmsc:60K37\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDas Votermodell im Votermedium: Das Finite System Scheme\nThe Votermodel in a Votermedium: The Finite System Scheme\nNeumaier, Alexander\nStochastisches Teilchensystem\nIrrfahrtsproblem\nddc:510\nIn dieser Arbeit soll eine Untersuchung des Votermodells im Votermedium auf dem Torus erfolgen. Uns interessiert das Langzeitverhalten dieses Systems. Dabei zeigt sich, dass bei einer festen Torusgr\"o\"se dieses Modell in allen Dimensionen stets in eines der beiden trivialen Ma\"se $\\delta_0$, beziehungsweise $\\delta_1$ l\"auft. Man muss, um ein solches Verhalten zu vermeiden, die Torusgr\"o\"se zur Systemzeit geeignet in Beziehung setzen. Diese Vorgehensweise wurde von Cox und Greven in \\cite{cg} eingef\"uhrt und mit der Bezeichnung {\\it finite system scheme} versehen. Es zeigt sich dann, dass ein dimensionsabh\"angiges Langzeitverhalten auftritt. In dieser Arbeit werden die Dimensionen $d \\ge 3$ untersucht. F\"ur die Dimensionen $d \\le 2$ geben wir aber einige Bemerkungen, welches Verhalten wir erwarten. Bei den Beweisen f\"ur das Votermodell kann man h\"aufig die Dualit\"at des Votermodells zu den verschmelzenden Irrfahrten ausn\"utzen. Deshalb gehen wir zu einer anderen Perspektive \"uber, und betrachten das Medium vom Irrfahrer aus. Wir weisen dann eine {\\it mittlere} Rate f\"ur die Irrfahrten im Votermedium nach. Um dann befriedigende Ergebnisse f\"ur das Votermodell im Votermedium auf dem Torus zu erhalten, muss diese mittlere Rate eine stetige Funktion von bestimmten Anfangsverteilungen des Votermediums sein. Wir k\"onnen diese Stetigkeit zwar nicht nachweisen, arbeiten aber heraus, welche Methoden weiterhelfen k\"onnten. Es r\"uckt ein weiterer Prozess in den Blickpunkt, der der Schl\"ussel f\"ur die L\"osung der auftretenden Probleme sein k\"onnte. Der Mechanismus dieses Modells besteht aus Teilchen-Geburten im Ursprung und verschmelzenden Irrfahrten. Ziel ist nun die Berechnung der Markov-Halbgruppe dieses Systems, angewendet auf Momentenfunktionen. Wir bestimmen dann das Langzeitverhalten des Votermodells im Votermedium auf dem Torus unter der Annahme der Stetigkeit der mittleren Rate. Daf\"ur ben\"otigen wir Fisher-Wright-Diffusionen im Fisher-Wright-Medium und den Dualprozess zu diesem Modell, das durch den reinen Todesprozess im Fisher-Wright-Medium gegeben ist. Des Weiteren ben\"otigen wir Aussagen \"uber die Verteilung der Anzahl der verschmelzenden Irrfahrten im Votermedium.\nWe study the voter model in a voter medium on the torus, $d \\ge 3$. This is a two-type process, in which every component is given as a voter model. The voter model is an interacting particle system, and we are interested in the longtime behaviour of the model. For a fixed size of the torus the process converges to deterministic configurations $\\delta_0$ or $\\delta_1$. To obtain convergence to non trivial distributions, one has to consider dimension dependent time scales, a method called {\\it finite system scheme}. It was introduced by Cox and Greven \\cite{cg}. If the time scale is proportional to $N^d$, when $[-N,N]^d$ is the size of the torus, the process exhipits a diffusion through the invariant measures of the infinite system. Using duality, we represent the voter model in the voter medium as coalescing random walks in a voter model. Therefor the method of the environment viewed from the particle can be used to approximate random walks in a voter medium through classical random walks. One problem we can't solve is the continuous dependence of the effective rates of the random walks in a voter medium from certain initial distributions of the voter model in a voter medium We study then the longtime behaviour of the voter model in a voter medium under the assumption of the continuity of the average rate. For this purpose we need Fisher-Wright diffusions in a random medium and its dual process, which is given as the Kingman coalescent in a random medium.\n2006-05-03\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/273\nurn:nbn:de:bvb:29-opus-3589\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-3589\nhttps://opus4.kobv.de/opus4-fau/files/273/DissertationNeumaier.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:280\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:570\nmsc\nmsc:92F05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nModulation of E-cadherin activity in colon and other tumor cells and its effects on phenotype and gene expression patterns\nModulation der Aktivit\u00e4t von E-Cadherin sowie dessen Auswirkungen auf den Ph\u00e4notyp und Genexpressionsmuster in Kolon- und anderen Tumorzellen\nKuphal, Felix\nZelladh\u00e4sion\nZell-Adh\u00e4sionsmolek\u00fcl\nGenexpression\nddc:570\nE-cadherin, the major cell adhesion molecule of adherens junctions, mediates homophilic adhesion between epithelial cells. At the cell junctions, it associates with beta-catenin, which also functions as a transcriptional co-activator of the canonical Wnt signaling pathway, and beta-catenin to establish a link to the actin cytoskeleton. E-cadherin is a major determinant of epithelial differentiation during embryonic development and its loss is frequently involved in tumor progression. Apart from its role in cell adhesion, E-cadherin has an impact on transient signaling events, for instance involving Rho-family GTPases, receptor tyrosine kinases or Wnt signaling. However, it is still largely unknown whether E-cadherin-mediated cell-cell adhesion also alters gene expression. The aim of this work was to establish and functionally characterize model systems to study signal transduction events that are elicited by E-cadherin-mediated cell-cell adhesion. Full-size E-cadherin was expressed constitutively in the E-cadherin-negative cell lines EJ28 and C33-A, and conditionally in L929 fibroblasts. Forced E-cadherin expression led to a more differentiated phenotype, characterized by marked changes in cell morphology, cytoskeletal architecture and cell migration. As a complementary approach, E-cadherin-mediated cell-cell contacts were abrogated in several well differentiated epithelial cell lines via over-expression of dominant negative E-cadherin or via RNA interference-mediated knock-down of E-cadherin. In this study, the established model systems in different cell lines were exploited to study transcriptional alterations elicited by E-cadherin. In addition, they may prove useful tools to study other cell biological issues, such as cell migration or invasiveness. To get an idea of whether E-cadherin alters gene transcription, global gene expression was investigated by genome-wide DNA microarray analysis. L20-22 fibroblasts, which expressed E-cadherin in an inducible fashion, did not show altered gene expression even after several distinct periods of E-cadherin expression. Thus, the dramatic effects of E-cadherin on the cellular phenotype have to be implemented on a post-transcriptional level in this system. As E-cadherin-based cell junctions and Wnt signaling both share beta-catenin as an essential component, I investigated the circumstances under which the loss of E cadherin plays a role in Wnt-dependent transcription. Using E-cadherin-positive epithelial cell lines that display either no basal or constitutively activated Wnt signaling, I could demonstrate that the siRNA-mediated knock-down of E-cadherin induced the translocation of beta- catenin to the nucleus and an enhancement of beta-catenin/TCF-dependent reporter activity only in DLD-1 cells, in which the tumor suppressor gene APC is mutated. These data indicate that the loss of E-cadherin expression might become particularly relevant in colorectal tumors, as this type of cancer shows a very high percentage of loss of function mutations of APC.\nE-Cadherin ist das wichtigste Zelladh\u00e4sionsmolek\u00fcl der \"adherens junctions\" epithelialer Zellen. Es vermittelt Zell-Zell-Kontakte durch homophile Interaktion, und stellt durch Assoziation mit zytoplasmatischen Proteinen der Catenin-Familie eine Verbindung mit dem Aktin-Zytoskelett her. Von diesen Interaktoren besitzt beta-Catenin eine zus\u00e4tzliche Funktion als Co-Activator des Wnt-Signalwegs. W\u00e4hrend der Embryonalentwicklung ist E-Cadherin f\u00fcr die Differenzierung epithelialer Gewebe unerl\u00e4sslich. Verlust der E-Cadherin-Expression tr\u00e4gt zudem oft zur Tumorprogression bei. In seiner Funktion bei der Zelladh\u00e4sion ist E-Cadherin an transienten Signaltransduktions\u00acprozessen beteiligt. So beeinflusst es z.B. GTPasen der Rho-Familie, Rezeptortyrosinkinasen oder den Wnt-Signalweg. Es ist jedoch weitgehend unbekannt, ob die durch E-Cadherin vermittelte Zell-Zell-Adh\u00e4sion nachhaltig Ver\u00e4nderungen der Genexpression hervor ruft. Um die Signaltransduktionsprozesse zu analysieren, die durch E-Cadherin vermittelte Zell-Zell-Adh\u00e4sion ausgel\u00f6st werden, sollten in dieser Arbeit zellul\u00e4re Modellsysteme etabliert und funktionell charakterisiert werden. Dazu wurde E-Cadherin entweder konstitutiv in den E-Cadherin-negativen Tumorzelllinien EJ28 und C33-A, oder induzierbar in L929-Fibroblasten exprimiert. Die \u00dcberexpression von E-Cadherin hatte einen differenzierteren Ph\u00e4notyp zur Folge, der durch \u00c4nderungen der Zellmorphologie, der Architektur des Zytoskeletts und des Zellmigrationverhaltens gekennzeichnet war. In einem komplement\u00e4ren Ansatz wurden E-Cadherin vermittelte Zell-Zell-Kontakte in differenzierten Epithelzelllinien herunterreguliert, was zum einen durch \u00dcberexpression einer dominant-negativ wirkenden E-Cadherin-Variante oder zum anderen durch RNA-Interferenz erreicht wurde. Die in verschiedenen Zelllinien etablierten Modellsysteme wurden im Weiteren dazu verwendet, durch E-Cadherin bedingte \u00c4nderungen des Transkriptionsmusters zu untersuchen. Dar\u00fcberhinaus sind diese Modellsysteme bestens geeignet auch zellbiologische Fragen, wie z.B. zur Zellmigration oder Invasivit\u00e4t, zu beantworten. Um herauszufinden, ob die Modulation der E-Cadherin-Expression tats\u00e4chlich Genexpressions\u00acver\u00e4nderungen bewirkt, wurden globale Genexpressionsanalysen mit Hilfe von DNA-Microarrays durchgef\u00fchrt. L20-22-Fibroblasten, die E-Cadherin induzierbar exprimieren, zeigten keine \u00c4nderungen des Transkriptionsmusters. Dieses Ergebnis war unabh\u00e4ngig vom Zeitraum der E-Cadherin-Expression. Die dramatischen Auswirkungen, die E-Cadherin auf den zellul\u00e4ren Ph\u00e4notyp in diesem System hat, werden also nicht auf transkriptioneller, sondern auf post-transkriptioneller Ebene ausgel\u00f6st. Da sowohl die E-Cadherin-basierten Zell-Zell-Kontakte als auch der Wnt-Signalweg beta-Catenin als essentielles Molek\u00fcl beinhalten, wurden au\u00dferdem die Umst\u00e4nde untersucht, unter welchen sich der Verlust von E-Cadherin auf die Wnt-abh\u00e4ngige Transkription auswirkt. Dazu wurde E-Cadherin in epithelialen Zelllinien, die entweder keine oder aber konstitutiv hohe Wnt-Aktivit\u00e4t zeigen, durch siRNA ausgeschaltet. In der kolorektalen Karzinomzelllinie DLD-1 bewirkte der E-Cadherin \"Knock-Down\" eine Anreicherung von beta-Catenin im Zellkern und dadurch die Erh\u00f6hung der Transkription von Wnt-Zielgenen. Wie in der Mehrzahl kolorektaler Tumore ist in diesen Zellen der Abbau von beta-Catenin durch Mutationen des Tumorsuppressorgens APC gest\u00f6rt. Diese Ergebnisse deuten darauf hin, dass sich der Verlust von E-Cadherin besonders auf die Progression von kolorektalen Tumoren auswirken k\u00f6nnte.\n2006-05-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/280\nurn:nbn:de:bvb:29-opus-3701\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-3701\nhttps://opus4.kobv.de/opus4-fau/files/280/FelixKuphalDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:322\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:90B50\nmsc:90C29\nmsc:90C31\nmsc:90C59\nmsc:91B06\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nParametergesteuerte L\u00f6sung nichtlinearer multikriterieller Optimierungsprobleme\nParameter Controlled Solving of Nonlinear Multi-Objective Optimization Problems\nEichfelder, Gabriele\nMehrkriterielle Optimierung\nZwei-Ebenen-Optimierung\nParametrische Optimierung\nNichtlineare Optimierung\nSensitivit\u00e4tsanalyse\nStrahlentherapie\nddc:510\nBei multikriteriellen Optimierungsproblemen mit mehreren sich widersprechenden Zielsetzungen gibt es i.Allg. nicht nur eine Minimall\u00f6sung, die alle Zielfunktionen gleichzeitig optimal erf\u00fcllt, sondern die L\u00f6sungsmenge, die sog. effiziente Menge, ist sehr gro\u00df. Dabei ist es f\u00fcr einen Entscheidungstr\u00e4ger oft wichtig, die gesamte Effizienzmenge zu kennen, da diese wichtige Informationen \u00fcber das Problem beinhaltet. Das Ziel dieser Arbeit ist es daher eine Approximation der L\u00f6sungsmenge zu bestimmen, die bzgl. bestimmter Qualit\u00e4tskriterien m\u00f6glichst gut ist, was durch eine N\u00e4herung mit nahezu \u00e4quidistanten Punkten erreicht wird. Es wird dazu ein parameterabh\u00e4ngiges skalares Ersatzproblem nach Pascoletti und Serafini betrachtet. Aufbauend auf neuen Sensitivit\u00e4tsergebnissen bestimmen wir einen Algorithmus zur Parametersteuerung und damit zur Generierung nahezu \u00e4quidistanter Approximationspunkte. Dabei seien im Zielraum des nichtlinearen multikriteriellen Optimierungsproblems beliebige Halbordnungen induziert durch spitze konvexe abgeschlossene Kegel zugelassen. Die Vorteile dieses neuen Verfahrens demonstrieren wir zun\u00e4chst an einigen Testproblemen, bevor wir es zur L\u00f6sung eines aktuellen Problems aus der Medizin, der optimalen Bestrahlungsplanung zur Behandlung eines Prostatakarzinoms, nutzen. Als weitere Anwendung entwickeln wir eine L\u00f6sungsmethodik f\u00fcr nichtlineare multikriterielle Bilevel-Optimierungsprobleme und l\u00f6sen damit ein bikriterielles Bilevel-Problem aus der Medizintechnik.\nIn multi-objective optimization we investigate optimization problems with more than one objective function. As a consequence there is, in general, not only one best solution minimizing all objective functions at the same time, and the solution set called efficient set is very large. Often it is important for the decision maker to have information about the whole efficient set because this provides a useful insight into the problem structure. Thus our aim is to determine an approximation of this set which satisfies certain quality criteria as good as possible. This is achieved by almost equidistant approximation points. Therefore we consider a parameter dependent scalarization approach according to Pascoletti and Serafini. Using new sensitivity results we present an algorithm for controlling the choice of the parameters and with that for generating almost equidistant points of the efficient set. In doing so we allow any partial ordering defined by a convex pointed closed cone in the objective space of the nonlinear multiobjective optimization problem. The effectiveness of this new method is demonstrated at some test problems and what is more we apply it to a recent problem in intensity modulated radiotherapy about prostate cancer treatment. As a further application we develop a new procedure for solving multi-objective bilevel optimization problems and we apply this to a bicriteria bilevel problem in medical technology.\n2006-09-04\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/322\nurn:nbn:de:bvb:29-opus-4335\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-4335\nhttps://opus4.kobv.de/opus4-fau/files/322/GabrieleEichfelderDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:346\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:570\nmsc\nmsc:92C40\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nRNA-binding of the human cytomegalovirus transactivator protein UL69 and its role in mRNA export\nRNA-Bindung des Transaktivatorproteins UL69 des human Cytomegalovirus und seine Rolle f\u00fcr mRNA Export\nToth, Zsolt\nMessenger-RNS\nRNS-Bindung\nCytomegalie-Virus\nGenexpression\nddc:570\nThe human cytomegalovirus protein UL69 is a member of a family of homologous regulatory proteins within the Herpesviridae which includes the proteins ICP27 of herpes simplex virus type 1 and EB2 of Epstein-Barr virus. Both ICP27 and EB2 have been shown to promote the nuclear export of a set of viral mRNAs via binding to the cellular mRNA export factor REF. Furthermore, a direct RNA-binding activity of ICP27 and EB2 was found to be essential for their stimulating effects on viral mRNA export. Recently, pUL69 has been demonstrated to share several properties with its herpesviral homologues suggesting that pUL69 might function as a viral mRNA export factor of human cytomegalovirus. It has been demonstrated that (i) pUL69 shuttles between the nucleus and the cytoplasm, (ii) the nucleocytoplasmic shuttling activity is essential for pUL69-mediated activation of gene expression and (iii) pUL69 interacts with cellular proteins involved in mRNA export. However, a direct role of pUL69 in mRNA export has not yet been shown. This study demonstrates that pUL69 can induce the cytoplasmic accumulation of an unspliced CAT reporter mRNA in transfected cells suggesting that pUL69 functions as an RNA export factor. One of the characteristic features of the viral mRNA export factors is the direct interaction with their RNA targets. Several experiments were performed to demonstrate that pUL69 can interact with RNA both in vivo and in vitro via a complex N-terminal RNA-binding domain consisting of three arginine-rich motifs. Interestingly, the RNA-binding domain of pUL69 overlaps with both the NLS and the binding site of the cellular mRNA export factors UAP56 and URH49. Whereas the deletion of the UAP56/URH49-binding site abolished pUL69-mediated mRNA export in transfected cells, an RNA-binding deficient pUL69 mutant which can still interact with UAP56/URH49 retained its RNA export activity. This suggests that, in contrast to its homologues, direct RNA interaction is not a prerequisite for the RNA export activity of pUL69, leastwise in a transient transfection system. To address the in vivo role of pUL69 for the HCMV replication cycle, a UL69 deficient HCMV (\uf044UL69) was generated. The analysis of expression of representative immediate early, early and late HCMV proteins in \uf044UL69-infected cells revealed that pUL69 appears to be required for the efficient expression of a subset of early and several late HCMV genes. The RNA transcripts of these genes could represent targets for pUL69-mediated mRNA export.\nDas Protein pUL69 des humanen Cytomegalovirus ist ein Mitglied einer Familie von homologen Regulatorproteinen innerhalb der Herpesviridae, die auch die Proteine ICP27 des Herpes simplex-Virus Typ I und EB2 des Epstein-Barr-Virus umfa\u00dft. F\u00fcr ICP27 und EB2 war gezeigt werden, da\u00df sie den nukle\u00e4ren Export eines Subsets viraler RNAs durch Bindung an den zellul\u00e4ren mRNA Exportfaktor REF vermitteln. Weiterhin erwies sich eine direkte RNA-Bindungsaktivit\u00e4t von ICP27 und EB2 als essenziell ist f\u00fcr den RNA-Export. Aktuelle Studien ergaben, da\u00df pUL69 mehrere Eigenschaften mit den homologen Proteinen anderer Herpesviren teilt: (i) es kann zwischen Zellkern und Zytoplasma wandern; (ii) die nukle\u00e4re Exportaktivit\u00e4t ist essenziell f\u00fcr die pUL69-vermittelte Aktivierung der Genexpression; (iii) pUL69 interagiert mit zellul\u00e4ren mRNA Exportfaktoren. Eine direkte Rolle von pUL69 f\u00fcr den nukle\u00e4ren mRNA Export war jedoch bisher noch nicht nachgewiesen worden. In dieser Arbeit wird gezeigt, da\u00df pUL69 nach Transfektion die zytoplasmatische Akkumulation einer ungesplei\u00dften CAT-Reporter mRNA induzieren kann. Weiterhin konnte durch unterschiedliche experimentelle Ans\u00e4tze nachgewiesen werden, da\u00df pUL69 sowohl in vitro als auch in vivo unter Vermittlung einer komplexen, N-terminalen Dom\u00e4ne, die aus drei Arginin-reichen Motiven besteht, RNA binden kann. Interessanterweise \u00fcberlappt die RNA-Bindungsdom\u00e4ne des pUL69 sowohl mit dem nukle\u00e4ren Lokalisationssignal (NLS) als auch mit der Bindungsstelle der zellul\u00e4ren mRNA Exportfaktoren UAP56 und URH49. W\u00e4hrend die Deletion der UAP56/URH49-Bindungsstelle zu einem Verlust der pUL69-vermittelten RNA-Exportaktivit\u00e4t f\u00fchrte, blieb bei einer RNA-bindungsdefizienten Mutante, die noch mit UAP56/URH49 interagierte, der RNA-Export erhalten. Dieser Befund weist darauf hin, da\u00df die direkte RNA-Bindung von pUL69, im Gegensatz zu den homologen Proteinen, keine Voraussetzung f\u00fcr die RNA-Exportaktivit\u00e4t dieses Proteins darstellt. Um die in vivo-Bedeutung von pUL69 f\u00fcr die virale Replikation zu untersuchen, wurde ein UL69-defizientes HCMV konstruiert. Die Expressionsanalyse repr\u00e4sentativer Gene der initialen, fr\u00fchen oder sp\u00e4ten Phase des Replikationszyklus zeigte, da\u00df pUL69 vor allem eine Rolle f\u00fcr die Expression von sp\u00e4ten Genen spielt. Die RNA-Transkripte dieser sp\u00e4ten Gene stellen m\u00f6glicherweise, \u00e4hnlich wie bei ICP27 und EB2, Ziele des pUL69-vermittelten RNA-Exports dar.\n2006-11-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/346\nurn:nbn:de:bvb:29-opus-4644\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-4644\nhttps://opus4.kobv.de/opus4-fau/files/346/PhD%20thesis_Zsolt%20Toth.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:479\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\nmsc\nmsc:76S05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nInvestigation of inhomogeneity in preparative liquid chromatographic columns\nUntersuchung von Inhomogenit\u00e4ten in pr\u00e4parativen fl\u00fcssigchromatographischen S\u00e4ulen\nAstrath, Dirk-Uwe\nChromatographie\nFl\u00fcssig-Fest-Chromatographie\nPr\u00e4parative Chromatographie\nChromatographies\u00e4ule\nFestbett\nComputertomographie\nNumerische Str\u00f6\nddc:600\nLiquid chromatography is a unit operation with a high separation efficiency that is based on a homogenous structure of the packed bed. Because of this, the work is devoted to study the suitability of two different, non-invasive approaches for the evaluation of the packing properties. The first approach comprised the investigation of the suitability of tomographic measurement techniques to yield relevant information about the packing structure. On the basis of experimental investigations it could be shown that computed tomography measurements as well as nuclear magnetic resonance studies are suited to get information about the properties of the packed bed. The second approach was about the influence of irregular column packings on the signals monitored by peripheral sensors. In order to study these effects experimentally, procedures to mimic inhomogeneities in the experiment were successfully developed. From the following batch chromatographic investigations it could be learned that different kinds of column bed inhomogeneity result in distinct patterns of the signal shift monitored by the peripheral sensors. This renders the identification of the type of irregularity possible. In addition to the experimental studies, computational fluid dynamics simulations were carried out in order to expand the available data base. Firstly, the feasibilities of the commercial CFD code StarCD were expended by means of usercoding to allow the modeling of chroma-tographic processes. Subsequently the modeling of inhomogeneous columns was successfully validated by comparison of the simulation and the experimental results. Additional simulations enabled parameter studies of the effect of the inhomogeneity\u2019s properties on the separation result.\nDie Fl\u00fcssigchromatographie ist eine thermische Grundoperation deren hohe Trenneffizienz stark von der homogenen Struktur des gepackten Bettes abh\u00e4ngt. Aus diesem Grunde wurden im Rahmen der vorliegenden Arbeit zwei unterschiedliche Ans\u00e4tze zur einflussfreien Beurteilung der Packungseigenschaften chromatographischer S\u00e4ulen untersucht. Die erste Ansatz umfasste die Untersuchung der Eignung tomographischer Me\u00dfmethoden zur Bereitstellung relevanter Informationen \u00fcber die Packungsstruktur der S\u00e4ulen. Anhand von experimentellen Untersuchungen konnte hierbei nachgewiesen werden, dass sowohl computertomographische Untersuchungen als auch Magnetresonanztomographiemessungen zur Bereitstellung von Informationen \u00fcber die Packungseigenschaften geeignet sind. Der andere Ansatz beinhaltete die Studie des Einflusses von unregelm\u00e4\u00dfigen Packungen auf die Messsignale, die mittels peripherer Sensoren aufgezeichnet werden. Zur experimentellen Untersuchung dieser Einfl\u00fcsse wurden erfolgreich Vorgehensweisen entwickelt, Packungsfehler definiert im Experiment nachzustellen. Anhand der anschlie\u00dfenden batchchromatographischen Untersuchungen lie\u00df sich ableiten, dass unterschiedliche Inhomogenit\u00e4tsmerkmale zu unterschiedlichen Mustern in der Signalverschiebung der Sensoren f\u00fchren und somit eine Identifikation erlauben. Zus\u00e4tzlich zu den experimentellen Untersuchungen wurden zur Erweiterung der verf\u00fcgbaren Datenbasis Computational Fluid Dynamics Simulationen durchgef\u00fchrt. Hierzu wurden zu-n\u00e4chst die Programmmerkmale des kommerziellen CFD-Codes StarCD mittels Usercoding so erweitert, dass eine Beschreibung chromatographischer Vorg\u00e4nge m\u00f6glich war. Anschlie\u00dfend wurde die Modellierung inhomogener S\u00e4ulen anhand der vorhandenen Messergebnisse erfolgreich validiert. Weiterf\u00fchrende Simulationen erlaubten die gezielte parametrische Untersu-chungen des Einflusses von Eigenschaften der Fehlstelle auf das Trennergebnis.\n2007-07-19\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/479\nurn:nbn:de:bvb:29-opus-6584\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-6584\nhttps://opus4.kobv.de/opus4-fau/files/479/Dirk-UweAstrathDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:487\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:11G45\nmsc:11R37\nmsc:19F05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nClass Field Theory for Arithmetic Schemes\nKlassenk\u00f6rpertheorie arithmetischer Schemata\nHofmann, Walter\nKlassenk\u00f6rpertheorie\nArithmetische Geometrie\nddc:510\nThis present work describes the \\etale\\ fundamental group of possibly singular arithmetic schemes. Let $\\cS\\subseteq\\Spec\\cO$ be an open subscheme of the spectrum of the ring of integers $\\cO$ of an algebraic number field, and let $\\cX$ be a separated, reduced and connected scheme which is flat, proper and of finite type over $\\cS$. Denote by $\\pioneab(\\cX)$ the abelianized \\etale\\ fundamental group and let $\\cC_\\cX$ be the \\idele\\ class group of $\\cX$ as defined in~\\cite{wiesendabelian}. $\\cC_\\cX^\\circ$ is connected component of zero of $\\cC_\\cX$. We describe the kernel and co\\-kernel of the map $\\cC_\\cX/\\cC_\\cX^\\circ\\to\\pioneab(\\cX)$ by means of embedding it in an exact sequence: \\begin{align}\\nonumber \\Het^2(\\cX,\\Q/\\Z)^*\\to\\HoK_2(\\cX,\\hat\\Z)\\to\\cC_\\cX/\\cC^\\circ_\\cX\\to\\pioneab(\\cX)\\to\\HoK_1(\\cX,\\hat\\Z)\\to0 \\end{align} $\\HoK_q$ are certain cohomology groups defined in this work.\nDie vorliegende Arbeit besch\u00e4ftigt sich mit der Klassenk\u00f6rpertheorie arithmetischer Schemata, d.h. mit separierten, reduzierten und zusammenh\u00e4ngenden Schemata $\\cX$, die von endlichem Typ \u00fcber $\\Spec\\Z$ oder allgemeiner \u00fcber einem offenen Teil $\\cS$ des Spektrums eines Ganzheitsrings eines algebraischen Zahlk\u00f6rpers liegen. In \\cite{hofmannwiesend,wiesendabelian} wurde zun\u00e4chst f\u00fcr Fl\u00e4chen und dann f\u00fcr Schemata beliebiger Dimension eine Klassengruppe definiert: \\begin{align} \\cC_\\cX:=\\coker\\left(\\bigoplus_{C\\subseteq\\cX}\\kappa(C)^\\times\\to \\bigoplus_{x\\in\\cX}\\Z\\oplus\\bigoplus_{C\\subseteq\\cX}\\bigoplus_{v\\in C_\\infty}\\kappa(C)^\\times_v\\right). \\end{align} hierbei durchl\u00e4uft $C$ alle irreduziblen Kurven auf $\\cX$, und $x$ l\u00e4uft \u00fcber alle abgeschlossenen Punkte von $\\cX$. Die Stellenmenge $C_\\infty$ enth\u00e4lt gerade diejenigen Stellen von $\\kappa(C)$, die \\emph{nicht} Punkten der Normalisierung von $C$ entsprechen. Die Abbildung wird in Definition~\\ref{def:classgroup} erl\u00e4utert. Diese Klassengruppe beschreibt die endlichen \\etale n \u00dcberlagerungen von $\\cX$: Ist $\\cX$ regul\u00e4r, integer und flach \u00fcber $\\cS$, so gibt es einen Isomorphismus \\begin{align}\\label{seq:intro} \\cC_\\cX/\\cC_\\cX^\\circ\\overset\\cong\\longrightarrow\\pioneab(\\cX), \\end{align} wobei $\\cC_\\cX^\\circ$ die Zusammenhangskomponente der Null von $\\cC_\\cX$ und $\\pioneab$ die abelsch gemachte Fundamentalgruppe ist. Dieses Ergebnis wurde zun\u00e4chst f\u00fcr $\\dim\\cX=2$ und modulo $n$ in \\cite{hofmannwiesend} gezeigt, und sp\u00e4ter wie hier verwendet in \\cite{wiesendabelian}. Ziel dieser Arbeit ist es nun, die Reziprozit\u00e4tsabbildung~\\eqref{seq:intro} im Falle eines nicht regul\u00e4ren Schemas $\\cX$ zu untersuchen. Dazu werden zun\u00e4chst gewisse Kohomologiegruppen $\\HK^q(\\cX,\\cF)$ eingef\u00fchrt, ihr Verhalten unter projektiven Limiten untersucht und eine Verbindung zu \\etale n Kohomologiegruppen $\\Het(\\cX,\\cF)$ hergestellt. Mit Hilfe dieser wird dann in Theorem~\\ref{th:cftmodn} eine exakte Sequenz aufgestellt, die die Abweichung der Abbildung~\\eqref{seq:intro} von einem Isomorphismus beschreibt. W\u00e4hrend zun\u00e4chst Theorem~\\ref{th:cftmodn} modulo $n$ formuliert ist, wird in Theorem~\\ref{th:cft} die Aussage dann allgemein gezeigt.\n2007-08-27\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/487\nurn:nbn:de:bvb:29-opus-6671\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-6671\nhttps://opus4.kobv.de/opus4-fau/files/487/dissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:563\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nmsc\nmsc:62M45\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nRobuste Lernverfahren bei schlecht konditionierten Problemstellungen f\u00fcr neuronale PCA- und ICA-Netzwerke mit Adaption der Lernraten\nRobust learning rules in case of ill-conditioned problems for neural PCA- and ICA-networks with adaptation of the learning rates\nMayer, Hubert\nNeuronales Netz\nSystemtheorie\nStochastischer Prozess\nddc:620\nVerfahren zur blinden Signalverarbeitung (BSP: Blind Signal Processing) gewinnen zusehends an Bedeutung. Innerhalb der BSP spielt die Hauptkomponentenanalyse (PCA: Principal Component Analysis) sowie die Analyse stochastisch unabh\u00e4ngiger Komponenten (ICA: Independent Component Analysis) eine zentrale Rolle. Aufgrund der parallelen Arbeitsweise neuronaler Netzwerke wird die erforderliche Lerndauer im wesentlichen durch die Konditionierung der Optimierungsaufgabe bestimmt. Zur Erstellung von Lernverfahren f\u00fcr eine invertierbare bzw. orthogonale Gewichtsmatrix werden nat\u00fcrliche stochastische Gradientenverfahren, die hierbei die zugrundeliegende Zielfunktion optimieren und bei dem um eine diagonale Gewichtsmatrix erweiterten neuronalen Netzwerk die Invarianz gegen\u00fcber der Erzeugung des vektoriellen Ausgabeprozesses erhalten, eingesetzt. Bestehende zeitdiskrete Verfahren zur Adaption der Lernrate werden durch zeitkontinuierliche Systeme ausgedr\u00fcckt, analysiert und modifiziert. Weiterhin werden zeitkontinuierliche Adaptionsregeln f\u00fcr die sogenannten lokalen Lernraten erstellt. Die allgemein vorab erl\u00e4uterten Methoden werden zur Erstellung neuartige PCA-Netzwerke einschlie\u00dflich ihrer Lernverfahren, die alle bzw. die ersten Hauptkomponenten extrahieren, eingesetzt. Hierbei wird durch Erweiterung des neuronalen Netzwerks zur vollst\u00e4ndigen Hauptkomponentenanalyse um eine Schicht mit der gleichen Gewichtsmatrix die Orthogonalit\u00e4tsbedingung der Gewichtsmatrix aufgenommen. Im zweiten Fall gehen die Lernverfahren aus der Maximierung bzw. Minimierung derselben Zielfunktion bez\u00fcglich zweier Matrizen, deren Produkt die zu optimierende Gewichtsmatrix bildet, hervor. Die Lernverfahren jener ICA-Netzwerke, die zwei zus\u00e4tzliche diagonale Gewichtsmatrizen enthalten und zur Extraktion einzelner Quellprozesse dienen, werden mittels des nat\u00fcrlichen Gradientenverfahrens f\u00fcr nicht invertierbare Gewichtsmatrizen mit orthonormalen Zeilen gewonnen. Zur Darlegung der Funktions- und Leistungsf\u00e4higkeit der PCA-Netzwerke und deren Lernverfahren werden diese im Zeitdiskreten auf einem Digitalrechner simuliert, wobei die Konditionszahl der Autokorrelationsmatrix Rxx den Bereich von 1600 bis 10^12 umfa\u00dft. Hierdurch wird der Vergleich der verschiedenen Lernverfahren f\u00fcr die Gewichtsmatrix und diagonale Gewichtsmatrix mit und ohne Momententerm sowie der Regeln zur Adaption der Lernraten erm\u00f6glicht. Die ICA-Netzwerke werden zur Extraktion einzelner Quellprozesse angewendet, wobei das sogenannte Prewhitening durch die erstellten PCA-Netzwerke realisiert wird. Ferner kann durch \u00c4nderungen im Lernverfahren ein gau\u00dfverteilter Quellprozesses extrahiert werden. Schlie\u00dflich best\u00e4tigen die Ergebnisse der PCA- und ICA-Netzwerke die Robustheit der hierzu geh\u00f6rigen Lernverfahren gegen\u00fcber schlecht konditionierten Autokorrelationsmatrizen Rxx bzw. Mixing-Matrizen, die gegebenenfalls sogar singul\u00e4r sind.\nMethods for Blind Signal Processing (BSP) are becoming noticeably more important. Within the BSP, the Principal Component Analysis (PCA) and the Independent Component Analysis (ICA) are of particular importance. Due to parallel operation of neural networks, the necessary time of learning is primarily determined by conditioning of the optimization problem. Learning rules for an invertible or an orthogonal weights matrix are developed under terms of natural stochastic gradient algorithms. These gradient algorithms optimize the underlying objective function and maintain the invariance relating to generation of the output processes, if the neural network is extended by a diagonal weights matrix. Existing discrete-time algorithms for adaptation of learning rate are transferred into continuous-time systems, which are subsequently analyzed and modified. Furthermore, adaptation rules for the so-called local learning rates are introduced. Applying above introduced approaches, novel PCA networks including their learning rules are derived. In case of those PCA networks extracting all principal components, the orthogonality constraint with respect to the weights matrix is included by extension of the PCA network with a layer containing the same weights matrix. if only the first principal components have to be extracted, learning rules of associated PCA networks result from minimization and maximization, respectively of the same objective function with respect to two matrices. The product of these two matrices forms the weights matrix which have to be optimized. Learning rules of those ICA networks for extraction of particular source processes, which contain two additional diagonal weights matrices, are developed by means of the natural gradient algorithm for non-invertible weights matrices with orthonormal rows. In order to demonstrate the functional capability and efficiency of the PCA networks and their learning rules, discrete-time simulations for these systems are produced by a digital computer.The condition number of the autocorrelation matrix Rxx spans the range from 1600 to 10^12. This procedural manner permits comparison of learning rules for the weights matrix and diagonal weights matrix with and without momentum term, as well as comparison of adaptation rules for learning rates. The ICA networks are applied to extraction of particular source processes, whereas the prewhitening is implemented by the created PCA networks. Moreover, a gaussian distributed source process may be extracted by an appropriate modification of the learning rule. Finally, the results of the PCA and ICA networks confirm the robustness of the corresponding learning rules with respect to ill-conditioned autocorrelation matrices Rxx and mixing-matrices respectively, which may actually be singular.\n2008-02-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/563\nurn:nbn:de:bvb:29-opus-7666\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-7666\nhttps://opus4.kobv.de/opus4-fau/files/563/dissert.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:605\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:65M12\nmsc:65M55\nmsc:65M60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDivergence-Free Mixed Finite Elements for the Incompressible Navier-Stokes Equation\nDivergenzfreie gemischte Finite Elemente f\u00fcr die inkompressible Navier-Stokes-Gleichung\nLinke, Alexander\nFinite-Elemente-Methode\nGalerkin-Methode\nFinite-Volumen-Methode\nMehrgitterverfahren\nddc:510\nThis dissertation is concerned with the numerical approximation of the incompressible Navier-Stokes equation. As our main contribution, we propose and analyze a new stabilized mixed finite element scheme for computing incompressible laminar flows, and investigate several properties of this new scheme. The finite element analysis is presented for the Oseen model problem. The scheme is based on the Scott-Vogelius mixed finite element and on symmetric stabilization operators for dominant convection, proposed in the last recent years. In particular, the scheme delivers divergence-free pointwise velocity approximations, with an approximation quality that is completely independent of the pressure. Therefore, the cumbersome grad-div stabilization drops out from the discrete equations, and standard multi-grid approaches are possible for the solution of the evolving linear systems. We propose such a multi-grid method and deliver a multi-grid analysis for the full symmetric part of the discrete Oseen equation, including the symmetric stabilization operator for dominant convection. We further propose a coupled FEM-FVM scheme for convection-diffusion problems in an incompressible flow field. For the numerical computation of a stationary incompressible Navier-Stokes equation we propose the above stabilized Scott-Vogelius scheme, and for a stationary or nonstationary scalar convection-diffusion equation, we propose a Voronoi-box-based finite volume scheme on boundary conforming Delaunay meshes. Since Scott-Vogelius finite element approximations are divergence-free pointwise, we can prove a discrete maximum principle for the discrete convection-diffusion equation. Finally, we present several numerical examples, in order to illustrate in which situations the proposed stabilized Scott-Vogelius scheme delivers more accurate numerical approximations than other approaches. Here, a 2D example of a colliding flow might deserve further interest, since it seems to have some physical relevance. Further, we present a 3D application from electrochemistry, where the coupled FEM-FVM scheme seems to be promising due to the establishment of a discrete maximum principle.\nDiese Dissertation befasst sich mit der numerischen Approximation der inkompressiblen Navier-Stokes-Gleichung. Als Hauptbeitrag der Arbeit f\u00fchren wir eine neue, stabilisierte gemischte Finite-Elemente-Diskretisierung f\u00fcr die numerische Behandlung von inkompressiblen laminaren Str\u00f6mungen ein, analysieren sie, und untersuchen einige Eigenschaften dieser Diskretisierung. In der Finite-Element-Analysis untersuchen wir das lineare Oseen-Modell-problem. Die Diskretisierung basiert auf dem Scott-Vogelius-Element, und benutzt symmetrische Stabilisierungsoperatoren, um die diskreten Oseen-Gleichungen gegen\u00fcber dominanter Konvektion zu stabilisieren. Eine Besonderheit der Diskretisierung besteht darin, dass sie punktweise divergenz-freie Geschwindigkeitsapproximationen liefert, und damit die Approximationsg\u00fcte der Geschwindigkeiten von der Druckapproximation v\u00f6llig entkoppelt. Aufgrund der Divergenzfreiheit f\u00e4llt die l\u00e4stige Grad-Div-Stabilisierung aus den diskreten Oseen-Gleichungen heraus, und Standard-Mehrgitter-Methoden sind anwendbar, um die entstehenden linearen Gleichungssysteme zu l\u00f6sen. Wir stellen eine solche Standard-Mehrgitter-Methode vor, und entwickeln eine Mehrgitter-Analysis f\u00fcr den gesamten symmetrischen Teil der diskreten Oseen-Gleichung, inklusive dem symmetrischen Stabilisierungsoperator. Weiter stellen wir eine gekoppelte Finite-Volumen/Finite-Element-Diskretisierung vor, mit der Konvektions-Diffusions-Probleme in einem inkompressiblen Str\u00f6mungsfeld behandelt werden k\u00f6nnen. F\u00fcr die numerische Approximation der Navier-Stokes-Gleichung wird das vorgeschlagene stabilisierte Scott-Vogelius-Element verwendet, und f\u00fcr die gekoppelte skalare Konvektions-Diffusions-Gleichung setzen wir eine Voronoi-Box-basierte Finite-Volumen-Methode auf randkonformen Delaunaygittern ein. Da das Scott-Vogelius-Element punktweise divergenz-freie Approximationen liefert, k\u00f6nnen wir ein lokales diskretes Maximumprinzip f\u00fcr die angekoppelte skalare Konvektions-Diffusions-Diskretisierung beweisen. Endlich stellen wir einige numerische Beispiele vor, um veranschaulichen zu k\u00f6nnen, in welchen Situationen das vorgestellte Scott-Vogelius-Element numerisch genauere Approximation liefert als andere Ans\u00e4tze. Besonders scheint ein 2D-Beispiel eines kollidierenden Fluids Beachtung zu verdienen, da es vergleichsweise physikalisch bedeutsam ist. Weiter stellen wir Ergebnisse einer 3D-Anwendungsrechnung aus der Elektrochemie vor, bei der das gekoppelte Finite-Volumen/Finite-Element-Schema aufgrund des diskreten Maximumsprinzips eine interessante Diskretisierungsvariante zu sein scheint.\n2008-04-08\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/605\nurn:nbn:de:bvb:29-opus-8874\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-8874\nhttps://opus4.kobv.de/opus4-fau/files/605/AlexanderLinkeDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:671\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:15A29\nmsc:34A34\nmsc:65J15\nmsc:82D45\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nForward and Inverse Problems in Piezoelectricity\nVorw\u00e4rts und Inverse Probleme der Piezoelektrizit\u00e4t\nLahmer, Tom\nParameteridentifikation\nPiezoelektrizit\u00e4t\nInkorrekt gestelltes Problem\nRegularisierungsverfahren\nOptimale Versuchsplanung\nddc:510\nFor understanding and predicting the behavior of piezoelectric devices, efficient numerical simulation tools, in particular the finite element method, have been developed and are used widely. Time consuming and expensive experiments, necessary for developing new piezoelectric products, sensors and actuators, are avoided by numerically solving the mathematical formulation of the underlying physical model. This model consists of a coupling between two physical quantities, namely electric field and mechanical strain and is given by a set of partial differential equations with appropriate boundary and initial conditions. Well-posedness results concerning the solutions of the underlying PDEs are presented in this work for time-dependent, harmonic, and static computations where appropriate damping and loss mechanisms are taken into account. The accuracy of the simulation, however, relies sensitively on the material parameters governing the interaction of the physical quantities. Without the knowledge of the exact parameters no quantitative predictions can be made by numerical computations. So far, these parameters have been estimated by measurements proposed by the IEEE Standard or the European Norm CENELEC from well-defined test samples. The special shapes recommended there allow for simplifications, namely reduction to onedimensional problems, in the model. Explicit formulas allowing for parameter extraction from resonance characteristics and other measureable quantities are developed. However, these results do not provide sufficiently precise information on the material coefficients. This can be seen by comparing three dimensional simulation results with parameters gained by the classical methods with measurements of the electrical impedance or mechanical displacement. In order to overcome these insufficient exactness, efforts are invested in the inverse problem, namely the simulation-based parameter identification for piezoelectric materials. The research and methods developed in this thesis consider both the linear and the nonlinear case. For the latter functional dependencies of the materials properties from the field quantities are assumed, a model which in particular is suitable for large-signal driven resonators at high frequencies, e.g. in sonar applications. The inherent instability of the inverse problem is treated with special care by applying appropriate regularizing methods. An iterative multilevel algorithm based on modified Landweber methods is developed here. The algorithm extends ideas of O. Scherzer and shows to be very effective in combination with the detection of parameter curves as they occur in nonlinear applications. Convergence results in the case of noisy data, and regularizing properties are given for this algorithm. Sensitivity analyses and methods of optimal experiment design show the reliability of the identified parameters and give rules how to improve identification results effectively without remarkably increasing the computational effort.\nSeit geraumer Zeit werden numerische Simulationswerkzeuge eingesetzt, insbesondere die Finite Elemente Methode, um piezoelektrische Bauteile und deren Funktionsweisen genau zu verstehen und um deren Verhalten vorherzusagen. Zeitaufw\u00e4ndige und teure Experimente, die f\u00fcr die Entwicklung neuartiger piezoelektrischer Produkte, Sensoren und Aktoren, n\u00f6tig sind, k\u00f6nnen durch die numerische Simulation des mathematischen Modells ersetzt werden. Jenes besteht aus der Koppelung zweier physikalischer Gr\u00f6\u00dfen, des elektrischen Felds einerseits und der mechanischen Dehnung andererseits. Das mathematische Modell wird durch ein System von partiellen Differentialgleichungen mit entsprechenden Rand- und Anfangswerten beschrieben. F\u00fcr jeweils den zeitabh\u00e4ngingen, harmonischen und statischen Fall werden in dieser Arbeit Existenz- und Eindeutigkeitsaussagen zu den L\u00f6sungen der zugrundeliegenden partiellen Differentialgleichungen formuliert. Passende D\u00e4mpfungs- und Verlustmechanismen werden dabei ber\u00fccksichtigt. Die Genauigkeit der Simulation h\u00e4ngt jedoch sehr von den Materialparametern ab, welche das Zusammenspiel der physikalischen Gr\u00f6\u00dfen steuern. Ohne eine genaue Kenntnis dieser Parameter k\u00f6nnen keine quantitativen Aussagen gemacht werden. Bislang wurden die Materialparameter nach Vorgaben des IEEE Standards und der europ\u00e4ischen Norm CENELEC aus Messungen an wohldefinierten Probek\u00f6rpern bestimmt. Die vorgeschlagenen speziellen Geometrien erlauben vereinfachte eindimensionale Modelle. Explizite Formeln wurden entwickelt, welche es erm\u00f6glichen, die Parameter aus Resonanzerscheinungen und anderen messbaren Gr\u00f6\u00dfen zu bestimmen. Jedoch bieten deren Ergebnisse keine hinreichend exakten Informationen \u00fcber die Materialparameter. Dies wird schnell verdeutlicht, wenn man dreidimensionale Simulationen mit den aus klassischen Methoden ermittelten Parametern durchf\u00fchrt und diese Ergebnisse mit gemessenen elektrischen Impedanzen und mechanischen Verschiebungen vergleicht. Um diese unzureichende Genauigkeit zu verbessern, wird das dazugeh\u00f6rige inverse Problem, n\u00e4mlich die simulationsbasierte Identifizierung der Materialparameter f\u00fcr das piezoelektrische Problem behandelt. Die Untersuchungen und Methoden, welche in dieser Arbeit entwickelt werden, ber\u00fccksichtigen sowohl den linearen wie auch den nichtlinearen Fall. Im zweiten Fall wird von funktionalen Abh\u00e4ngigkeiten der Materialeigenschaften von den Feldgr\u00f6\u00dfen ausgegangen, einem Modell, welches speziell f\u00fcr gro\u00dfsignalbetriebene Wandler in Hochfrequenzbereichen, z.B., sonaren Anwendungen, passend ist. Die inh\u00e4rene Instabilit\u00e4t des inversen Problems wird mit spezieller Vorsicht behandelt, indem geeignete Regularisierungsmethoden angewandt werden. Ein iterativer Multilevelalgorithmus, welcher auf modifizierten Landweber Methoden basiert und die Ideen von O. Scherzer erweitert, zeigt sich als effektiv in Bezug auf die Charakterisierung von Parameterkurven, wie sie bei nichtlinearen Anwendungen auftreten. F\u00fcr diesen Algorithmus werden Aussagen \u00fcber die Konvergenz im Fall von verrauschten Daten und regularisierende Eigenschaften pr\u00e4sentiert. Sensitivit\u00e4tsanalysen und Methoden der optimalen Versuchsplanung zeigen die Vertrauensw\u00fcrdigkeit der ermittelten Parameter und geben Hinweise, wie die Identifizierbarkeit effektiv verbessert werden kann ohne den rechnerischen Aufwand merkbar zu erh\u00f6hen.\n2008-07-17\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/671\nurn:nbn:de:bvb:29-opus-9585\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-9585\nhttps://opus4.kobv.de/opus4-fau/files/671/tomLahmerDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1494\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:500\nmsc\nmsc:92-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDevelopment of innovative bioassay principles for pharmaceutical quality control\nEntwicklung innovativer Bioassayprinzipien f\u00fcr die pharmazeutische Qualit\u00e4tskontrolle\nZumpe, Cornelia\nBioassay\nQualit\u00e4tskontrolle\nAuswertemodelle\nRead-out Systeme\nSTAT5 Phosphorylierungsassay\nInterleukin-7\nddc:500\nThe quality of biopharmaceuticals is usually monitored by using biological assays. The aim of this study was to optimize existing methods for biological assays and to implement alternative assay formats. First, analysis models and read-out systems for proliferation assays, which are frequently applied for quality control purposes, were compared. Regarding the comparison of analysis models, the parallel line model was compared to the logistic models four- and five-parameter fit. The results of this comparison revealed a slightly better accuracy and precision of the logistic models in comparison to the parallel line model; however differences were not statistically significant different. On the other side, the logistic models showed a significantly higher failure rate. Regarding the comparison of read-out systems, it was tested in this study, whether luminescence or fluorescence systems could be of advantage in comparison to the colorimetric system, originally in use for detection of the response of proliferation assays at Merck Serono. From the results of this study, it can be concluded that the luminescence technique seems to be the most advantageous of the three read-outs for potency assays. It is extremely sensitive, therefore leading to cell savings and provides the highest signal to noise ratios, leading to very precise results. Moreover, it is the fastest of the three methods for all assays investigated (CTLL-2, DiFi and Kit 225 potency assay), since the plate can already be measured 5 - 10min after addition of substrate. As an alternative to the commonly used proliferation assays, an intracellular phosphorylation assay was developed for a cytokine fusion protein including Interleukin (IL)-7, since the originally used proliferation assay suffered from a very long incubation time and a poor robustness. Using western blotting, STAT5 was identified as a target for development of this assay. This study was further expanded to a comparison of IL-7 signaling in different cell lines (Kit 225, PB-1 and 2E8) and to a comparison between the signaling pathways of the two \u03b3c cytokines IL-7 and IL-2. The study showed that some of the investigated targets were induced by IL-2 stimulation only, and not by IL-7 stimulation. Regarding the comparison between the different cell lines, interesting differences in the expression pattern and kinetics of activation by IL-7 could be observed. However, in this study, the only pathway identified to be activated in all three cell lines by IL-7, was the STAT5 pathway and the associated induction of pim-1, making STAT5 a suitable target for development of the phosphorylation assay and the IL-7 signal transduction cascade an interesting target for further explorations. The developed STAT5 phosphorylation assay showed very accurate and precise results. The main advantage of this assay are its rapidness - reducing the total time of the assay from six to three days \u2013 and the higher robustness in comparison to the proliferation assay. It can serve as a fast and reliable alternative to classical end-point assays and is well-suited for use in quality control of biopharmaceuticals.\nDie Qualit\u00e4t von Biopharmazeutika wird \u00fcblicherweise durch Bioassays kontrolliert. Das Ziel dieser Arbeit war, bestehende Methoden f\u00fcr Bioassays zu optimieren und alternative Assayformate zu implementieren. Zun\u00e4chst wurden Auswertemethoden und Read-out Systeme f\u00fcr Proliferationsassays, die h\u00e4ufig f\u00fcr Qualit\u00e4tskontrollzwecke eingesetzt werden, miteinander verglichen. Hinsichtlich des Vergleichs von Auswertemodellen wurde das Parallel Line Model mit den beiden logistischen Modellen, Vier- und F\u00fcnf-Parameter Fit verglichen. Die Ergebnisse dieser Studie zeigten eine etwas h\u00f6here Genauigkeit und Pr\u00e4zision der logistischen Modelle im Vergleich zum Parallel Line Model; die Unterschiede waren jedoch statistisch nicht signifikant. Andererseits waren die Fehlerraten der logistischen Modelle signifikant h\u00f6her. F\u00fcr den Vergleich verschiedener Read-out Systeme wurde in dieser Studie getestet, ob Lumineszenz oder Fluoreszenz Systeme vorteilhaft gegen\u00fcber dem kolorimetrischen System sind, das bei Merck Serono urspr\u00fcnglich f\u00fcr die Detektion der Antwort von Proliferationsassays verwendet wurde. Aus den Ergebnissen dieser Studie kann geschlussfolgert werden, dass die Lumineszenztechnik die vielversprechendste der drei Read-out Methoden f\u00fcr Bioassays ist. Sie ist extrem sensitiv und mindert somit den Verbrauch an Zellen, liefert die h\u00f6chsten Signal-Rausch-Verh\u00e4ltnisse und f\u00fchrt zu sehr pr\u00e4zisen Resultaten. Au\u00dferdem ist sie die schnellste der drei Methoden f\u00fcr alle untersuchten Assays (CTLL-2, DiFi und Kit 225 Assay), da die Platte bereits 5 - 10min nach Substratzugabe gemessen werden kann. Als Alternative zu den allgemein verwendeten Proliferationsassays wurde ein intrazellul\u00e4rer Phosphorylierungsassay f\u00fcr ein Zytokinfusionsprotein mit Interleukin (IL)-7 Anteil entwickelt, da die Inkubationszeit des urspr\u00fcnglichen Proliferationsassays sehr lang war und die Robustheit des Assays nicht angemessen war. Mittels Westernblotanalyse wurde STAT5 als Target f\u00fcr diesen Assay identifiziert. Diese Studie wurde ausgeweitet zu einem Vergleich der IL-7 Signaltransduktionswege in verschiedenen Zelllinien (Kit 225, PB-1 und 2E8) und zu einem Vergleich der Signaltransduktionswege der beiden \u03b3c Zytokine, IL-7 und IL-2. Die Ergebnisse der Studie zeigten, dass einige der untersuchten Targets nur durch IL-2 Stimulation und nicht durch IL-7 Stimulation induziert wurden. In den verschiedenen Zelllinien konnten interessante Unterschiede der Expressionsmuster und Aktivierungskinetiken durch IL-7 beobachtet werden. Jedoch war der einzige, identifizierbare Signaltransduktionsweg, der in dieser Studie durch IL-7 aktiviert wurde, der STAT5 Signalweg und die damit verbundene Induktion von Pim-1. Dies machte STAT5 zu einem geeigneten Target f\u00fcr die Entwicklung eines Phosphorylierungsassays und die IL-7 Signaltransduktionskaskade zu einem interessanten Ziel f\u00fcr weitere Untersuchungen. Der entwickelte STAT5 Phosphorylierungsassay zeigte sehr genaue und pr\u00e4zise Ergebnisse. Die wesentlichen Vorteile dieses Assays sind seine Schnelligkeit \u2013 da die Gesamtdauer des Assays von sechs auf drei Tage reduziert werden konnte \u2013 und die h\u00f6here Robustheit im Vergleich zum Proliferationsassay. Er ist somit eine schnelle und zuverl\u00e4ssige Alternative im Vergleich zu den klassischen Proliferationsassays und ist gut geeignet f\u00fcr die Qualit\u00e4tskontrolle von Biopharmazeutika.\n2010-12-07\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1494\nurn:nbn:de:bvb:29-opus-21669\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-21669\nhttps://opus4.kobv.de/opus4-fau/files/1494/original_CorneliaZumpeDissertation.docm\nhttps://opus4.kobv.de/opus4-fau/files/1494/CorneliaZumpeDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1533\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:60B05\nmsc:60J25\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDynamics of Genealogical Trees for Type- and State-dependent Resampling Models\nDynamiken genealogischer B\u00e4ume f\u00fcr typen- und zustandsabh\u00e4ngige Resampling-Modelle\nPiotrowiak, Sven\nMarkov-Prozess\nddc:510\nWe study the evolution of the genealogy of an infinitely large population with different types in a neutral model, where the dynamics are affected by the types of the individuals or even additionally by the current composition of the population. More precisely, we study type- or even state-dependent resampling models in a large population limit, where we focus on the following two scenarios: The Weighted Sampling model, a model with infinitely many possible types, where the resampling rates depend only on the types of the individuals, and the Ohta-Kimura model, a model with only two types, where the resampling rates depend also on the type frequencies. We code the genealogy by means of ultrametric spaces which exhibit tree-like structures. To be exact, we introduce the concept of marked metric measure spaces in order to obtain a suitable state space allowing to consider also type-dependent dynamics. The desired processes, the tree-valued Weighted Sampling dynamics and the tree-valued Ohta-Kimura dynamics, are then given as unique solutions of martingale problems. Moreover, we show that these processes arise indeed as diffusion limit of the corresponding tree-valued resampling models for finite populations. As an application, we study the long-term behavior of our introduced processes which turns out to be essentially the same as for tree-valued Fleming-Viot dynamics, given by a Kingman coalescent tree. In addition, we characterize the set of invariant measures allowing to describe also the genealogy of an infinitely old population.\nWir untersuchen die Evolution der Genealogie einer unendlich gro\u00dfen Population mit verschiedenen Typen in einem neutralen Modell, wobei die Dynamik von den Typen der Individuen oder sogar von der gegenw\u00e4rtigen Zusammensetzung der Population beeinflusst wird. Dazu betrachten wir typen- und zustandsabh\u00e4ngige Resampling-Modelle im Diffusionslimes, wobei genauer die folgenden beiden Szenarien im Mittelpunkt der Untersuchung stehen: Das Weighted Sampling-Modell erlaubt es, unendlich viele verschiede Typen zuzulassen, wobei die Resampling-Raten nur von den Typen der Individuen abh\u00e4ngen. Im Ohta-Kimura-Modell existieren nur zwei verschieden Typen, jedoch h\u00e4ngen die Resampling-Raten zus\u00e4tzlich von den relativen H\u00e4ufigkeiten der Typen ab. Die Genealogie einer Population kodieren wir mit Hilfe von ultrametrischen R\u00e4umen, welche eine baumartige Struktur aufweisen. Wir f\u00fchren genauer das Konzept der markierten metrischen Ma\u00dfr\u00e4ume ein, um einen geeigneten Zustandsraum f\u00fcr typenabh\u00e4ngige Dynamiken zu erhalten. Die gew\u00fcnschten Prozesse, der baumwertige Weighted Sampling-Prozess bzw. der baumwertige Ohta-Kimura-Prozess, werden dann als eindeutige L\u00f6sungen von Martingalproblemen charakterisiert. Au\u00dferdem zeigen wir, dass man diese Prozesse tats\u00e4chlich als Diffusionlimes der entsprechenden baumwertigen Resampling-Modelle f\u00fcr endliche Populationen erhalten kann. Dar\u00fcber hinaus untersuchen wir das Langzeitverhalten der eingef\u00fchrten Prozesse, welches im Wesentlichen dem Langzeitverhalten eines baumwertigen Fleming-Viot-Prozesses entspricht, welches wiederum durch einen Kingman-Koaleszenten beschrieben wird. Wir charakterisieren auch die Menge der invarianten Ma\u00dfe, was es uns erm\u00f6glicht, die Genealogie einer unendlich alten Population zu konstruieren.\n2011-01-17\ndoctoralthesis\ndoc-type:doctoralThesis\ntext/x-tex\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1533\nurn:nbn:de:bvb:29-opus-22607\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-22607\nhttps://opus4.kobv.de/opus4-fau/files/1533/original_SvenPiotrowiakDissertation.tex\nhttps://opus4.kobv.de/opus4-fau/files/1533/SvenPiotrowiakDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1582\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nmsc\nmsc:62K25\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nComputer Aided Robust Design \u2013 Verkn\u00fcpfung rechnerunterst\u00fctzter Entwicklung und virtueller Fertigung als Baustein des Toleranzmanagements\nComputer Aided Robust Design - Linking of Computer Aided Design and virtual Manufacturing as Part of Dimensional Management\nStockinger, Andreas\nComputersimulation\nMonte-Carlo-Simulation\nToleranz <Technik>\nFinite-Elemente-Methode\nNichtlineare Finite-Elemente-Methode\nStatistik\nddc:620\nDa die Produktion von Bauteilen aus technologischen und wirtschaftlichen Gr\u00fcnden stets durch Abweichungen von der Nenngestalt begleitet ist, werden seitens der Entwicklung die zul\u00e4ssigen Abweichungen durch Toleranzen eingeschr\u00e4nkt. Ziel des Toleranzmanagements und insbesondere der virtuellen, entwicklungsbegleitenden Funktionsabsicherung ist es, die Wirtschaftlichkeit und Prognosef\u00e4higkeit der Toleranzvorgaben sicherzustellen. Im Rahmen dieser Arbeit wird durch einen integrativen Simulationsansatz eine Kopplung von Methoden des Robust Design und der Toleranzrechnung vorgeschlagen, um fr\u00fchzeitig ein robustes Toleranzkonzept herbeif\u00fchren zu k\u00f6nnen: stochastische Betrachtungen von Teilefertigungsprozessen, Montage- und F\u00fcgeprozessen bis hin zu Aussagen \u00fcber das statistische Verhalten der Baugruppen unter Betriebslasten erlauben es, rechnerunterst\u00fctzt die Ma\u00dfhaltigkeit und Funktionserf\u00fcllung auf Bauteil- und Baugruppenebene zu ermitteln. Damit kann die Robustheit der einzelnen Schritte in der Prozesskette und schlie\u00dflich der Produktfunktion im Entwicklungsprozess fr\u00fchzeitig bestimmt werden. Ausgehend von Grundlagenbetrachtungen und dem Stand der Forschung und Technik werden systematisch der Handlungsbedarf und das Simulationskonzept abgeleitet. An Beispielen wird die vorgeschlagene Simulationskette exemplarisch durchlaufen und anhand experimenteller Teiluntersuchungen validiert. Schlussendlich werden die verf\u00fcgbaren Methoden des Dimensional Managements identifiziert und den Prozessschritten im Produktenwicklungsmodell zugeordnet, um das Toleranzmanagement in Entwicklungsvorhaben effektiv unterst\u00fctzen zu k\u00f6nnen.\nThe production of machined parts is accompanied by deviations from nominal geometry due to technological and economical restrictions. Therefore tolerances are used in product development to constrain the admissible range of deviations. Dimensional management and virtual simulation methods in particular aim to ensure the economic efficiency and suitability of tolerance specifications. An integrated approach of tolerance simulation is proposed in this thesis: Methods of robust design and tolerance analysis are used to virtually derive a robust product and tolerancing concept. Stochastic analysis of part manufacturing processes, assembly and joining processes, as well as of the functional behaviour of products under conditions of use is performed. This allows the dimensional accuracy and the robustness of product function to be evaluated at an early stage in the product development process. Starting from the state of the art the required action to improve the analysis and synthesis process is derived. This finally leads to the proposed simulation concept. Case studies serve as a basis for numeric and experimental evaluation of the proposed approach. Finally the available and developed methods used in dimensional management can be assigned to the stages of the product development process in order to improve the dimensional management process in product development projects.\n2011-02-07\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1582\nurn:nbn:de:bvb:29-opus-22636\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-22636\n978-3-18-340901-3\nhttps://opus4.kobv.de/opus4-fau/files/1582/dissertation_stockinger2011_unibibsmall.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1617\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:22E65\nmsc:53B05\nmsc:53C35\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nBanach Symmetric Spaces\nBanach-symmetrische R\u00e4ume\nKlotz, Michael\nSymmetrischer Raum\nLie-Tripelsystem\nUnendlichdimensionale Lie-Gruppe\nBanach-Mannigfaltigkeit\nBanach-Lie-Algebra\nAutomorphismengruppe\nSpieg\nddc:510\nIn this thesis, we develop the basic Lie theory of symmetric spaces that are modelled on Banach spaces. A symmetric space in the sense of O. Loos is a smooth manifold $M$ endowed with a multiplication map $\\mu\\colon M\\times M\\to M$ such that each left multiplication map $\\mu_x:=\\mu(x,\\cdot)$ (with $x\\in M$) is an involutive automorphism of $(M,\\mu)$ with the isolated fixed point $x$. Inspection of the infinitesimal properties leads to the concept of Lie triple systems. These naturally arise as (-1)-eigenspaces of symmetric Lie algebras. There is a covariant functor $\\Lts$ from the category of pointed symmetric spaces to the category of Lie triple systems. We show that the automorphism group of a connected Banach symmetric space $M$ is a Banach-Lie group that acts smoothly on $M$. In particular, we see that $M$ is a Banach homogeneous space. It can be identified with the quotient of its automorphism group by a stabilizer subgroup. This enables us to gain several statements by deducing them from analogous ones concerning Lie groups. The following main results are obtained: - Integration of a morphism $\\Lts(M_1,b_1)\\to \\Lts(M_2,b_2)$ of Lie triple systems if $M_1$ is 1-connected (i.e., connected and simply connected). - Integration of a closed triple subsystem $\\mathfrak{n}\\leq \\Lts(M,b)$. - Quotients of symmetric spaces. - Integrability Criterion for Lie triple systems.\nIn dieser Doktorarbeit entwickeln wir die Grundlagen einer Lie-Theorie f\u00fcr symmetrische R\u00e4ume, die auf Banachr\u00e4umen modelliert sind. Ein symmetrischer Raum im Sinne von O. Loos ist eine glatte Mannigfaltigkeit $M$, die so mit einer Multiplikationsabbildung $\\mu\\colon M\\times M\\to M$ versehen ist, dass jede Linksmultiplikation $\\mu_x:=\\mu(x,\\cdot)$ (mit $x\\in M$) ein involutiver Automorphismus von $(M,\\mu)$ mit isoliertem Fixpunkt $x$ ist. Eine Betrachtung der infinitesimalen Eigenschaften f\u00fchrt zu dem Konzept der Lie-Tripelsysteme. Diese treten in nat\u00fcrlicher Weise als (-1)-Eigenr\u00e4ume von symmetrischen Lie-Algebren auf. Es gibt einen kovarianten Funktor $\\Lts$ von der Kategorie der punktierten symmetrischen R\u00e4ume in die Kategorie der Lie-Tripelsysteme. Wir zeigen, dass die Automorphismengruppe eines zusammenh\u00e4ngenden Banach-symmetrischen Raumes $M$ eine auf $M$ glatt wirkende Banach-Lie-Gruppe ist. Insbesondere sehen wir, dass $M$ ein Banach-homogener Raum ist. Er kann mit dem Quotienten seiner Automorphismengruppe nach einer Stabilisatoruntergruppe identifiziert werden. Das erm\u00f6glicht es uns, einige Aussagen dadurch zu gewinnen, dass wir sie aus analogen Aussagen \u00fcber Lie-Gruppen herleiten. Folgende wesentliche Ergebnisse werden erzielt: - Integration von einem Morphismus $\\Lts(M_1,b_1)\\to \\Lts(M_2,b_2)$ von Lie-Tripelsystemen, wenn $M_1$ 1-zusammenh\u00e4ngend (d.h. zusammenh\u00e4ngend und einfach zusammenh\u00e4ngend) ist. - Integration von einem abgeschlossenen Tripeluntersystem $\\mathfrak{n}\\leq \\Lts(M,b)$. - Quotienten symmetrischer R\u00e4ume. - Integrabilit\u00e4tskriterium f\u00fcr Lie-Tripelsysteme.\n2011-02-17\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1617\nurn:nbn:de:bvb:29-opus-23616\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-23616\nhttps://opus4.kobv.de/opus4-fau/files/1617/original_Originalformat.zip\nhttps://opus4.kobv.de/opus4-fau/files/1617/MichaelKlotzDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1657\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:310\nmsc\nmsc:62M10\nmsc:62M20\nmsc:62P20\nmsc:65C30\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Wiwi_ohneAngabe\nAdvances in the Analysis of Energy Commodities and of Multivariate Dependence Structures\nFortschritte in der Analyse von Energie-Rohstoffen und von multivariaten Abh\u00e4ngigkeiten\nSchl\u00fcter, Stephan\nZeitreihenanalyse\nCopula\nGARCH\nPrognose\nWavelets\nQuantile\nddc:310\nIn the first chapter of the dissertation a new stochastic long-term/short-term model for short-term electricity prices is introduced and applied to four major European indices. Evidence is given that all time series contain certain periodic patterns, and it is shown how to use the wavelet transform for filtering purpose. The wavelet transform is also applied to separate the long-term trend from the short-term oscillation in the seasonal-adjusted log-prices. Moreover, dynamic volatility is found in all time series, which is incorporated by using a bivariate GARCH model with constant correlation. The residuals are modeled using the normal-inverse Gaussian distribution. In the second chapter an overview over different wavelet based time series forecasting methods is given. The methods are tested on four data sets, each with its own characteristics. Eventually, it can be seen that using wavelets does improve the forecasting quality, especially for longer time horizons than one day ahead. However, there is no single superior method; the performance depends on the data set and the forecasting time horizon. In the third chapter a new formula for extreme Student t quantiles is derived. The derivation is based on the proof for the Gaussian quantile and on the fact that the Student t distribution arises as the limit of a variance-mixture of normals. In the fourth chapter a theoretical framework and a solved example for valuing a European gas storage facility is presented. For modeling the gas price a mean reverting process with GARCH volatility is chosen. Based on this process dynamic programming methods are applied to derive partial differential equations for valuing the storage facility. As an example a storage site in Epe, Germany, is chosen. In this context the effects of multiple contract types for renting a storage site are investigated and a sensitivity analysis is performed. In the fifth chapter multivariate copula models are discussed. Using three different four-variate data sets it is analyzed, which model fits best to data sets of dimensions higher than two. In the last chapter the weak tail dependence coefficient of the elliptical generalized hyperbolic distribution is derived.\nIm ersten Kapitel wird ein neues stochastisches Modell f\u00fcr kurzfristige Strompreise vorgestellt, dass die Preise als Summe einer langfristigen und einer kurzfristigen Entwicklung interpretiert. Das Konzept wird auf vier europ\u00e4ische Preisindizes angewandt. Es wird gezeigt, dass alle Zeitreihen gewisse Saisonalit\u00e4ten enthalten, und es wird erkl\u00e4rt wie diese mit Hilfe der Wavelet-Transformation eliminiert werden k\u00f6nnen. Letztere kann zudem dazu verwendet werden, den langfristigen Trend von der kurzfristigen Schwankung zu trennen. Es zeigt sich, dass in allen Zeitreihen die Volatilit\u00e4t zeitvariierend ist, was durch die Verwendung des GARCH-Modells in beiden Komponenten ber\u00fccksichtigt wird. Anhand der Daten wird schlie\u00dflich gezeigt, dass das neue Konzept besser als die g\u00e4ngigen abschneidet. Im zweiten Kapitel wird ein \u00dcberblick \u00fcber verschiedene Wavelet-basierte Prognosemethoden gegeben. Die Methoden werden an vier verschiedenen Datens\u00e4tzen getestet. Es zeigt sich, dass die Verwendung von Wavelets die Prognosequalit\u00e4t im Allgemeinen verbessert, v.a. f\u00fcr Zeithorizonte von mehr als einem Tag. Allerdings gibt es kein einzelnes \u00fcberragendes Konzept; die Leistung der Methoden variiert von Datensatz zu Datensatz und mit dem Prognosehorizont. Im dritten Kapitel wird eine neue Formel f\u00fcr extreme Student-t-Quantile abgeleitet. Die Formel basiert auf dem Beweis f\u00fcr extreme Quantile der Normalverteilung und auf der Tatsache, dass die Student-t-Verteilung sich als Mischung von Normalverteilungen darstellen l\u00e4sst. Im vierten Kapitel wird ein theoretischer Ansatz und ein gel\u00f6stes Beispiel f\u00fcr die Bewertung eines europ\u00e4ischen Erdgasspeichers vorgestellt. Als Gaspreismodell wird ein Prozess mit \u201eMean Reversion\u201c und GARCH-Volatilit\u00e4t bestimmt. Methoden der Dynamischen Programmierung werden verwendet um partielle Differentialgleichungen abzuleiten, die dazu dienen, den Speicherwert zu bestimmen. Als Anwendungsbeispiel wird eine Speicherst\u00e4tte in Epe, Deutschland, ausgew\u00e4hlt. In diesem Kontext werden unterschiedliche Vertragskonstellationen f\u00fcr die Miete einer Speicherst\u00e4tte untersucht, und es wird eine Sensitivit\u00e4tsanalyse durchgef\u00fchrt. Im f\u00fcnften Kapitel werden multivariate Copulamodelle diskutiert. Unter Verwendung dreier verschiedener vier-variater Datens\u00e4tze wird analysiert, welches Konzept am besten Strukturen h\u00f6herer Dimension, d.h. gr\u00f6\u00dfer als zwei, abbilden kann. Im letzten Kapitel der Arbeit wird der schwache Tail-Abh\u00e4ngigkeitskoeffizient der elliptischen generalisierten hyperbolischen Verteilung hergeleitet.\n2011-03-22\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1657\nurn:nbn:de:bvb:29-opus-24352\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-24352\nhttps://opus4.kobv.de/opus4-fau/files/1657/original_StephanSchlueterDissertation.pdf\nhttps://opus4.kobv.de/opus4-fau/files/1657/StephanSchlueterDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1691\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:310\nmsc\nmsc:62M10\nmsc:62M20\nmsc:62P20\nmsc:65C30\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Wiwi_ohneAngabe\nAdvances in the Analysis of Energy Commodities and of Multivariate Dependence Structures\nFortschritte in der Analyse von Energie-Rohstoffen und von multivariaten Abh\u00e4ngigkeiten\nSchl\u00fcter, Stephan\nZeitreihenanalyse\nCopula\nGARCH\nPrognose\nWavelets\nQuantile\nddc:310\nIn the first chapter of the dissertation a new stochastic long-term/short-term model for short-term electricity prices is introduced and applied to four major European indices. Evidence is given that all time series contain certain periodic patterns, and it is shown how to use the wavelet transform for filtering purpose. The wavelet transform is also applied to separate the long-term trend from the short-term oscillation in the seasonal-adjusted log-prices. Moreover, dynamic volatility is found in all time series, which is incorporated by using a bivariate GARCH model with constant correlation. The residuals are modeled using the normal-inverse Gaussian distribution. In the second chapter an overview over different wavelet based time series forecasting methods is given. The methods are tested on four data sets, each with its own characteristics. Eventually, it can be seen that using wavelets does improve the forecasting quality, especially for longer time horizons than one day ahead. However, there is no single superior method; the performance depends on the data set and the forecasting time horizon. In the third chapter a new formula for extreme Student t quantiles is derived. The derivation is based on the proof for the Gaussian quantile and on the fact that the Student t distribution arises as the limit of a variance-mixture of normals. In the fourth chapter a theoretical framework and a solved example for valuing a European gas storage facility is presented. For modeling the gas price a mean reverting process with GARCH volatility is chosen. Based on this process dynamic programming methods are applied to derive partial differential equations for valuing the storage facility. As an example a storage site in Epe, Germany, is chosen. In this context the effects of multiple contract types for renting a storage site are investigated and a sensitivity analysis is performed. In the fifth chapter multivariate copula models are discussed. Using three different four-variate data sets it is analyzed, which model fits best to data sets of dimensions higher than two. In the last chapter the weak tail dependence coefficient of the elliptical generalized hyperbolic distribution is derived.\nIm ersten Kapitel wird ein neues stochastisches Modell f\u00fcr kurzfristige Strompreise vorgestellt, dass die Preise als Summe einer langfristigen und einer kurzfristigen Entwicklung interpretiert. Das Konzept wird auf vier europ\u00e4ische Preisindizes angewandt. Es wird gezeigt, dass alle Zeitreihen gewisse Saisonalit\u00e4ten enthalten, und es wird erkl\u00e4rt wie diese mit Hilfe der Wavelet-Transformation eliminiert werden k\u00f6nnen. Letztere kann zudem dazu verwendet werden, den langfristigen Trend von der kurzfristigen Schwankung zu trennen. Es zeigt sich, dass in allen Zeitreihen die Volatilit\u00e4t zeitvariierend ist, was durch die Verwendung des GARCH-Modells in beiden Komponenten ber\u00fccksichtigt wird. Anhand der Daten wird schlie\u00dflich gezeigt, dass das neue Konzept besser als die g\u00e4ngigen abschneidet. Im zweiten Kapitel wird ein \u00dcberblick \u00fcber verschiedene Wavelet-basierte Prognosemethoden gegeben. Die Methoden werden an vier verschiedenen Datens\u00e4tzen getestet. Es zeigt sich, dass die Verwendung von Wavelets die Prognosequalit\u00e4t im Allgemeinen verbessert, v.a. f\u00fcr Zeithorizonte von mehr als einem Tag. Allerdings gibt es kein einzelnes \u00fcberragendes Konzept; die Leistung der Methoden variiert von Datensatz zu Datensatz und mit dem Prognosehorizont. Im dritten Kapitel wird eine neue Formel f\u00fcr extreme Student-t-Quantile abgeleitet. Die Formel basiert auf dem Beweis f\u00fcr extreme Quantile der Normalverteilung und auf der Tatsache, dass die Student-t-Verteilung sich als Mischung von Normalverteilungen darstellen l\u00e4sst. Im vierten Kapitel wird ein theoretischer Ansatz und ein gel\u00f6stes Beispiel f\u00fcr die Bewertung eines europ\u00e4ischen Erdgasspeichers vorgestellt. Als Gaspreismodell wird ein Prozess mit \u201eMean Reversion\u201c und GARCH-Volatilit\u00e4t bestimmt. Methoden der Dynamischen Programmierung werden verwendet um partielle Differentialgleichungen abzuleiten, die dazu dienen, den Speicherwert zu bestimmen. Als Anwendungsbeispiel wird eine Speicherst\u00e4tte in Epe, Deutschland, ausgew\u00e4hlt. In diesem Kontext werden unterschiedliche Vertragskonstellationen f\u00fcr die Miete einer Speicherst\u00e4tte untersucht, und es wird eine Sensitivit\u00e4tsanalyse durchgef\u00fchrt. Im f\u00fcnften Kapitel werden multivariate Copulamodelle diskutiert. Unter Verwendung dreier verschiedener vier-variater Datens\u00e4tze wird analysiert, welches Konzept am besten Strukturen h\u00f6herer Dimension, d.h. gr\u00f6\u00dfer als zwei, abbilden kann. Im letzten Kapitel der Arbeit wird der schwache Tail-Abh\u00e4ngigkeitskoeffizient der elliptischen generalisierten hyperbolischen Verteilung hergeleitet.\n2011-04-11\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1691\nurn:nbn:de:bvb:29-opus-25019\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-25019\nhttps://opus4.kobv.de/opus4-fau/files/1691/original_DissertationStephanSchlueter.pdf\nhttps://opus4.kobv.de/opus4-fau/files/1691/DissertationStephanSchlueter.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1762\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:60H10\nmsc:60J80\nmsc:62B10\nmsc:94A17\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nGeneralized-Relative-Entropy Type Distances between some Branching Processes and their Diffusion Limits\nVerallgemeinerte Relative Entropiema\u00dfe f\u00fcr diverse Verzweigungsprozesse und deren Diffusionslimiten\nKammerer, Niels Bahne\nEntropie <Informationstheorie>\nGalton-Watson-Prozess\nDiffusionsapproximation\nddc:510\nWe derive exact values respectively bounds of Hellinger integrals and power divergences -- which are generalizations of the relative entropy -- between two members of some fixed class $K$ of stochastic processes. Furthermore, we give some applications to asymptotical distinguishability (contiguity, entire separation) as well as to Bayesian decision making and Neyman-Pearson testing. In the first part, $K=K_{1}$ consists of all (discrete-time and discrete-space) Poissonian Galton-Watson branching processes without immigration (GW) and with immigration (GWI), where we allow for all types of criticality. In the second part, $K=K_{2}$ is the class of (continuous-time and continuous-space) Cox-Ingersoll-Ross-type (CIR) diffusion processes which are also known as Feller-square-root-type diffusions. Moreover, we obtain results for sequences in a special subclass $K_{3}\\subsetneq K_{1}$ of GW(I) processes which converge to CIR processes; in particular, the outcoming (bounds of the) limit Hellinger integrals, limit power divergences and limit relative entropies are compared with their counterparts within class $K_{2}$.\nIn der vorliegenden Arbeit berechnen wir exakte Werte bzw. Ober- und Unterschranken von Hellinger-Integralen und Potenz-Divergenzen -- die Verallgemeinerungen der relativen Entropie konstituieren -- zwischen zwei Mitgliedern einer fixierten Klasse $K$ von stochastischen Prozessen. Des Weiteren vollf\u00fchren wir Anwendungen in Bezug auf die asymptotische Unterscheidbarkeit (contiguity, entire separation) sowie auf die Bayessche Entscheidungstheorie und die Neyman-Pearsonsche Testtheorie. Im ersten Teil besteht $K=K_{1}$ aus allen (zeit- und raum-diskreten) Poissonschen Galton-Watson-Verzweigungsprozessen ohne Immigration (GW) und mit Immigration (GWI), wobei wir alle Kritikalit\u00e4tstypen zulassen. Im zweiten Teil ist $K=K_{2}$ die Klasse aller (zeit- und raum-kontinuierlichen) Cox-Ingersoll-Ross Diffusionsprozesse (CIR), die auch unter dem Namen Feller-Quadratwurzel-Diffusion bekannt sind. Unter anderem generieren wir auch Resultate f\u00fcr Folgen aus einer Teilklasse $K_{3}\\subsetneq K_{1}$ von GW(I)-Verzweigungsprozessen die gegen CIR-Diffusionsprozesse konvergieren; im Speziellen vergleichen wir die gewonnenen (Schranken der) Grenzwert-Hellinger-Integrale, Grenzwert-Potenz-Divergenzen und Grenzwert-Relative-Entropien mit deren Analoga aus Klasse $K_{2}$.\n2011-06-14\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1762\nurn:nbn:de:bvb:29-opus-26105\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-26105\nhttps://opus4.kobv.de/opus4-fau/files/1762/original_TeX_Files.zip\nhttps://opus4.kobv.de/opus4-fau/files/1762/NielsKammererDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1826\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:49J35\nmsc:49K40\nmsc:49M37\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nFree Material Optimization for Shells and Plates\nFreie Materialoptimierung f\u00fcr Schalen und Platten\nGaile, Stefanie\nElastizit\u00e4t\nElastische Schale\nStrukturoptimierung\nFinite-Elemente-Methode\nddc:510\nWithin this thesis we develop mathematical models and numerical methods for the Free Material Optimization problem for shells and plates. In Chapter 1 we provide a motivation arising from structural engineering to address this problem and classify the Free Material Optimization approach within other common methods in structural design optimization. In Chapter 2 we introduce the foundations of differential geometry and continuum mechanics necessary for a reliable prediction of the shell's elastic behavior. The chosen description is based on the theory of Cosserat continua, a direct approach to the shell as a two-dimensional midsurface in physical space endowed with director vectors to provide additional degrees of freedom in order to model bending and shear deformations. We restrict ourselves to displacements that fulfill the Reissner-Mindlin kinematical assumption: the material lines, that are represented by the director vectors, remain straight and unstretched during deformation. This requirement leads to a first-order approximation of the three-dimensional elasticity theory including shear effects, which is known as Naghdi's shell model. Chapter 3 is dedicated to the development of a Free Material Optimization formalism for Naghdi shells. Free Material Optimization is a subbranch of structural optimization and accordingly deals with the problem of finding the stiffest structure for a given design domain and a predefined set of loads constructed from a limited amount of material. To this end we consider the entire elasticity tensors C and D in their most general form as optimization variables and include only the basic requirements for linear elastic material in the constraints. This freedom in the design space leads to the ultimately best design, although the optimal material typically does not preexist in nature and approximations of the optimal structure have to be intricately manufactured e.g. by using tapelayering techniques or constructing composites. Moreover, since we have no information about the density of an arbitrary material, we require another measure for the amount of used material and employ a summed trace of the elasticity tensors for this purpose. We show existence of an optimal solution for the obtained minimum compliance problem as well as equivalence to the dual of a nonlinear convex semidefinite program. Moreover we introduce the minimum weight problem formulation which has recently gained much attention due to the development of numerical solvers that render this problem's structure computationally tractable. In Chapter 4 we focus on the numerical solution of the preceding problem formulations. To this end we apply a finite element method to obtain discretized versions of the previously introduced optimization problems. After a sensitivity analysis we are able to compute solutions by employing the nonlinear semidefinite programming code PENSCP, an efficient solver for problems arising from Free Material Optimization. We test our software on a collection of numerical test examples frequently used in the structural optimization of shells and show the validity of our results by comparing them with solutions originating from other prominent material optimization approaches. In Chapter 5 we discuss the extension of the Free Material Optimization problem for shells by multidisciplinary optimization constraints. We consider linear displacement constraints in a discrete context, which can be utilized to manipulate the shape of the deformed structure. In the case of stress constraints we distinguish between in-plane stresses and out-of-plane stresses and apply them in order to avoid material damage or even failure due to high stresses. For the formulation of eigenfrequency constraints we introduce a dynamic model describing the free vibrations of Naghdi shells. Therefrom we deduce a semidefinite matrix constraint that can be employed to raise the natural frequency of the structure to prohibit resonance excitation and ultimately a resonance disaster. The final type of constraints regarded by us are buckling constraints, which address the susceptibility of shells to geometrical imperfections and load perturbations. The thesis is concluded by a summary emphasizing continuative questions for future research in Chapter 6.\nIm Rahmen dieser Dissertation entwickeln wir mathematische Modelle und numerische Methoden, um das freie Materialoptimierungsproblem f\u00fcr Schalen und Platten zu l\u00f6sen. In Kapitel 1 stellen wir die aus der Bautechnik stammende Motivation f\u00fcr dieses Problem vor und ordnen den Ansatz der freien Materialoptimierung in Bezug zu anderen gebr\u00e4uchlichen Methoden in der Strukturoptimierung ein. Im 2. Kapitel stellen wir die Grundlagen der Differentialgeometrie und der Kontinuumsmechanik vor, die f\u00fcr eine zuverl\u00e4ssige Vorhersage des elastischen Verhaltens der Schale n\u00f6tig sind. Die gew\u00e4hlte Beschreibung basiert auf der Theorie der Cosserat-Medien, einem direkten Zugang zur Schale in Form einer zweidimensionalen Mittelfl\u00e4che im physischen Raum, die mit Direktorvektoren ausgestattet ist. Wir beschr\u00e4nken uns auf Verschiebungen, die die kinematischen Annahmen der Reissner-Mindlinschen Theorie erf\u00fcllen: die durch die Direktorvektoren repr\u00e4sentierten Materiallinien bleiben w\u00e4hrend der Deformation gerade und ver\u00e4ndern dabei auch nicht ihre L\u00e4nge. Diese Bedingung f\u00fchrt zu einer N\u00e4herung erster Ordnung der dreidimensionalen Elastizit\u00e4tstheorie, bei der Schereffekte ber\u00fccksichtigt werden und die als Naghdis Schalenmodell bekannt ist. Das 3. Kapitel ist der Entwicklung eines Formalismus f\u00fcr die freie Materialoptimierung von Naghdi-Schalen gewidmet. Freie Materialoptimierung ist ein Teilgebiet der Strukturoptimierung und besch\u00e4ftigt sich dementsprechend mit dem Problem, f\u00fcr einen gegebenen Designbereich und festgelegte Lastf\u00e4lle die steifste Struktur zu finden, die aus einer beschr\u00e4nkten Menge an Material gebaut ist. Zu diesem Zweck betrachten wir die kompletten Elastitzit\u00e4tstensoren C und D in ihrer allgemeinsten Form als Optimierungsvariablen und ber\u00fccksichtigen nur die grundlegenden Bedingungen f\u00fcr lineares elastisches Material in den Nebenbedingungen. Diese Freiheit im Entwurfsraum erlaubt das Erreichen der ultimativ besten Struktur, jedoch existiert das optimale Material typischerweise nicht in der Natur. Daher m\u00fcssen N\u00e4herungen der optimalen Struktur aufwendig gefertigt werden, beispielsweise durch die Verwendung von Tapelayering-Techniken oder durch die Konstruktion von Verbundwerkstoffen. Dar\u00fcber hinaus besitzen wir keine Information \u00fcber die Dichte eines beliebigen Materials und ben\u00f6tigen daher ein alternatives Ma\u00df f\u00fcr die Menge des verwendeten Materials. Zu diesem Zweck verwenden wir eine kombinierte Spur der Elastizit\u00e4tstensoren. Im Folgenden beweisen wir zum einen die Existenz von L\u00f6sungen f\u00fcr das Problem der minimalen Nachgiebigkeit als auch \u00c4quivalenz zum dualen eines nichtlinearen konvexen semidefiniten Programms. Zudem f\u00fchren wir das Problem des minimalen Gewichts ein, das in letzter Zeit an Relevanz gewonnen hat, da numerische L\u00f6ser entwickelt wurden, die mit dieser Problemformulierung umgehen k\u00f6nnen. In Kapitel 4 besch\u00e4ftigen wir uns mit einer numerischen L\u00f6sung der vorangegangenen Problemformulierungen. Dazu verwenden wir eine Finite-Elemente-Methode, um diskrete Versionen der zuvor eingef\u00fchrten Optimierungsprobleme zu erhalten. Nach einer Sensitivit\u00e4tsanalyse sind wir in der Lage, mithilfe des nichtlinearen semidefiniten L\u00f6sers PENSCP, der effiziente Methoden f\u00fcr die aus der freien Materialoptimierung stammenden Probleme besitzt, L\u00f6sungen zu berechnen. Wir \u00fcberpr\u00fcfen unsere Software mithilfe einer Ansammlung von h\u00e4ufig verwendeten Testbeispielen aus der Strukturoptimierung von Schalen und zeigen die Stichhaltigkeit unserer Resultate, indem wir sie mit L\u00f6sungen vergleichen, die aus anderen etablierten Methoden der Materialoptimierung stammen. Im 5. Kapitel diskutieren wir die Erweiterung des freien Materialoptimierungsproblems f\u00fcr Schalen durch zus\u00e4tzliche Nebenbedingungen. Wir betrachten lineare Nebenbedingungen f\u00fcr die Verschiebungen, die dazu verwendet werden k\u00f6nnen, die Form der deformierten Struktur zu beeinflussen. Im Fall der Nebenbedingungen f\u00fcr Spannungen unterscheiden wir zwischen den Spannungen in und au\u00dferhalb der Ebene, und benutzen sie, um Materialsch\u00e4den oder sogar Materialversagen aufgrund zu hoher Spannungen zu vermeiden. Zur Formulierung der Nebenbedingungen f\u00fcr die Eigenfrequenzen der Struktur stellen wir ein dynamisches Modell vor, das die freien Schwingungen von Naghdi-Schalen beschreibt. Daraus leiten wir eine semidefinite Matrixnebenbedingung her, die dazu verwendet werden kann, die nat\u00fcrlichen Eigenfrequenzen der Struktur anzuheben, um so die Anregung durch Resonanz und schlu\u00dfendlich eine Resonanzkatastrophe zu verhindern. Die letzte Art von Nebenbedingungen, die wir betrachten, sind Knicknebenbedingungungen, die sich mit der Anf\u00e4lligkeit von Schalen f\u00fcr geometrische Unregelm\u00e4\u00dfigkeiten und Lastst\u00f6rungen befassen. Im 6. Kapitel wird diese Doktorarbeit durch eine Zusammenfassung abgeschlossen, die auf weiterf\u00fchrende Fragestellungen f\u00fcr zuk\u00fcnftige Forschungsprojekte eingeht.\n2011-07-28\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1826\nurn:nbn:de:bvb:29-opus-27245\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-27245\nhttps://opus4.kobv.de/opus4-fau/files/1826/original_StefanieGaileDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/1826/StefanieGaileDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2155\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:65K10\nmsc:90C30\nmsc:90C35\nmsc:90C90\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nReal-Time Control of Hydrodynamic Process Models on Finite Volume Networks\nEchtzeitsteuerung von hydrodynamischen Prozessmodellen auf Finite Volumen Netzwerken\nHild, Johannes\nFlachwasser\nOptimale Kontrolle\nNichtlineare partielle Differentialgleichung\nFinite-Volumen-Methode\nddc:510\nWe study the shallow water equations augmented with pollution transport in one space dimension on prismatic channels with nontrivial channel width. We use both, the characteristic and the weak formulation, to construct an explicit solution of the corresponding Riemann problem. This solution is an important ingredient for Godunov's finite volume scheme, which is used to discretize a real world sewer system into a network of finite volume elements. We especially focus on a finite volume model for the network vertices, which implies weak coupling conditions at each vertex and is based on the solution of a transjunctional Riemann problem. In order to develop a process model for sewer systems of practical relevance, we introduce generalized numerical flux functions, which allow the modeling of wave reflections at nonprismatic channel junctions and walls. The generalized numerical flux functions also enable the modeling of controllable special structures like pumps, weirs and valves. The resulting process model is especially suited for real world sewer systems and is mathematically represented by a system of ordinary differential equations. The controllable process model is subject to an optimal control problem for real-time application. We discretize the process model with a general linear one-step scheme in time and use adjoint calculus to provide explicit formulas for the gradient and Hessian of the time discrete cost functional. We verify our approach with a self-written C++ application, which is tailored to optimize finite volume networks in real-time, and provide numerical results for two sewer systems, which are both based on practical application. We apply a receding horizon strategy to both test cases and compare the real-time control results with the previously computed solutions of an offline control approach.\nWir untersuchen die Flachwassergleichungen inklusive Schmutztransport im eindimensionalen Raum auf prismatischen Kan\u00e4len mit nichttrivialer Kanalbreite. Wir verwenden sowohl die charakteristische als auch die schwache Formulierung der Flachwassergleichungen, um eine explizite L\u00f6sung des zugeh\u00f6rigen Riemannproblems zu konstruieren. Diese L\u00f6sung ist ein wichtiger Bestandteil f\u00fcr das Finite-Volumen-Verfahren von Godunov, welches verwendet wird, um ein praxisbezogenes Kanalnetz in ein Finite-Volumen-Netzwerk zu diskretisieren. Wir konzentrieren uns vor allem auf die Finite-Volumen-Modellierung von Netzwerkknoten, welche schwache Kopplungsbedingungen an jedem Knoten impliziert und auf der L\u00f6sung eines knoten\u00fcbergreifenden Riemannproblems basiert. Um ein Prozessmodell f\u00fcr praxisrelevante Kanalisationssysteme herzuleiten, f\u00fchren wir verallgemeinerte numerische Flussfunktionen ein, mit denen wir Wellenreflexionen an nichtprismatischen Kanalknoten und Kanalw\u00e4nden modellieren k\u00f6nnen. Au\u00dferdem erm\u00f6glichen die verallgemeinerten numerischen Flussfunktionen die Modellierung von Sonderbauwerken wie steuerbare Pumpen, Wehre und Ventile. Das daraus resultierende, steuerbare Prozessmodell ist besonders daf\u00fcr geeignet, praxisrelevante Kanalisationssysteme darzustellen, und l\u00e4sst sich mathematisch als ein System von gew\u00f6hnlichen Differentialgleichungen beschreiben. Das steuerbare Prozessmodel wird als Nebenbedingung eines Optimalsteuerungsproblems f\u00fcr Echtzeitanwendungen eingesetzt. Wir diskretisieren das Prozessmodell mit einem verallgemeinerten linearen Einschrittverfahren in der Zeit und nutzen das Adjungiertenkalk\u00fcl um explizite Formeln f\u00fcr den Gradienten und die Hessematrix des zeitdiskreten Zielfunktionals herzuleiten. Wir \u00fcberpr\u00fcfen unseren Ansatz mit einer selbstgeschriebenen C++ Anwendung, welche speziell daf\u00fcr entwickelt wurde um Finite-Volumen-Netzwerke in Echtzeit zu optimieren, und dokumentieren die numerischen Ergebnisse zweier Kanalnetze, die beide aus der praktischen Anwendung stammen. Wir wenden die Strategie des rollenden Horizonts auf beide Testnetze an und vergleichen die Ergebnisse der Echtzeitsteuerung mit den zuvor berechneten L\u00f6sungen einer Offlinesteuerungsstrategie.\n2012-03-29\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2155\nurn:nbn:de:bvb:29-opus-32048\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-32048\nhttps://opus4.kobv.de/opus4-fau/files/2155/original_JohannesHildDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/2155/JohannesHildDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2162\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35J87\nmsc:90C30\nmsc:90C31\nmsc:90C90\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nOptimization of the crack resistance in composite materials\nOptimierung der Risswiderstandsf\u00e4higkeit von Verbundwerkstoffen\nPrechtel, Marina\nVariationsungleichung\nRissbildung\nInklusion\nOptimierung\nNichtglatte Optimierung\nddc:510\nIn brittle composite materials mesoscopic failure mechanisms like debonding of the matrix-fiber interface or fiber breakage can result in crack deflection and hence in the improvement of the macroscopic damage tolerance. More generally it is known that high values of fracture energy dissipation lead to toughening of the material. This motivates our goal to design brittle composite materials yielding maximal energy dissipation for a given static load case. We focus especially on the effect of variation of fiber shapes on the crack paths and thus on the fracture energy. For a systematic approach we set up a shape optimization problem formulation. We derive first order information in the form of a shape gradient or a sub-gradient for different formulations of the shape optimization problem respectively. While the cost function of the optimization problem is represented by the fracture energy the state problem consists in the determination of the potentially discontinuous displacement field in the two dimensional cracked domain. The displacement field includes the information about crack openings which influence the energy dissipation significantly. This effect is due to consideration of cohesiveness. The cohesive tractions and the dissipated energy depend nonlinearly on the crack openings normal as well as tangential to the crack surface. This is an extension to existing approaches. Some of these approaches solely take cohesiveness due to normal crack openings into account. Others rely on Griffith theory and thus model energy dissipation independent of crack openings. Our approach results in an extended set of constitutive equations for description of the behavior of a cracked structure. The cohesive effects and a non-penetration condition, which is imposed to avoid interpenetration of opposite crack sides, lead to additional inequalities. We show that a displacement field which fulfills the constitutive equations can be obtained as solution of a variational inequality or equivalently as minimum of the total energy by solution of an energy minimization problem. Furthermore, we prove existence and uniqueness of solutions under commonly accepted conditions. In order to determine the crack path for a given load case numerically, we apply a finite element discretization in combination with so-called cohesive elements. In contrast to other approaches to fracture where the crack path inside of the considered domain is known our numerical approach allows to determine an unknown crack path. The energy minimization method described above has the particular strength that the complete crack path is a result of one minimization process. We numerically determine optimal fiber shapes for specific problem settings as solutions of discretized shape optimization problems. Our numerical results reveal on the one hand that our simulation techniques generate reasonable crack paths. On the other hand the shape optimization results clearly show that our objective to maximize the fracture energy by optimization of fiber shapes is achieved. Finally, we notice that we include different scales in our approach. The uncracked domain is modeled by linear elasticity while the area close to the crack is described on a smaller scale taking into account nonlinear cohesive effects. Furthermore, the cohesive parameters are determined directly on atomistic scale using density functional calculations. In conclusion, our results can help to guide the manufacturing process of materials with a high fracture toughness.\nMesoskopische Schadensprozesse in spr\u00f6den Verbundwerkstoffen, wie das Abl\u00f6sen von Fasern oder Faserbruch, k\u00f6nnen zu Rissablenkungen f\u00fchren und somit die makroskopische Schadenstoleranz erh\u00f6hen. Es ist grunds\u00e4tzlich bekannt, dass eine vermehrte Energiedissipation w\u00e4hrend der Rissausbreitung zu erh\u00f6hter Rissz\u00e4higkeit f\u00fchrt. Hieraus leitet sich unsere Zielsetzung ab, spr\u00f6ode Verbundwerkstoffe dahingehend zu entwerfen, dass f\u00fcr einen speziellen statischen Lastfall eine m\u00f6glichst hohe Energiedissipation erzielt wird. Dabei betrachten wir im Speziellen den Einfluss der Ver\u00e4nderung der Faserform auf den Risspfad und damit auf die Bruchenergie. Um einen systematischen Zugang zu erhalten stellen wir ein Faserformoptimierungsproblem auf. Wir leiten Information erster Ordnung in Form eines Shapegradienten oder eines Sub-Gradienten abh\u00e4ngig von der Problemformulierung ab. W\u00e4hrend die Kostenfunktion durch die Bruchenergie selbst gegeben ist, besteht das Zustandsproblem in der Bestimmung des m\u00f6glicherweise unstetigen Verschiebungsfeldes in einem zweidimensionalen gerissenen Gebiet. Dieses Verschiebungsfeld beinhaltet Informationen \u00fcber Riss\u00f6ffnungen, welche die dissipierte Energie signifikant beeinflussen. Dieser Effekt beruht auf der Betrachtung von Koh\u00e4sivit\u00e4at. Die koh\u00e4siven Kr\u00e4fte sowie die dissipierte Energie h\u00e4ngen nicht-linear von den Riss\u00f6ffnungen in normaler und tangentialer Richtung zur Rissoberfl\u00e4che ab, was eine Erweiterung zu bestehenden Ans\u00e4tzen darstellt. Manche dieser Ans\u00e4tze ziehen nur die Abh\u00e4ngigkeit der dissipierte Energie von Riss\u00f6ffnungen in normaler Richtung zur Rissoberfl\u00e4che in Betracht. Andere basieren auf der Theorie von Griffith und modellieren deshalb die dissipierte Energie als unabh\u00e4ngig von den Riss\u00f6ffnungen. Damit f\u00fchrt unser Ansatz zu einer erweiterten Menge an Konstitutivgleichungen zur Beschreibung des Verhaltens der gerissenen Struktur. Die koh\u00e4siven Effekte und die non-penetration Bedingung, welche zur Vermeidung des gegenseitigen Eindringens gegen\u00fcberliegender Rissseiten verwendet wird, f\u00fchren zu zus\u00e4tzlichen Ungleichungen. Wir zeigen, dass das Verschiebungsfeld, welches alle Konstitutivgesetze erf\u00fcllt, entweder als L\u00f6sung einer Variationsungleichung oder \u00e4quivalent als Minimum der Gesamtenergie \u00fcber die L\u00f6sung eines Minimierungsproblems erhalten werden kann. Ferner beweisen wir Existenz und Eindeutigkeit der L\u00f6sung unter \u00fcblichen Bedingungen. Zur numerischen L\u00f6osung des Rissproblems verwenden wir eine FE-Diskretisierung in Kombination mit sogenannten koh\u00e4siven Elementen. Im Unterschied zu anderen Ans\u00e4tzen zur Behandlung von Rissen, bei welchen der Risspfad vorgegeben ist, erlaubt unser numerischer Ansatz die Bestimmung eines vorab unbekannten Risspfades. Die oben beschriebene Energieminimierungsmethode besitzt den speziellen Vorteil, dass der komplette Risspfad als Ergebnis eines einzigen Energieminimierungsprozesses erhalten werden kann. Optimale Faserformen bestimmen wir numerisch als L\u00f6sungen von diskretisierten Faserformoptimierungsproblemen. Unsere numerischen Ergebnisse vedeutlichen zum einen, dass unsere Risssimulationsmethoden angemessene Risspfade erzeugen. Zum anderen zeigen die Formoptimierungsergebnisse, dass unser Ziel der Maximierung der Bruchenergie durch Optimierung der Faserform erreicht worden ist. Schlie\u00dflich verweisen wir noch auf die Mehrskaligkeit unseres Ansatzes. Das ungerissene Gebiet wird \u00fcber ein lineares Elastizit\u00e4tsmodell beschrieben, w\u00e4hrend das Gebiet nahe des Risses auf einer kleineren Skala betrachtet wird, welche die Einbeziehung nicht-linearer koh\u00e4siver Effekte erlaubt. Dar\u00fcber hinaus werden die koh\u00e4siven Parameter \u00fcber Dichtefunktionalberechnungen direkt auf der atomaren Skala bestimmt. Zusammenfassend k\u00f6nnen aus unseren Ergebnissen Richtlinien f\u00fcr den Herstellungsprozess f\u00fcr Materialien mit hoher Bruchz\u00e4higkeit abgeleitet werden.\n2012-04-18\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2162\nurn:nbn:de:bvb:29-opus-32185\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-32185\nhttps://opus4.kobv.de/opus4-fau/files/2162/original_MarinaPrechtelDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/2162/MarinaPrechtelDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2202\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:05E10\nmsc:17B67\nmsc:20C08\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nKazhdan-Lusztig Kombinatorik und Impulsgraphen\nKazhdan-Lusztig combinatorics in the moment graphs setting\nLanini, Martina\nDarstellungstheorie\nAlgebraische Kombinatorik\nddc:510\nImpulsgraphen, sowie Kazhdan-Lusztig-Polynome, liegen an der Schnittstelle von algebraischer Kombinatorik, Darstellungstheorie und geometrischer Darstellungstheorie. W\u00e4hrend die Kombinatorik der Kazhdan-Lusztig Theorie schon seit dem grundlegenden Artikel von Kazhdan und Lusztig (1979), das diese Polynome definiert, untersucht wurde, wurden Impulsgraphen noch nicht als kombinatorische Objekte betrachtet. Das Ziel dieser Arbeit ist, zuerst eine axiomatische Theorie \u00fcber Impulsgraphen und Garben auf ihnen zu entwickeln und anschlie\u00dfend diese auf die Untersuchung einer fundamentalen Klasse von Impulsgraphen, n\u00e4mlich die -regul\u00e4ren und parabolischen- Bruhat-Impulsgraphen, anzuwenden. Sie sind mit jeder symmetrisierbaren Kac-Moody Algebra verbunden und die zugeh\u00f6rigen Braden-MacPherson Garben beschreiben die projektiven unzerlegbaren Objekten der - regul\u00e4ren oder singul\u00e4ren- Kategorie O. Dies ist f\u00fcr uns der wichtigste Grund, zusammen mit ihrem inneren kombinatorischen Interesse, Bruhat-Impulsgraphen zu untersuchen. Im ersten Kapitel definieren und beschreiben wir die Kategorie der k-Impulsgraphen auf einem Gitter. Zentraler Punkt dieses Teils ist die Definition des Begriffs von Morphismus. Das zweite Kapitel ist \u00fcber die Kategorie der Garben auf einem k-Impulsgraph. Wir definieren die Pullback und Push-Forward Funktoren und wir beweisen, dass sie gute Eigenschaften haben. Wie in der klassischen Garbentheorie ist der Pullback links-adjungiert zum Push-Forward. Au\u00dferdem zeigen wir, dass der Pullback eines Isomorphismus die wichtigste Klasse von Garben auf einem k-Impulsgraph, genauer die unzerlegbaren Braden-MacPherson Garben, erh\u00e4lt. Dieses Ergebnis wird ein grundlegendes Instrument im Kapitel 5 werden. Im folgenden Kapitel betrachten wir die Familie von Bruhat k-Impulsgraphen, die mit einer symmetrisierbaren Kac-Moody-Algebra verbunden sind. Das interessanteste Ergebnis des ersten Teils dieses Kapitels ist die Realisierung von parabolischen Bruhat-Impulsgraphen als Quotienten - im Sinne von Kapitel 1 - des regul\u00e4ren Bruhat-Impulsgraph. Im zweiten Teil des Kapitels untersuchen wir den affinen Fall. Insbesondere bekommen wir eine explizite Beschreibung von gewissen endlichen Intervallen des Bruhat-Impulsgraphs die zur affine Grassmannschen assoziiert sind. In Kapitel 4 verallgemeinern wir eine Kategorifizierung von Fiebig der Hecke-Algebra auf den parabolischen Fall. Der grundlegende Schritt ist die Definition eines involutiven Automorphismus der Strukturalgebra des parabolischen Bruhat-Impulsgraphen. In den letzten zwei Kapiteln kategorifizieren wir gewisse Eigenschaften der Kazhdan--Lusztig-Polynome durch Garben auf Impulsgraphen. Insbesondere werden die Ergebnisse aus Kapitel 5 \u00fcber die Kombinatorik und Eigenschaften der unzerlegbaren Braden-MacPherson Garbe sowie die Aussagen aus Kapitel 1 \u00fcber den Pullback Funktor angewendet. Der Beweis des Hauptergebnisses des Kapitels 6 ist ziemlich aufwendig und benutzt neben Ergebnissen von Fiebig die bis dahin entwickelten Techniken aus den vorherigen Kapiteln.\nMoment graphs, as well as Kazhdan-Lusztig polynomials, straddle the intersection of algebraic combinatorics, representation theory and geometric representation theory. While the combinatorial core of the Kazhdan-Lusztig theory has been investigated for thirty years, after the seminal paper of Kazhdan-Lusztig (1979) where these polynomials were defined, moment graphs have not yet been studied as combinatorial objects. The aim of this thesis is first to develop an axiomatic theory of moment graphs and sheaves on them and then to apply it to the study of a fundamental class of moment graphs: the -regular and parabolic- Bruhat (moment) graphs. They are attached to any symmetrisable Kac-Moody algebra and the associated indecomposable Braden-MacPherson sheaves describe the indecomposable projective objects in the corresponding deformed -regular or singular- category O. This is for us the most important reason to consider Bruhat graphs, together with their intrinsic combinatorial interest. In the first chapter, we define and describe the category of k-moment graphs on a lattice. The fundamental point of this part is the definition of the notion of morphism. The second chapter is about the category of sheaves on a k-moment graphs. We give the definition of the pullback and push-forward functors and we prove that they have nice properties. In particular, as in classical sheaf theory, the pullback ist left adjoint to the push-forward. Moreover, we show that the pullback of an isomorphism preserves the most important class of sheaves on a k-moment graph: the indecomposable Braden-MacPherson sheaves. This result will be a fundamental tool in Chapter 5. In the following chapter we study the family of Bruhat (k-moment) graphs associated to a simmetrisable Kac-Moody algebra. The most interesting result of the first part of this chapter is the realisation of parabolic Bruhat graphs as quotients - in the sense of Chapter 1- of the regular one. The second part of the chapter is devoted to the study of the affine case. In particular, we describe certain finite intervals of the Bruhat graph corresponding to the Affine Grassmannian in a very precise way. In Chapter 4 we generalise to the parabolic setting a categorification, due to Fiebig, of the Hecke algebra. The fundamental step is the definition of an involutive automorphism of the structure algebra of parabolic moment Bruhat graphs.\n2012-05-16\ndoctoralthesis\ndoc-type:doctoralThesis\ntext/x-tex\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2202\nurn:nbn:de:bvb:29-opus-32779\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-32779\nhttps://opus4.kobv.de/opus4-fau/files/2202/original_MartinaLaniniDissertation.tex\nhttps://opus4.kobv.de/opus4-fau/files/2202/MartinaLaniniDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2301\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:01A55\nmsc:51M04\nmsc:51N15\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nRational families of circles and bicircular quartics\nRationale Kreisscharen und bizirkulare Quartiken\nWerner, Thomas Rainer\nAnalytische Geometrie\nKreis\nKreisschar\nddc:510\nThis dissertation deals with special plane algebraic curves, with so called bicircular quartics. These are curves of degree four that have singularities in the circular points at infinity of the complex projective plane IP_2(IC). The main focus lies on real curves, i.e. such curves that are invariant under complex conjugation. Many of the statements on bicircular quartics presented in this work are well known since the end of the 19th century, but the way of proving at that time did not fully employ the language of the then well developed projective geometry. The primary goal of this text is the formulation of the classical statements on bicircular quartics in modern language. In order to achieve this the theoretical framework is built beginning with the space of circles in the language of projective geometry. Within the space of circles are discussed at first linear and then quadratic families of circles. In the following the theorems on bicircular quartics and their degenerate form, the circular cubics, are proved by means of geometrical statements on the mentioned families of circles in the projective space of circles. An ancillary goal of this work is the provision of tools that facilitate an easy depiction of bicircular quartics and rational families of circles with the help of a computer.\nDiese Dissertation besch\u00e4ftigt sich mit speziellen ebenen algebraischen Kurven, mit sogenannten bizirkularen Quartiken. Das sind Kurven vierten Grades, die Singularit\u00e4ten in den unendlich fernen Kreispunkten der komplex-projektiven Ebene IP_2(IC) besitzen. Das Hauptaugenmerk liegt dabei auf reellen Kurven, d.h. solchen Kurven, die unter der komplexen Konjugation invariant sind. Viele der in dieser Arbeit vorgestellten Aussagen \u00fcber bizirkulare Quartiken sind bereits seit Ende des 19. Jahrhunderts wohl bekannt, die Beweisf\u00fchrung von damals nutzte jedoch nicht konsequent die Sprache der seinerzeit bereits entwickelten projektiven Geometrie aus. Das prim\u00e4re Ziel dieser Arbeit ist die Formulierung der klassischen Aussagen \u00fcber bizirkulare Kurven in moderner Sprache. Zu diesem Zwecke wird das theoretische Ger\u00fcst beginnend beim Raum der Kreise in der Sprache der projektiven Geometrie aufgebaut. Im Raum der Kreise werden zun\u00e4chst lineare, dann quadratische Kreisscharen diskutiert. Im Folgenden werden die Theoreme \u00fcber bizirkulare Quartiken und \u00fcber ihre Entartungsform, die zirkularen Kubiken, mit der Hilfe von geometrischen Aussagen \u00fcber die oben genannten Kreisscharen im projektiven Raum der Kreise bewiesen. Ein untergeordnetes Ziel dieser Arbeit ist die Bereitstellung von Werkzeugen, die eine einfache Darstellung von bizirkularen Kurven und von rationalen Kreisscharen am Rechner erm\u00f6glichen.\n2012-07-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2301\nurn:nbn:de:bvb:29-opus-34320\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-34320\nhttps://opus4.kobv.de/opus4-fau/files/2301/original_ThomasWernerDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/2301/ThomasWernerDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2327\n2021-05-19\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35L50\nmsc:76N25\nmsc:93C20\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nStabilization of the Gas Flow in Networks: Boundary Feedback Stabilization of Quasilinear Hyperbolic Systems on Networks\nStabilisierung des Gasflusses in Netzwerken: Feedbackstabilisierung quasilinearer hyperbolischer Systeme auf Netzwerken mit Randsteuerungen\nDick, Markus\nGastransport\nStabilisierung\nLjapunov-Funktion\nR\u00fcckkopplung\nNetzwerk\nPartielle Differentialgleichung\nSteuerung\nddc:510\nWe study the stabilization of quasilinear hyperbolic PDE systems on networks. To do so we develop stabilizing boundary feedback controls. We study feedback controls without and with time-varying delays. To analyze the system evolution we introduce strict Lyapunov functions which are weighted and squared L2- and H1-norms of the solutions of the quasilinear systems. The exponential weights in the Lyapunov functions contain the system eigenvalues. The feedback controls with time delay are represented by delay terms in the Lyapunov functions. The boundary controls yield an exponential decay of the corresponding Lyapunov function on a given finite time interval. Furthermore, we prove the exponential decay of the L2- and H1-norms of the solutions and give exponential estimates for the C0- and C1-norms. We apply our results on the stabilization of the isothermal Euler equations with friction, a hyperbolic system of balance laws that models the gas flow through pipes. For this system we analyze stationary states and classical nonstationary solutions. We present methods to stabilize the isothermal Euler equations locally around a given stationary state both on a single pipe and on fan-shaped pipe networks with compressor stations.\nIn dieser Arbeit besch\u00e4ftigen wir uns mit der Stabilisierung von quasilinearen hyperbolischen partiellen Differentialgleichungen auf Netzwerken. F\u00fcr die Stabilisierung verwenden wir Feedbackrandsteuerungen mit und ohne zeitabh\u00e4ngigen Verz\u00f6gerungen. Um die zeitliche Entwicklung der L\u00f6sungen der Differentialgleichungen zu untersuchen, benutzen wir quadrierte L2- und H1-Normen als Lyapunovfunktionen. Die Lyapunovfunktionen besitzen exponentielle Gewichte, welche die Systemeigenwerte enthalten. Die Feedbacksteuerungen mit Verz\u00f6gerung werden durch Zusatzterme in den Lyapunovfunktionen ber\u00fccksichtigt. Die Feedbacksteuerungen bewirken den exponentiellen Abfall der Lyapunovfunktionen auf einem vorgegebenen endlichen Zeitintervall. F\u00fcr die L\u00f6sungen der Differentialgleichungen zeigen wir au\u00dferdem den exponentiellen Abfall der L2- und H1-Normen und leiten exponentielle Absch\u00e4tzungen f\u00fcr deren C0- und C1-Normen her. Wir wenden unsere Ergebnisse auf die Stabilisierung des Flusses in Gasr\u00f6hren an. Dieser wird durch die isothermen Eulergleichungen mit Reibung, einem hyperbolischen System von Bilanzgleichungen, modelliert. F\u00fcr dieses System untersuchen wir die Existenz und das Verhalten von station\u00e4ren und klassischen nichtstation\u00e4ren Zust\u00e4nden. Wir entwickeln Methoden zur Stabilisierung der isothermen Eulergleichungen in einer Umgebung eines vorgegebenen station\u00e4ren Zustandes. Dabei betrachten wir sowohl den Gasfluss durch einzelne R\u00f6hren als auch durch f\u00e4cherf\u00f6rmige Netzwerke mit Verdichteranlagen.\n2012-08-03\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2327\nurn:nbn:de:bvb:29-opus-34644\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-34644\nhttps://opus4.kobv.de/opus4-fau/files/2327/original_MarkusDickDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/2327/MarkusDickDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2375\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nmsc\nmsc:97M60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med_ohneAngabe\nKn\u00f6cherne Verankerung und Prim\u00e4rstabilit\u00e4t von dentalen Implantaten im Oberkiefer des Menschen - Eine experimentelle Untersuchung\nBone anchorage and primary stability of dental implantsin the human upper jaw - an experimental examination\nLagarie, Sebastian\nImplantat\nOberkiefer\nddc:610\nHintergrund und Ziele Obwohl die Stabilit\u00e4t eine wichtige Determinante bei Insertion und kaufunktioneller Versorgung eines dentalen Implantates darstellt, existiert bislang kein valides Verfahren, dass diesen Parameter in der klinischen Anwendung reproduzierbar quantitiativ zu erfassen vermag. Dokumentiert werden daher eher Funktionen der Stabilit\u00e4t wie die taktile Wahrnehmung des Operateurs, der Einbringwiderstand (Torque) als Drehmoment oder die durch ein technisches Verfahren generierte Resonanzfrequenz (RFA). Ziel der experimentellen Untersuchung am humanen Oberkiefer war es daher, Torque und RFA als indirekte Methoden mit histologischen Daten sowie der direkten Messung der Stabilit\u00e4t durch ein optisches Belastungsmessverfahren zu vergleichen. Material und Methode Die vorliegende experimentelle Kadaverstudie wurde an 11 humanen Oberkieferpr\u00e4paraten vorgenommen. Insgesamt wurden jeweils 20 Implantate der Durchmesser 3,4 mm, 3,8 mm, 4,5 mm, 5,5 mm inseriert. Nach Implantatbettaufbereitung wurden die Implantate eingebracht und der Eindrehwiderstand ermittelt. Anschlie\u00dfend folgte die Messung der Resonanzfrequenz. Bei der photogrammetrischen Verformungsmessung wurde zun\u00e4chst eine definierte, gerichtete Kraft von 10 N appliziert und die Ver\u00e4nderung der Implantatposition optisch durch ein Verformungsmesssystem registriert. Nachfolgend wurde die Kraft um jeweils 10 N bis maximal 140 N gesteigert und die optische Messung wiederholt. Nach Abschluss der Messungen erfolgten die histologischen Untersuchungen am Knochenpr\u00e4parat bez\u00fcglich der absoluten Knochenh\u00f6he und der periimplant\u00e4ren Knochenfl\u00e4che. Ergebnisse und Beobachtungen Bei Implantatinsertion wurden maximale Drehmomente zwischen 5 und 50 Ncm erreicht mit einer H\u00e4ufung im Bereich zwischen 10 und 25 Ncm (Median 15 Ncm). Die RFA zeigte mittlere Werte von 6065 \u00b1 378 Hz. F\u00fcr den Durchmesser 3,4 mm zeigten sich Frequenzen von 5862 \u00b1 174 Hz, f\u00fcr 3,8 mm 6214 \u00b1 499 Hz, f\u00fcr 4,5 mm 5920 \u00b1 325 Hz und f\u00fcr 5,5 mm 6165 \u00b1 352 Hz. Die Unterschiede waren nicht statistisch signifikant. In der Belastungsanalyse erzeugte eine nach der Implantatachse ausgerichtete Kraft von 10 N eine mittlere Auslenkung von 4,3 \u00b5m bzw. von 56,1 \u00b5m bei maximaler Krafteinwirkung von 140 N. Im Bereich von 40 bis 140 N zeigten sich signifikante Korrelationen zwischen der Auslenkung unter Belastung und dem Einbringtorque sowie der periimplant\u00e4ren Knochenfl\u00e4che (jeweils p<0,001). Auch wiesen die Implantate, die unter einer Belastung von 100 N um 50 \u00b5m und mehr ausgelenkt wurden, signifikant geringere periimplant\u00e4re Knochenfl\u00e4chen (p<0,001) und Drehmomente auf (p<0,001). Die Resonanzfrequenz korrelierte mit Auslenkungen unter Belastung in einem begrenzten Kraft-applikationsbereich von 60 N bis 100 N (jeweils p<0,05). In der Grenzwertbetrachtung zeigte sich jedoch kein statistisch signifikanter Bezug zu Implantatauslenkung oder histomorphometrischen Daten. Praktische Schlussfolgerungen Die vorliegende experimentelle Untersuchung am humanen Oberkiefer zeigt, da\u00df Torque und RFA als indirekte Verfahren zur Quantifizierung der Prim\u00e4rstabilit\u00e4t keine koh\u00e4renten Ergebnisse liefern und nicht per se mit der direkten Erfassung mittels photogrammetrischer Verformungsmessung korrelieren. Bez\u00fcglich des statistisch signifikanten Zusammenhangs zu den Auslenkungsdaten der Belastungsanalyse war der Torque der Resonanzfrequenzanalyse \u00fcberlegen. Ziel weiterf\u00fchrender Studien sollte die \u00dcbertragung der Belastungsanalyse auf die klinische Situation sowie der Vergleich von Torque, RFA und Auslenkungsvektoren am Patienten sein.\nBackground Although the stability is an important determinant at insertion time and for functional supply of a dental implant, no valid procedure exists that can quantify these parameter in the clinical application reproducible. Rather functions of the stability are documented, like the tactile perception of the surgeon, the screw-in-resistance as torque or the resonance frequency generated by a technical procedure (RFA). The aim of the experimental study on human upper jaw was to compare torque and RFA as indirect methods to histological data as well as the direct measurement of the stability by an optical load measuring procedure. Materials and methods The present experimental cadaver study was performed on 11 human upper jaw preparations. A total of 20 implants in each of the diameters 3.4 mm, 3.8 mm, 4.5 mm, 5.5 mm were inserted. After implant site preparation the implants were placed and the screw-in-resistance was determined. After that the measurement of the resonance frequency followed. With the photogrammetric deformation measurement a initially defined, directed strength of 10 N was applied first and the change of the implant position was registered optically by a photogrammetric measurement system. Subsequently, the force was increased by steps of 10 N up to a maximum of 140 N and the optical measurement was repeated. After the measurements were finished histological studies on bone preparation as the absolute bone height and peri-implant bone area followed. Results and observations At implant-placement the maximum implant torque of 5 to 50 Ncm were achieved with an accumulation in the range between 10 and 25 Ncm (median 15 Ncm). The RFA showed mean values of 6065 \u00b1 378 Hz. For the 3.4 mm diameter frequencies of 5862 \u00b1 174 Hz were exhibited, for 3.8 mm 6214 \u00b1 499 Hz, for 4.5 mm and 5920 \u00b1 325 Hz for 5.5 mm 6165 \u00b1 352 Hz. The differences were not statistically significant. At the stress analysis a force of 10 N in the direction of the implant axis produced an average deviation of 4.3 \u00b5m and at maximum force of 140 N a deviation of 56.1 \u00b5m. In the range of 40 to 140 N, significant correlations were shown between the deflection under load, the peri-implant bone area and the torque (each p <0.001). Implants, which were deflected 50 \u00b5m und more under a load of 100 N, showed significantly lower peri-implant bone surfaces (p<0.001) and torques (p<0.001). The resonance frequency correlated with deflections under load in a limited application of forces from 60 N to 100 N (each p<0.05). However, in the analysis of the limit values no statistically significant relation to implant-movement or histomorphometric data was shown. Conclusion The present experimental study on human upper jaw shows that torque and RFA as indirect methods for the quantification of the primary stability do not provide coherent results and do not correlate per se with the direct acquisition by means of photogrammetric deformation measurement. Regarding to the statistically significant relationship to the deformation data the torque was superior to the resonance frequency analysis. Further studies should aim the transfer of the deformation measurement to the clinical situation and the comparison of torque, RFA and the vectors of displacement on the patient.\n2012-09-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2375\nurn:nbn:de:bvb:29-opus-35053\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-35053\nhttps://opus4.kobv.de/opus4-fau/files/2375/original_SebastianLagarieDissertation.docx\nhttps://opus4.kobv.de/opus4-fau/files/2375/SebastianLagarieDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2385\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:34E20\nmsc:34F15\nmsc:35P15\nmsc:81U05\nmsc:81V55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nResonances in the two centers Coulomb system\nResonanzen fuer das Quantenmechanische Zweizentren-Problem\nSeri, Marcello\nResonanzen\nSturm-Liouville\nSpectrum\nQuantenmechanische\nCoulomb\nSchroedinger Operator\nDifferentialgleichung\nddc:510\nIn dieser Doktorarbeit wurden die Resonanzen f\u00fcr das quantenmechanische Zweizentren-Problem in zwei und drei Dimensionen studiert. Im ersten Teil der Arbeit vervollst\u00e4ndigen wir das klassische Bild mit dem Verzweigungsdiagramm und dem Phasenportrait f\u00fcr positive Energien. Das ist f\u00fcr sich genommen interessant, aber es ist auch wichtig zu verstehen, ob quantenmechanische Resonanzen existieren, und wo sie sich befinden. Dann konzentrieren wir uns auf die Untersuchung des quantenmechanischen Modells. In diesem Teil entwickeln wir die Techniken und die Strukturen, um eine Definition der Resonanzen bereitzustellen und die Absch\u00e4tzungen f\u00fcr ihre Lokalisierung auszuf\u00fchren. Die Resonanzen werden als verallgemeinerte komplexe Eigenwerte mit kleinen Imagin\u00e4rteilen definiert. Sie werden durch verschiedene approximationstechniken auf-gez\u00e4hlt und in verschiedenen asymptotischen Regimen von Parametern mit einem Fehler der Ordnung $h^{\\frac32}$ explizit lokalisiert.\nIn this work we investigate the existence of resonances for two-centers Coulomb systems with arbitrary charges in two and three dimensions, defining them in terms of generalized complex eigenvalues of a non-selfadjoint deformation of the two-center Schr\u00f6dinger operator. After giving a description of the bifurcation of the classical system for positive energies, we construct the resolvent kernel of the operators and we prove that they can be extended analytically to the second Riemann sheet. The resonances are then defined and studied with numerical methods and perturbation theory.\n2012-10-02\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2385\nurn:nbn:de:bvb:29-opus-35468\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-35468\nhttps://opus4.kobv.de/opus4-fau/files/2385/original_diss_seri.zip\nhttps://opus4.kobv.de/opus4-fau/files/2385/diss_seri.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2683\n2018-11-12\ndoc-type:article\nbibliography:false\nddc\nddc:500\nmsc\nmsc:82C40\nmsc:82D05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nApplication of the Method of Generating Functions to the Derivation of Grad\u2019s N-Moment Equations for a Granular Gas\nNoskowicza, S. H.\nSerero, Dan\n-\nddc:500\nA computer aided method using symbolic computations that enables the calculation of the source terms (Boltzmann) in Grad\u2019s method of moments is presented. The method is extremely powerful, easy to program and allows the derivation of balance equations to very high moments (limited only by computer resources). For sake of demonstration the method is applied to a simple case: the one-dimensional stationary granular gas under gravity. The method should find applications in the field of rarefied gases, as well. Questions of convergence, closure are beyond the scope of this article.\n2012-11-15\narticle\ndoc-type:article\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2683\nurn:nbn:de:bvb:29-opus-38642\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-38642\nhttps://opus4.kobv.de/opus4-fau/files/2683/noskowicz_application_3864.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2684\n2018-11-12\ndoc-type:article\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35L50\nmsc:76N25\nmsc:93C20\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nExistence of classical solutions and feedback stabilization for the flow in gas networks\nGugat, Martin\nHerty, Micha\u00ebl\n-\nddc:510\nWe consider the flow of gas through pipelines controlled by a compressor station. Under a subsonic flow assumption we prove the existence of classical solutions for a given finite time interval. The existence result is used to construct Riemannian feedback laws and to prove a stabilization result for a coupled system of gas pipes with a compressor station. We introduce a Lyapunov function and prove exponential decay with respect to the L2-norm.\n2012-11-15\narticle\ndoc-type:article\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2684\nurn:nbn:de:bvb:29-opus-38655\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-38655\nhttps://opus4.kobv.de/opus4-fau/files/2684/gugat_existence_3865.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2869\n2018-11-12\ndoc-type:article\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35Lxx\nmsc:76N15\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nConservation law constrained optimization based upon Front-Tracking\nGugat, Martin\nHerty, Micha\u00ebl\nKlar, Axel\nLeugering, Gunter\n-\nddc:510\nWe consider models based on conservation laws. For the optimization of such systems, a sensitivity analysis is essential to determine how changes in the decision variables influence the objective function. Here we study the sensitivity with respect to the initial data of objective functions that depend upon the solution of Riemann problems with piecewise linear flux functions. We present representations for the one\u2013sided directional derivatives of the objective functions. The results can be used in the numerical method called Front-Tracking.\n2013-01-28\narticle\ndoc-type:article\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2869\nurn:nbn:de:bvb:29-opus-41035\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-41035\nhttps://opus4.kobv.de/opus4-fau/files/2869/gugat_conservation_4103.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3236\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:60H35\nmsc:65F18\nmsc:65F22\nmsc:65F50\nmsc:65F60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nAsymptotic error estimates of the 3D MUSIC algorithm in the case of systematic errors\nAsymptotische Fehlerschranken des 3D MUSIC Algorithmus im Fall von systematischen Fehlern\nLuff, Patrick\nParametersch\u00e4tzung\nSignalverarbeitung\nSignalunterraum\nddc:510\nParametersch\u00e4tzung behandelt die Bestimmung des Zustands eines gegebenen Systems aufgrund von indirekten Sensormessungen. Unter den vielen Methoden in Verwendung wird der Multiple Signal Classification (MUSIC) Algorithmus als einer der erfolgreichsten in mehreren Anwendungsgebieten angesehen, darunter Magnetoenzephalographie, Elektroenzephalographie, Radar und drahtlose Kommunikation. Der Algorithmus durchsucht das Arbeitsvolumen mit einem Modell einer Einzelquelle und berechnet die \u00c4hnlichkeit zwischen dem Quellenunterraum und einem gen\u00e4herten Signalunterraum. Es wird dabei angenommen, dass die Quelle r\u00e4umlich statisch ist. Die meisten praktischen Anwendungen haben mit Ungenauigkeiten des zugrunde liegenden physikalischen Modells zu k\u00e4mpfen. Diese Ungenauigkeiten beeinflussen die Parametersch\u00e4tzung und f\u00fchren zu einem inkorrekten Ergebnis, sogar im Fall von rauschfreien Sensoren. Diese Arbeit konzentriert sich auf die Modellierung and Quantifizierung dieser Sch\u00e4tzfehler im asymptotischen Fall. Daf\u00fcr wird angenommen, dass die Modellfehler Zufallsvariablen sind und Erwartungswert und Momente h\u00f6herer Ordnung des Sch\u00e4tzfehlers werden gen\u00e4hert. Bereits bestehende Ans\u00e4tze erster Ordnung werden auf potentiell zweite Ordnung erweitert. Zus\u00e4tzlich wird dreidimensionale Parametersch\u00e4tzung betrachtet. Wie sich herausstellt, werden im dreidimensionalen Fall Ableitungen von Eigen- und Singul\u00e4rsystemen ben\u00f6tigt. F\u00fcr beide Systeme werden in dieser Arbeit L\u00f6sungen in geschlossener Form angegeben. Auftretende numerische Probleme werden in einem separaten Kapitel behandelt. Die vorgestellten numerischen Vereinfachungen machen eine Berechnung von asymptotischen Sch\u00e4tzfehlern in einer praktikablen Zeitspanne \u00fcberhaupt erst m\u00f6glich. Als ein untergeordneter Beitrag wird in dieser Arbeit der Fall einer sich bewegenden Quelle behandelt. Verwendet wird ein stichprobenbasierter Partikelfilter-Algorithmus, der die A-posteriori-Wahrscheinlichkeitsdichte des Zustands des Systems aufgrund der eingegangenen Messungen n\u00e4hert. Eine neue Technik f\u00fcr die Gewichtung der Stichproben basierend auf dem MUSIC-Algorithmus wird vorgestellt. Die vorgestellten Konzepte und Algorithmen werden an einer Anzahl von Testf\u00e4llen validiert.\nParameter estimation deals with the problem of determining the state of a given system through indirect measurements by sensors. Among the abundance of methods in use, the multiple signal classification (MUSIC) algorithm is considered as one of the most successful ones in various areas, like magnetoencephalography, electroencephalography, radar or wireless communication. The algorithm is able to search the operating volume with a one source model and computes the similarity of the source subspace to an estimated signal subspace. The source is assumed to be spatially static. Most practical applications suffer from inaccuracies of the underlying physical model. These will disturb the parameter estimation even in the case of noiseless sensors. This thesis focuses on modeling and quantifying these parameter estimation errors in the asymptotic case. Therefore, the modeling errors are considered as random variables and expectation and higher moments of the parameter estimation error are approximated. Existing first order approaches are augmented to near second order and the case of three dimensional parameter estimation is treated. The three dimensional case necessitates the use of derivatives of eigensystems and singular systems, for which closed-form solutions are provided. Arising problems in numerical complexity are treated in a separate chapter. The presented numerical simplifications make a calculation of asymptotic error estimates in a feasible time possible in the first place. As a minor contribution of this thesis, we investigate the case of a moving source in the operating volume. Our method of choice is a sample based particle filter algorithm which estimates the posterior probability density of the system state given the measurements. We propose a new technique of weighting the samples based on the MUSIC algorithm. The presented concepts and algorithms are validated through a number of test cases.\n2013-03-22\ndoctoralthesis\ndoc-type:doctoralThesis\ntext/x-tex\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3236\nurn:nbn:de:bvb:29-opus-43724\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-43724\nhttps://opus4.kobv.de/opus4-fau/files/3236/original_PatrickLuffDissertation.tex\nhttps://opus4.kobv.de/opus4-fau/files/3236/PatrickLuffDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3288\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:60F17\nmsc:60G44\nmsc:60J60\nmsc:60K35\nmsc:92D10\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDynamics of Genealogical Trees for Autocatalytic Branching Processes\nDynamiken genealogischer B\u00e4ume f\u00fcr autokatalytische Verzweigungsprozesse\nGl\u00f6de, Patric Karl\nWahrscheinlichkeitstheorie\nPopulationsgenetik\nDiffusionsapproximation\nMartingal\nStochastisches Teilchensystem\nddc:510\nWe introduce stochastic processes modelling the evolution of the genealogy and total mass of finite and infinite autocatalytic branching populations. In such populations, at any time, each of the individuals alive has an infinitesimal death rate depending on the current population size and, upon its death, produces a random number of offspring. Formally, processes take values in the space of ultrametric measure spaces equipped with the polar Gromov-weak topology. While the dynamics of discrete tree-valued processes are constructed explicitly as piecewise deterministic Markov processes, the dynamics of infinite populations are characterised by means of martingale problems. Key issues are proving well-posedness for the martingale problems and finding invariance principles linking finite and infinite populations. In fact, we show that infinite populations arise as scaling limits of large finite populations in the sense of weak convergence on path space. It turns out that there is a close relationship between the genealogies of infinite autocatalytic branching populations and the Fleming-Viot process. We also study a general class of martingale problems, to which we refer as skew product martingale problems, and prove a uniqueness result. This result is applied to the tree-valued processes studied in this thesis but it also applies to more general settings. Since total mass processes play an important role in the study of tree-valued processes a separate chapter is devoted to real-valued autocatalytic branching dynamics.\nWir betrachten Populationen, deren Individuen sich autokatalytisch verzweigen. Zu jedem Zeitpunkt hat jedes noch lebende Individuum eine von der aktuellen Populationsgr\u00f6\u00dfe abh\u00e4ngige infinitesimale Todesrate. Bei seinem Tod hinterl\u00e4sst es eine zuf\u00e4llige Anzahl von Nachkommen. Wir f\u00fchren stochastische Prozesse ein, welche die Evolution der Genealogie und der totalen Masse endlicher und unendlicher Populationen modellieren. Formal nehmen die betrachteten Prozesse Werte im Raum der ultrametrischen Ma\u00dfr\u00e4ume an, der mit der polaren Gromov-schwachen Topologie ausgestattet ist. W\u00e4hrend wir die Dynamiken diskreter baumwertiger Prozesse explizit als st\u00fcckweise deterministische Markov-Prozesse konstruieren, werden die Dynamiken f\u00fcr unendliche Populationen durch Martingalprobleme charakterisiert. Wir zeigen unter anderem, dass diese Martingalprobleme wohlgestellt sind. Wir beweisen au\u00dferdem Invarianzprinzipien, die besagen, dass sich unendliche Populationen als Skalierungslimiten gro\u00dfer endlicher Populationen ergeben und zwar im Sinne der schwachen Konvergenz auf dem Pfadraum. Es stellt sich heraus, dass es einen engen Zusammenhang gibt zwischen den Genealogien unendlicher autokatalytischer Verzweigungspopulationen und des Fleming-Viot-Prozesses. Neben baumwertigen autokatalytischen Verzweigungsprozessen besch\u00e4ftigen wir uns auch mit einer allgemeinen Klasse von Martingalproblemen, die wir als Schiefprodukt-Martingalprobleme bezeichnen, und beweisen ein allgemeines Eindeutigkeitsresultat. Dieses wird auf konkrete Martingalprobleme aus der vorliegenden Arbeit angewandt, gilt aber auch in einem allgemeineren Kontext. Da die totale Masse-Prozesse eine wichtige Rolle bei der Analyse der baumwertigen Prozesse spielen, ist ein Kapitel dem Studium reellwertiger autokatalytischer Verzweigungsdynamiken gewidmet.\n2013-03-28\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3288\nurn:nbn:de:bvb:29-opus-45453\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-45453\nhttps://opus4.kobv.de/opus4-fau/files/3288/original_PatricKarlGloedeDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/3288/PatricKarlGloedeDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3306\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35B27\nmsc:35R30\nmsc:74P05\nmsc:74Q05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nTwo-scale material design - From theory to practice\nMaterialdesign auf zwei Skalen - Von der Theorie zur Praxis\nSchury, Fabian\nHomogenisierung <Mathematik>\nTopologieoptimierung\nddc:510\nIn this thesis we study the material design problem in different variations. First, we consider periodic material design via inverse homogenization. It is necessary to regularize the corresponding optimization problem. For this purpose, we use the so-called slope constraints', i.e. bounds for the partial derivatives of the design variable. For the optimization problem regularized in this manner, we show existence of a solution. After defining an appropriate discretization scheme, we show convergence of the discretized solutions to the solutions in function space. We then extend the method and present a two-scale approach for the design of locally periodic structures, also called structurally graded materials. We combine inverse homogenization for the meso-scale with free material optimization (FMO) for the macro-scale. FMO allows to find, at every point in the design domain, the optimal material tensor (under the assumption of linear elasticity). In a second step, this space-dependent tensor is then approximated by structures via the method of inverse homogenization. We develop a decoupled two-scale algorithm to solve the whole problem. We put special emphasis on producing manufacturable designs. This necessitates special provisions at several steps in the algorithm. Among other things, we again enforce bounds on the partial derivatives of the design variable to regularize the FMO problem. For the problem regularized in this way, as before we show existence of a solution and convergence of an appropriate discretization scheme. Furthermore, we discuss several practical aspects which ensure the feasibility and the viability of the procedure. In this approach, the information between the scales is transferred in the form of material tensors. In a certain sense, this might be too much information. This may result in meso-structures which are too complex with respect to the size of the smallest structural feature. Therefore, we develop a second two-scale approach based on topology gradients in order to optimize elliptic orthotropic inclusions in a surrounding domain of isotropic material. Now, the information between the scales is passed in the form of optimal rotation angles. This procedure is very advantageous from a manufacturing point of view, since the problem of smallest feature size can be completely avoided. In addition, this approach to the two-scale problem is computationally very efficient, since the algebraic system has to be solved only once for any number of inclusions. For all three topics, i.e. periodic inverse homogenization with slope constraints and the two two-scale approaches based on FMO and the topology gradient, we present numerical studies and experiments to study, in detail, the properties of the procedures. In particular, we underline their practical applicability.\nIn dieser Arbeit besch\u00e4ftigen wir uns mit dem Problem des Materialdesigns in verschiedenen Varianten. Zun\u00e4chst betrachten wir das periodische Materialdesign mittels inverser Homogenisierung. F\u00fcr das zugeh\u00f6rige Optimierungsproblem ist es notwendig, eine Form der Regularisierung anzuwenden. Hierf\u00fcr werden die sogenannten 'slope constraints', also Beschr\u00e4nkungen der partiellen Ableitung der Designvariable, verwendet. F\u00fcr das so regularisierte Problem zeigen wir dann die Existenz einer L\u00f6sung. Wir definieren ein geeignetes Diskretisierungschema und zeigen, dass die diskretisierten L\u00f6sungen gegen die L\u00f6sungen im Funktionenraum konvergieren. Daraufhin wird diese Methode erweitert und ein zweiskaliger Ansatz pr\u00e4sentiert, der das Design von lokal periodischen Strukturen erlaubt. Dabei koppeln wir die inverse Homogenisierung f\u00fcr die Mesoskala mit der freien Materialoptimierung (FMO) auf der Makroskala. Die FMO erlaubt, punktweise im Gebiet den optimalen Materialtensor (unter der Annahme eines linear elastischen Materialgesetzes) zu bestimmen. In einem zweiten Schritt wird dieser Tensor dann durch Strukturen approximiert, die aus der inversen Homogenisierung gewonnen werden. Wir beschreiben einen entkoppelten zweiskaligen Algorithmus, um das Gesamtproblem zu l\u00f6sen. Da die Fertigbarkeit der optimalen Struktur von besonderem Interesse ist, ist es notwendig, an mehreren Stellen im Algorithmus besondere Vorkehrungen zu treffen. Unter anderem verwenden wir, wiederum als Regularisierung, Beschr\u00e4nkungen der partiellen Ableitungen der Designvariable im FMO-Problem. F\u00fcr das solcherma\u00dfen regularisierte Problem zeigen wir erneut die Existenz einer L\u00f6sung, sowie die Konvergenz eines geeigneten Diskretisierungsschemas. Des weiteren besch\u00e4ftigen wir uns mit praktischen Aspekten, die die Zul\u00e4ssigkeit und Realisierbarkeit des Verfahrens sicherstellen. Da die \u00dcbertragung der Information zwischen den Skalen f\u00fcr diesen Ansatz mit Hilfe von Materialtensoren geschieht, kann es passieren, dass in gewissem Sinne zu viel Information \u00fcbertragen wird. Dies kann zur Folge haben, dass die Mesostruktur zu komplex wird im Bezug auf die Gr\u00f6\u00dfe der kleinsten Stegbreite. Daher entwickeln wir einen zweiten Zweiskalenansatz, basierend auf Topologiegradienten, um elliptische, orthotrope Einschl\u00fcsse in einem Gebiet isotropen Materials zu optimieren. Die Informationen werden hierbei in Form von optimalen Rotationswinkeln zwischen den Skalen \u00fcbermittelt. Da das Problem der kleinsten Stegbreite komplett umgangen werden kann, ist dieses Verfahren ist von gro\u00dfem Vorteil f\u00fcr die Fertigbarkeit der entstehenden Struktur. Dar\u00fcberhinaus ist das Verfahren numerisch sehr effizient, da das algebraische System nur einmal gel\u00f6st werden muss f\u00fcr die Optimierung einer beliebigen Anzahl von Einschl\u00fcssen. F\u00fcr alle drei Ans\u00e4tze, also periodische inverse Homogenisierung mit slope constraints, sowie die beiden Zweiskalenans\u00e4tze basierend auf FMO und Topologiegradienten, werden numerische Studien und Experimente pr\u00e4sentiert, um die unterschiedlichen Eigenschaften der Verfahren detailliert darzustellen und zu untersuchen. Insbesondere wird ihre Praxistauglichkeit herausgearbeitet.\n2013-04-15\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3306\nurn:nbn:de:bvb:29-opus-45787\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-45787\nhttps://opus4.kobv.de/opus4-fau/files/3306/original_FabianSchuryDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/3306/FabianSchuryDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3313\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:500\nmsc\nmsc:92-02\nmsc:92F05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nCharacterisation of functional and sensory properties of lupin proteins\nCharakterisierung der funktionellen und sensorischen Eigenschaften von Lupinenproteinen\nMittermaier, Stephanie\nEnt\u00f6lung\nAromastoffe\nddc:500\nSeeds of sweet lupins are valuable sources for the production of lupin protein isolates due to their high protein content and their high nutritive value. Besides, lupin proteins exhibit excellent functional properties regarding protein solubility and emulsifying properties. However, the sensory characteristics of the lupin protein isolates and changes during storage impede their commercial availability. Therefore, the aim of the present work was to characterise impact factors on the functional properties of protein isolates during processing. Additionally, the odour-active compounds most likely responsible for the characteristic flavour of lupin flours and protein isolates were identified using high resolution gas chromatography-olfactometry/mass spectrometry. Furthermore, de\u2011oiling with organic solvents and supercritical CO2 was investigated as possibilities to improve the flavour of the isolates. First of all, the effects of different lupin species (L. albus cv. TypTop, L. luteus cv. Bornal) and lupin varieties of L. angustifolius L. on the chemical composition of flours and isolates as well as on the functional characteristics of the produced isolates were investigated. Diverging protein functionalities were obtained for protein isolates derived from different lupin species. The protein isolates of L. angustifolius L. revealed excellent emulsifying properties, whereas only moderate emulsifying characteristics were observed for the protein isolates derived from the other species. Compared with the LPI of the other species, the proteins of L. albus cv. TypTop formed viscous gels at a concentration of 15% (w/w). Thus, the LPI with different functional profiles are suitable for various food applications, e.g. as emulsifiers in case of L. angustifolius L. and as a gelling agent in case of L. albus cv. TypTop. The diverging functionalities seem to be caused by the presence of different protein fractions with varying molecular weights as shown by one-dimensional polyacrylamide gel electrophoresis. Altogether, greater variations were obtained between lupin species than between lupin varieties, but also environmental conditions during growth seemed to influence dry matter recoveries during protein isolation. Due to its availability and the highest protein recovery among the narrow-leafed lupin species, L. angustifolius cv. Boregine was chosen for further investigations. Thereby, the present thesis focussed on the identification of odour-active compounds in lupin flour and potential changes during storage and isolate production. The odorants, which were identified for the first time in lupin flour and protein isolates, comprised compounds of various chemical classes including aldehydes, ketones, carboxylic acids, 3-alkyl-2-methoxypyrazines and lactones. According to the different chemical properties and the specific structural features of the identified odorants different metabolic and reaction pathways like lipoxygenase-mediated reactions, oxidation of fatty acids or secondary plant metabolism leading to these substances can be assumed. Besides, the aroma profile of the protein isolate changed significantly to higher intensities of fatty, hay-like, green and oat flakes-like odour impressions in relation to the lupin flour samples. Consequently, higher FD-factors were obtained for saturated and unsaturated aldehydes in the isolate compared to lupin flour representing oxidation of fatty acids, which is most likely related to the activity of lipoxygenase. In order to improve the aroma of the LPI, lipid oxidation should be avoided, which might be accomplished by either enzyme inactivation or by de-oiling of lupin flakes. In the present thesis the effect of de-oiling by the application of various organic solvents as well as supercritical CO2 on the flavour and the functional properties of the isolates were investigated. Only de-oiling with ethanol and 2-propanol resulted in slightly decreased protein solubilities of the flakes, which subsequently resulted in lower protein recoveries. Furthermore, independent of the de\u2011oiling process all isolates revealed excellent functional properties. The overall acceptance of the LPI produced from supercritical CO2-extracted flakes was rated higher than that of the protein isolates derived from organic solvent de-oiled and from full-fat lupin flakes, respectively. Therefore, de-oiling with supercritical CO2 is a preferable alternative to de-oiling with organic solvents considering the protein recoveries, the functional and the sensory properties of the isolates. The present thesis highlighted the potential of narrow-leafed lupin varieties, in particular L. angustifolius cv. Boregine, as a valuable source for the efficient production of highly functional protein isolates with improved flavour due to de-oiling with supercritical CO2. However, the inactivation of enzymes for flavour improvement was not part of the present work and should be addressed in future.\nAufgrund ihres hohen Protein- und N\u00e4hrstoffgehalts stellen S\u00fc\u00dflupinen eine wertvolle Quelle f\u00fcr die Herstellung von Proteinisolaten dar. Au\u00dferdem verf\u00fcgen Lupinenproteine \u00fcber hervorragende Proteinl\u00f6slichkeiten und Emulgiereigenschaften. Allerdings erschweren ihre sensorischen Eigenschaften und Ver\u00e4nderungen dieser w\u00e4hrend der Lagerung ihre kommerzielle Verf\u00fcgbarkeit. Ein Ziel der vorliegenden Arbeit war daher, Einflussfaktoren auf die funktionellen Eigenschaften von Proteinisolaten w\u00e4hrend der Herstellung zu untersuchen. Au\u00dferdem wurden die aroma-aktiven Substanzen, welche f\u00fcr das charakteristische Aromaprofil von Lupinenmehlen und -proteinisolaten verantwortlich sind, mittels Gaschromatographie-Olfaktometrie und zwei-dimensionaler Gaschromatographie-Massenspektrometrie identifiziert. Weiterhin wurde als M\u00f6glichkeit zur Verbesserung der sensorischen Eigenschaften der Lupinenproteinisolate die Ent\u00f6lung mit Hilfe von organischen L\u00f6semitteln und \u00fcberkritischem CO2 untersucht. Zun\u00e4chst wurde der Einfluss unterschiedlicher Lupinensorten (L. albus cv. TypTop, L. luteus cv. Bornal) und -variet\u00e4ten von L. angustifolius L. auf die Zusammensetzung der Mehle und Isolate sowie auf die funktionellen Eigenschaften der daraus hergestellten Proteinisolate untersucht. Insgesamt wiesen die Proteinisolate von L. angustifolius L. herausragende Emulgiereigenschaften auf, wohingegen bei den Isolaten der anderen Lupinensorten nur m\u00e4\u00dfige Emulgiereigenschaften beobachtet werden konnten. Im Vergleich zu den Isolaten der anderen Lupinensorten bildeten die Proteine von L. albus cv. TypTop bei einer Konzentration von 15 Gew.-% viskose Gele. Die Proteinisolate mit ihren unterschiedlichen funktionellen Eigenschaften sind f\u00fcr den Einsatz in verschiedensten Lebensmittelsystemen geeignet, wie beispielsweise als Emulgator im Fall von L. angustifolius L. und als Gelbildner im Fall von L. albus cv. TypTop. Diese divergierenden funktionellen Eigenschaften der Proteinisolate k\u00f6nnten auf die Anwesenheit unterschiedlicher Proteinfraktionen mit verschiedenem Molekulargewicht zur\u00fcckzuf\u00fchren sein, was mittels 1-dimensionaler Gelelektrophorese gezeigt wurde. Insgesamt zeigte sich, dass zwischen den einzelnen Lupinensorten gr\u00f6\u00dfere Unterschiede bestanden als zwischen Variet\u00e4ten; jedoch beeinflussten auch die Umweltbedingungen w\u00e4hrend des Wachstums die Trockenmasse-Ausbeuten w\u00e4hrend des Isolierungsprozesses. Aufgrund der Verf\u00fcgbarkeit und der h\u00f6chsten Proteinausbeute aller untersuchten Schmalbl\u00e4ttrigen Lupinenvariet\u00e4ten wurde L. angustifolius cv. Boregine f\u00fcr die weiterf\u00fchrenden sensorischen Untersuchungen ausgew\u00e4hlt. Dabei konzentrierte sich die vorliegende Arbeit auf die Identifizierung von Aromastoffen in Lupinenmehl und die Identifizierung von m\u00f6glichen Ver\u00e4nderungen w\u00e4hrend der Lagerung und der Herstellung von Proteinisolaten. Die Aromastoffe, die zum ersten Mal in Lupinenmehlen und -proteinisolaten identifiziert wurden, bestanden aus Verbindungen verschiedenster chemischer Strukturklassen wie Aldehyde, Ketone, Carbons\u00e4uren, 3-Alkyl-2-Methoxypyrazine und Lactone. Aufgrund der unterschiedlichen chemischen Eigenschaften und der spezifischen strukturellen Merkmale der identifizierten Aromastoffe k\u00f6nnen unterschiedliche Reaktionswege f\u00fcr die Bildung dieser Substanzen angenommen werden. Diese Reaktionswege beinhalten h\u00f6chstwahrscheinlich Lipoxygenase-vermittelte Reaktionen, Oxidation von Fetts\u00e4uren, Abbau von Aminos\u00e4uren und Produkte des sekund\u00e4ren Pflanzenstoffwechsels. Au\u00dferdem ver\u00e4nderte sich das Aromaprofil der Lupinenproteinisolate im Vergleich zum Profil des Lupinenmehls signifikant hin zu st\u00e4rkeren Intensit\u00e4ten von fettigen, heuartigen, gr\u00fcnen und haferflockenartigen Geruchseindr\u00fccken. Ebenso wurden in den Isolaten im Vergleich zu den Lupinenmehlen h\u00f6here FD-Faktoren f\u00fcr ges\u00e4ttigte und unges\u00e4ttigte Aldehyde ermittelt. Um das Aroma der Lupinenproteinisolate zu verbessern, sollte die Oxidation von Fetten vermieden werden, was entweder durch Enzyminaktivierung oder durch Ent\u00f6lung der Lupinenflocken erreicht werden k\u00f6nnte. In der vorliegenden Arbeit wurde der Einfluss einer Ent\u00f6lung mit verschiedenen organischen L\u00f6semitteln und \u00fcberkritischem CO2 auf die funktionellen und sensorischen Eigenschaften der Isolate untersucht. Lediglich eine Ent\u00f6lung mit Ethanol oder 2-Propanol verursachte eine Reduzierung der Proteinl\u00f6slichkeiten der Lupinenflocken, was zu geringeren Proteinausbeuten f\u00fchrte. Dar\u00fcber hinaus besa\u00dfen alle Proteinisolate \u2013 unabh\u00e4ngig von der Ent\u00f6lungsmethode \u2013 herausragende funktionelle Eigenschaften. Die Gesamtbeliebtheit der Isolate, die aus CO2-extrahierten Flocken hergestellt wurden, war h\u00f6her als die Akzeptanz der Isolate, die mit L\u00f6semittel-ent\u00f6lten Flocken, und die mit vollfetten Flocken, hergestellt wurden. Daher ist eine Ent\u00f6lung mit \u00fcberkritischem CO2 einer Ent\u00f6lung mit organischen L\u00f6semitteln im Hinblick auf die Proteinausbeuten, auf die funktionellen und sensorischen Eigenschaften der Isolate vorzuziehen.\n2013-04-23\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/vnd.oasis.opendocument.text\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3313\nurn:nbn:de:bvb:29-opus-45838\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-45838\nhttps://opus4.kobv.de/opus4-fau/files/3313/original_MittermaierStephanieDissertation.odt\nhttps://opus4.kobv.de/opus4-fau/files/3313/MittermaierStephanieDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3438\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nmsc\nmsc:92C55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med_ohneAngabe\nDosisreduktion in der thorakalen Computertomographie: Bildqualit\u00e4t von gefilterter R\u00fcckprojektion und iterativen Rekonstruktionen im intraindividuellen Vergleich\nDosereduction in computed tomography of the chest: Intraindividual comparison of image quality of filtered back projection reconstruction and iterative reconstruction.\nStahl, Christian\nRadiologie\nIonisierende Strahlung\nStrahlung\nStrahlendosis\nComputertomographie\nct\nBrustkorb\nDosis\nRekonstruktion\nIRIS\nIterativ\nddc:610\nHintergrund und Ziele In einem prospektiven Vergleich sollten Bildqualit\u00e4t, Bildrauschen und diagnostische Wertigkeit thorakaler Halbdosis (HD) Datens\u00e4tze, die mit einem bildbasierten, iterativen Algorithmus (IRIS) rekonstruiert wurden, mit durch gefilterte R\u00fcckprojektion (FBP) rekonstruierten Volldosis (FD) und HD Datens\u00e4tzen verglichen werden. Material und Methoden 52 Patienten (14 Frauen und 38 M\u00e4nner; Durchschnittsalter 62 \u00b1 13 Jahre) mit klinischer Indikation zur Thorax-CT wurden an einem Dual Source (DS) CT untersucht. Der R\u00f6hrenstrom wurde bei konstanter R\u00f6hrenspannung von 120 Kilovolt (kV) gleichm\u00e4\u00dfig auf die beiden R\u00f6ntgenr\u00f6hren-Detektoreinheiten aufgeteilt. Aus einer Untersuchung wurden jeweils 3 Datens\u00e4tze rekonstruiert: FD-FBP durch Auswertung von Rohdaten aus beiden, sowie HD-FBP und HD-IRIS aus Rohdaten eines einzelnen R\u00f6ntgenr\u00f6hren-Detektor-Systems. Die objektive Bewertung der Bildqualit\u00e4t erfolgte durch Messung des Bildrauschens. Die subjektive Evaluation wurde unabh\u00e4ngig von 2 Untersuchern analog der \"European guidelines on quality criteria for CT\", sowie hinsichtlich des Auftretens von Bildartefakten und der Abgrenzbarkeit und Randsch\u00e4rfe pathologischer L\u00e4sionen vorgenommen. Ergebnisse und Beobachtungen Das Bildrauschen in HD-IRIS und FD-FBP unterschied sich nicht signifikant voneinander (p = 0,254). In der HD-FBP Gruppe nahm das Bildrauschen dagegen hochsignifikant um 41,3 % zu (p < 0,001). FD-FBP und HD-IRIS zeigten bez\u00fcglich der scharfen Darstellung von pulmonalen und mediastinalen Strukturen, mit Ausnahme der kleinsten Bronchial\u00e4ste, keine signifikanten Unterschiede. Die Abgrenzbarkeit und Randsch\u00e4rfe der untersuchten L\u00e4sionen zeigte keine signifikante Diskrepanz zwischen HD-IRIS und FD-FBP Datens\u00e4tzen. In den HD-FBP Bildern wurden die anatomischen Strukturen und die Kantensch\u00e4rfe der L\u00e4sionen im Vergleich zu FD-FBP signifikant schlechter bewertet. Streifenartefakte waren in den HD-FBP Datens\u00e4tzen, eine pixelige Darstellungsform in den HD-IRIS Datens\u00e4tzen signifikant h\u00e4ufiger (beide p < 0,001). Praktische Schlussfolgerung Bei Halbierung der Strahlendosis erreichte die FBP in der Darstellung der Lunge eine akzeptable, IRIS eine der FD-FBP vergleichbare Qualit\u00e4t. Eine scharfe Darstellung mediastinaler Strukturen und die notwendige Kantensch\u00e4rfe kleiner L\u00e4sionen wurden bei 50 % Dosisreduktion nur im iterativen Algorithmus erreicht. IRIS erm\u00f6glicht dank Rausch- und Artefaktreduktion eine der FD-FBP vergleichbare Bildqualit\u00e4t und diagnostische Wertigkeit bei halber Strahlendosis. Der leicht ver\u00e4nderte Bildaspekt durch IRIS hatte dabei keinen Effekt auf die diagnostische Bildqualit\u00e4t. Moderne, iterative Rekonstruktionsalgorithmen k\u00f6nnen in Zukunft weitere Dosiseinsparungen bei Rekonstruktionszeiten, die mit dem klinischen Ablauf vereinbar sind, erm\u00f6glichen.\nObjectives The purpose of the study was to prospectively compare image quality, image noise and diagnostic value of iterative reconstruction in image space (IRIS) in half-dose (HD) datasets with full-dose (FD) and HD filtered back projection reconstruction (FBP) in computed tomography (CT) of the chest. Materials and Methods Chest CT examinations were performed for 52 patients (14 women, 38 men, mean age 62 \u00b1 13 years) on a dual-source (DS) system. Data were acquired with both tubes operating at a constant tube voltage of 120 kV. Tube current was split up equally for both tube-detector-systems. Three image datasets were reconstructed from raw data: FD-FBP datasets applying data from both tube-detector-systems, HD-FBP and HD-IRIS datasets using data from only one tube-detector-system. Objective image quality analysis was performed by measurement of the image noise. Subjective image quality was assessed by two investigators according to the European guidelines on quality criteria for CT. Both of them performed additional evaluation of artifacts, lesion conspicuity and lesion edge sharpness. Results No significant differences between HD-IRIS and FD-FBP were found for the image noise (p = 0.254). HD-FBP datasets were associated with significantly increased image noise (+ 41.3 %; p < 0.001). Visually sharp anatomic reproduction of lung and mediastinal structures was, except of the small bronchioli, not significantly different in FD-FBP and HD-IRIS. Lesion conspicuity and lesion edge sharpness in HD-IRIS images were comparable to those in FD-FBP images. Compared with FD datasets, visually sharp reproduction of anatomical structures and lesion edge sharpness were rated significantly worse in the HD-FBP datasets. Streak artifacts were significantly increased in HD-FBP, while HD-IRIS datasets showed a slightly altered noise pattern. Conclusion Qualitative image quality at 50% dose reduction allows for acceptable reproduction of the lung tissue using FBP, while reconstructions with IRIS accounts for an image quality comparable to FD-FBP. Especially the visually sharp reproduction of mediastinal structures and lesion edge sharpness at HD was conserved by IRIS. Image quality and diagnostic acceptability can be obtained for images acquired at 50 % dose reduction when reconstructed with IRIS. Diagnostic acceptability was not influenced by the slightly pixilated appearance of HD-IRIS. Modern iterative reconstruction algorithms might allow for further dose reduction in thoracic CT within acceptable reconstruction time.\n2013-06-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3438\nurn:nbn:de:bvb:29-opus-47495\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-47495\nhttps://opus4.kobv.de/opus4-fau/files/3438/original_ChristianStahlDissertation.docx\nhttps://opus4.kobv.de/opus4-fau/files/3438/ChristianStahlDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3530\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nmsc\nmsc:94A05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nRateless Coded Cooperative Communications for Wireless Networks\nKooperative Datenkommunikation f\u00fcr drahtlose Netzwerke mittels ratenlosen Codes\nRavanshid, Azad\nCodierungstheorie\nKanalcodierung\nQuellencodierung\nDaten\u00fcbertragung\nMobilfunk\nddc:620\nZusammenfassung: In der vorliegende Arbeit untersuchen werden praktische Implementierung von kooperativer Kommunikation, sogenannte Decode-and-Forward (DF) sowie Compress-and-Forward (CF) Relay-\u00dcbertragung, in Verbindung mit ratenlosen Codes untersucht. Zuerst wird die halb-duplex DF \u00dcbertragung unter Einsatz von ratenlosen Codes betrachtet, welche dem Relay erlauben, den Umschaltzeitpunkt zwischen Empfangen und Senden des Quellensignals basierend auf den jeweiligen \u00dcbertragungsqualit\u00e4ten zu optimieren. Diese Variation von DF Relay-\u00dcbertragung wird auch als Dynamic DF (DDF) bezeichnet. Die Detektion der Signale von der Informationsquelle und dem Relay am Informationsempf\u00e4nger kann auf unterschiedliche Art und Weise erfolgen. In dieser Arbeit werden auf systematische Art und Weise zwei aus der Fachliteratur bekannte Methoden, n\u00e4mlich sogenanntes Energy Combining und Information Combining behandelt. Diese Beschreibung erlaubt es, eine neuartige Detektionsmethode, welche wir Mixed Combining nennen, abzuleiten. Die verschiedenen Combining-Methoden f\u00fchren zu unterschiedlichen erreichbaren Raten, die analytisch bestimmt werden. Anhand dieser Kapazit\u00e4tzanalyse wird aufgezeigt, unter welchen Rahmenbedingungen Mixed Combining den anderen Methoden \u00fcberlegen ist und wie es optimiert werden kann. Als zweiten wesentlichen Beitrag dieser Arbeit wird eine Analyse der erreichbaren Datenrate f\u00fcr DDF mit den verschiedenen Combining-Methoden und eine spezifische Implementierung von ratenosen Codes, sogenannten Raptor Codes entwickelt. Diese Analyse beruht auf einer Adaption der Density-Evolution Technik. Dar\u00fcber hinaus wird ein Optimierungsproblem f\u00fcr das Design von Raptor Codes f\u00fcr DDF formuliert und gel\u00f6st. Diese optimierten Codes dienen als Benchmark f\u00fcr DDF mit universellen Raptor-Code Designs. Die numerische Ergebnisse f\u00fcr die Beispiele mit Drei- und Vier-Hop Relay-Netzwerken zeigen, dass die vorgeschlagene Mixed-Combining Methode eine signifikante Steigerung der erreichbaren Datenrate erm\u00f6glicht und DDF mit universellen Raptor Codes in der Lage ist, diese Steigerung in der Praxis zu realisieren. Compress-and-Forward bezeichnet ein weiteres grundlegendes Relay-Schema, das auf dem Konzept von Quellencodierung mit Seiteninformation basiert, n\u00e4mlich Slepian-Wolf Codierung (SWC) und Wyner-Ziv Codierung (WZC). Mit Hilfe von SWC/WZC kann ein Relay die Korrelation zwischen seinem empfangenen Signal sowie dem bereits empfangenen Signal an der Destination zur Datenkompression zu nutzen versuchen. Im Gegensatz zu DF ist der Umschaltzeitpunkt in CF a-priori festgelegt. Es kann jedoch immer noch sinnvoll sein, ein CF Relaying Netzwerk mit ratenlosen Codes zu betreiben, da die Ratenanpassung von ratenlosen Codes nicht nur zur Grenze der Kanalkapazit\u00e4t hin erfolgen kann (als Kanalcode), sondern auch an die Grenze der Komprimierbarkeit (als Quellencode). Dies f\u00fchrt zu einer umfassenden Ratenanpassung, die als Fine-Tuning bezeichnet wird. In dieser Arbeit werden die theoretischen Grundlagen und Voraussetzungen von Compress-and-Forward Relay Strategien dargestellt. Die Methoden und praktischen implementierungen von SWC und WZC werden speziell f\u00fcr ratenlose Anwendungen dargestellt. Die Grenzen der maximal erreichbaren Raten von spitzenleistungsbegrenzten halb-duplex CF Strategien werden hergeleitet und neue CF Strategien mit ratenloser Codierung werden f\u00fcr systematische Raptor Codes hergeleitet. Es wird gezeigt, dass hinsichtlich der maximalen Rate systematische Raptor Codes f\u00fcr SWC und WZC eine deutlich h\u00f6here Leistungsf\u00e4higkeit erreichen als nicht-systematische Raptor Codes. Desweiteren wird durch Einf\u00fchrung eines parallelen Decodierschemas eine Density-Evolution Analyse f\u00fcr CF \u00dcbertragung mit systematischen Raptor Codes entwickelt, auf deren Grundlage die \u00dcbertragungsparameter weiter optimiert werden. Wie erwartet, zeigen die numerischen Ergebnisse eine deutlich verbesserte Leistungsf\u00e4higkeit aufgrund der Parameteroptimierung.\nAbstract: In this thesis we investigate two main practical implementations of cooperative communications, so called Decode-and-Forward (DF) and Compress-and-Forward (CF) relaying, in conjunction with rateless codes. Dynamic decode-and-forward (DDF) is a version of decode-and-forward relaying in which the duration of the listening phase at relays is not fixed. In this thesis, we investigate half-duplex DDF relaying based on rateless codes. The use of rateless codes allows relays to autonomously switch from listening to the source node to transmitting to the destination node. We first revisit different signal combining strategies applied at the destination node, namely energy and information combining known from literature, and propose a new combining method which we refer to as mixed combining. The different combining methods give rise to different achievable rates, i.e., constrained channel capacities, for which we provide analytical expressions. The capacity analysis reveals the conditions under which mixed combining is superior and how it can be optimized. We then consider Raptor codes as a specific implementation of rateless codes and develop a density-evolution approximation to predict the data-rate performance of these codes in DDF relaying. Furthermore, we devise an optimization of the output symbol degree distribution of Raptor codes that is mainly used to benchmark the performance of Raptor codes with a fixed degree distribution. Numerical results for exemplary three-node and four-node relay networks show that the proposed mixed combining provides significant gains in achievable data rate and that Raptor codes with a fixed degree distribution are able to realize these gains and to approach closely the constrained-capacity limits. Compress-and-forward refers to another relevant relaying scheme, which is based on the concept of source coding with side information; namely Slepian-Wolf coding (SWC) and Wyner-Ziv coding (WZC). Using SWC/WZC, the relay attempts to exploit the correlation between its received signal, and the already received signal at the destination in order to compress its data. In contrast to DF, the duration of listening phase is a-priori defined. However, it may still be reasonable to facilitate CF relaying framework with rateless codes, since rateless codes ensure to adapt their rates not only to the capacity bounds (as a channel code) but also to compression bounds (as a source code), which leads to an overall rate-adaption, the so called fine tuning. In this thesis we provide theoretical backgrounds and preliminaries on compress-and-forward relaying strategy. We outline the principles and practical methods of implementing of SWC and WZC, mainly in a rateless scenario. We derive the bounds on maximum achievable rate of half-duplex CF scheme under peak power constraint (PPC), and propose a novel rateless coded CF scheme using systematic Raptor codes. We show that in terms of overall maximum achievable rate, using systematic Raptor codes for SWC/WZC significantly outperforms non-systematic ones. Furthermore, by introducing a parallel decoding method, density evolution analysis for CF transmission with systematic Raptor code is developed, which serves as the basis for optimization of transmission parameters. As expected, numerical results indicate clear performance improvement due to the optimized parameter.\n2013-08-15\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3530\nurn:nbn:de:bvb:29-opus-48894\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-48894\nhttps://opus4.kobv.de/opus4-fau/files/3530/original_AzadRavanshidDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/3530/AzadRavanshidDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3537\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:570\nmsc\nmsc:92C37\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nDie Funktion HIV-1 Nef-induzierter Exosomen\nThe function of HIV-1 Nef-induced exosomes\nBr\u00e4u, Tanja\nHIV\nddc:570\nExosomen spielen eine gro\u00dfe Rolle bei der interzellul\u00e4ren Kommunikation zwischen Zellen im Zusammenhang mit Tumorerkrankungen sowie viralen Infektionen wie HIV. Ziel dieser Arbeit war es, die unbekannte physiologische Funktion der durch HIV-1 Nef-induzierten Exosomen-Sekretion in Hinblick auf die HIV-Pathogenese aufzukl\u00e4ren. Es konnte zu Beginn eine Nef-induzierte Exosomen-Sekretion beobachtet werden, die eine massive TNF\u03b1-Freisetzung bei ruhenden, nicht-infizierten Nachbarzellen initiierte. Die Suche nach einer TNF\u03b1-prozessierenden Protease f\u00fchrte zur Metalloprotease ADAM17/TACE, welche durch Nef und einen rekrutierten Nef-assoziierten Kinase Komplex (NAKC) aktiviert und zusammen mit Komponenten des Komplex in Exosomen transferiert wird. TACE wird nach dessen Aktivierung durch Nef von der Zelloberfl\u00e4che in endosomale Kompartimente internalisiert und in einer zur Zelloberfl\u00e4che inversen Orientierung in Exosomen freigesetzt wird. Um die Aktivierung von TACE durch Abspaltung der N-terminalen Prodom\u00e4ne zu analysieren, wurden Furin-Schnittstellen deletiert, die putativ als Schnittstellen zur Entfernung der Prodom\u00e4ne dienten. Dadurch zeigte sich, dass m\u00f6glicherweise eine autokatalytische, strukturabh\u00e4ngige Aktivit\u00e4t von TACE selbst f\u00fcr die Abspaltung der Prod\u00f6m\u00e4ne und somit dessen Aktivierung verantwortlich ist. Die Analyse von Exosomen aus dem Plasma nicht-vir\u00e4mischer HIV- Patienten zeigte, dass auch in vivo Nef und TACE mit Vesikeln assoziiert vorlagen und diese ebenfalls die Freisetzung von proinflammatorischem TNF\u03b1 bewirkten. Eine ADAM-abh\u00e4ngige TNF\u03b1-Freisetzung bei Zielzellen konnte ebenso f\u00fcr Exosomen einer prim\u00e4ren Melanomzelllinie best\u00e4tigt werden. In dieser Arbeit konnte eine neue physiologische Funktion von Exosomen gezeigt werden: der Integrin-abh\u00e4ngige Transfer von ADAM-Metalloproteasen in Exosomen, wie er bei einer HIV-Infektion als auch bei prim\u00e4ren Melanomzellen gezeigt werden konnte, spielt m\u00f6glicherweise eine Rolle bei der Generierung eines proinflammatorischen Milieus, welches die Replikation des Virus bzw. das Wachstum des Tumors stimuliert. Dieses neugewonnene Wissen gibt einen tieferen Einblick in die Pathogenese von HIV-Infektionen und Tumorerkrankungen und k\u00f6nnte vor allem bei der Entwicklung neuer Therapiem\u00f6glichkeiten Anwendung finden.\nExosomes play a big role in intercellular communication between cells in the context of tumour diseases as well as in viral infections such as HIV. The aim of this thesis was to decipher the unknown physiological function of HIV-1 Nef-induced exosome secretion in respect of HIV pathogenesis. There could be observed a Nef-induced exosome secretion in the beginning that was able to initiate a massive release of proinflammatory TNF\u03b1 from resting non-infected bystander cells. The search for a TNF\u03b1-processing protease led to the metalloproteinase ADAM17/TACE that is activated through Nef and a recruited Nef-associated kinase complex (NAKC) and shuttled in exosomes together with components of the complex. Upon activation through Nef TACE is internalized from the cell surface in endosomal compartments and released in exosomes in an inverse orientation regarding cell surface. In order to analyze TACE activation via N-terminal prodomain cleavage, furin cleavage sites, serving as putative cleavage sites for prodomain removal, were deleted. It turned out that an autocatalytic structure-depending TACE activity might be responsible for prodomain cleavage and a subsequent activation. The analysis of exosomes isolated from plasma of non-viraemic HIV patients revealed that also in an in vivo situation Nef and TACE are associated together wirh exosomes. These exosomes were also capable of inducing a proinflammatory TNF\u03b1 secretion. An ADAM-dependent TNF\u03b1 release from bystander cells could further be confirmed for exosomes from a primary melanoma cell line. In this thesis a new physiological function of exosomes could be described: the integrin-dependent transfer of ADAM metalloproteinases to exosomes, like it was shown in an HIV infection als well as for primary melanoma cells, probably plays a role in generating a proinflammatory environment stimulating viral replication and tumour growth, respectively. This newly gained knowledge provides deeper insight into the pathogenesis of HIV infections and tumour diseases and could be applied in the development of new therapeutic options.\n2013-08-23\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3537\nurn:nbn:de:bvb:29-opus-48741\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-48741\nhttps://opus4.kobv.de/opus4-fau/files/3537/original_TanjaBraeuDissertation.pdf\nhttps://opus4.kobv.de/opus4-fau/files/3537/TanjaBraeuDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3608\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:65N08\nmsc:65N15\nmsc:65N22\nmsc:76M10\nmsc:76T20\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nSimulation of particulate electrodynamic flows with the Subspace Projection Method\nSimulation von partikelbehafteten elektrodynamischen Str\u00f6mungen mit der Unterraumprojektionsmethode\nPrignitz, Rodolphe\nFinite-Elemente-Methode\nFehlerabsch\u00e4tzung\nAlgorithmus\nFluid\nPartikel\nddc:510\nA novel finite element method for the 3d simulation of (many) particles in a Newtonian carrier liquid is presented. The method features the celebrated one domain approach to simplify the spatial discretization, a newly developed subspace projection method to account for the rigid body motion within the particles and an operator splitting to decouple the nonlinearities. Combined with local mesh refinement the method results in a fast and accurate algorithm which is, in addition conceptually simple to implement. Validation is achieved using the sedimentation of a single particle and comparing the resulting drag coefficient with theoretical and experimental results. Furthermore, a viscometer is considered where the effective viscosity of a particle laden fluid is compared with analytic results. Furthermore, a method for the solution of the Nernst\u2013Planck\u2013Poisson equations is presented. These equations describe the distribution of the concentration of charged substances in a fluid. Validation is carried out by stationary solutions of the equations. Finally, both methods are combined for the simulation of particulate electrodynamic flows.\nDiese Arbeit befasst sich mit einer neuartigen Finite Elemente Methode zur 3d Simulation einer gro\u00dfen Zahl von Partikeln in einem Newtonschen Tr\u00e4gerfluid. Die Methode nutzt den bekannten Ein Gebiet Ansatz zur vereinfachen \u00f6rtlichen Diskretisierung, eine neu entwickelte Unterraumprojektionsmethode (SPM) zur Erzwingung der Festk\u00f6rperbewegung innerhalb der Partikel und ein Operator Splitting zur Entkopplung der Nichtlinearit\u00e4ten. In Kombination mit lokaler Gitteradaptivit\u00e4t resultiert die Methode in einen schnellen, genauen und zudem konzeptionell einfach zu implementierenden Algorithmus. Zur Validierung der Methode wird die Sedimentation eines Einzelpartikels betrachtet. Der daraus resultierende Widerstandsbeiwert wird mit theoretischen und experimentellen Ergebnissen verglichen. Mit der Simulation eines Viskometers wird die effektive Viskosit\u00e4t eines mit vielen Partikeln versetzten Fluids berechnet und ebenfalls mit analytischen Resultaten verglichen. Des weiteren wird eine Methode zur L\u00f6sung der Nernst\u2013Planck\u2013Poisson Gleichungen vorgestellt. Diese dienen der Beschreibung der Konzentrationsverteilung von geladenen Stoffen innerhalb eines Fluids unter externen Potentialdifferenzen. Eine Validierung wird mittels station\u00e4rer Testf\u00e4lle durchgef\u00fchrt. Zum Schluss werden beide Methoden zur Simulation von ionenhaltigen, partikelbehafteten Str\u00f6mungen benutzt.\n2013-08-30\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3608\nurn:nbn:de:bvb:29-opus4-36088\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-36088\nhttps://opus4.kobv.de/opus4-fau/files/3608/RodolphePrignitzDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3616\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35B51\nmsc:35K86\nmsc:46E35\nmsc:49N60\nmsc:54C25\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nExistence and Gradient Estimates in Parabolic Obstacle Problems with Nonstandard Growth\nExistenz und Gradientenabsch\u00e4tzungen f\u00fcr parabolische Hindernisprobleme mit Nichtstandard-Wachstum\nErhardt, Andr\u00e9 H.\nExistenz\nRegularit\u00e4t\nnichtlineare parabolische Differentialgleichungen\nddc:510\nWe consider parabolic obstacle problems with nonstandard growth. Here, we proof the existence of localizable solutions. Moreover, we show the higher integrability of localizable solutions and the Calder\u00f3n-Zygmund theory in parabolic obstacle problems with nonstandard growth.\nWir betrachten parabolische Hindernisprobleme mit nichtstandard Wachstum. Hierf\u00fcr beweisen wir die Existenz von lokalisierbaren L\u00f6sungen. Des Weiteren, zeigen wir die H\u00f6here Integierbarkeit von lokalisierbaren L\u00f6sungen und die Calder\u00f3n-Zygmund Theorie f\u00fcr parabolische Hindernisprobleme mit nichtstandard Wachstum.\n2013-09-02\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3616\nurn:nbn:de:bvb:29-opus4-36161\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-36161\nhttps://opus4.kobv.de/opus4-fau/files/3616/Andr%C3%A9ErhardtDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3749\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:621\npacs\npacs:40.00.00\nmsc\nmsc:78-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nStimulated Raman Scattering in Gas Filled Hollow-Core Photonic Crystal Fibres\nStimulierte Raman-Streuung in mit Gas gef\u00fcllten Hohlkernfasern\nThang, Nguyen\nElektromagnetismus\nOptik\nddc:621\nThe development of hollow-core photonic crystal fibers (HC-PCF) has led to the observation of many interesting phenomena in the field of gas-based nonlinear optics. In particular, studies of gas-based stimulated Raman scattering (SRS) have benefited greatly from the unprecedentedly low Raman threshold offered by low loss HC-PCF. In my talk, I present research results on stimulated Raman scattering (SRS) in forward as well as backward interaction geometries. First, I will overview a two-stage pulse compression scheme in a hydrogen-filled HC-PBC using a backward SRS geometry. Using this technique, a pulse compression factor of 20 times is demonstrated. Moreover, a new dynamical process resulting in a train of Raman pulses with flexibly controllable peak intensities has been observed in transient backward SRS. Next, using a forward SRS geometry, I will show how a broad, phase-coherent, purely rotational Raman frequency comb can be generated in a relatively simple set-up consisting of a microchip pump laser source and two hydrogen-filled HC-PCFs. Finally, I consider the effect of the collision between gas molecules and the fibre core wall on the forward Raman linewidth at low gas pressures.\nIn dieser Arbeit nutze ich die einzigartigen Eigenschaften von photonischen Hohlkernfasern (engl.: hollow-core photonic crystal fibre: HC-PCF), um stimulierte Raman-Streuung (SRS) in gasf\u00f6rmigen Medien zu untersuchen. HC-PCF bieten exzellente M\u00f6glichkeiten wie beispielsweise den Einschluss von Licht und Materie auf engstem Raum und \u00fcber lange Wechselwirkungsl\u00e4ngen im mikrometer-gro\u00dfen Faserkern, sowie geringe Transmissionverluste und einstellbare Transmissionb\u00e4nder. Dies erlaubt uns extrem hohe Raman-Konversionseffizienzen zu erreichen und die optischen Prozesse f\u00fcr die gew\u00fcnschten Frequenzbereiche zu optimieren. Dar\u00fcber hinaus k\u00f6nnen wir SRS-Bereiche erforschen, die auf herk\u00f6mmliche Weise nicht zug\u00e4nglich sind.\nIch werde zun\u00e4chst einen \u00dcberblick \u00fcber die Leitungsmechanismen und die Herstellungsverfahren von photonischen Hohlkernfasern geben. Es gibt haupts\u00e4chlich zwei verschiedene HC-PCF-Typen. Bandl\u00fccken-Hohlkernfasern (engl.: hollow-core photonic bandgap fibre: PBG-PCF) haben sehr geringe Leistungsverluste und leiten in einem schmalen Frequenzband. Hohlkernfasern mit Kagom\u00e9-Gitterstruktur (Kagom\u00e9-PCF) erlauben breitbandige Lichtleitung, allerdings bei h\u00f6heren Verlusten. Es ist bekannt, dass die Licht-Materie-Wechselwirkungseffizienz f\u00fcr nichtlineare Effekte in HC-PCF um mehrere Gr\u00f6\u00dfenordnungen h\u00f6her ist als mit herk\u00f6mmlichen Methoden. Im Anschluss an diese Kapitel wird der theoretische Hintergrund zur SRS im Detail erkl\u00e4rt, ausgehend von sowohl klassischem als auch quantenmechanischem Bild. Dabei werden unter anderem die Maxwell-Bloch-Gleichungen hergeleitet, die die raumzeitliche Ausbreitung der Licht-Gas-Wechselwirkung bei SRS beschreiben.\nZu Anwendungszwecken habe ich durch stimulierte Raman-R\u00fcckstreuung (engl.: backward stimulated Raman scattering: BSRS) zwei aufeinanderfolgende Pulskompressionen in Wasserstoff-gef\u00fcllten PBG-PCF durchgef\u00fchrt. Damit konnte auf effiziente Weise ein Signalpuls erzeugt werden, der zwanzigmal k\u00fcrzer als der Ausgangpuls der Pumpquelle war. Dar\u00fcber hinaus konnte ein neuer dynamischer Prozess bei der transienten BRSR beobachtet werden, welcher einen Raman-Pulszug mit flexibel kontrollierbarer Spitzenintensit\u00e4t erzeugt. Au\u00dferdem gelang es uns einen zugleich breiten und koh\u00e4renten Raman-Frequenzkamm ausschlie\u00dflich mit Hilfe von\nRotations\u00fcberg\u00e4ngen zu erzeugen. Der verh\u00e4ltnism\u00e4\u00dfig einfache Aufbau besteht haupts\u00e4chlich aus einer Mikrochip-Pumplaserquelle und zwei Wasserstoff-gef\u00fcllten HC-PCF. Abschlie\u00dfend befasse ich mich mit der spektralen Linienbreite der stimulierten Raman-Vorw\u00e4rtsstreuung (engl., forward stimulated Raman scattering: FSRS) bei geringem Gasdruck, die ma\u00dfgeblich von Zusammenst\u00f6\u00dfen der Gasmolek\u00fcle mit der Wand des Faserkerns beeinflusst wird.\n2013-09-30\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3749\nurn:nbn:de:bvb:29-opus4-37493\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-37493\nhttps://opus4.kobv.de/opus4-fau/files/3749/Thang%20Thesis.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3763\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:540\npacs\npacs:31.70.Dk\nmsc\nmsc:81-02\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Chemie\nTheoretical Study of Electronic Properties of Carbon Allotropes\nTheoretische Studien der elektronischen Eigenschaften von Kohlenstoff-Allotropen\nDral, Pavlo\nConfiguration Interaction\nAb-initio-Rechnung\nQuantenchemie\nTheoretische organische Chemie\nDichtefunktionalformalismus\nSemiempirische Methode\nPolycyclische Aromaten\nExziton\nFullerene\nElektronentransfer\nReaktionsmechanismus\nPorphyrine\nddc:540\nThis doctoral thesis describes theoretical investigations of the different physicochemical and above all electronic properties of numerous already discovered and yet to be synthesized modern carbon allotropes, their model compounds and derivatives.\nIn the last century it was ascertained that carbon is not only the most important chemical element for the existence of living beings, but is also becoming increasingly more important for electronics and especially in recent decades for molecular nanoelectronics. Its unique ability to form an unlimited number of chemical compounds results in seemingly infinitely many allotropes that have very different properties. Carbon allotropes that are known till now can be classified first of all by the hybridization of orbitals of carbon atoms: sp-carbon can at least theoretically form linear acetylenic carbon, sp2-carbon \u2013 numerous allotropes with graphenic surfaces such as graphite, graphene, carbon nanotubes and fullerenes, sp3-carbon \u2013 diamond. Their properties can be tuned further via chemical functionalization. Smaller model compounds of sp-carbon allotropes such as polyynes and cumulenes, sp2-carbon allotropes as polycyclic aromatic hydrocarbons, sp3-carbon allotropes as diamondoids are also of large interest, because they can be investigated theoretically and experimentally not only easier, but have also themselves remarkable properties. Moreover, the novel allotropes consisting of the combinations of sp-, sp2- and sp3-hybridized carbons as sp-sp2-graphdiyne, sp-sp3-yne-diamond, sp2-sp3-hexagonite and sp-sp2-sp3-carbon built of fullerene balls connected through carbon chains are thinkable and extended segments of some of them were already synthesized.\nCarbon allotropes, their model compounds and derivatives find more and more often application for the nanoelectronics and electronics as elements of transistors, sensors and memory storage devices, for energy conversion as building blocks of solar cells and for energy storage. Therefore, these substances have been investigated very intensively experimentally and theoretically in the last years. The importance of the studies of the carbon allotropes in research and development was rewarded by the Nobel Prizes in Chemistry in 1996 and in Physics in 2012. The former Nobel Prize was awarded to Robert F. Curl, Harold Kroto and Richard E. Smalley for the discovery of fullerenes and the latter one was given to Andre Geim and Konstantin Novoselov \u201efor the fundamental experiments with two-dimensional material graphene\u201d.\nIn the present work diverse electronic properties of carbon allotropes and related systems that are important for nanoelectronics, energy conversion and storage were studied with different ab initio, semiempirical and density functional theory (DFT) quantum chemical methods. Semiempirical configuration interaction (CI) and DFT-based methods were used for describing excited states of the molecular nanosystems based on the above compounds.\nDetailed ab initio and DFT studies of the excited states of the relatively large nanosystems with many more than a hundred atoms is too computationally expensive with the current development of computer techniques and semiempirical CI methods are therefore sometimes the only choice for such systems. Thus, new semiempirical Unrestricted (HF) Natural Orbitals (UNO) \u2013 CI methods were developed in this work, to solve the challenging task to select the correct active orbitals for semiempirical CI. Moreover, UNO\u2013CIS methods have generally better accuracy than conventional CI methods and comparable or better accuracy than DFT. UNO\u2013CI methods were implemented into semiempirical MO-program VAMP.\nThe optical band gaps of the polyyne series related to the sp-carbon allotrope linear acetylenic carbon were studied with semiempirical UNO\u2013CI and CI methods in the present work. It was shown that the theoretical values of the properties studied are in very good agreement with experimentally available values and observations.\nAfterwards, different model compounds of the sp2-carbon allotropes were considered. Optical band gaps of many polycyclic aromatic hydrocarbons (PAHs) were calculated with semiempirical UNO\u2013CI and CI methods and compared with experimental data and time-dependent (TD) DFT calculations. Next, inclusion energies of heteroatoms and some groups into the interior of PAHs were calculated with the DFT methods. The influence of such doping on such electronic properties as spin state, diradical character, electron affinities (EAs), ionization potentials (IPs), different types of band gaps, exciton binding energy and aromaticity was examined at the semiempirical and DFT levels. What\u2019s more, exceptional properties of the unusual radical ion pair NH4(+)@C60(\u2022-) were theoretically studied, its possible synthesis suggested and the corresponding reaction steps including intermediate endofullerenes potentially interesting for spintronics were calculated. Photoinduced electron transfer (PIET) in systems involving model systems of sp2-carbon allotropes including fullerene C60 and doped PAHs important for energy conversion applications was studied by DFT and semiempirical CI and UNO\u2013CI methods.\nFurthermore, electron transfer processes between electron donating diamondoids including adamantane and oxadiamondoids that are substructures of undoped and oxygen-doped sp3-carbon allotrope diamond, respectively, and electron accepting nitronium-containing compounds were studied. This study explained the experimentally observed reactivity of diamondoids and the distribution of the products of the corresponding reactions.\nFinally, sp2 carbon allotropes with graphenic surfaces are suggested as the plausible candidates for the hydrogen storage that is important for the environmentally friendly energy storage technology. First, careful calibration of the second order M\u00f8ller\u2013Plesset perturbation theory (MP2), DFT and semiempirical methods was performed to find the most accurate methods able to reproduce experimentally observed change of fullerene electron affinity under hydrogenation. Second, the DFT and semiempirical methods confirmed that experimentally observed 1,9-dihydro[60]fullerene is the most stable isomer among 23 possible regioisomers of C60H2 and the study of the influence of electron doping on the hydrogenation of fullerenes with the same methods explained the decomposition of C60H2 under electron reduction. Third, the importance of choosing a DFT functional that describes binding extra electrons correctly in highly negatively charged fullerene derivatives for predicting relative stabilities of the latter species was demonstrated.\nThe close cooperation with experimental studies within the present doctoral thesis proved the effectiveness and even synergic effect of theoretical studies on research and development of the modern applications for nanoelectronics, energy conversion and storage based on carbon allotropes and related systems.\nIn summary, the theoretical methods used and developed can explain the experimentally observed properties very well and have been applied for the prediction of the properties of the unknown compounds.\nIn der vorliegenden Doktorarbeit wird die theoretische Untersuchung der verschiedenen physikalisch-chemischen und vor allem elektronischen Eigenschaften von zahlreichen bereits entdeckten und noch zu synthetisierenden neuartigen Kohlenstoff-Allotropen, deren Modelverbindungen und Derivate dargestellt.\nIm letzten Jahrhundert wurde festgestellt, dass Kohlenstoff nicht nur das wichtigste chemische Element f\u00fcr die Existenz von Lebewesen ist, sondern auch zunehmend wichtiger f\u00fcr Elektronik und besonders in letzten Jahrzehnten f\u00fcr molekulare Nanoelektronik wird. Seine einzigartige F\u00e4higkeit, unbegrenzte Mengen chemischer Verbindungen zu bilden, f\u00fchrt auch dazu, dass es auch scheinbar unendlich viel Allotropen mit sehr unterschiedlichen Eigenschaften hat. Die bis jetzt bekannten Kohlenstoff-Allotropen k\u00f6nnen vor allem nach Hybridisierung der Orbitalen ihrer Kohlenstoffatome klassifiziert werden: sp-Kohlenstoff kann zumindest theoretisch linearen azetylenischen Kohlenstoff bilden, sp2-Kohlenstoff \u2013zahlreiche Allotropen mit graphenischen Oberfl\u00e4chen wie Graphit, Graphen, Kohlenstoffnanor\u00f6hre und Fullerene, sp3-Kohlenstoff \u2013 Diamant. Ihre Eigenschaften k\u00f6nnen weiter durch chemische Funktionalisierung gesteuert werden. Kleinere Modelverbindungen von sp-Kohlenstoff-Allotropen wie Polyine und Kumulene, sp2-Kohlenstoff wie polyzyklische aromatische Kohlenwasserstoffe, sp3-Kohlenstoff wie Diamantoide sind auch von gro\u00dfem Interesse, weil sie nicht nur oft einfacher theoretisch und experimental untersucht werden k\u00f6nnen, sondern auch selbst bemerkenswerte Eigenschaften haben. Au\u00dferdem sind die neuartige Kohlenstoff-Allotropen, die aus der Kombination von sp-, sp2- und sp3-hybridisierten Kohlenstoffen zusammengesetzt sind, wie sp-sp2-Graphdiin, sp-sp3-in-Diamant, sp2-sp3-Hexagonit und sp-sp2-sp3-Kohlenstoffe, die aus mit Kohlenstoffketten verbundenen Fullerenkugeln bestehen, denkbar und erweiterte Ausschnitte von einigen davon wurden bereits synthetisiert.\nKohlenstoff-Allotropen, ihre Modelverbindungen und Derivaten finden immer h\u00e4ufiger Anwendung f\u00fcr Nanoelektronik und Elektronik, z. B. bei Bestandteilen von Transistoren, Sensoren und Speicherger\u00e4ten, f\u00fcr Energiewandlung, wie es bei Bestandteilen von Solarzellen zu finden ist und f\u00fcr Energiespeicherung. Dementsprechend werden diese Substanzen in den letzten Jahren sehr intensiv experimental und theoretisch untersucht. Die Bedeutung der Studien von Kohlenstoff-Allotropen in Forschung und Entwicklung wurde mit den Nobelpreisen f\u00fcr Chemie im Jahre 1996 und f\u00fcr Physik im Jahre 2010 ausgezeichnet. Der erste Nobelpreis wurde Robert F. Curl, Harold Kroto und Richard E. Smalley f\u00fcr die Entdeckung der Fullerene verliehen und der zweite wurde an Andre Geim und Konstantin Novoselov \u201ef\u00fcr grundlegende Experimente mit dem zweidimensionalen Material Graphen\u201c vergeben.\nIn dieser Arbeit werden Kohlenstoff-Allotropen und deren verwandten Verbindungen auf ihre wichtigen Eigenschaften f\u00fcr die Nanoelektronik bzw. Energiewandlung und -speicherung mit verschiedenen quantenchemischen Methoden wie ab initio und semiempirische sowie Dichtefunctionaltheorie (DFT) Verfahren untersucht. Semiempirische Konfigurations-wechselwirkungsmethoden (Configuration Interaction, CI) und DFT-Methoden werden verwendet, um die angeregten Zust\u00e4nde von molekularen Nanosystemen, die auf die oben genannten Verbindungen basiert sind, zu beschreiben.\nDetaillierte ab initio- und DFT-Studien der angeregten Zust\u00e4nde von relativ gro\u00dfen molekularen Nanosystemen mit weit \u00fcber hundert Atomen sind mit der heutigen Entwicklung der Computertechnik zu rechenintensiv und deshalb sind semiempirische CI-Methoden (Configuration Interaction, CI) manchmal die einzige Wahl f\u00fcr solche Systeme. Demzufolge wurden neue semiempirische Unrestricted (HF) Natural Orbitals (UNO) \u2013 CI-Methoden entwickelt, die die anspruchsvolle Aufgabe der Auswahl der richtigen aktiven Orbitale f\u00fcr CI l\u00f6sen. Dar\u00fcber hinaus liefern UNO\u2013CI-Methoden in der Regel h\u00f6here Genauigkeit als die konventionellen CI-Methoden und vergleichbare oder h\u00f6here Genauigkeit als DFT. UNO\u2013CI-Methoden wurden in das semiempirische MO-Programm VAMP implementiert.\nDanach wurden in der vorliegenden Arbeit die optischen Bandl\u00fccken von der homologen Reihe der Polyine, die mit linearem azetylenischem Kohlenstoff (sp-Kohlenstoff-Allotrop) verwandt sind, mit semiempirischen UNO\u2013CI- und CI-Methoden untersucht. Die theoretischen Werte der studierten Eigenschaften stimmen sich sehr gut mit experimentell verf\u00fcgbaren Werten und Beobachtungen \u00fcberein.\nAnschlie\u00dfend wurden verschiedene Modelverbindungen der sp2-Kohlenstoff-Allotropen betrachtet. So wurden die optischen Bandl\u00fccken von vielen polyzyklischen aromatischen Kohlenwasserstoffen (polycyclic aromatic hydrocarbons, PAHs) mit DFT-, semiempirischen UNO\u2013CI- und CI-Methoden berechnet und sowohl mit experimentalen Werten als auch mit DFT-Berechnungen verglichen. Dann wurden die Energien der Versetzung von Heteroatomen und einigen Gruppen ins Innere von PAHs mit DFT-Methoden berechnet. Die Auswirkung einer solchen Dotierung auf die elektronischen Eigenschaften, wie die der Spin-Zust\u00e4nde, der diradikalischen Charaktere, der Elektronenaffinit\u00e4ten (EA), der Ionisierungspotentialen (IP), der verschiedenen Arten von Bandl\u00fccken, der Excitonbindungsenergien und der Aromatizit\u00e4t der PAHs, wurde mit semiempirischen und DFT-Methoden erforscht. Dazu wurden die besonderen Eigenschaften des ungew\u00f6hnlichen radikalischen Ionenpaar NH4(+)@C60(\u2022-) theoretisch untersucht, seine m\u00f6gliche Synthese vorgeschlagen und entsprechende Reaktionsschritte, die die potentiell f\u00fcr Spintronik interessanten offen-schaligen Endofullerene wie Intermediate einschlie\u00dfen, berechnet. Der f\u00fcr die Energiewandlungsanwendungen wichtige photoinduzierte Elektronentransfer (PIET) in den aus Modelsystemen von sp2-Kohlenstoff-Allotropen bestehenden Systemen (Fulleren C60 und dotierte PAHs einschlie\u00dfend) wurde mit DFT und semiempirischen CI- und UNO\u2013CI-Methoden untersucht.\nAu\u00dferdem wurden die Elektronentransferprozesse zwischen Elektronen gebenden Diamantoiden einschlie\u00dflich Adamantan und Oxadiamondoiden, welche die Substrukturen von undotiertem bzw. sauerstoffdotiertem sp3-Kohlenstoff-Allotrop Diamant darstellen, und dem Elektronen akzeptierenden nitroniumhaltigen Verbindungen erforscht. Dabei wurde die experimentell beobachtete Reaktivit\u00e4t der Diamantoiden sowie die Verteilung der Produkte entsprechender Reaktionen erkl\u00e4rt.\nSchlie\u00dflich werden Kohlenstoff-Allotropen mit graphenishen Oberfl\u00e4chen als viel versprechende Kandidaten f\u00fcr die Wasserstoffspeicherung, die wichtig f\u00fcr die umweltfreundliche Energiespeichertechnik ist, vorgeschlagen. Erstens wurden die M\u00f8ller\u2013Plesset St\u00f6rungstheorie zweiter Ordnung (MP2), DFT- und semiempirischen Methoden sorgf\u00e4ltig kalibriert, um die genauesten Methoden zu finden, die die experimentell beobachtete \u00c4nderung der Elektronenaffinit\u00e4t von Fulleren unter Hydrierung reproduzieren k\u00f6nnen. Zweitens best\u00e4tigte die Studie der Auswirkung der Elektronendotierung auf die Hydrierung von Fullerenen mit DFT und semiempirischen Methoden, dass das experimentell beobachtete 1,9-Dihydro[60]fulleren das stabilste Isomer unter 23 m\u00f6glichen Regioisomeren von C60H2 ist, und erkl\u00e4rte die Zersetzung von C60H2 unter Elektronenreduktion. Drittens wurde die Wichtigkeit der Wahl von DFT-Funktional f\u00fcr die korrekte Beschreibung der Bindung von Extraelektronen in hoch negativ geladenen Fullerenederivaten dargelegt, um die relativen Stabilit\u00e4ten letztgenannten Verbindungen zuverl\u00e4ssig vorauszusagen.\nDie enge Zusammenarbeit mit experimentellen Untersuchungen im Rahmen vorliegender Doktorarbeit zeigte die Effektivit\u00e4t und sogar synergetische Effekte der theoretischen Studien f\u00fcr die Forschung und Entwicklung von auf Kohlenstoff-Allotropen und deren verwandten Verbindungen basierten neuartigen Anwendungen f\u00fcr Nanoelektronik, Energiewandlung und -speicherung.\nZusammenfassend l\u00e4sst sich sagen, dass die verwendeten und entwickelten theoretischen Methoden die experimentell beobachteten Eigenschaften sehr gut erkl\u00e4ren k\u00f6nnen sowie f\u00fcr die Vorhersage der Eigenschaften von unbekannten Verbindungen verwendet werden k\u00f6nnen.\n2013-05-10\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3763\nurn:nbn:de:bvb:29-opus4-37630\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-37630\nhttps://opus4.kobv.de/opus4-fau/files/3763/PavloDralDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3992\n2021-11-19\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nmsc\nmsc:74B20\nmsc:74G15\nmsc:74S05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nEntwicklung und Untersuchung polytoper Finiter Elemente f\u00fcr die nichtlineare Kontinuumsmechanik\nDevelopment and Investigation of polytope finite elements for nonlinear continuum mechanics\nKraus, Markus\nPolytop\nPolygon\nPolyeder\nNichtlineare Kontinuumsmechanik\nFinite-Elemente-Methode\nDiskretisierung\nQualit\u00e4tsma\u00df\nInterpolation\nddc:620\nF\u00fcr die Approximation der L\u00f6sung partieller Differentialgleichung hat sich in den letzten Jahrzehnten die Methode der Finiten Elemente als eines der wichtigsten Verfahren etabliert. Die dabei verwendeten Elementformen beschr\u00e4nken sich auf wenige einfache Geometrieprimitive, f\u00fcr die eine Vielzahl effizienter und leistungsf\u00e4higer Elementformulierungen im Gebiet der nichtlinearen Kontinuumsmechanik entwickelt wurden. Die Gruppe der polytopen Finite Elemente umfasst alle polygonalen und polyedrischen Elementformen und stellt eine Erweiterung der bisherigen Elemente auf eine nahezu beliebige Elementgestalt dar. Jenseits der reinen Geometrieapproximation oder einer positiven Beeinflussung der numerischen Simulationsergebnisse k\u00f6nnen diese Elemente auch zus\u00e4tzliche Werkstoffinformationen, wie element- bzw. gef\u00fcgekornspezifische Materialmodelle, unmittelbar ber\u00fccksichtigen.\nAnhand der Grundlagen der nichtlinearen Kontinuumsmechanik und der Methode der Finiten Elemente werden wesentliche Entwicklungsziele identifiziert. Die Diskretisierung eines Bauteils kann neben einer Rekonstruktion realer Gef\u00fcge oder einer deterministischen Vernetzung auch durch stochastische Vernetzungsverfahren realisiert werden. Zur Bewertung der Diskretisierungen werden Qualit\u00e4tsma\u00dfe definiert und untersucht. Die Interpolation in polygonalen Gebieten kann durch existierende zweidimensionale Verfahren durchgef\u00fchrt werden, f\u00fcr komplexe dreidimensionale Elemente wird ein allgemeines Interpolationsverfahren entwickelt. Die Verfahren werden hinsichtlich ihrer Leistungsf\u00e4higkeit und Rechenzeiten untersucht. Zur Integration der diskretisierten Grundgleichungen kann f\u00fcr beliebige dreidimensionale Polyeder wegen fehlender Referenzkonfigurationen, auf denen optimierte und effiziente Quadraturverfahren existieren oder ermittelt werden k\u00f6nnten, nur die Unterteilung des Gebiets in polytope Integrationszellen mit der Projektion bekannter Quadraturpunkte universell angewendet werden.\nInsgesamt werden diese Methoden f\u00fcr die Entwicklung polytoper Elementformulierungen verkn\u00fcpft. Dabei werden bekannte Ans\u00e4tze und Verfahren auf allgemeine polytope Elementformen adaptiert und erweitert. Die Funktionsf\u00e4higkeit und das Potential der Elementformulierungen werden durch numerische Untersuchungen und anhand verschiedener Anwendungsbeispiele der Kontinuumsmechanik demonstriert. Die Formulierung mit einem erweiterten Verzerrungsansatz zeigt im Rahmen der durchgef\u00fchrten Untersuchungen hervorragende mechanische Systemantworten.\nIn the last decades, the finite element method has become one of the most important techniques for the approximation of the solution of partial differential equations. Within this analysis, the usually used element shapes are limited to a few geometric primitives, for which plenty of efficient and capable element formulations have been developed for nonlinear continuum mechanics. The group of polytope finite elements comprises all polygonal and polyhedral element shapes and represents a significant extension of existing formulations to almost arbitrary element configurations. Beyond sole approximation of the part geometry or the positive influence on the numerical results, these elements may incorporate additional material information, e.g. by assigning element or grain specific material models.\nBased on the foundations of nonlinear continuum mechanics and the finite element method substantial development milestones are defined. The discretization of a device can - besides an extensive reconstruction of real microstructures or a deterministic meshing - also be realized for three-dimensional parts by an automatic and stochastic mesh generation process. For the evaluation of the discretizations, quality measures are defined and examined. The interpolation inside polytope domains can be realized by existing two-dimensional procedures and by a newly established general interpolation scheme that is also suitable for complex three-dimensional elements. All interpolation schemes are examined with respect to their capability and computational costs. Due to missing generic element domains, in which optimized and efficient quadrature schemes exist or may be developed, only a domain decomposition into polytope quadrature cells with projection of known quadrature schemes opens an universal access for solving the fundamental equations.\nThese methods are combined for the development of polytope element formulations, where known approaches and ideas are adapted and extended for general polytope element shapes. The functionality and potential of the proposed element formulations are demonstrated by numerical studies and different applications. The formulation with an enhanced assumed strain approach provide outstanding mechanical system responses within the scope of the considered experiments.\n2013-12-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3992\nurn:nbn:de:bvb:29-opus4-39923\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-39923\nhttps://opus4.kobv.de/opus4-fau/files/3992/MarkusKrausDissertation.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-sa/3.0/de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4039\n2020-09-21\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nccs\nccs:G.\npacs\npacs:78.20.-e\nmsc\nmsc:35L65\nmsc:35Q70\nmsc:35R09\nmsc:74S10\nmsc:93C20\nopen_access\nopen_access:open_access\ncollections\ncollections:FAUUP\ninstitutes\ninstitutes:Nat_Mathematik\nOptimization of Particle Synthesis - New Mathematical Concepts for a Controlled Production of Functional Nanoparticles\nOptimierung in der Partikelsynthese - Neue mathematische Konzepte f\u00fcr eine gezielte Herstellung funktionaler Nanopartikel\nGr\u00f6schel, Michael\nOptimale Kontrolle\nNichtlineare Kontrolltheorie\nNichtlineare Optimierung\nNumerische Mathematik\nMathematische Modellierung\nNanopartikel\nProzessoptimierung\nParameteridentifikation\nTeilchentechnologie\nSteife nichtlineare Differentialgleichung\nBilineares System\nAdjungierte Differentialgleichung\nFinite-Volumen-Methode\nVerteilungsfunktion\nGlattheit Mathematik\nddc:510\nEmbedded in an interdisciplinary research project, the present work investigates the modeling, simulation, and optimization of specific processes for the production of functional nanoparticles. The examined material systems represent a selection of important core building blocks for future nanotechnologies. Depending on the application, a well-defined particle size distribution of the final product is required. The involved mechanisms (e.g. reaction, growth, ripening, and agglomeration) are described in the modeling by a hyperbolic partial integro-differential equation coupled to one or more ordinary differential equations. The successful validation against experimental data combined with the techniques of optimal control theory thereby allow for a targeted manipulation of the underlying processes. The developed methods establish in many cases for the first time a systematic approach for the production of tailor-made nanoparticles.\n2013-12-09\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4039\nurn:nbn:de:bvb:29-opus4-40395\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-40395\n978-3-944057-12-5\nhttps://opus4.kobv.de/opus4-fau/files/4039/DissertationMichaelGroeschel.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4100\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:530\npacs\npacs:04.50.-h\npacs:04.50.Gh\npacs:04.60.-m\npacs:45.20.Jj\nmsc\nmsc:83C05\nmsc:83E15\nmsc:83E50\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Physik\nHigher dimensional and supersymmetric extensions of loop quantum gravity\nH\u00f6herdimensionale und supersymmetrische Erweiterungen der Schleifenquantengravitation\nThurn, Andreas\nQuantengravitation\nSupergravitation\nddc:530\nIn this work, we extend loop quantum gravity (LQG) both, to higher dimensions and supersymmetry (i.e. supergravity theories), thus overcoming the current limitation to 3+1 dimensions with standard model matter fields. On the one hand, this gives a proof of principle that LQG is in accordance with these two theoretical concepts, and on the other hand hopefully allows contact with superstring/M - theory, which necessarily is supersymmetric and formulated in ten or eleven spacetime dimensions. Symmetry arguments suggest that supergravity theories in the corresponding dimensions constitute the low energy effective field theory limit of superstring/M - theory. This makes a study of the loop quantisation thereof, which we start here, a promising endeavour at the border between the two approaches.\nIn more detail, our findings are the following: firstly, a new canonical formulation for general relativity in D + 1 spacetime dimensions (D \u2265 2) on a Yang Mills theory phase space is presented for the first time, with the core properties that 1. the canonical variables encoding the metric information are a real connection and its real conjugate momentum, in particular satisfying the standard canonical Poisson bracket relations, 2. the gauge group can be chosen to be a compact group (namely SO(D + 1)) for both, Lorentzian and Euclidean signature spacetimes, and 3. the system of constraints is first class (in Dirac\u2019s terminology). Up to now, such a formulation was only known for D = 3 (and D = 2), corresponding to Ashtekar Barbero variables, constituting the classical foundation of the loop quantisation programme.\nThe quantisation procedure itself is formulated almost independently of the number of spacetime dimensions and the choice of compact gauge group, and therefore the lack of higher dimensional analogues of LQG only was caused by the missing classical canonical formulation satisfying 1. - 3. Thus it is not surprising and we show explicitly that the new formulation we present can be quantised using the methods developed in the loop community straightforwardly to obtain LQG theories in higher dimensions.\nThe formulation which we present is genuinely new in that it does not reduce to the Ashtekar Barbero formulation for D = 3, and furthermore for D > 2 comes with an additional set of constraints, the so called simplicity constraints, which pose the only conceptually new challenge when quantising. Interestingly, these constraints are not at all unknown in (quantum) gravity research, and in particular are a standard ingredient in the covariant approach to LQG called spin foam models. The formulation in this sense builds a novel bridge between the covariant and canonical approaches to LQG. The quantum anomalies known for this constraint from spin foams are recovered, which lead to problems when implementing it at the quantum level. We present some new proposals of how to deal with these problems.\nIn the second part of this work, we give an extension of the above framework to the loop quantisation of a large class of Lorentzian signature supergravities, including in particular the D + 1 = 4 N = 8, D + 1 = 11 N = 1 and D + 1 = 10 N = 1 theories. Concretely, we incorporate standard and also non-standard matter fields, which appear in supergravity theories due to the requirement of supersymmetry, into the afore developed framework of higher dimensional LQG.\nCoupling to standard model matter fields has already been achieved in usual LQG and the results obtained there carry over to the case at hand. The only exception is the treatment of Dirac fermions, which needs slight adjustment: coming from an action principle, the Dirac field transforms in the spinor representation of the gauge group SO(1, D) for the physically relevant Lorentzian theory, but due to the strong similarity of the Lorentzian and the Euclidean Clifford algebras, the gauge group can be exchanged for SO(D + 1) to fit in with the gravitational degrees of freedom.\nTypical non-standard fields appearing in supergravity theories are the spin 3/2 Rarita Schwinger field (\u201cgravitino\u201d) on the fermionic side, and (Abelian) higher p-form fields as novel bosonic fields (i.e. generalisations of the Maxwell field to higher form degree).\nThe former usually is a Majorana fermion (i.e. it is its own antiparticle) and therefore belongs to a real representation space of SO(1, D). In order to formulate supergravities in terms of SO(D + 1) gauge theories, we again have to exchange the gauge group SO(1, D) with SO(D + 1), but there is no action of SO(D + 1) on these real representation spaces, which hugely complicates the passage when compared to the case of Dirac fermions. We present a solution to this problem and for the first time, to the best of the author\u2019s knowledge, provide a background independent Hilbert space representation for the gravitino field.\nConcerning novel bosonic fields, we exemplarily treat the three-form field (\u201cthree index photon\u201d) of D + 1 = 11 N = 1 supergravity. Due to an additional Chern Simons term in the action, this field is not a simple generalisation of the Maxwell field to three-forms, but actually becomes self interacting and the equivalent of the electric field is not gauge invariant. We propose a reduced phase space quantisation with respect to the equivalent of the Gau\u00df constraint, and the background independent representation we use is given by a state of Narnhofer-Thirring type, which already has been used in the loop literature in Thiemann\u2019s treatment of the closed bosonic string.\nIn the third part of this work, as a first application of the new variables, we extend the isolated horizon treatment (a quasi-local notion of black holes) in LQG to higher dimensions. In D = 3, the use of Ashtekar Barbero variables induces a Chern Simons theory on the horizon and the quantisation thereof and subsequent state counting led to the derivation of the famous Bekenstein Hawking entropy formula for black holes from LQG. Here, we study (non-distorted) isolated horizons in 2(n + 1) dimensional spacetimes and find that using the new variables induces an SO(2(n+1)) Chern Simons theory thereon. Since this theory, unlike its D = 3 counterpart, has local degrees of freedom, the quantisation and finally rederivation of the entropy formula become significantly more intricate and are left for further research.\nWe want to stress that several aspects of both, the higher dimensional as well as the supersymmetric extension, definitely deserve further study to actually catch up with the current status of usual canonical LQG. In the non-supersymmetric case, this concerns mainly the implementation of the simplicity constraint and its interplay with the dynamics. In the supersymmetric case, of course the supersymmetry constraint needs intensive study, in particular its role in the quantum super Dirac algebra. We hope that the generalisation of LQG to higher dimensions and supersymmetry achieved in this work will spark further development to clarify the mentioned open problems and finally lead to new interrelations between LQG and superstring/M - theory.\nIn dieser Arbeit verallgemeinern wir Schleifenquantengravitation (LQG) sowohl auf h\u00f6here Dimensionen als auch auf Supersymmetrie (d.h. Supergravitationstheorien). Damit wird die bestehende Limitation der LQG auf 3+1 Dimensionen und Materiefelder des Standardmodells aufgehoben. Dies beweist einerseits, dass LQG prinzipiell mit diesen beiden theoretischen Konzepten in Einklang gebracht werden kann. Andererseits weckt es Hoffnung, dass neue Ankn\u00fcpfungspunkte zu Superstring- und M - Theorie erm\u00f6glicht werden, da diese Theorien notwendigerweise supersymmetrisch sind und in zehn beziehungsweise elf Raumzeitdimensionen formuliert werden m\u00fcssen. Symmetrieargumente legen nahe, dass sich diese Theorien im Niederenergielimes effektiv durch Supergravitationstheorien in eben diesen Dimensionen beschreiben lassen. Die Untersuchung der Schleifenquantisierung der entsprechenden Supergravitationstheorien, mit der wir in dieser Arbeit beginnen, stellt daher ein vielversprechendes Unterfangen an der Grenze zwischen den beiden Ans\u00e4tzen dar.\nPr\u00e4ziser formuliert lauten unsere Ergebnisse wie folgt: Wir pr\u00e4sentieren erstmalig eine kanonische Formulierung der allgemeinen Relativit\u00e4tstheorie in D + 1 Raumzeitdimensionen (D \u2265 2) auf einem Yang Mills Phasenraum mit den zentralen Eigenschaften: 1. Die kanonischen Variablen, die die metrische Information tragen, sind ein reeller Zusammenhang und ein dazu konjugierter reeller Impuls, die insbesondere die kanonischen Poissonklammerrelationen erf\u00fcllen. 2. Als Eichgruppe kann sowohl f\u00fcr die lorentzsche als auch die euklidische Theorie eine kompakte Gruppe gew\u00e4hlt werden (in unserem Fall SO(D + 1)). 3. Die Zwangsbedingungen sind alle von erster Klasse (in Diracs Terminologie). Eine solche Formulierung war bisher nur in drei und vier Dimensionen bekannt, die Ashtekar Barbero Formulierung, welche die klassische Basis f\u00fcr LQG darstellt.\nDas Programm der Schleifenquantisierung selbst ist fast g\u00e4nzlich unabh\u00e4ngig von der Anzahl der Raumzeitdimensionen und der Wahl der kompakten Eichgruppe formuliert, weswegen das Fehlen von h\u00f6herdimensionalen Analoga der LQG alleine dem Nichtvorhandensein der klassischen kanonischen Formulierung zuzuschreiben ist, welche die obigen Anforderungen 1. - 3. erf\u00fcllt. Darum ist es nicht verwunderlich, dass die Methoden der Schleifenquantisierung direkt auf die hier pr\u00e4sentierte Formulierung anwendbar sind, um h\u00f6herdimensionale Schleifenquantengravitationstheorien zu erhalten. Dies arbeiten wir explizit aus.\nDie Formulierung, die wir pr\u00e4sentieren, ist insofern wirklich neu, als dass sie sich f\u00fcr die Wahl D = 3 nicht auf die bekannte Ashtekar Barbero Formulierung reduziert. Stattdessen finden wir f\u00fcr D > 2 eine zus\u00e4tzliche Zwangsbedingung, die sogenannte \u201dSimplicity\u201c Zwangsbedingung, die die einzige konzeptionell neue Herausforderung bei der Quantisierung darstellt. Diese Zwangsbedingung ist interessanterweise keineswegs unbekannt in der (Quanten-) Gravitationsforschung und taucht insbesondere generell in Spinschaummodellen auf, die auch kovarianter Ansatz zur LQG genannt werden. In diesem Sinne stellt unsere Formulierung eine neue Verbindung zwischen kovarianter und kanonischer LQG her. F\u00fcr diese Zwangsbedingung treten Quantenanomalien auf, die schon von den Spinschaummodellen her bekannt sind und die zu Problemen bei der Implementierung der Zwangsbedingung auf Quantenebene f\u00fchren. Wir stellen einige neue L\u00f6sungsans\u00e4tze hierf\u00fcr vor.\nDen zweiten Teil dieser Arbeit stellt die Erweiterung des obigen Rahmenwerks auf die Schleifenquatisierung einer ganzen Klasse von lorentzschen Supergravitationstheorien dar, die insbesondere die D + 1 = 4 N = 8, die D+1=11 N =1 und die D+1=10 N =1 Theorien umfasst. Konkreter untersuchen wir dazu die Kopplung von Standard- und au\u00dfergew\u00f6hnlichen Materiefeldern an die bis dahin untersuchte Vakuumgravitationstheorie, die in Supergravitationstheorien wegen den Anforderungen der Supersymmetrie vorkommen.\nDie Kopplung von Standardmaterie wurde f\u00fcr die LQG bereits erforscht und die Ergebnisse aus der vierdimensionalen Theorie sind auch auf die neue Formulierung anwendbar. Die einzige Ausnahme bilden Diracfermionen, bei denen nachgebessert werden muss: Ausgehend von einer Wirkung transformieren sie in der Spinordarstellung der Eichgruppe SO(1, D), aber wegen der starken \u00c4hnlichkeit der lorentzschen und euklidischen Clifford Algebren kann die Eichgruppe gegen SO(D + 1) getauscht werden. Das Diracfeld f\u00fcgt sich so in die Behandlung des gravitativen Anteils der Theorie ein.\nBez\u00fcglich der au\u00dfergew\u00f6hnlichen Materiefelder tritt in Supergravitationstheorien im fermionischen Sektor typischerweise das Spin 3/2 Rarita Schwinger Feld (\u201dGravitino\u201c) auf und auf bosonischer Seite sind h\u00f6here p-Form Felder (d.h. Verallgemeinerungen des Maxwellfeldes auf h\u00f6here Formgrade) zu finden.\nErsteres ist normalerweise ein Majoranafermion (d.h. es ist sein eigenes Antiteilchen) und geh\u00f6rt damit zu einem reellen Darstellungsraum der SO(1, D). Um nun auch Supergravitationstheorien als SO(D + 1) Eichtheorien zu formulieren, muss die Eichgruppe SO(1, D) erneut gegen SO(D + 1) getauscht werden. Aber auf den reellen Darstellungsr\u00e4umen existiert keine Wirkung der Gruppe SO(D + 1), was den Eichgruppenwechsel verglichen mit dem Fall des Diracfeldes enorm erschwert. Wir finden eine L\u00f6sung f\u00fcr dieses Problem und konstruieren, nach bestem Wissen des Autors erstmalig, eine hintergrundunabh\u00e4ngige Hilbertraumdarstellung f\u00fcr das Gravitino.\nAls Beispiel f\u00fcr die neuartigen bosonischen Felder betrachten wir das Dreiformfeld (\u201dDreiindexphoton\u201c) der D + 1 = 11 N = 1 Supergravitation. Dieses Feld stellt keine triviale Erweiterung des Maxwellfeldes auf Dreiformen dar, da es wegen eines zus\u00e4tzlichen Chern Simons Terms in der Wirkung selbstwechselwirkend ist. Das f\u00fchrt unter anderem auch dazu, dass das \u00c4quivalent des elektrischen Feldes nicht eichinvariant ist. Wir f\u00fchren eine Quantisierung des bez\u00fcglich des Pendants der Gau\u00df Zwangsbedingung reduzierten Phasenraumes durch. Eine hintergrundunabh\u00e4ngige Darstellung erhalten wir durch Verwendung eines Zustandes vom Narnhofer-Thirring Typ, wie er in der Literatur zur Schleifenquantisierung bereits von Thiemann in seiner Behandlung des geschlossenen bosonischen Strings benutzt wurde.\nIm dritten Teil der Arbeit erweitern wir schlie\u00dflich als erste Anwendung der neuen Variablen die Behandlung von isolierten Horizonten (einer quasi- lokalen Beschreibung schwarzer L\u00f6cher) in der LQG auf h\u00f6here Dimensionen. In vier Raumzeitdimensionen induziert der Gebrauch der Ashtekar Barbero Variablen eine Chern Simons Theorie auf dem Horizont. Eine Quantisierung der entsprechenden Horizontfreiheitsgrade und anschlie\u00dfendes Z\u00e4hlen der Mikrozust\u00e4nde f\u00fchrte zur Herleitung von Bekensteins und Hawkings ber\u00fchmter Entropieformel f\u00fcr schwarze L\u00f6cher innerhalb der LQG. In dieser Arbeit untersuchen wir (nicht-deformierte) isolierte Horizonte in 2(n+1)-dimensionalen Raumzeiten und finden, dass aus dem Gebrauch der neuen Variablen eine SO(D+1) Chern Simons Theorie auf dem Horizont resultiert. Diese hat jedoch, im Gegensatz zu ihrem dreidimensionalen Gegenst\u00fcck, lokale Freiheitsgrade, was die Quantisierung und Herleitung der Entropieformel signifikant erschwert. Beide Punkte m\u00fcssen in zuk\u00fcnftiger Forschungsarbeit weiter untersucht werden.\nEs ist zu betonen, dass einige Aspekte sowohl von der h\u00f6herdimensionalen als auch von der supersymmetrischen Erweiterung weiterer Forschung bed\u00fcrfen, um den gleichen Stand wie die aktuelle vierdimensionale LQG zu erreichen. Im nicht supersymmetrischen Fall betrifft dies haupts\u00e4chlich die Implementierung der Simplicity-Zwangsbedingung und sein Zusammenspiel mit der Dynamik. Im supersymmetrischen Fall werfen vor allem die Supersymmetrie Zwangsbedingung und insbesondere ihre Rolle in der Quanten-Super-Diracalgebra neue Fragen auf. Wir hoffen, dass die in dieser Arbeit erzielte Verallgemeinerung der LQG auf h\u00f6here Dimensionen und Supersymmetrie weitere Forschung zur Kl\u00e4rung dieser Fragen anregt und schlie\u00dflich zu neuen Ankn\u00fcpfungspunkten zwischen LQG und Superstring/M - Theorie f\u00fchrt.\n2014-01-09\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4100\nurn:nbn:de:bvb:29-opus4-41005\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-41005\nhttps://opus4.kobv.de/opus4-fau/files/4100/AndreasThurnDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4118\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:512\nmsc\nmsc:17-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nThe Jantzen sum formula at the critical level\nDia Jantzen-Summenformel im kritischen Level\nK\u00fcbel, Johannes\nKac-Moody-Algebra\nDarstellungstheorie\nddc:512\nThe present thesis deals with the representation theory of a symmetrizable Kac-Moody algebra. We restrict ourselves to the highest weight modules whose structure is given by the BGG category O. In the case of an affine Kac-Moody algebra, one distinguishes between representations of positive, negative and critical level. An important role for the calculation of characters of simple highest weight modules is played by the Verma modules in positive and negative level. At the critical level, it is helpful to introduce the restricted category O, a certain subcategory of the usual category O. The usual Verma modules are then replaced by restricted Verma modules that appear as maximal restricted quotients of aforementioned Verma modules.\nThe Jantzen filtration provides a good tool for studying simple subquotients of a Verma module. In case of a Verma module in positive level, we show that the structure of subquotients of this filtration is given by the coefficients of certain Kazhdan-Lusztig polynomials, by associating it with the Andersen filtration in negative level.\nAt the critical level, we define an analogue of the Jantzen filtration for restricted Verma modules and prove an alternating sum formula for it. As a corollary from this we get the `Linkage Principle', which was already proven by Arakawa and Fiebig. A special case of the sum formula for subgeneric restricted Verma modules allows us to calculate the center of a deformed block of the restricted category O.\nDie vorliegende Arbeit besch\u00e4ftigt sich mit der Darstellungstheorie einer symmetrisierbaren Kac-Moody Algebra. Hierbei beschr\u00e4nken wir uns auf die H\u00f6chstgewichtsmoduln, deren Struktur durch die BGG Kategorie O gegeben ist. Im Falle einer affinen Kac-Moody Algebra unterscheidet man zwischen Darstellungen mit positivem, kritischem und negativem Level. Eine wichtige Rolle zur Bestimmung von Charakteren einfacher H\u00f6chstgewichtsmoduln spielen hierbei die Vermamoduln im positiven und negativen Level. Im kritischen Level ist es hilfreich, die restringierte Kategorie O, eine gewisse Unterkategorie der \u00fcblichen Kategorie O, einzuf\u00fchren. Dabei werden die \u00fcblichen Vermamoduln durch restringierte Vermamoduln ersetzt, die als maximale restringierte Quotienten der Vermamoduln auftauchen.\nDie Jantzen-Filtrierung liefert ein gutes Hilfsmittel zur Berechnung von einfachen Subquotienten in einem Vermamodul. Im Falle eines Vermamoduls im positiven Level zeigen wir, dass die Struktur der Subquotienten dieser Filtrierung durch Koeffizienten gewisser Kazhdan-Lusztig Polynome gegeben ist, indem wir sie mit der Andersen-Filtrierung im negativen Level in Verbindung bringen.\nIm kritischen Level definieren wir analog eine Jantzen-Filtrierung auf den restringierten Vermamoduln und beweisen eine alternierende Summenformel f\u00fcr diese. Als Korollar hiervon erhalten wir das `Linkage Principle', welches schon von Arakawa und Fiebig bewiesen wurde. Ein Spezialfall der Summenformel f\u00fcr subgenerische, restringierte Vermamoduln erlaubt es uns, das Zentrum eines deformierten Blocks der restringierten Kategorie O zu berechnen.\n2014-01-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4118\nurn:nbn:de:bvb:29-opus4-41182\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-41182\nhttps://opus4.kobv.de/opus4-fau/files/4118/JohannesKuebelDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4259\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\npacs\nmsc\nmsc:01-02\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nBernhard Nathanael Gottlob Schreger (1766-1825). Leben und Werk\nSchmidt, Cornelia Marlen\nPerson als Schlagwort[SWP]\nddc:610\nGegenstand dieser Arbeit ist das Leben und Werk des Chirurgen Bernhard Nathanael Gottlob Schreger (1766-1825). Nachdem Schreger in Leipzig sein Medizinstudium absolviert hatte, erreichte ihn 1793 ein Ruf nach Altdorf, wo er vier Jahre eine au\u00dferordentliche Professur f\u00fcr Anatomie, Chirurgie und Geburtshilfe innehatte. Anschlie\u00dfend nahm er einen Ruf als ordentlicher Professor f\u00fcr Medizin und Chirurgie in Erlangen an. Hier verbrachte er die letzten 28 Jahre bis zu seinem Lebensende.\nW\u00e4hrend Schregers Schaffenszeit wurden ihm einige Ehrungen zuteil. Im Jahre 1804 fand seine Ernennung zum preu\u00dfischen Hofrat statt; zahlreiche wissenschaftliche Gesellschaften, darunter die \u201eKaiserlich Leopoldinische Gesellschaft der Naturforscher\u201c, w\u00e4hlten ihn zum Mitglied. Dar\u00fcber hinaus z\u00e4hlte er 1808 zu den Mitbegr\u00fcndern der \u201ePhysikalisch-medizinischen Soziet\u00e4t\u201c in Erlangen.\nSchreger war Zeuge einer Umbruchzeit um die Wende des 18. zum 19. Jahrhundert. Ver\u00e4nderungen auf kultureller, staatlicher und sozialer Ebene zogen Neuerungen in einem Land nach sich, das sich vom Heiligen R\u00f6mischen Reich deutscher Nation zum Deutschen Bund mit einer eigenen Verfassungsgrundlage entwickelt hatte. Die Reichsstadt N\u00fcrnberg und das dazu geh\u00f6rige Altdorf sowie die brandenburgisch-preu\u00dfische Kleinstadt Erlangen kamen in diesem Zeitraum unter die neue bayerische Krone. Unter den Gedanken der Aufkl\u00e4rung setzte die Industrialisierung ein und die Welt der Naturwissenschaften wurde neu geordnet. Zudem zeichnete sich eine verst\u00e4rkte Hinwendung zur Wissenschaft und zu einer Forschung ab, die durch Empirismus und Besinnung auf den eigenen Verstand praktiziert werden sollte. Naturgesetze sollten experimentell jederzeit reproduzierbar sein. Sowohl dieser neue Wissenschaftsbegriff als auch ein ver\u00e4ndertes Gesundheitsbewusstsein revolutionierten die Medizin. Die Bildungsreformen in Preu\u00dfen von Wilhelm von Humboldt (1767-1835) und seine Denkschrift \u00fcber die Organisation des Medizinalwesens von 1809 trieben diese Prozesse voran. So ist es nicht verwunderlich, dass Schregers Zeitgenossen diese Periode als einen Zeitraum der rasanten Entwicklung erlebten.\nSchreger hinterlie\u00df zahlreiche Kasuistiken, die einen Blick in seine T\u00e4tigkeit als Chirurg zulassen. Anhand dieser detailgetreuen Fallbeschreibungen ist es m\u00f6glich, die Arbeit eines Chirurgen um die Jahrhundertwende vom 18. zum 19. Jahrhundert nachzuvollziehen. Schreger entwickelte nicht nur neue Operationsmethoden, sondern auch neuartige Instrumente, mit denen er Operationsabl\u00e4ufe verbessern wollte. Unter den damaligen Umst\u00e4nden bestritten Schreger und seine Kollegen s\u00e4mtliche Eingriffe ohne Asepsis, Antisepsis oder An\u00e4sthesie. Viele Operationen bargen deswegen ein hohes Risiko, da immer die Gefahr von Blutverlust, Infektionen und Traumata bestand.\nIn dieser Arbeit werden neben Schregers therapeutischen T\u00e4tigkeiten neue Aspekte untersucht, die ihn als facettenreichen Gelehrten auszeichnen. Sein Schaffen als Wissenschaftler und Universit\u00e4tslehrer tragen zu dieser Vielseitigkeit bei. Ein Chirurg, der neben seiner therapeutischen T\u00e4tigkeit wissenschaftlich publizierte und als akademischer Lehrer an der Universit\u00e4t arbeitete, erscheint aus heutiger Sicht nicht au\u00dfergew\u00f6hnlich. Schreger befand sich jedoch in einer Umbruchzeit, in der die Chirurgie erst im Begriff war, sich zur Wissenschaft zu entwickeln; ein akademischer Chirurg war keinesfalls die Regel. Korrelierend zu diesem Prozess hatte Schreger seine beachtliche Karriere in dieser Zeit des allgemeinen Wandels aufbauen k\u00f6nnen.\nSchreger lebte in einer Zeit, in der die Medizin von unterschiedlichen wissenschaftlichen Str\u00f6mungen beeinflusst wurde. Die Lehren des schottischen Arztes John Brown (1735-1788), des Vitalisten William Cullen (1710-1790) und des italienischen Physiologen und Naturwissenschaftlers Luigi Galvani (1737-1798) wurden von Schreger aufmerksam studiert. Die Humoralpathologie entwickelte sich zur Solidarpathologie und die Anatomie r\u00fcckte bei den Chirurgen in den Blickpunkt des Interesses. Schreger unternahm seit Beginn seiner Schaffenszeit anatomisch-pathologische Nachbeurteilungen mit Hilfe von Sektion und Autopsie. Als \u201eBegr\u00fcnder der chirurgischen Anatomie\u201c, wie Heinrich Rohlfs (1827-1898) ihn nannte, legte er zudem Wert auf Kenntnisse in der Zergliederungskunst als wichtige Voraussetzung f\u00fcr die sichere Aus\u00fcbung der Chirurgie. Robert Campbell best\u00e4tigte Ende des 19. Jahrhunderts, dass der junge Chirurg ein \u201eakkurater Anatom\u201c sein m\u00fcsse und nicht spekulativ sondern praktisch die Anatomie zu betreiben habe: \u201eAndernfalls muss er sich als blo\u00dfer St\u00fcmper erweisen.\u201c\nIn der ersten H\u00e4lfte des 19. Jahrhunderts etablierte sich die Form der Forschungsuniversit\u00e4t, an der \u00fcberliefertes Wissen nicht nur weitergegeben, sondern auch durch neue Erkenntnisse erweitert wurde. Die Universit\u00e4ten in Leipzig, Altdorf und Erlangen stellten f\u00fcr Schreger eine Plattform dar, auf der er wissenschaftliche Kenntnisse sowohl erlangen, als auch lehren konnte. Das Erlanger Clinicum chirurgicum als universit\u00e4re Institution erm\u00f6glichte die Fortf\u00fchrung seiner Lehrt\u00e4tigkeit. Schregers Gr\u00fcndung dieser ersten chirurgischen Klinik in Erlangen mit Hilfe von Friedrich Wendt (1738-1818) im Jahr 1815 geh\u00f6rt zweifellos zu seinen nachhaltigsten Taten. Auf diese Weise schuf er eine chirurgische Ausbildungsst\u00e4tte f\u00fcr den praxisnahen Unterricht am Krankenbett. Das Clinicum chirurgicum wurde die erste klinische Institution der Fakult\u00e4t. Au\u00dferdem er\u00f6ffnete sich f\u00fcr Schreger durch das Clinicum chirurgicum die M\u00f6glichkeit, wertvolle wissenschaftliche Erfahrungen zu sammeln, die er publizieren konnte.\nIn dieser Arbeit wird die M\u00f6glichkeit ergriffen, das umfangreiche Schaffen eines Universit\u00e4tschirurgen darzustellen, der sich im Spannungsfeld zwischen der alten Chirurgie und der Chirurgie einer neuen Epoche befand. Als akademischer Chirurg und klinischer Universit\u00e4tslehrer war er nicht mehr Teil der alten Chirurgie, w\u00e4hrend er den Durchbruch der neuen Chirurgie mit An\u00e4sthesie und Asepsis gleichwohl nicht mehr erlebte. Gleichzeitig befand er sich in einer Umbruchzeit, die sich \u00fcber die wissenschaftliche Ebene hinaus auf s\u00e4mtliche gesellschaftlichen und wirtschaftlichen Bereiche ausbreitete. Vor diesem Hintergrund soll Schregers T\u00e4tigkeit als Lehrer, Wissenschaftler und schlie\u00dflich als chirurgischer Therapeut beleuchtet werden. Die Betrachtung Schregers aus diesen verschiedenen Blickwinkeln erm\u00f6glicht letztendlich einen vielschichtigen Eindruck von Leben und Werk eines akademischen Chirurgen um 1800.\nSubject of this thesis is the life and opus of the surgeon Bernhard Nathanael Gottlob Schreger (1766-1825). Having completed his studies at the University in Leipzig, Schreger got a reputation to Altdorf in 1793, where he held for four years a non-tenured professorship in anatomy, surgery and midwifery. After this, he took up a position as a tenured professor for medicine and surgery in Erlangen. There, he spent the rest of his life.\nAlong the way, he has received a numerous honours. In the year 1804 took place his appointment to the prussian court counsellor; a large number of scientific societies, including the \u201cKaiserlich Leopoldinische Gesellschaft der Naturforscher\u201c elected him as a member. Furthermore, he was one of the founders of the \u201ePhysikalisch-medizinischen Soziet\u00e4t\u201c in Erlangen.\nSchreger witnessed a time of radical change at the turn of the 19th century. Changes at the cultural, social and political level resulted innovations in a country, that had developed from the Holy Roman Empire of the German Nation to the German Federation with their own constitutional foundation. At this time, the bavarian reign ruled the imperial city Nuremberg and the associated Altdorf as well as the brandenburgisch-prussian provincial town Erlangen. With the idea of the Enlightenment the industrialisation began and the world of the natural sciences has been reordered. In addition, a stronger interest in a science and research, that should be practised by empiricism and reflection on the own mind, became apparent. Natural laws should have been reproducible at any time. Both, this new concept of science and a changing health-consciousness revolutionised the medicine. The education reforms in Prussia of Wilhelm von Humboldt (1767-1835) and his memorandum on the organisation of the medical care (1809) pushed these processes. It is therefore not surprising that Schregers contemporaries experienced this time as a period of rapid development.\nSchreger has left numerous casuistries, that allow a view in his activity as a surgeon. On the basis of these case descriptions it is able to relate to the work of a surgeon at the turn of the 19th century. Schreger developed not only new operational techniques, but also new instruments to improve surgical procederes. Under the circumstances of the time all operations have been contested bey Schreger and his colleagues without asepsis, antisepsis and anaesthesia. Many operations involved a high risk because of the always existent danger of blood loss, infection and trauma.\nAdditionally to Schregers therapeutic skills, new issues, that show his function as a manifold scholar, are discussed in this thesis. His work as a scientist and professor contribute to this versatility. From today's point of view, a surgeon, who published scientific papers and worked as a professor beside his therapeutic activities, seems not unusually. However, Schreger was in a time of transition, when surgery was about to develop to a science; an academic surgeon was not the norm, at all. Correlated to this process, Schreger was able to develop a remarkable academic career.\nSchreger lived in a time, when different scientific approaches affected the medicine. The teachings of the Scottish physician John Brown (1735-1788), of the vitalist William Cullen (1710-1790) and of the Italian Physiologists and natural scientist Luigi Galvani (1737-1798) have been carefully evaluated by Schreger. The humoral pathology developed to the solidism and for the surgeons, the anatomy moved into the centre of attention. Since the beginning of his work, Schreger undertook anatomo-pathological revisions by means of autopsies. As the \u201cfounder of surgical anatomy\u201d, as Heinrich Rohlfs (1827-1898) called him, he placed value in knowledges in anatomy as an essential precondition for a succesful surgery. Robert Campbell confirmed at the end of the 19th century, that the young surgeon must be an accurate anatomist, otherwise he would be a real bumbler.\nIn the first half of the 19th century, the form of research university was established. There, traditional knowledge was not only handed down but also expanded with further knowledge. For Schreger, the universities of Leipzig, Altdorf and Erlangen were a platform, on which he could both, gaining scientific knowledge and doing his lessons as a professor. The Erlanger \u201cClinicum chirurgicum\u201d as an academic institution made the continuation of his teaching activities possible. Without a doubt, Schregers foundation of this first surgical clinic in Erlangen by means of Friedrich Wendt (1738-1818) in the year 1815 was one of his most sustainable achievements. That way, he created a surgical educational institution for bedside-teaching. The Clinicum chirurgicum became the first clinical institution of the faculty. Furthermore, for Schreger the Clinicum chirurgicum opend up the possibility to gather scientific experiences, he could publish. This thesis shows the extensive work of a university surgeon, who was beeing pulled between the old surgery and the surgery of a new age. As an academical surgeon and clinical professor he wasn\u2019t part of the old surgery anymore, nevertheless, he did not live to see the breakthrough of the new surgery with anesthesia and asepsis. At once, he was in a period of transition, that influenced beyond the scientific level the social and economic field. Against this backdrop, Schregers activity as a teacher, scientist and, finally, surgical therapist should be illuminated. Analysing Schreger from different points of view enables a complex impression of the life and work of an academic surgeon around 1800.\n2014-02-14\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4259\nurn:nbn:de:bvb:29-opus4-42599\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-42599\nhttps://opus4.kobv.de/opus4-fau/files/4259/Ver%C3%B6ffDrArbeitSchmidt.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4297\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nddc:660\nccs\nccs:B.\npacs\npacs:00.00.00\nmsc\nmsc:28-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Chemieing\nIn-situ Diagnostik von Zeolithbildungsprozessen auf Basis der Messung von Ultraschalld\u00e4mpfung und -geschwindigkeit\nIn-situ diagnostics of zeolite formation processes based on the measurement of Ultrasonic attenuation and velocity\nBaser, Hasan H\u00fcseyin\nUltraschall\nIn-situ\nKinetik\nZeolith A\nddc:620\nddc:660\nIn dieser Arbeit wurden die Synthesen von Zeolithen mittels Messung von US-D\u00e4mpfung und -geschwindigkeit untersucht. Die Zielstellung der Arbeit war zu \u00fcberpr\u00fcfen, ob Ultra-schall als eine diagnostische Methode zur Prozesskontrolle der Zeolithsynthese eingesetzt werden kann und aus den in-situ Ultraschallergebnissen kinetische und reaktionsmechanisti-\nsche Informationen erhalten werden k\u00f6nnen.\nAusgew\u00e4hlt wurden die hydrothermalen Synthesen von Zeolith A nach dem Sol-Gel-Prozess und die kolloidalen Synthesen von Zeolith A und Silikalith-1 L\u00f6sungen sowie die solvothermale Synthese von CuBTC.\nDie in-situ Ultraschalldaten korrelierten in den Sol-Gel-synthesen mit der Entwicklung der Kristallinit\u00e4t und in den kolloidalen Synthesen mit der Zunahme der Produktmenge.\nDie US-Geschwindigkeit reagierte mehr auf die chemischen und physikalischen \u00c4nderungen (z.B. Ionenkonzentrationen, die Dichte und die Temperatur) in der fl\u00fcssigen Phase. Die US-\nD\u00e4mpfung korrelierte besser die \u00c4nderungen in der festen Phase.\nSomit bietet die Anwendung des Ultraschalls als eine in-situ diagnostische Methode die M\u00f6glichkeit, die \u00c4nderungen des Synthesezustandes einer Zeolithkristallisation sowohl in der\nfl\u00fcssigen Phase als auch in der festen Phase w\u00e4hrend des gesamten Kristallisationsvorganges beobachten zu k\u00f6nnen.\nIn Kristallisationsprozessen von Zeolithen bildet sich oftmals zun\u00e4chst eine metastabile Phase, die sich schlie\u00dflich in eine unerw\u00fcnschte stabilere Phase umwandelt. Aus diesem\nGrund w\u00e4re es w\u00fcnschenswert, den Kristallisationsprozess der Synthese auf eine kosteng\u00fcnstige, robuste und einfacher Weise zu verfolgen. Bei allen Synthesen in dieser Arbeit\nkonnten der Anfang und das Ende der Kristallisation sowie die Dauer der Inkubations- bzw. Kristallisationsdauer exakt in-situ detektiert werden. Somit eignet sich die Methode zur Prozesskontrolle nicht nur in Laborsynthesen sondern auch in den industriellen Gro\u00dfanlagen.\nDurch die erhaltene hohe Anzahl an Datenpunkten konnten die kinetischen Daten wie die Aktivierungsenergie und Reaktionsgeschwindigkeit bzw. Ordnung auf einfacher und sicherer Weise mit geringem Fehler erhalten werden. Durch die in-situ Analyse wurde die Synthese nicht gest\u00f6rt und die Daten konnten trotz der gro\u00dfen Menge schnell ausgewertet werden,\nwas man f\u00fcr eine ex-situ Analyse gebrauchen h\u00e4tte. Somit konnte gezeigt werden, dass die Ultraschalldiagnostik auch als ein Tool zur Erhaltung von kinetischen Daten eingesetzt werden kann.\nObwohl die hier angewendete Ultraschalltechnik keine struktursensitive Analysemethode ist, konnte man aus den Ergebnissen wichtige Information auch \u00fcber den Verlauf der Kristallisationsentwicklung erhalten werden.\nDurch den Einsatz der Ultraschallmessung in dieser Arbeit konnten wichtige Informationen zu den Kristallisationsvorg\u00e4ngen von Zeolithen und MOF\u00b4s bekommen und ein besseres Verst\u00e4ndnis der Arbeitsweise des in-situ Ultraschallmonitorings erhalten werden.\nIn this work the synthesis of the zeolite by means of measurement of ultrasonic attenuation and velocity were examined. The aim of this study was to determine whether ultrasound can be used as a diagnostic method for process control of zeolite synthesis and if information regarding the reaction kinetics and mechanism can be obtained from the in - situ ultrasonic results.\nThe selected synthesis techniques were the hydrothermal synthesis of zeolite A, according to the sol gel and the colloidal synthesis route, the colloidal synthesis of silicalite-1 and the sol-\nvothermal synthesis of CuBTC. In situ ultrasonic data correlated in the sol-gel synthesis with the development of crystallinity and in the colloidal synthesis with an increase in the amount\nof product. The ultrasonic velocity was responding more to the chemical and physical changes (e.g. ion concentration, density and temperature) in the liquid phase. The ultrasonic atten-\nuation was found to correlate better with the changes in the solid phase. The applicability of ultrasound was thereby demonstrated as a diagnostic in-situ method to monitor the changes in the synthesis of zeolite crystallization in both the liquid phase and the solid phase during the whole crystallization process.\nIn crystallization processes of zeolites a metastable phase is often initially formed, which eventually transforms into an undesired stable phase. For this reason, it would be desirable\nto follow the crystallization of the synthesis process in a cost effective, robust and simple way. In all syntheses in this work, the beginning and the end of the crystallization and the\nduration of incubation and crystallization time could be accurately detected in situ. Thus, the method was found to be suitable for process control, not only in laboratory synthesis but also\nin the large industrial suitable.\nDue to the resulting high number of data points, kinetic data such as activation energy and reaction rate or order could be obtained in a simple and safe manner with minimal error. Due to the in - situ analysis, the synthesis was not disturbed and the data could be analyzed quickly in spite of the large data amount required for an ex - situ analysis. Thus, it could be shown that diagnostic ultrasound may also be used as a tool for maintenance of kinetic data.\nAlthough the ultrasound technology applied here is not a structure-sensitive method of analysis, important information could be obtained from the results concerning the course of crys-\ntallization development.\nThrough the use of ultrasound measurement in this work important information regarding the crystallization processes of zeolites and MOFs could be achieved and a better understanding\nof the operation of the in - situ ultrasonic monitoring are obtained.\n2014-02-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4297\nurn:nbn:de:bvb:29-opus4-42979\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-42979\nhttps://opus4.kobv.de/opus4-fau/files/4297/Dissertation_Hasan%20Baser-Final.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4340\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:B.0\npacs\npacs:00.00.00\nmsc\nmsc:94-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nEinfl\u00fcsse des Filterentwurfs auf die Empfangsqualit\u00e4t breitbandiger Funk\u00fcberwachungsempf\u00e4nger\nDehm-Andone, Gunther\nFilter <Elektrotechnik>\nFrequenzfilter\nFilterschaltung\nddc:620\nDiese Arbeit beschreibt Ans\u00e4tze zur Reduktion des Formfaktors, der Kosten und des\nImplementierungsaufwandes eines Empf\u00e4ngers zur Anwendung in COMINT1-Systemen.\nDabei werden eine gezielte Anforderungsanalyse und die dazu passenden L\u00f6sungskonzepte\neinzelner Baugruppen, speziell der Filterkomponenten, vorgestellt. Die markt\u00fcblichen\nFunktionalit\u00e4ten f\u00fcr das als Abh\u00e4nge- und Scanning-Empf\u00e4nger entworfene System wird\ndurch eine ausgewogene Balance sowohl zwischen Analog- und Digitalteil als auch bei der\nDimensionierung der Analogkomponenten bereitgestellt. Als wichtigste Parameter stellen\nsich dabei der sich \u00fcber zwei Dekaden erstreckende Eingangsfrequenzbereich und die\nerforderliche Echtzeitbandbreite und Signalqualit\u00e4t dar. Die gewonnene Systemtopologie\nund deren Umsetzung erm\u00f6glichen letztlich die Erschlie\u00dfung neuer Anforderungsaspekte\nin der Funk\u00fcberwachung. Eine kosteneffiziente Hardware solcher COMINT-Empf\u00e4nger,\nwie es der vorliegende KAIMAN2-Empf\u00e4nger ist, er\u00f6ffnet die Einsatzm\u00f6glichkeit in weitr\u00e4umigen\nund fl\u00e4chendeckenden Szenarien mit einer Vielzahl an Funk\u00fcberwachungsstationen.\nUm die performate Funktion des Empf\u00e4ngers zu erreichen, ist eine Filterung in verschiedenen\nStufen des Empf\u00e4ngers unumg\u00e4nglich. Diese beeinflusst jedoch die erw\u00e4hnte\nSignalqualit\u00e4t. Daher wird hier erstmalig eine umfangreiche Analyse des Einflusses\ndes Filterentwurfs und der Filterimperfektionen auf Systemparameter von COMINTEmpf\u00e4ngern\ndargestellt. Speziell auf den Signalqualit\u00e4tsindikator EVM3 wird hier Augenmerk\ngerichtet. Die Implementierung mittels optimaler Technologie und Topologie\nund die Validierung der daraus abgeleiteten Filterl\u00f6sungen wird im Anschluss pr\u00e4sentiert\nund zeigt, dass die gefundenen Umsetzungen dem Vergleich zu kommerziell erh\u00e4ltlichen\nFiltern standhalten und ihnen in einigen Punkten \u00fcberlegen sind. Auch der Vergleich\nkommerzieller COMINT-Empf\u00e4nger mit dem hier entwickelten KAIMAN-Empf\u00e4nger im\nGesamtsystem zeigt, dass eine vergleichbare Performanz bei kompakterem Formfaktor\nerzielt werden konnte.\nThis work describes approaches to reduce the form factor, costs and implementation effort\nof a receiver for application in COMINT4 systems. Thus, a purposeful requirement analysis\nand its suitable solution concepts for single modules, especially filter components, is\npresented. The customary functionality of the system, that is designed as monitoring and\nscanning receiver, is provided by a balance of the analog and digital part as well as among\nthe analog components. The most important parameters are the input frequency range\nof two decades and the required instantaneous bandwidth and signal quality. In the end,\nthe gained system topology and its realization permit the opening of new applications\nin the radio surveillance domain. A cost efficient hardware of COMINT receivers, such\nas the KAIMAN5 receiver, establishes the application possibility in large and inclusive\nscenarios with a multitude of surveillance stages.\nTo reach this functionality, filtering in several stages of the receiver is inevitable. However,\nthis influences the signal quality. Hence, for the first time a comprehensive analysis\nof the influence of filter design and filter imperfections on system parameters of COMINT\nreceivers is presented. Focus is layed on the signal quality indicator EVM6. The\nimplementation by means of optimal technology and topology and the validation of the\nderived filter solution is presented and shows that the offered realization sustains comparison\nwith commercially available filters and outclasses them in some points. Also the\ncomparison of the whole KAIMAN system with commercial COMINT systems shows\nthat comparable performance could be achieved with a more compact form factor.\n2014-03-13\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4340\nurn:nbn:de:bvb:29-opus4-43407\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-43407\nhttps://opus4.kobv.de/opus4-fau/files/4340/Dehm-AndoneDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4521\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:550\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nExperimental measurements and numerical modeling of a Ground Source Heat Pump system\nExperimentelle Messungen und numerische Modellierung eines Erdw\u00e4rmepumpen-Systems\nLuo, Jin\nGround Source Heat Pump system\nddc:550\nIn recent years, Ground Source Heat Pump (GSHP) systems have been widely applied in commercial and residential buildings for space heating as well as cooling. GSHP system is a kind of geothermal systems which has the superiority of high energy efficiency and environmental friendliness. Compared to the traditional air conditioning systems, GSHP system can achieve higher energy efficiency. The initial costs for a vertical GSHP systems which depend mainly on the size, length and numbers of Borehole Heat Exchangers (BHE) are, however, even higher than that of air conditioning systems. Therefore, the study of thermal and economic performance of BHE is of crucial importance to optimize the size of GSHP systems.\nIn current estimation of system performance the BHE is generally examined with the same borehole diameters. However, the thermal efficiency as well as economic performance can change with different borehole diameters. The ground is also generally considered to be homogenous in most former studies. In fact, thermal performance of BHE can change drastically with different geological stratums (e.g. claystone layer is commonly has worse thermal performance than sandstone layer). Besides, the energy loss in the horizontal connecting pipes and unbalanced heating/cooling load of building are also very important for performance of the GSHP systems. Therefore, a study aiming to overcome the aforementioned drawbacks of the former studies is necessary.\nIn this thesis, the performance of a GSHP system has been studied via both experimental measurements and numerical modeling. The GSHP system is installed in an office building in Nuremberg, Germany. There are 18 BHEs installed in this system which can be grouped into 3 blocks in accordance with the borehole diameter: block I of 121 mm, block II of 165 mm and block III of 180 mm. In order to examine the efficiency of the system, a monitoring system is installed. Sensors such as flowmeters and temperature sensor are used to detect the operating conditions of the system. On the other hand, two wells are drilled to monitor the ground response during operation of the GSHP system and to investigate the ground properties. Based on the monitoring data, thermal performance of the BHEs and the whole GSHP system is analyzed and discussed. Furthermore, numerical models are developed using the program FEFLOW to simulate heat transfer of the horizontal connecting pipes as well as the BHE performance in a layered underground.\nMajor conclusions of this thesis are presented as follows: 1. The comparison of thermal performance among three blocks of BHEs indicates that block II (165 mm) has a 1.64% and block III a 3.45% (180 mm) higher performance than that of block I (121 mm), respectively; 2. The economic evaluation of those three blocks suggests that the economic profitability of BHEs decreases with enlarging the borehole diameter; 3. Based on the on-site data and numerical results, there exists substantial energy loss of horizontal connecting pipes for a GSHP system in cold periods. The daily energy loss can vary in magnitudes, as determined by pipe burial depth. On the other hand, up to 50% energy loss can be avoided by adopting a 25 mm thick insulation; 4. In a layered subsurface, BHE performance in an aquiclude layer has only 74.1% of the energy efficiency as compared to that of aquifer layers. 5. Over years of the system operation, it is observed that the heating performance reduces while the cooling performance enhances due to the imbalanced heating and cooling energy demand for the building.\nThe findings obtained from this thesis provide useful information for design and implementation of future GSHP systems in terms of improving energy efficiency as well as reducing costs. Also, this study gives a view in investigation of the ground conditions both in laboratory and in borehole field for GSHP systems.\n2014-04-14\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4521\nurn:nbn:de:bvb:29-opus4-45217\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-45217\nhttps://opus4.kobv.de/opus4-fau/files/4521/PhD%20thesis-Jin%20Luo.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4534\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35J47\nmsc:35L15\nmsc:49M37\nmsc:90-08\nmsc:90C31\nmsc:90C90\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Mathematik\nTopology Preserving Multi-Layer Shape and Material Optimization\nTopologieerhaltende Mehrschicht Form- und Material-Optimierung\nSchmidt, Bastian\nStrukturoptimierung\nddc:510\nIn this thesis, we present a new approach for parametric shape optimization in combination with material parameter optimization. The main novelty of the shape optimization method is the description of the admissible set. In contrast to \u2018classical\u2019 parametric shape optimization, where the boundary is (partially) represented by the graph of a function, we use a reference configuration, which we then deform.\nIn the reference configuration, curves are used to describe the outer boundary and interfaces of a domain. To model the deformation, we again use curves, which are interpreted as an added displacement. We add a simple \u2018deformation constraint\u2019 that controls the amount of admissible deformation. This constraint also ensures topology preservation, i.e. all admissible deformed configurations have the same topology as the reference configuration, which is necessary to guarantee that the deformed boundary and interfaces still describe a domain and its partition. For cost functions depending on the solution of a linear elasticity problem, we extend the existence results from the classical shape optimization approach to our new method. We take a detailed look at the regularity of all admissible domains and present conditions to restrict the admissible set, which ensure the preservation of given regularity, e.g. uniform cone property or domains of class C k , without losing the existence result.\nUsing a finite element discretization for the state problem and a spline discretization of the curves in the admissible set together with a discretized version of the deformation constraint, we state a discretized optimization problem, which allows the convergence statement as known for the classical shape optimization approach.\nThe method is extended by exchanging the static elasticity problem for the initial value problem of linear elastodynamics, and we transfer the existence and convergence statements to this transient setting. Furthermore, we present enhancements that allow this approach to be adopted for three dimensional problems.\nWe present details of our numerical implementation, including a description of the admissible set using linear constraints for an efficient solution of the optimization problem and an extensive sensitivity analysis.\nThe thesis concludes with several academic examples focusing on specific capabilities of the presented method and, finally, the application of the method to optimize a multi-layer silicone vocal fold model, which can be examined in a wind tunnel to better understand human phonation.\nMit dieser Arbeit stellen wir einen neuartigen Ansatz zur parametrischen Form-Optimierung kombiniert mit der Optimierung von Materialparametern vor. Die grundlegende Neuheit liegt in der Beschreibung der zul\u00e4ssigen Menge. Im Gegensatz zur \u2018klassischen\u2019 parametrischen Form-Optimierung, bei der der Rand einesGebietes (teilweise) durch den Graph einer Funktion beschrieben wird, nutzen wir eine Referenzkonfiguration, in der der \u00e4u\u00dfere Rand und Unterteilungen eines Gebietes durch Kurven beschrieben werden, die wir anschlie\u00dfend deformieren. Um diese Deformation zu modellieren benutzen wir wiederum Kurven, die Verschie bungen der urspr\u00fcnglichen Kurven darstellen.\nWir benutzen eine einfache \u2018Deformations-Restriktion\u2019, die die Gr\u00f6\u00dfe der zul\u00e4ssigen Verschiebungen beschr\u00e4nkt, sie garantiert au\u00dferdem die Topologieerhaltung der Deformation, d.h. alle deformierten Gebiete haben die selbe Topologie, wie die Referenzkonfiguration. Dies ist n\u00f6tig, um sicherzustellen, dass der noch der Deformation immer noch ein Gebiet mit Unterteilung beschrieben wird.\nF\u00fcr Kostenfunktionen, die von der L\u00f6sung eines linearen Elastizit\u00e4tsproblems abh\u00e4ngen erweitern wir die Existenzresultate aus der klassischen Form-Optimierungs-Theorie. Wir betrachten dabei auch die Regularit\u00e4t der zul\u00e4ssigen Gebiete und geben Bedingungen an, die die zul\u00e4ssige Menge so einschr\u00e4nken, dass die Erhaltung der Regularit\u00e4t des Ausgangsgebiets, z.B. f\u00fcr die gleichm\u00e4\u00dfige Kegelbedingung oder C k -Gebiete, gesichert ist und die Existenzaussage erhalten bleibt.\nWir verwenden eine Finite-Elemente Diskretisierung f\u00fcr das Zustandsproblem und eine Spline Diskretisierung f\u00fcr die Kurven der zul\u00e4ssigen Menge und geben eine diskretisierte Version der Deformations-Restriktion an, um ein diskretisiertes Optimierungsproblem zu formulieren, das die Konvergenzaussagen wie in der klassischen Form-Optimierung erlaubt.\nAnschlie\u00dfend, wird die Methode auf das transiente Elastizit\u00e4tsproblem erweitert und die Existenz und Konvergenzaussagen entsprechend \u00fcbertragen. Au\u00dferdem pr\u00e4sentieren wir Erweiterungen um den Ansatz f\u00fcr dreidimensionale Probleme nutzen zu k\u00f6nnen.\nWir erl\u00e4utern die Details unserer numerischen Implementation, unter anderem eine Beschreibung der zul\u00e4ssigen Menge durch lineare Nebenbedingungen, was eine effiziente L\u00f6sung des Optimierungsproblem erlaubt. Weiterhin f\u00fchren wir eine ausf\u00fchrliche Sensitivit\u00e4tsanalyse durch.\nZum Abschluss zeigen wir einige akademische Beispiele, um einzelne Aspekte des Ansatzes genauer zu beleuchten, und eine Anwendung der Methode um ein mehr-schichtiges Silikon Stimmlippenmodell zu optimieren, das im Windkanal verwendet werden kann, um die menschliche Stimmgebung n\u00e4her zu untersuchen.\n2014-04-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4534\nurn:nbn:de:bvb:29-opus4-45344\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-45344\nhttps://opus4.kobv.de/opus4-fau/files/4534/DissertationBastianSchmidt%20%282%29.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4667\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\npacs\npacs:82.00.00\nmsc\nmsc:80Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Chemieing\nEntwicklung neuer Absorbentien f\u00fcr die Aufreinigung von Biogas auf Erdgasqualit\u00e4t\nDevelopment of novel solvents for the upgrade of biogas to natural gas quality\nV\u00f6lkl, Johannes\nIonische Fl\u00fcssigkeit\nCOSMO-RS\nCO2-Absorption\nL\u00f6sungsmittel\nSimulation\nModellierung\nddc:600\nIn der vorliegenden Arbeit wurde ein Ansatz zur Entwicklung neuer Absorbentien f\u00fcr die CO2-Absorption aus Biogas erarbeitet. Anhand von Kriterien aus der praktischen Anwendung sollen neue L\u00f6sungsmittel gezielt ausgew\u00e4hlt, synthetisiert und charakterisiert werden, die diese Anforderungen erf\u00fcllen. Dabei standen Molek\u00fcle aus den Stoffklassen der Ionischen Fl\u00fcssigkeiten und hyperverzweigten Polymere im Fokus. Zur Vorauswahl wurden computergest\u00fctzte Methoden genutzt, um geeignete Kandidaten zu identifizieren und den experimentellen Aufwand zu minimieren. Die Arbeit gliederte sich in 5 Themenkomplexe:\n1. F\u00fcr die Vorhersage von thermophysikalischen Eigenschaften der betrachteten Stoffe wurden entsprechende Methoden identifiziert. F\u00fcr Reinstoffeigenschaften, wie die W\u00e4rmekapazit\u00e4t, sind dies Gruppenbeitragsmethoden. Zur Berechnung von Wechselwirkungseigenschaften, wie die L\u00f6slichkeit von Wasser oder Gasen in den L\u00f6sungsmitteln, wurde das Modell COSMO-RS verwendet. Zur Verbesserung der Vorhersage der absoluten Werte mit diesem Modell wurde das Konzept der relativen L\u00f6slichkeit eingef\u00fchrt und angewendet. Durch die Anwendung von quantenmechanischen Methoden konnte die Gleichgewichtslage und W\u00e4rmet\u00f6nung von chemischen Reaktionen mit CO2 vorhergesagt werden.\n2. F\u00fcr den Gesamtprozess der CO2-Absorption und L\u00f6sungsmittelregeneration wurden Prozessmodelle in Aspen Plus f\u00fcr physisorptive und reaktive L\u00f6sungsmittel aufgestellt und validiert. Dar\u00fcber hinaus wurde ein Modell zur Modellierung von Gaspermeationsprozessen durch Membranen erstellt. Damit sollen Membranverfahren und absorptive Verfahren miteinander verglichen werden k\u00f6nnen.\n3. Die Vorhersagemethoden und die Prozessmodellierung wurden in einem Screening zusammengefasst. Dazu sind zuerst die thermophysikalischen Einflussgr\u00f6\u00dfen auf das Prozessverhalten durch Variation dieser Daten in entsprechenden Simulationen quantifiziert und qualifiziert worden. In einem Screening wurde eine gro\u00dfe Anzahl von realen und hypothetischen, d.h. noch nicht synthetisierten, L\u00f6sungsmittel untersucht. Dabei wurden sowohl physisorptive als auch reaktive Absorbentien ber\u00fccksichtigt. F\u00fcr jede Verbindung wurden die thermophysikalischen Eigenschaften vorhergesagt und damit das Prozessverhalten mit entsprechenden Prozessmodellen abgesch\u00e4tzt. Als Prozessmodelle wurden dazu speziell entwickelte Short-Cut-Methoden verwendet. F\u00fcr physisorptive L\u00f6sungsmittel ergeben sich keine geeigneten Kandidaten, der abgesch\u00e4tzte W\u00e4rmebedarf und Methanverlust ist zu hoch. F\u00fcr reaktive Ionische Fl\u00fcssigkeiten werden vielversprechende Strukturen identifiziert. F\u00fcr die Membranverfahren wurden ein- und mehrstufige Verfahren sowie Hybridverfahren in Kombination mit der Absorption evaluiert.\n4. In Kooperation mit dem Lehrstuhl f\u00fcr Organische Chemie der Universit\u00e4t Erlangen-N\u00fcrnberg wurden die geeignetsten Ionischen Fl\u00fcssigkeiten synthetisiert und im Rahmen dieser Arbeit experimentell charakterisiert. Dar\u00fcber hinaus wurden von der Firma Dendritech neue hyperverzweigte Polymere zur Verf\u00fcgung gestellt. Zur Bestimmung der Gasl\u00f6slichkeit wurde eine Apparatur verwendet, die nach dem isochoren Messprinzip arbeitet. Daneben wurden hydrodynamische Eigenschaften wie die Dichte oder Viskosit\u00e4t untersucht.\n5. Abschlie\u00dfend wurden mit den experimentellen Daten die Prozessmodelle neu ausgewertet und die Prozessparameter optimiert. Ausgehend von den simulierten Werten f\u00fcr den W\u00e4rme- und Strombedarf sind die untersuchten Verfahren der physisorptiven W\u00e4sche (drucklos und druckgetrieben), der chemischen W\u00e4sche, der zweistufigen Membranverfahren und der Hybridverfahren in ihren Betriebskosten miteinander verglichen worden. Dabei weisen die in dieser Arbeit neu entwickelten reaktiven Ionischen Fl\u00fcssigkeiten die niedrigsten Kosten bei der h\u00f6chsten Produktgasqualit\u00e4t auf.\nIn the present work a new approach for the development of novel absorbents for the absorption of CO2 from biogas was designed. On the basis of criteria from the practical application novel solvents should be selected, synthesised and characterised specifically to fulfil these specifications. The focus was on Ionic Liquids and hyperbranched polymers. For the preselection computer aided methods were used to identify suitable candidates and minimize the experimental efforts. Five different aspects were investigated:\n1. For the prediction of thermophysical properties of the considered solvents corresponding methods were identified. Pure component data like the heat capacity were calculated with group contribution methods. For the prediction of interaction properties like the solubility of water or gases in the solvents the model COSMO-RS was used. To improve the quantitative prediction the concept of relative solubility was introduced and applied. With the application of quantum mechanical methods, the chemical equilibrium as well as the heat of reaction could be described.\n2. For the process of CO2-absorption and solvent regeneration models in Aspen Plus for physical and reactive solvents were developed and validated. Additionally a model for the simulation of gas permeation processes by membranes was compiled. Thereby membrane processes and absorption should be compared.\n3. The prediction methods and the modelling were combined in a screening. First the influence of the thermophysical data on the process behaviour was quantified and qualified by varying these data in corresponding simulations. In a screening approach a large number of existing and hypothetical absorbents were investigated. Therefore physical as well as reactive solvents were considered. For each molecule the thermophysical data is predicted and with these data their process behaviour is estimated with corresponding models. For the modelling novel developed short cut methods were used. No physical solvent was found to be a suited candidate: the estimated heat demand and methane losses are too high. For reactive Ionic liquids promising structures were identified. The membrane processes were evaluated as one stage and two stage processes as well as in combination with absorption in hybrid processes.\n4. In cooperation with the Chair of Organic Chemistry, the University Erlangen-N\u00fcrnberg, the most suitable Ionic Liquids were synthesised and experimental characterised in the present work. Additionally the company Dendritech provided novel hyperbranched polymers. For the determination of the gas solubility an apparatus was used, which is working based on the isochoric principle. Additionally, the hydrodynamic properties like density and viscosity were investigated.\n5. Using the experimental data the simulations were updated and the process parameters were optimised. Based on the calculated values for the heat and electricity demand the investigated process of the physical absorption (without pressure and pressure driven), the chemical absorption, the two stage membrane process and the hybrid processes were compared by their cost of operation. The reactive Ionic Liquids, which were developed in the present work, are showing the lowest cost combined with the highest product gas quality.\n2014-05-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4667\nurn:nbn:de:bvb:29-opus4-46674\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-46674\nhttps://opus4.kobv.de/opus4-fau/files/4667/Dissertation_Johannes_Voelkl.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4767\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:570\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nAPOBEC3G and Emergence of Pathogenic SIV\nAPOBEC3G und die Emergenz eines pathogenen SIVs\nKrupp, Annabel\nRetroviren\nddc:570\nIn nature, new diseases arise when existing viruses adapt to infect new host species. For example, pandemic human immunodeficiency virus type 1 (HIV-1) and AIDS are the result of such a cross-species transmission of a lentivirus from its reservoir (chimpanzees) to humans. Lentiviruses are abundant in African primates and pose a constant danger of additional transmissions to humans, due to the expansion of settlements and growing contact of primates and humans. Therefore the investigation of the origins, natural history and the influence of host genes on the transmission of these lentiviruses will help us to understand, and perhaps even predict future zoonoses.\nCellular restriction factors, which render cells intrinsically resistant to viruses, potentially impose genetic barriers to cross-species transmission and emergence of viral pathogens in nature. One such factor is APOBEC3G (A3G). To overcome A3G-mediated restriction, many lentiviruses encode Vif, a protein that targets A3G for degradation by the proteasome. As is typical for many restriction factor genes, primate A3G displays strong signatures of positive selection. This is interpreted as evidence that the primate A3G locus reflects a long-term evolutionary \u201carms-race\u201d between lentiviruses and their primate hosts. The emergence of SIVmac and outbreaks of simian AIDS in outbred colonies of macaques in the 1970s provides an unusual opportunity to examine the impact of specific, host-encoded restriction factors in the context of an emerging pathogen.\nThis study provides direct evidence that A3G functioned as a barrier to cross-species transmission, both suppressing viral replication and selecting for evolution of viral resistance mutations during emergence of SIVmac. Specifically, this study found that rhesus macaques have multiple, functionally distinct A3G alleles, and that emergence of SIVmac and simian AIDS required adaptation of the virus to evade APOBEC3G-mediated restriction. The evidence provided includes the first comparative analysis of APOBEC3G polymorphism and function in both a reservoir and recipient host species (sooty mangabeys and rhesus macaques, respectively), and identification of adaptations unique to Vif proteins of the SIVmac lineage that specifically antagonize A3G alleles found in rhesus macaque. Demonstrating that interspecies variation in a known restriction factor selected for viral counter-adaptations in the context of a documented case of cross-species transmission, lends strong support to the evolutionary \u201carms-race\u201d hypothesis. Importantly, this study confirms that A3G divergence can be a critical determinant of interspecies transmission and emergence of primate lentiviruses, including viruses with the potential to infect and spread in human populations.\nDie Entstehung neuer Krankheiten kann durch die Anpassung vorhandener Viren an neue Spezies geschehen. Das pandemische menschliche Immunodefizienzvirus Typ 1 (HIV-1) ist das Ergebnis einer solchen arten\u00fcbergreifenden \u00dcbertragung. Ein von Schimpansen stammendes Virus passte sich an seinen neuen Wirt, den Menschen, an und wurde damit zum Verursacher von AIDS. Bis heute scheint weder eine Heilung noch eine Impfung in greifbarer N\u04d3he. HIV verwandte Lentiviren sind reichlich in afrikanischen Primaten verbreitet und stellen durch die Expansion von menschlichen Siedlungen und dem damit vermehrten Kontakt zwischen Mensch und Tier eine st\u04d3ndige Gefahr der erneuten \u00dcbertragung auf Menschen dar. Daher ist die Erforschung der Urspr\u00fcnge, Geschichte und der Rolle von Wirtsgenen in der \u00dcbertragung von Lentiviren wichtig, denn es erweitert unser Verst\u04d3ndnis und kann uns vielleicht sogar erm\u04e7glichen zuk\u00fcnftige Zoonosen voraus zu sagen.\nZellul\u04d3re Proteine, welche die Virusreplikation hemmen k\u04e7nnen, werden als antivirale Restriktionsfaktoren bezeichnet und stellen m\u04e7glicherweise Barrieren zur arten\u00fcbergreifenden \u00dcbertragung von Viren dar und verhindern damit die Entstehung neuer viraler Krankheitserreger. Ein bekannter Restriktionfaktor ist APOBEC3G (A3G). Um den antiviralen Effekt von A3G zu hemmen, exprimieren Lentiviren das Vif-Protein. Durch die Interaktion von Vif mit A3G wird dieses f\u00fcr den Abbau durch das Proteasom markiert. Wie viele andere Restriktionsfaktorgene, zeigt der A3G-Lokus in Primaten eine starke Signatur positiver Selektion. Dies kann als Beweis f\u00fcr eine kompetetive Anpassung des A3G Genlokus an Lentiviren interpretiert werden. Die Entstehung von SIVmac und Ausbr\u00fcche von AIDS in unterschiedlichen Kolonien von Makaken in den 1970er Jahren bietet heute eine einmalige Gelegenheit die Auswirkungen von spezifischen Restriktionsfaktoren im Rahmen eines entstehenden Krankheitserreges zu untersuchen.\nDiese Studie liefert einen direkten Beweis daf\u00fcr, dass A3G tats\u04d3chlich als eine Barriere f\u00fcr eine arten\u00fcbergreifende \u00dcbertragung agierte, indem es die virale Replikation unterdr\u00fcckte und fuer die Entwicklung viraler Resistenzmutationen w\u04d3hrend der Entstehung von SIVmac selektionierte. Ein wichtiges Ergebnis dieser Studie ist, dass Rhesusaffen mehrere, funktionell unterschiedliche A3G Allele besitzen. Daher erforderte die Entstehung von SIVmac und AIDS in Affen eine Anpassung von Seiten des Virus\u2019 um der Restriktion von A3G zu entgehen. Diese Arbeit beinhaltet die erste Vergleichsanalyse von A3G Polymorphismen und deren Funktion sowohl im Reservoirwirt Russmangabe also auch im Empf\u04d3ngerwirt Rhesusaffe. Zudem wurden spezifische Anpassungen der Vif Proteine aus der SIVmac Linie identifiziert, welche in der Lage sind, spezifisch A3G Allele von Rhesusaffen zu antagonisieren. Die hier pr\u04d3sentierten Ergebnisse zeigen, dass genetische Variation in einem bekannten Restriktionsfaktor verantwortlich ist f\u00fcr die Gegenadaptation des Virus im Zuge einer dokumentierten arten\u00fcbergreifenden \u00dcbertragung, und st\u00fctzen somit die Hypothese des genetischen Wettr\u00fcstens.\n2014-10-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4767\nurn:nbn:de:bvb:29-opus4-47670\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-47670\nhttps://opus4.kobv.de/opus4-fau/files/4767/Annabel%20Krupp%20Dissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4827\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:900\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:01-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nJ\u00fcdische Zahn\u00e4rzte in N\u00fcrnberg und F\u00fcrth im Nationalsozialismus. Leben und Schicksal.\nSalzner, Marina\nNationalsozialismus\nddc:900\na) Hintergrund und Ziele\nDie vorliegende Arbeit beschreibt Leben und Schicksal der j\u00fcdischen Zahn\u00e4rzte in N\u00fcrnberg und F\u00fcrth zur Zeit des Nationalsozialismus. Au\u00dferdem zeigt sie die Lebensumst\u00e4nde j\u00fcdischer Studenten und Zahn\u00e4rzte zu dieser Zeit in Hinblick auf die Entwicklung der j\u00fcdischen Gemeinden in N\u00fcrnberg und F\u00fcrth, das Leben an der Universit\u00e4t und die Entwicklungen in der Zahn\u00e4rzteschaft.\nb) Methoden\nAuf Basis archivalischer Quellen aus sechs Archiven und aktueller Forschungsliteratur wurde die Gesamtzahl der j\u00fcdischen Zahn\u00e4rzte und Dentisten in N\u00fcrnberg und F\u00fcrth ermittelt. Anschlie\u00dfend wurde f\u00fcr jede Person eine Biographie verfasst.\nc) Ergebnisse und Beobachtungen\nVon den 29 ermittelten Zahn\u00e4rzten und Zahn\u00e4rztinnen \u00fcberlebten 16 den Holocaust, neun fielen ihm zum Opfer und vier verstarben bereits in den Anfangszeiten des ,Dritten Reichs\u201b bzw. ihr genaues Schicksal ist nicht bekannt.\nd) Praktische Schlussfolgerungen\nDiese Arbeit soll einen Beitrag zu den Forschungen \u00fcber den Nationalsozialismus leisten. Au\u00dferdem ist sie ein Gedenkbuch f\u00fcr die vorgestellten Personen und somit ein Teil der Erinnerungskultur.\n2014-06-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4827\nurn:nbn:de:bvb:29-opus4-48272\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-48272\nhttps://opus4.kobv.de/opus4-fau/files/4827/DissertationSalznerMarina.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4843\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:35Lxx\nmsc:49Jxx\nmsc:49Kxx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Mathematik\nOptimal control of nonlinear nonlocal conservation laws on networks\nOptimale Steuerung von nichtlinearen und nichtlokalen Erhaltungsgleichungen auf Netzwerken\nKeimer, Alexander\nSteuerungstheorie\nOptimierung\nErhaltungsgleichungen\nddc:510\nIn this work we consider a class of nonlinear, nonlocal hyperbolic conservation laws. These equations have already been used several times to model supply chains and thus their regularity and stability properties have also been a subject of recent research.\nWe extend this class of models to so called ``multi-commodity'' models, i.e. models that simultaneously simulate the processing/transportation of several, different commodities.\nThis extension leads to a coupling of the different commodities through the so called ``work in progress'' (WIP), the sum of all goods that are momentarily localized in the supply chain. The velocities of the processing/transportation of the different goods are dependent on this WIP.\nThe resultant multi-commodity model is further extended to networks, an extension which seems - in the context of supply chains - quite natural, as more complicated supply chains are often modeled by networks.\nThe model is complemented by an objective functional, which measures distribution in the network, the fulfilling of a certain demand at the end of the network (so called sink), and the number of goods sent into the network (at the so called source). Hence the objective is for the most part a tracking type functional.\nMathematically, we consider an optimal control problem, i.e. an infinite dimensional optimization problem subject to nonlinear, nonlocal and hyperbolic pde constraints and show in the first chapters of this work the existence of an optimal control in certain Banach spaces.\nTo do this we must study the regularity of the solution of the pde under consideration, where the regularity depends on initial and boundary data. We distinguish between $L^{p}$-, $BV$- and $W^{1,p}$-regularity and prove the corresponding existence and uniqueness results for these spaces.\nFollowing, first order optimality conditions are formally computed.\nThe KKT (Karush-Kuhn-Tucker) conditions, represented by the so called adjoint equations amongst others, have some very interesting properties.\nTo be well defined, the adjoint equations demand more regularity than we could expect for the forward equations, the nonlinear, nonlocal and hyperbolic pde. But the higher regularity itself requires higher regular boundary conditions, which can lead to compatibility problems in the corners of the space time horizon.\nTherefore, we prove $BV$-regularity for forward and backward equations, and ensure that the KKT system is well defined, i.e. the regularity of the boundary values of the forward equations fits the required regularity for the boundary values of the backward equations and vice versa, such that both pdes are well defined.\nIn the final chapter we study the existence of a solution to the deduced optimality system, i.e. the system of forward and backward pdes coupled through the network topology and the boundary conditions therein.\nAs it turns out, the existence of a solution in $H^{1}$ can only be shown for sufficiently small time horizons, since otherwise the equations are no longer continuous in the corresponding Banach spaces and the typical fixed-point theorems cannot be applied.\nTo guarantee the feasibility of the solutions (for instance, the flow on the network has to be nonnegative, since it represents the flow of goods), we also have to apply a projection operator.\nThe existence of a fixed-point in $BV$ is not considered also due to problems of continuity.\nAn outlook to open problems and possible generalizations conclude this work.\nWir besch\u00e4ftigen uns in dieser Arbeit mit einer Klasse von nichtlinearen, nichtlokalen, hyperbolischen Erhaltungsgleichungen.\nDiese Gleichungen wurden bereits f\u00fcr die Simulation von Supply Chain Modellen genutzt und vielfach in Hinblick auf Regularit\u00e4t und Stabilit\u00e4t untersucht. Eine Erweiterung dieser Modellklasse auf sogenannte ``multi-commodity'' Modelle, also auf Modelle, in denen die Verarbeitung/der Transport mehrerer, verschiedener G\u00fcter simultan modelliert werden kann, liegt dementsprechend nahe und wird im Folgenden durchgef\u00fchrt.\nDabei h\u00e4ngt die Verarbeitungsgeschwindigkeit der einzelnen G\u00fcter der Supply Chain vom sogenannten WIP, ``work in progress'', ab, der Gesamtzahl aller (verschiedener) G\u00fcter, die momentan in der Supply Chain lokalisiert sind.\nDie Koppelung durch den WIP resultiert in einer Koppelung jeder einzelnen G\u00fcterklasse durch die entsprechende (Verarbeitungs-)Geschwindigkeit.\nDas so konstruierte multi-commodity Modell wird dann auf Netzwerke ausgedehnt, eine im Kontext der Supply Chains kanonische Erweiterung, lassen sich doch komplexere Supply Chains - ``Versorgungsketten'' - sehr gut durch Netzwerke repr\u00e4sentieren.\nVervollst\u00e4ndigt wird das Modell schlie\u00dflich durch eine Kostenfunktion, die das Nichterf\u00fcllen eines gewissen, vorgegebenen Bedarfs am sog. Ausgang des Netzwerkes penalisiert, w\u00e4hrend gleichzeitig auch das Senden von G\u00fctern in das Netzwerk gewichtet und bei Abweichung von einem gegeben Profil bestraft wird. Schlie\u00dflich wird die G\u00fcterverteilung im Netzwerk optimiert.\nIn mathematischer Sprache stellen wir ein Optimalsteuerungsproblem auf, d.h. ein unendlichdimensionales Optimierungsproblem mit partielle Differentialgleichungen als Nebenbedingungen, und zeigen in den ersten Kapiteln dieser Arbeit die Existenz einer Optimall\u00f6sung dieses Optimalsteuerungsproblems in bestimmten Banachr\u00e4umen.\nDazu untersuchen wir die Regularit\u00e4t der L\u00f6sung der zugrundeliegenden partiellen Differentialgleichungen in Abh\u00e4ngigkeit von Anfangs- und Randwerten. Wir unterscheiden zwischen $L^{p}$, $BV$ und $W^{1,p}$ Regularit\u00e4t und geben f\u00fcr die verschiedenen R\u00e4ume die entsprechenden Existenzresultate.\nIm Folgenden werden dann formal Ableitungsinformationen erster Ordnung berechnet.\nDie zugeh\u00f6rigen KKT-Bedingungen, unter anderem durch die sog. adjungierten Gleichungen repr\u00e4sentiert, werfen hierbei einige, interessante Probleme auf.\nSo ben\u00f6tigen die adjungierten Gleichungen eine h\u00f6here Regularit\u00e4t als man f\u00fcr die Vorw\u00e4rtsgleichungen erwarten w\u00fcrde. Die h\u00f6here Regularit\u00e4t verlangt nach h\u00f6her regul\u00e4ren Randbedingungen, die wiederum Kompatibilit\u00e4tsprobleme an den Ecken des Orts-Zeit-Gebietes generieren.\nWir nutzen deshalb $BV$ L\u00f6sungen f\u00fcr die Vorw\u00e4rts- und R\u00fcckw\u00e4rtsgleichungen und erreichen damit immerhin, dass das entstandene KKT-System wohldefiniert ist, d.h. dass die Regularit\u00e4t der Randwerte der Vorw\u00e4rtsgleichungen zur Regularit\u00e4t der R\u00fcckw\u00e4rtsgleichungen passen und umgekehrt und die zugeh\u00f6rigen Differentialgleichungen wohldefiniert sind.\nIm letzten Kapitel besch\u00e4ftigen wir uns dann mit der Existenz einer L\u00f6sung des hergeleiteten Optimalit\u00e4tssystems.\nWie sich herausstellt, kann die Existenz einer L\u00f6sung in $H^{1}$ nur f\u00fcr hinreichend kleinen Zeithorizont gezeigt werden, da die beteiligten Gleichungen ansonsten nicht mehr ``stetig'' in den zugrundeliegenden Banachr\u00e4umen sind und die typischen Fixpunkts\u00e4tze (Banach, Schauder) dementsprechend nicht mehr anwendbar.\nUm die Zul\u00e4ssigkeit der optimalen L\u00f6sung zu garantieren (so hat die L\u00f6sung z.B. nicht-negativ zu sein, repr\u00e4sentiert sie doch den ``G\u00fcterfluss'' in einem Netzwerk), m\u00fcssen wir im allgemeinen Fall eine Projektion nachschalten.\nDie Existenz eines Fixpunktes in $BV$ wird nicht betrachtet, da auch dann auf gewisse Stetigkeitseigenschaften verzichtet werden m\u00fcsste, die f\u00fcr die Anwendung der \u00fcblichen Fixpunkts\u00e4tze (Banach, Schauder) unumg\u00e4nglich w\u00e4re.\nEin Ausblick auf offene Fragen und m\u00f6gliche weitere Verallgemeinerungen schlie\u00dft die Arbeit ab.\n2014-06-30\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4843\nurn:nbn:de:bvb:29-opus4-48439\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-48439\nhttps://opus4.kobv.de/opus4-fau/files/4843/diss_keimer.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4983\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:I.\npacs\npacs:06.20.-f\nmsc\nmsc:93-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nBayes-Filter zur Genauigkeitsverbesserung und Unsicherheitsermittlung von dynamischen Koordinatenmessungen\nBayesian filter for improved accuracy and uncertainty estimation of dynamic coordinate measurements\nGarcia, Elmar\nMesstechnik, Koordinatenmesstechnik, Systemtheorie\nddc:620\nIn der vorliegenden Arbeit werden neue Methoden und Verfahren der bayesschen Sch\u00e4tzung f\u00fcr die Koordinatenmesstechnik entwickelt, umgesetzt und validiert. Messungen sind aufgrund allgegenw\u00e4rtig wirkender bekannter und unbekannter Einfl\u00fcsse (St\u00f6rungen oder Rauschprozesse) stets abweichungsbehaftet und die tats\u00e4chlichen Koordinaten nicht exakt bestimmbar. Der Messvorgang liefert Informationen \u00fcber die Position in Form von Beobachtungen, die allerdings eine abweichungsbehaftte Abbildung der tats\u00e4chlichen Position repr\u00e4sentieren. Da bei dieser Transformation Informationen verloren gehen, ist eine exakte, eindeutige Berechnung der tats\u00e4chlichen Position aus den vom Messprozess generierten Beobachtungen nicht m\u00f6glich. Sie kann jedoch mathematisch gesch\u00e4tzt werden, da der funktionale Zusammenhang zwischen den tats\u00e4chlichen Objektkoordinaten und den Beobachtungen erhalten bleibt. Das Ziel dieser Arbeit ist die Implementierung und Bereitstellung von Algorithmen zur dynamischen, messdatenbasierten und sensor\u00fcbergreifenden Genauigkeitsverbesserung und Messunsicherheitsermittlung von Koordinatenmessungen. Die L\u00f6sung erfordert vom Anwender weder das Aufstellen einer Modellgleichung des Messprozesses, noch a priori Verteilungsannahmen, erlaubt aber deren Spezifikation bei Bedarf. Durch diese probabilistische Messdatenverarbeitung und -auswertung k\u00f6nnen Abweichungen in den Messdaten reduziert und gleichzeitig Unsicherheitsaussagen abgeleitet werden.\nIn this work, new methods and algorithms of Bayesian estimation are developed, implemented and validated for coordinate metrology. Measurements are due to ubiquitous known and unknown influences (disturbances or noise sources) always inaccurate and thus the actual coordinates can not be determined exactly. The measurement process provides information about the position in form of observations, however this is an imperfect mapping of the true position. Since information is lost in this transformation, the true object coordinates can not be computed completely from the obtained observations. But it is possible to estimate the measurand, because the functional relationship between the true object coordinates and the observations remains.\nThe objective of this work is the implementation and provision of algorithms for dynamic, measurement data\u2013based and sensor\u2013independent accuracy improvement and measurement uncertainty evaluation of coordinate measurements. The solution requires neither an explicit modelling of the measurement process nor a priori distribution assumption, but allows the specification if necessary. This probabilistic measurement data processing and evaluation allows to reduce the deviations in measured data and to deduce measurement uncertainty evaluations.\n2014-07-24\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4983\nurn:nbn:de:bvb:29-opus4-49833\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-49833\nhttps://opus4.kobv.de/opus4-fau/files/4983/diss_pdf-a.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5001\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:37-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nContributions in Surface Dynamics\nBeitr\u00e4ge zur zweidimensionalen Dynamik\nPasseggi, Alejandro\nMathematics\nddc:510\nIn this thesis we present a serie of results concerning dynamics in surfaces given by homeomorphisms. The results are related to the study of minimal sets for the dynamics and the rotation theory on the two dimensional torus.\nIm Rahmen dieser Dissertation werden verschiedene Beitr\u00e4ge zur Dynamik auf zweidimensionalen Mannigfaltigkeiten vorgestellt. Im Zentrum stehen dabei insbesondere die Strukur minimaler Mengen und Rotationstheorie f\u00fcr Hom\u00f6omorphismen des zweidimensionalen Torus.\n2014-07-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5001\nurn:nbn:de:bvb:29-opus4-50012\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50012\nhttps://opus4.kobv.de/opus4-fau/files/5001/AlejandroPasseggiDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5062\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\npacs\npacs:47.11.Fg\npacs:47.55.dr\nmsc\nmsc:65N22\nmsc:76D45\nmsc:76M10\nmsc:76M30\nmsc:76R10\nmsc:76T99\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nSimulation of single drops with variable interfacial tension\nSimulation von Einzeltropfen unter dem Einfluss variabler Grenzfl\u00e4chenspannung\nB\u00e4umler, Kathrin\nGrenzfl\u00e4chenspannung\nFinite-Elemente-Methode\nMarangoni-Effekt\nNavier-Stokes-Gleichung\nDirekte numerische Simulation\nKonvektions-Diffusionsgleichung\nGrenzfl\u00e4che\nNumerische Mathematik\nTropfen\nTropfenform\nComputersimulation\nValidierung\nZweiphasenstr\u00f6mung\nddc:519\nIn der vorliegenden Arbeit wird eine numerische Unterraumprojektionsmethode zur Behandlung von Zweiphasenstr\u00f6mungen mit gleichzeitigem Stofftransport und Marangonikonvektion unter Ber\u00fccksichtigung deformierbarer Grenzfl\u00e4chen vorgestellt. Das Wissen \u00fcber Kenngr\u00f6\u00dfen wie Aufstiegsgeschwindigkeiten und Stoff\u00fcbergangskoeffizienten von Einzeltropfen ist beispielsweise notwendig zur Planung von Fl\u00fcssig/fl\u00fcssig\u2013Extraktionsprozessen, und damit eine zentrale Fragestellung in der Verfahrenstechnik. Die komplexen Grenzfl\u00e4cheneffekte am Einzeltropfen (durch Marangonikonvektion sind der Stofftransport und die Fluidynamik nichtlinear gekoppelt) machen deren Untersuchung auch aus numerischer Sicht interessant und erfordern den Einsatz neuer numerischer Methoden.\nZun\u00e4chst wird ein \u2013im Hinblick auf die anschlie\u00dfende Diskretisierung\u2013 geeignetes mathematisches Modell des umstr\u00f6mten Einzeltropfens entwickelt. Dieses Modell basiert auf der Verwendung spezieller Unterr\u00e4ume, einer zweiphasenspezifischen Entdimensionalisierung, sowie eines beschleunigten Koordinatensystems, das dem Aufstieg des Einzeltropfens folgt. Basierend auf der variationellen Formulierung wird eine Energieabsch\u00e4tzung des umstr\u00f6mten Einzeltropfens, die Wohlgestelltheit des Oseen\u2013Problems (das Oseen\u2013Problem ist ein erweitertes Stokes\u2013Problem, das durch Zeitdiskretisierung und Linearisierung der Navier\u2013Stokes Gleichungen entsteht), sowie die Inf\u2013sup Stabilit \u0308at nachgewiesen.\nKernbestandteil der Unterraumprojektionsmethode ist die Realisierung der Grenzfl\u00e4chenbedingungen durch den Einsatz von Projektionen, die je nach gew\u00fcnschter Anwendung (starre Kugel, sph\u00e4rischer Tropfen, deformierbarer Tropfen, sph\u00e4rische \u201cStagnant cap\u201d \u2013 ein Spezialfall in der Behandlung tensidbehafteter Systeme) zugeschaltet werden k\u00f6nnen. Die hohe Approximationsg\u00fcte in der Darstellung komplexer Grenzfl\u00e4chenph\u00e4nomene durch die Anwendung einer \u201caligned mesh method\u201d wird durch unstetige Druck\u2013 und Konzentrations\u2013 Ansatzfunktionen weiter erh\u00f6ht. Insbesondere die Darstellung eines unstetigen Druckes ist unerl\u00e4sslich zur Vermeidung der sogenannten \u201cspurious oscillations\u201d. F\u00fcr die Unterraumprojektionsmethode kann leicht die Inf\u2013sup Stabilit\u00e4tsbedingung f\u00fcr den Fall der Zweiphasenstr\u00f6mung nachgewiesen werden.\nIn der Simulation umstr\u00f6mter Einzeltropfen ist die numerische Behandlung der gekoppelten Impuls\u2013 und Stofftransportgleichungen durch konzentrationsinduzierte Marangonikonvektion hervorzuheben. Hierbei wird von der bisherigen Methode zur Implementierung der Grenzfl\u00e4cheneffekte abgewichen, und auf die Darstellung der Grenzfl\u00e4chenspannungen in Divergenzform zur\u00fcckgegriffen. Vorteilhaft bei der Behandlung in Divergenzform ist die Formulierung der Schubspannungsbilanz ohne tangentiale Ableitungen der Grenzfl\u00e4chenspannung.\nIn der Praxis treten h\u00e4ufig konvektionsdominante Systeme auf, wie beispielsweise das Stoffsystem Toluol/Aceton/Wasser. Hierbei ergeben sich im Zusammenhang mit der Finiten Elemente Methode numerische Schwierigkeiten. Wir untersuchen residuenbasierte Stabilisierungsmethoden im Hinblick auf ihre Anwendbarkeit in der Simulation von Zweiphasenstr\u00f6mungen. Desweiteren werden allgemeine Aspekte der Implementierung, wie beispielsweise die Parallelisierung mit Open MP, behandelt.\nDie numerische Methode wird in verschiedenen achsensymmetrischen Anwendungen validiert, beispielsweise in der Simulation bin\u00e4rer Systeme (Einzeltropfen ohne zus\u00e4tzlichen Stofftransport) mit niedriger, mittlerer, und hoher Grenzfl\u00e4chenspannung im Bereich nicht oszillierender Tropfen. Aufstiegsgeschwindigkeiten, Widerstandsbeiwerte sowie Tropfendeformation zeigen eine exzellente \u00dcbereinstimmung mit experimentellen Daten, beispielsweise weichen Endaufstiegsgeschwindigkeiten um weniger als 4% von den experimentellen Daten ab. In der Simulation von starren Kugeln, sph\u00e4rischen Tropfen, sph\u00e4rischen Tropfen mit Stagnant caps und thermokapillarer Str\u00f6mung kann ebenfalls eine gute \u00dcbereinstimmung mit bestehenden Korrelationen (oder \u2013 soweit vorhanden \u2013 analytischen L\u00f6sungen) aus der Literatur gezeigt werden. Die numerische Simulation der Aufstiegsgeschwindigkeit der thermokapillaren Str\u00f6mung weicht um weniger als 0.2% von der Vorhersage von Young, Goldstein und Block (1959) ab.\nEine qualitative Untersuchung des Effekts der variablen Grenzfl\u00e4chensspannung wird im terti\u00e4ren System Toluol/Aceton/Wasser durchgef\u00fchrt. Eine quantitative Validierung der Ergebnisse ist hier nicht m\u00f6glich. Ursache ist die ausgepr\u00e4gte Konvektionsdominanz des betrachteten Systems, sowie die Annahme von Rotationssymmetrie trotz der inh\u00e4rent dreidimensionalen Effekte der Marangonikonvektion. Dennoch zeigen die numerischen Simulationen, dass qualitativ der Einfluss der konzentrationsinduzierten Marangonikonvektion dargestellt werden kann.\nIn this thesis, a numerical method for the simulation of two\u2013phase flow problems with a deformable interface and coupled species transport is investigated. The coupling of species concentration and fluid dynamics originates from a concentration\u2013dependent interfacial tension coefficient \u2013 an effect known as concentration\u2013induced Marangoni convection.\nFor this purpose, a mathematical model of a single drop was formulated using subspaces of the conventional function spaces, a problem\u2013specific nondimensionalization, an accelerated frame of reference, and a divergence formulation of interfacial stresses.\nThe numerical method is based on a finite element method within an arbitrary Lagrangian\u2013 Eulerian framework. This class of methods results in an explicit interface representation, and forms the basis of the subspace projection method, a novel method for the implementation of various interface conditions in two\u2013phase flow problems via problem\u2013specific projections. Interface conditions that are investigated in this thesis comprise the flow around a rigid body, the simulation of spherically\u2013shaped fluid particles, interface conditions of deformable drops with and without Marangoni convection, and the simulation of spherical stagnant caps \u2013 a limiting case observed in single drop flow under the presence of surfactants.\nAn essential feature of the subspace projection method is its ability to represent discontinuous functions, enabled by using a computational grid with doubled interfacial nodes. Spurious oscillations \u2013often observed in two\u2013phase flow problems where the pressure is approximated by globally continuous functions\u2013 are considerably reduced.\nConcentration\u2013induced Marangoni convection (resulting from a non\u2013vanishing tangential gradient of interfacial stresses) is a challenging application in the numerical simulation of two\u2013 phase flows with a deforming interface. The implementation of interfacial stresses is based on a divergence formulation. This formulation accounts for normal and tangential stresses without the necessity of approximating surface derivatives of the interfacial tension.\nA validation of the numerical method is performed for different axisymmetric single drop flow applications.\nDifferent binary systems (single, buoyant, deformable drops with constant interfacial tension) are numerically investigated in terms of terminal rise velocities, drag coefficients, and deformation, comprising the range from low to high interfacial tension coefficients and drop diameters below the onset of shape oscillations. An excellent agreement of simulation results and experimental data is obtained. In case of terminal rise velocities, the deviation is below 4%.\nSimulations of ternary systems (binary systems with species transport) are performed with and without Marangoni convection. In case of high Peclet numbers, finite element simulations often suffer from instabilities due to convection dominance. Stabilizing methods are investigated with special regard to two\u2013phase flow applications.\nSimulations of fluid dynamics and species transport in single drop flow applications like rigid particles, spherically shaped drops, spherical stagnant caps, and thermocapillary migration, show good to excellent agreement to correlations from literature in systems with moderate Peclet numbers. The deviation of the thermocapillary rise velocity is below 0.2% to the prediction by Young, Goldstein and Block (1959).\nThe ternary system toluene/acetone/water with concentration\u2013induced Marangoni convection is numerically investigated and the results are compared to experimental data. In spite of the convection dominance of the corresponding species transport problem and the assumption of axisymmetry despite the inherent three\u2013dimensional nature of the Marangoni effect, a reasonable qualitative agreement was achieved.\n2014-08-14\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5062\nurn:nbn:de:bvb:29-opus4-50629\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50629\nhttps://opus4.kobv.de/opus4-fau/files/5062/Dissertation_Baeumler.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5068\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nddc:510\nddc:530\nddc:620\nccs\nccs:I.6.3\npacs\npacs:41.20.Jb\npacs:42.25.Fx\npacs:42.30.Kq\nmsc\nmsc:68T05\nmsc:78-04\nmsc:90-08\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Uebergreifend_ohneAngabe\ninstitutes:Tech_Informatik\nArtificial Evolution for the Optimization of Lithographic Process Conditions\nK\u00fcnstliche Evolution f\u00fcr die Optimierung von Lithographischen Prozessbedingungen\nF\u00fchner, Tim\nOptimierung\nPhotolithographie\nMehrkriterielle Optimierung\nEvolution\u00e4rer Algorithmus\nMemetischer Algorithmus\nIntegrierte Schaltung\nddc:004\nddc:510\nddc:530\nddc:620\nMiniaturization is a driving force both for the performance and for cost reductions of semiconductor devices. It is therefore carried on at an enormous pace. Gordon Moore proposed and later refined an estimation stating that the minimization of costs would lead to a doubling of the density of integrated circuits every two years. And in fact, this time scale---known as Moore's law---is still aspired at by the major players in the industry. Photolithography, one of the key process steps, has to keep up with this pace. In the past, the introduction of new technologies, including a smaller wavelength of the illumination system or higher numerical apertures (NA) of the projector, has led to a relatively straightforward downscaling approach. Today, optical lithography is confined to argon fluoride excimer lasers with a wavelength of 193 nanometers and an NA of 1.35. The introduction of next generation lithography approaches such as extreme ultraviolet lithography have been delayed and will not be applicable until several years from now. Further scaling hence leads to dramatically decreases process margins since patterns with dimensions of only a fraction of the wavelength have to be lithographically created.\nIn this work, computational methods are devised that are suited to drastically improve process conditions and hence to push resolution beyond former limitations. The lithographic process can be broadly grouped into the stepper components: the illumination system, the photomask, the projection system and the wafer stack. As shown in this dissertation, each element exhibits a number of parameters that can be subjected to optimization. To actually enhance resolution, however, a rigorous simulation and computation regime has to be established. The individual ingredients are discussed in detail in this thesis.\nAccordingly, the models required to describe the lithography process are introduced and discussed. It is shown that the numerical and algorithmic implementation can be regarded as a compromise between exactness and computation time. Both are critical to obtain predictive, yet feasible approaches. Another complication is the multi-scale and multi-physics nature of the first principle process models. Although it is sometimes possible to derive individual optimization-tailored, reduced models, such an approach is often prohibitive for a concise co-optimization of multiple aspects.\nIn this work, we thus examine an approach that allows for a direct integration of first principle models. We investigate the use of evolutionary algorithms (EAs) for that purpose. These types of algorithms can be characterized as flexible optimization approaches that mimic evolutionary mechanisms such as selection, recombination and mutation. Many variants of related techniques exist, of which a number are considered in this dissertation. One chapter of this thesis is dedicated to the discussion of different representations and genetic operators, including motivations of the choices made for the following studies.\nThe lithographic process is characterized not only by a large number of parameters but can also be evaluated by a wide range of criteria, some of which may be conflicting or incommensurable---such as figures of merits like performance and manufacturability. We therefore apply a multi-objective genetic algorithm (GA) that is specifically tailored to identifying ideal compromise solutions. The characteristics of multi-objective optimization, especially when performed with evolutionary algorithms, are discussed in this thesis.\nThere is no such thing as a universal optimizer. EAs, for example, can be considered highly flexible, but they fail to intensively exploit local information. In an attempt to get the best of both worlds, we combine evolutionary with local search routines. We thoroughly discuss our approach to these hybrid techniques and present a number of benchmark tests that demonstrate their successful applications.\nThe majority of optimization problems in lithography are characterized by computationally expensive fitness evaluations. The reduction of the number of candidate solutions is therefore critical to maintain a feasible optimization procedure. To this end, we devised a function approximation approach based on an artificial neural network. Specifically, the GA population constitutes the training pattern for the network. The resulted output is the approximated fitness function. While the global search using the GA is still conducted on the exact search space, the local search is carried out on this approximation, leading to a much reduced runtime. The efficiency and feasibility of this approach is demonstrated by a number of benchmark tests.\nThe algorithms, frameworks and programs developed in the scope of this work are deployed as software modules that are available through the computational lithography environment Dr.LiTHO of the Fraunhofer IISB. The general software structure is briefly discussed. In order to achieve feasible optimization runtimes, rigorous distribution and parallelization techniques need to be employed. For this dissertation, a number of different approaches are devised and discussed in this thesis.\nA variety of application examples demonstrate the benefits of the devised methods. In a first set of examples, source/mask optimization problems are formulated and solved. In contrast to related work, which is mainly aimed at developing models that are specifically tailored to the underlying optimization routines, the direct approach proposed here is capable of directly employing models that are typically used in lithography simulation. A multitude of results using different problem representations is presented. Additional model options including mask topography effects are demonstrated. It is shown that the approach is not restricted to simplistic aerial image-based evaluations but is able to take the process windows and thin-film effects into account. Moreover, an extension to future resolution enhancement techniques, for example, constructively using projector aberrations, is also demonstrated.\nIn another example series, three-dimensional mask optimizations are performed. There, the topography including the materials of the photomask absorber are subjected to optimization. Drastically improved configurations compared to both standard optical and EUV absorbers under various illumination conditions are obtained. In order to cover all aspects of the lithography process, the last section of this thesis is devoted to the optimization of the wafer stack. As an example, the anti-reflective coating applied at the bottom of the resist to reduce standing waves in the resist profile is optimized. Different configurations including single and bi-layer coating systems are examined and optimized for, especially for double patterning applications. Significant improvements in comparison to standard stacks are shown and discussed.\nThe thesis finally concludes with a discussion on the different optimization strategies and the optimization and simulation infrastructure developed for this work. Advantages and challenges of the methodology are highlighted and future directions and potentials are demonstrated.\nMiniaturisierung ist sowohl f\u00fcr die Leistungssteigerung als auch f\u00fcr die Kostensenkung von Halbleiterbauelementen von gro\u00dfer Bedeutung und wird daher mit einer enorm hohen Geschwindigkeit betrieben. Gordon Moore leitete daraus eine Sch\u00e4tzung ab, die besagt, dass die Hersteller gezwungen seien, etwa alle zwei Jahre die Dichte der integrierten Schaltungen zu verdoppeln. Und tats\u00e4chlich verfolgen die Hauptakteure der Industrie dieses Ziel -- bekannt als Moore's Law -- noch heute. Photolithographie, einer der wichtigsten Prozessschritte, hat sich diesem Ziel unterzuordnen.\nIn der Vergangenheit stellte die Einf\u00fchrung neuer Technologiestufen, einschlie\u00dflich kleinerer Wellenl\u00e4ngen der Beleuchtungssysteme oder h\u00f6here numerische Aperturen (NA) der Projektionssysteme, einen relativ einfachen Ansatz dar, Schaltungsstrukturen zu verkleinern. Heute allerdings muss sich die optische Lithographie auf den Einsatz von Argon-basierten Excimer-Laser mit einer Wellenl\u00e4nge von 193 Nanometer und einer NA von 1,35 beschr\u00e4nken. Die Einf\u00fchrung neuer Lithographie-Generationen, beispielsweiser unter Ausnutzung extrem ultravioletten (EUV) Lichtes, verz\u00f6gert sich, so dass mit ihr erst in mehreren Jahren zu rechnen ist. Eine weitere Verkleinerung der Bauelemente f\u00fchrt so zu einer deutlichen Versch\u00e4rfung der Anforderungen an den lithographischen Prozess, da Strukturen mit Abmessungen eines Bruchteiles der zur Verf\u00fcgung stehenden Wellenl\u00e4nge abgebildet werden m\u00fcssen.\nIn dieser Arbeit werden deshalb numerische Methoden entwickelt, die geeignet sind, Prozessbedingungen signifikant zu verbessern und damit Aufl\u00f6sungen jenseits vorheriger Beschr\u00e4nkungen zu erzielen. Der Lithographieprozess kann in folgende Komponenten unterteilt werden: das Beleuchtungssystem, die Photomaske, das Projektionssystem und das Schichtsystem auf der Halbleiterscheibe. Wie in dieser Dissertation gezeigt wird, weist jede dieser Komponenten eine gro\u00dfe Anzahl optimierbarer Parameter auf. Um tats\u00e4chlich eine Verbesserung der Aufl\u00f6sung zu erzielen, ist jedoch der Einsatz umfassender Simulationswerkzeuge unabdingbar. Deren einzelne Bestandteile werden in dieser Arbeit er\u00f6rtert.\nSo werden die ben\u00f6tigten Modelle, die den Lithographieprozess beschreiben, vorgestellt und diskutiert. Es wird gezeigt, dass die numerische und algorithmische Umsetzung aus einem Kompromiss zwischen Genauigkeit und Rechenzeit besteht. Beide Kriterien sind entscheidend bei der Entwicklung eines pr\u00e4diktiven und praktikablen Ansatzes. Eine weitere Komplikation ergibt sich aus der Multi-Skalen- und Multi-Physik-Eigenschaft pr\u00e4diktiver Prozessmodelle. Obwohl es bisweilen m\u00f6glich ist, reduzierte Modelle f\u00fcr ein spezielles Optimierungsproblem zu entwickeln, eignet sich ein solches Vorgehen im Allgemeinen nicht f\u00fcr die gleichzeitige Optimierung mehrerer Prozessaspekte.\nIn dieser Arbeit wird daher ein Ansatz untersucht, der die direkte Nutzung exakter Modelle erlaubt. Als Optimierungsverfahren werden dabei evolution\u00e4re Algorithmen (EA) entwickelt und verwendet. EAs bezeichnen probabilistische Verfahren, die evolution\u00e4re Mechanismen wie Selektion, Rekombination und Mutation imitieren und sich durch ein hohes Ma\u00df an Flexibilit\u00e4t auszeichnen. Da es zahlreiche EA-Varianten gibt, widmet sich ein Kapitel dieser Arbeit der Diskussion und Untersuchung verschiedener Darstellungsoptionen und genetischer Operatoren. Dabei wird insbesondere die f\u00fcr diese Arbeit getroffene Auswahl motiviert.\nDer lithographische Prozess umfasst nicht nur eine Vielzahl an Parametern, sondern bedarf auch der Bewertung hinsichtlich verschiedener Kriterien, von denen nicht wenige wechselseitig unvereinbar oder unvergleichbar sind. So sind beispielsweise Herstellbarkeit und Leistungsf\u00e4higkeit im Allgemeinen inkommensurabel. Daher wird ein multikriterieller genetischer Algorithmus (GA), der speziell auf die Suche nach Kompromissl\u00f6sungen zugeschnitten ist, implementiert und untersucht. Die Eigenschaften von Mehrzieloptimierung, insbesondere im Zusammenhang mit evolution\u00e4ren Algorithmen, werden in dieser Arbeit eingehend diskutiert.\nGenau so wenig wie andere Optimierer k\u00f6nnen EAs als universell bezeichnet werden: Sie zeichnen sich zwar durch hohe Flexibilit\u00e4t aus, sind aber anderen Verfahren bei der intensiven Ausnutzung lokaler Informationen oft unterlegen. Eine Kombination evolution\u00e4rer und lokaler Suchalgorithmen bietet sich deshalb an. Ein entsprechendes hybrides Verfahren wird in dieser Arbeit entwickelt, und dessen Leistungsf\u00e4higkeit wird mit Hilfe einer Reihe von Benchmark-Funktionen demonstriert.\nDie Mehrzahl lithographischer Optimierungsprobleme ist durch rechenintensive G\u00fcteauswertungen charakterisiert. Die Zahl der Auswertungen muss daher auf ein Minimum reduziert werden. Es wird zu dem Zweck ein Ansatz verfolgt, bei dem die Fitnessfunktion durch eine deutlich schneller auszuwertende Ersatzfunktion gen\u00e4hert wird. Dabei kommt ein k\u00fcnstliches neuronales Netz zum Einsatz, das die durch den GA erzeugte Population aus L\u00f6sungskandidaten als Trainingsinstanzen nutzt, um so ein Modell der Fitnessfunktion zu erzeugen. Dieses Modell wird dann f\u00fcr eine intensive lokale Suche verwendet, w\u00e4hrend die globale GA-Suche auf der urspr\u00fcnglichen, exakten Funktion durchgef\u00fchrt wird. Die Effizienz und die Machbarkeit dieses Ansatzes wird an einer Reihe von Vergleichstests nachgewiesen.\nDie f\u00fcr diese Arbeit entwickelten Algorithmen, Frameworks und Programme stehen im Rahmen der Fraunhofer IISB Lithographiesimulationsumgebung Dr.LiTHO als Software-Module zur Verf\u00fcgung. Der prinzipielle Aufbau der Recheninfrastruktur wird kurz diskutiert, insbesondere im Hinblick auf die entwickelten und verwendeten Verteilungs- und Parallelisierungsverfahren, ohne die praktikable Optimierungsl\u00e4ufe aufgrund der hohen Rechenzeiten nicht m\u00f6glich w\u00e4ren.\nEine Vielzahl von Anwendungsbeispielen zeigt die Vorteile der entwickelten Methoden. In einer Studie werden Beleuchtungsquellen/Photomasken-Optimierungsprobleme formuliert und gel\u00f6st. Im Gegensatz zu vergleichbaren Arbeiten, die zumeist auf vereinfachten, effizienten Modellen beruhen, wird hier ein direkter Ansatz verfolgt, der es erlaubt, exakte, in der Lithographiesimulation \u00fcbliche Modelle zu verwenden. Mehrere Darstellungsvarianten werden vorgestellt und anhand zahlreicher Ergebnisse diskutiert. Die Flexibilit\u00e4t des Ansatzes wird unter anderem durch die Ber\u00fccksichtigung von Maskentopographieeffekten demonstriert. Es wird ferner gezeigt, dass das Verfahren nicht auf die Auswertung von Luftbildern beschr\u00e4nkt ist, sondern auch andere Komponenten wie Prozessfenster oder D\u00fcnnfilmeffekte einbeziehen kann. Weitere Ergebnisse demonstrieren die Erweiterbarkeit des Verfahrens auf zuk\u00fcnftige Techniken zur Verbesserung der Aufl\u00f6sung, zum Beispiel, der Ausnutzung der Projektor-Aberrationskontrolle.\nZiel einer weiteren Reihe von Simulationsexperimenten ist die dreidimensionale Maskenoptimierung, in der zus\u00e4tzlich zur Quellen/Maken-Optimierung auch die Topographie und die Materialeigenschaften der Photomaske optimiert werden. Dabei k\u00f6nnen deutliche Verbesserungen im Vergleich zu Standardkonfigurationen erzielt werden. Optimierungsergebnisse sowohl f\u00fcr optische als auch f\u00fcr EUV-Lithographie werden pr\u00e4sentiert und diskutiert. Um alle Aspekte des Lithographieprozesses abzudecken, befasst sich der letzte Abschnitt der Arbeit mit dem Schichtsystem auf der Halbleiterscheibe. Als Beispiel wird die antireflektive Beschichtung auf der Unterseite des Photolackes optimiert. Diese Beschichtung wird eingesetzt, um eine Interferenz zwischen einfallendem und r\u00fcckreflektiertem Licht zu verhindern, die zu stehende Wellen f\u00fchrt. Verschiedene Anordnungen, darunter Einzel- und Zweischichtsysteme, werden untersucht und verbessert. Ziel dieser Studie ist es insbesondere, die Ver\u00e4nderungen des Schichtsystems unter heute h\u00e4ufig verwendeten Mehrfachbelichtungsverfahren exakt zu beschreiben und zu verbessern.\nDie Dissertation schlie\u00dft mit einer Diskussion sowohl der verschiedenen Optimierungsstrategien als auch der f\u00fcr diese Arbeit entwickelten Optimierungs- und Simulationsinfrastruktur. Vor- und Nachteile der Methodik werden hervorgehoben und m\u00f6gliche zuk\u00fcnftige Anwendungen und Erweiterungen vorgestellt.\n2014-08-15\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5068\nurn:nbn:de:bvb:29-opus4-50689\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50689\nhttps://opus4.kobv.de/opus4-fau/files/5068/DissFuehner.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5165\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:920\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:01-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Phil\nDer andere Kepler - Vom Aufstieg eines fr\u00fchneuzeitlichen Gelehrten mit Hilfe der Astrologie\nBauer, Katrin\nAstrologie\nddc:920\nDie Dissertation untersucht den Nutzen, den Johannes Kepler im Laufe seiner Karriere aus der Bearbeitung astrologischer Inhalte zog. Die Arbeit geht dabei in drei Schritten vor. Zun\u00e4chst wird analysiert, welchen Stellenwert astrologische Inhalte innerhalb von Kontaktaufnahmen f\u00fcr den Gelehrten spielten. Der zweite Abschnitt wendet sich der Erledigung von Auftragsarbeiten zu, wobei wiederum astrologische Fragestellungen bevorzugt in den Blick genommen werden. In einem dritten Hauptteil wird schlie\u00dflich die Positionierung Keplers in der wissenschaftlichen Welt thematisiert. Von besonderem Interesse sind dabei Rezensionen und gelehrter Austausch sowie eigene wissenschaftliche Werke.\nThe main topic of this work is Johannes Kepler's use of astrology for his own carreer. It contains three main parts. In the first part it is analized how Kepler commenced his contacts. The second main chapter gives an insight into the duties of an early modern mathematician. The last part emphasizes the importance of astrology for Kepler's standing in his coeval scientific field.\n2014-09-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5165\nurn:nbn:de:bvb:29-opus4-51657\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-51657\nhttps://opus4.kobv.de/opus4-fau/files/5165/DissertationKatrinBauer.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5339\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\nmsc\nmsc:90C11\nmsc:90C29\nmsc:90C33\nmsc:90C35\nmsc:91B15\nmsc:91B26\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Mathematik\nAuctions in Exchange Trading Systems: Modeling Techniques and Algorithms\nAuktionen in B\u00f6rsenhandelssystemen: Modellierungstechniken und Algorithmen\nM\u00fcller, Johannes Christian\nGemischt-ganzzahlige Optimierung\nKomplementarit\u00e4tsproblem\nZielprogrammierung\nAuktion\nAuktionspreis\nMarktmodell\nTerminmarkt\nStrommarkt\nErdgasmarkt\nddc:519\nModern exchange trading systems facilitate the market participants to electronically submit detailed information about their buy and sell preferences. With the help of so-called combinatorial orders, they can buy or sell several different items at once.\nTradable items are, for instance, company shares, futures contracts, electricity, or natural gas. The most basic combinatorial order is the fill-or-kill order in stock exchanges. This order is either executed entirely or not executed at all. That is, the participant either buys or sells all requested or offered items at once or none of them. Combinatorial orders are also available in electricity exchanges (e.g., block orders) and in futures exchanges (e.g., futures combination order).\nAn exchange usually determines a single price for each class of tradable items. For example, one price is determined for futures contracts of type A and a further price is determined for futures contracts of type B. In the absence of combinatorial orders, these prices can be determined separately. However, if participants submit combinatorial orders that comprise futures contracts of type A and B, then the prices must be determined simultaneously. This is necessary in order to make the prices for combinatorial orders consistent with the ones of the underlying futures contracts.\nThe first part of this work presents general modeling techniques and algorithms. For instance, we present a framework model for auctions in which the prices for several classes of items are determined simultaneously. Roughly speaking, we perform several auctions in parallel and integrate the combinatorial orders in such a way that they couple the parallel auctions among each other. For a certain class of combinatorial auctions this work provides model formulations that are solvable in polynomial time (e.g., the futures opening auction). For the general case, where the problem is NP-hard, we provide model formulations and algorithms which are fast enough such that they can be used in practice (e.g., in the European day-ahead electricity auction).\nIn the second part of this work, we present specific auction models for futures opening auctions, European day-ahead electricity auctions, and European natural gas auctions. Thereby we apply our general models and algorithms, go into problem specific details, and perform numerical tests on real-world instances.\nModerne B\u00f6rsenhandelssysteme erm\u00f6glichen es den Marktteilnehmern, ihre Kauf- und Verkaufspr\u00e4ferenzen in detaillierter Form elektronisch zu \u00fcbermitteln. Mit Hilfe von sogenannten kombinatorischen Geboten k\u00f6nnen beispielsweise mehrere verschiedene G\u00fcter simultan gekauft oder verkauft werden.\nHandelbare G\u00fcter sind zum Beispiel Aktien, Futures-Kontrakte, Strom oder Erdgas. Das einfachste kombinatorische Gebot ist die sogenannte Fill-or-kill-Order an der Wertpapierb\u00f6rse. Dieses Gebot wird entweder vollst\u00e4ndig oder gar nicht ausgef\u00fchrt. Das bedeutet, dass ein Teilnehmer entweder alle nachgefragten oder angebotenen Wertpapiere am St\u00fcck kauft oder verkauft, oder kein einziges. Kombinatorische Gebote werden auch an Stromb\u00f6rsen (z.B. das Blockgebot) und Terminb\u00f6rsen (z.B. die Futures-Kombinationsorder) angeboten.\nNormalerweise bestimmt eine B\u00f6rse einen Preis f\u00fcr jede Klasse von handelbaren G\u00fctern. Zum Beispiel wird ein Preis f\u00fcr Futures-Kontrakte vom Typ A und ein Preis f\u00fcr Futures-Kontrakte vom Typ B ermittelt. Falls keine kombinatorischen Gebote abgegeben wurden, dann sind die Preise voneinander unabh\u00e4ngig und k\u00f6nnen separat bestimmt werden. Falls jedoch kombinatorische Gebote abgegeben wurden, die sowohl Futures-Kontrakte vom Typ A als auch vom Typ B beinhalten, dann m\u00fcssen die Preise simultan berechnet werden. Dies ist notwendig, damit die Preise f\u00fcr die kombinatorischen Gebote konsistent zu den Preisen der zugrundeliegenden Futures-Kontrakte sind.\nIm ersten Teil dieser Arbeit stellen wir allgemeine Modellierungstechniken und Algorithmen vor. Unter anderem pr\u00e4sentieren wir allgemeine Rahmenmodelle f\u00fcr Auktionen, bei denen die Preise f\u00fcr mehrere Klassen von handelbaren G\u00fctern simultan berechnet werden. Grob gesagt werden mehrere Auktionen parallel durchgef\u00fchrt und dabei die kombinatorischen Gebote so eingebunden, so dass die Auktionen untereinander gekoppelt werden. F\u00fcr bestimmte Klassen von kombinatorischen Auktionen pr\u00e4sentieren wir Modellformulierungen, die in polynomieller Zeit gel\u00f6st werden k\u00f6nnen (z.B. die Futures-Er\u00f6ffnungsauktionen). F\u00fcr den allgemeinen Fall, in dem das Auktionsproblem NP-schwer ist, stellen wir Modellformulierungen und Algorithmen vor, die schnell genug sind, um in der Praxis verwendet werden zu k\u00f6nnen (z.B. in europ\u00e4ischen day-ahead Strommarktauktionen).\nIm zweiten Teil dieser Arbeit werden spezielle Auktionsmodelle f\u00fcr Futures-Er\u00f6ffnungsauktionen, europ\u00e4ische day-ahead Stromauktionen und europ\u00e4ische Erdgasauktionen vorgestellt. Dabei wenden wir unsere allgemeinen Modelle und Algorithmen an, gehen auf problemspezifische Details ein und f\u00fchren numerische Tests mit Probleminstanzen aus der Praxis durch.\n2014-10-20\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5339\nurn:nbn:de:bvb:29-opus4-53396\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-53396\n978-3-7375-1137-7\nhttps://opus4.kobv.de/opus4-fau/files/5339/mueller2014%20-%20Auctions%20in%20Exchange%20Trading%20Systems.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5467\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nddc:610\nmsc\nmsc:94Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nHigh Performance Iterative X-Ray CT with Application in 3-D Mammography and Interventional C-arm Imaging Systems\nHochperformante Iterative Computertomographie mit Anwendung in der 3-D Mammographie und C-arm CT Bildgebung\nKeck, Benjamin\nCUDA <Informatik>\nComputertomographie\nddc:000\nddc:610\nMedical image reconstruction is a key component for a broad range of medical imaging technologies. For classical Computed Tomography systems the amount of measured signals per second increased exponentially over the last four decades, whereas the computational complexity of the majority of utilized algorithms has not changed significantly.\nA major interest and challenge is to provide optimal image quality at the fewest patient dose possible. One solution and active research field towards solving that problem, are iterative reconstruction methods. Their complexity is a multiple compared to the classical analytical methods which were used in nearly all commercially available systems. In this thesis the application of graphics cards in the field of iterative medical image reconstruction is investigated. The major contributions are the demonstrated fast implementations for off-the-shelf hardware as well as the motivation of graphics card usage in upcoming generations of medical systems. The first realization describes the implementation of a commonly used analytical cone-beam reconstruction method for C-arm CT, before covering iterative reconstruction methods. Both analytical as well as iterative reconstruction methods share the compute-intensive back-projection step. In addition iterative reconstruction methods require a forward-projection step with similarly high computational cost. The introduced Compute Unified Device Architecture (CUDA) builds the basis for the presented GPU implementation of both steps. Different realization schemes are presented by combining both steps and applying minor modifications. The implementations of the SART, SIRT as well as OS-SIRT illustrate the realization of algebraic reconstruction methods. Further, a realization for the more advanced statistical reconstruction methods is described, introducing a GPU accelerated implementation of a maximum likelihood reconstruction using a concave objective function.\nThe achieved reconstruction performance is based on different detailed optimizations and exploitation of various technical features. In addition the performance results are evaluated for different hardware platforms - like the CPU - and for the proposed algorithms. The results implicate that for all presented reconstruction methods a significant speedup compared to a CPU realization is achieved. In example, we achieve at least a speedup factor of 10 for the presented OS-SIRT comparing a NVIDIA QuadroFX 5600 graphics card with a workstation equipped with two Intel Xeon Quad-Core E5410 processors. This is additionally supported by the comparison of the presented implementations to the CUDA alternative OpenCL underpinning the performance lead of GPUs using CUDA.\nA further contribution of this thesis is the exemplary clinical application of the pro- posed algorithms to two different modalities: C-arm CT and 3-D mammography. These applications demonstrate the potential and importance of GPU accelerated iterative medical image reconstruction. This thesis is concluded with a summary and an outlook on the future of GPU accelerated medical imaging processing.\nEine wichtige Kernkomponente der medizinischen Bildgebung bildet die medizinische Bildrekonstruktion. Betrachtet man die Anzahl der gemessenen Signale pro Sekunde f\u00fcr die klassische Computertomographie in den letzten vier Jahrzehnten, so l\u00e4sst sich ein exponentielles Wachstum feststellen. Im Vergleich dazu ver\u00e4nderte sich die Berechnungskomplexit\u00e4t der verwendeten Algorithmen nicht merklich. Ein wichtiger Aspekt f\u00fcr die Computertomographie ist die Reduktion der applizierten Patientendosis auf ein Minimum unter der Voraussetzung weiterhin eine optimale Bildqualit\u00e4t zu erhalten. Ein m\u00f6glicher L\u00f6sungsansatz hierf\u00fcr und Gegen- stand aktueller Forschung sind iterative Rekonstruktionstechniken. Die Berechnungskomplexit\u00e4t dieser ist dabei allerdings ein Vielfaches gegen\u00fcber der \u00fcberwiegend kommerziell verwendeten analytischen Rekonstruktionstechniken.\nZiel dieser Arbeit ist es, den m\u00f6glichen Einsatz von Grafikkarten zur beschleunigten Berechnung iterativer Rekonstruktionstechniken zu untersuchen. Der Nachweis und die Erl\u00e4uterung zu den erzielten performanten Realisierungen mit Hilfe dieser Technologie z\u00e4hlt zu den wichtigsten Erkenntnissen. Des weiteren kann der Einsatz dieser breit verf\u00fcgbaren Technologie in zuk\u00fcnftigen medizinischen Bildgebungssystemen nahe gelegt werden. Zus\u00e4tzlich zu den Erl\u00e4uterungen bez\u00fcglich der iterativen Rekonstruktionstechniken wird zun\u00e4chst die Realisierung einer weit verbreitenden analytischen Rekonstruktionstechnik f\u00fcr die C-arm CT Bildgebung beschrieben. Ein Bestandteil dieser Rekonstruktionstechnik, die R\u00fcckprojektion, ist eine berechnungsintensive Kernkomponente die sowohl bei analytischen als auch bei iterativen Rekonstruktionstechniken ben\u00f6tigt wird. Im Unterschied zu den analytischen Methoden ben\u00f6tigen iterative Rekonstruktionstechniken zus\u00e4tzlich eine weitere Kernkomponente, die (Vorw\u00e4rts-)Projektion. Diese stellt dabei einen \u00e4hnlich hohen Berechnungsaufwand dar, wie die R\u00fcckprojektion. Die Basis f\u00fcr die Implementierungen beider Kernkomponenten bildet dabei die erl\u00e4uterte parallele Programmiertechnik f\u00fcr Grafikkarten namens CUDA. Durch geringe Modifikationen und geschickte Kombination beider Kernkomponenten werden verschiedene Realisierungen erzielt. Die Implementierung der Rekonstruktionstechniken SART, SIRT und OS-SIRT stellt die Gruppe der algebraischen Rekonstruktionsalgorithmen dar. Des weiteren wird ein Implementierungbeispiel f\u00fcr die Gruppe der statistischen Rekonstruktionstechniken anhand einer Maximum Likelihood Rekonstruktion auf Basis einer konkaven Zielfunktion erl\u00e4utert.\nDie erzielten Rekonstruktionsgeschwindigkeiten basieren auf verschiedenen dargelegten Optimierungstechniken und der Verwendung neuer technischer Funktionen. Zus\u00e4tzlich werden die Rekonstruktionsgeschwindigkeiten zwischen verschiedenen Hardware-Plattformen und den verschiedenen Algorithmen verglichen. Die Ergebnisse implizieren, dass f\u00fcr alle vorgestellten Rekonstruktionstechniken mittels Grafikkarten ein signifikanter Geschwindigkeitszuwachs im Vergleich zu einer CPU erzielt werden kann. Zum Beispiel ist die Implementierung der OS-SIRT auf einer NVIDIA QuadroFX 5600 mindestens Faktor 10 schneller als die CPU Implementierung auf einer Workstation mit zwei Intel Xeon Quad-Core E5410 Prozessoren. Dies wird durch den Vergleich der erl\u00e4uterten Implementierungen zur CUDA Alter- native OpenCL zus\u00e4tzlich bekr\u00e4ftigt, der die Geschwindigkeitsvorteile von Grafik- karten unter der Verwendung von CUDA untermauert.\n2014-11-19\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5467\nurn:nbn:de:bvb:29-opus4-54678\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-54678\nhttps://opus4.kobv.de/opus4-fau/files/5467/BenjaminKeckDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5484\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nddc:610\nccs\nccs:B.4\npacs\npacs:40.00.00\nmsc\nmsc:94-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nDaten\u00fcbertragungskonzepte f\u00fcr ein Brust-CT\nSchilling, Harry\nDrehverbindung\nSchleifring\nddc:004\nddc:610\nHintergrund und Ziele\nEin am Institut f\u00fcr Medizinische Physik verfolgtes neuartiges Konzept f\u00fcr ein dediziertes CT der Brust zeigt in Simulationen und Experimenten erstmals auf der Basis von neuartigen und optimierten Bildgebungskomponenten die in der Diagnostik und im Screnning geforderten Leistungswerte hinsichtlich Bildqualit\u00e4t und Dosis. Damit scheint das Konzept geeignet, die Probleme der Mammographie, wie zum Beispiel eine reduzierte Sensitivit\u00e4t bei dichtem Brustgewebe, das Vorhanden sein von \u00dcberlagerungseffekten oder auch die h\u00e4ufig als schmerzhaft empfundene Kompression der Brust w\u00e4hrend der Untersuchung zu l\u00f6sen. Zur Umsetzung des Konzepts f\u00fcr das Brust-CT-System werden zwei alternative rotierende Tragwerkskonzepte f\u00fcr die Bildgebungseinheit favorisiert, welche beide zur Energie-, Signal- und Daten\u00fcbertragung einen sogenannten Dreh\u00fcbertrager ben\u00f6tigen.\nDas Ziel dieser Arbeit besteht in der Erforschung und Erarbeitung von unterschiedlichen \u00dcbertragungsprinzipien f\u00fcr die Energie-, Signal- und Daten\u00fcbertragung mit einem Dreh\u00fcbertrager. Dabei m\u00fcssen die Vorgaben aus der Spezifikation des Brust-CT-Systems eingehalten und umgesetzt werden.\nMethoden\nZun\u00e4chst wurden die beiden Tragwerkskonzepte und die Spezifikationsparameter des Brust-CT-Systems beschrieben, pr\u00e4zisiert und zusammengefasst.\nNach der Definition des Begriffs Dreh\u00fcbertrager wurde der Stand der Technik der \u00dcbertragungsprinzipien f\u00fcr die Energie-, Signal- und Daten\u00fcbertragung erkl\u00e4rt. Die \u00dcbertragungsprinzipien wurden untersucht und jeweils deren Vor- und Nachteile zusammengefasst. Im Ergebnis wurden die \u00dcbertragungsprinzipien ausgew\u00e4hlt und im Detail weiter erforscht, die f\u00fcr einen Dreh\u00fcbertrager f\u00fcr eines der beiden Tragwerkskonzepte als geeignet erschienen. Dazu wurden grundlegende Betrachtungen und Simulationen durchgef\u00fchrt und anschlie\u00dfend auch Versuchsaufbauten erstellt, um die praktische Anwendbarkeit des jeweiligen \u00dcbertragungsprinzips zu \u00fcberpr\u00fcfen. Danach erfolgte eine abschlie\u00dfende Auswahl der \u00dcbertragungsprinzipien f\u00fcr den oder die Dreh\u00fcbertrager des Brust-CT. Der Dreh\u00fcbertrager mit der besten Eignung f\u00fcr das Brust-CT wurde als Prototyp aufgebaut.\nErgebnisse und Beobachtungen\nEs zeigt sich, dass die 6Bildgebungskomponenten die gr\u00f6\u00dfte Herausforderung f\u00fcr den Dreh\u00fcbertrager darstellen.\nDie R\u00f6ntgenr\u00f6hre ben\u00f6tigt entweder eine Spannung von 60 KV bei einer Leistung von 3 KW, wenn der R\u00f6hrengenerator nicht mitrotiert, oder, falls der R\u00f6hrengenerator mitrotiert, eine Spannung von 400 V bei einer Leistung von 5 KW. Diese Spannungen und Leistungen lassen sich mit kontaktierenden Graphitb\u00fcrsten oder kontaktierenden Goldfederdr\u00e4hten \u00fcbertragen. Eine L\u00f6sung f\u00fcr eine direkte kontaktlose \u00dcbertragung der R\u00f6hrenspannung von 60 KV konnte nicht gefunden bzw. im Rahmen der Gesamtkosten nicht umgesetzt werden.\nDie Daten\u00fcbertragungsrate der Projektionsdaten des R\u00f6ntgendetektors von 28,6 GBit/s sind mit dem entwickelten und zum Patent angemeldeten einkanaligen optischen Dreh\u00fcbertrager auf der Rotationsachse problemlos direkt \u00fcbertragbar. Durch ein Wellenl\u00e4ngenmultiplex ist au\u00dferdem die \u00dcbertragung der Projektionsdaten mit drei 10 GBit Ethernet-Kan\u00e4len m\u00f6glich, jedoch f\u00fcr den spezifizierten Kostenrahmen zu teuer. F\u00fcr die Daten\u00fcbertragung mit einem Dreh\u00fcbertrager, welcher einen freien Innendurchmesser aufweist, gibt es unter der gegebenen Spezifikation deshalb f\u00fcr das Brust-CT keine L\u00f6sung.\nDes Weiteren ergab sich, dass f\u00fcr ein Brust-CT-System ein Dreh\u00fcbertrager mit kontaktierenden Goldfederdr\u00e4hten zur Leistungs- und Steuerdaten\u00fcbertragung am besten geeignet ist.\nPraktische Schlussfolgerungen\nDie durchgef\u00fchrten Untersuchungen zeigen, dass bei der entwickelten Ausf\u00fchrungen des Dreh\u00fcbertragers die spezifizierten Parameter f\u00fcr das Brust-CT mit einer U-Anordnung umgesetzt und mit den nach dem Stand der Technik zur Verf\u00fcgung stehenden \u00dcbertragungsprinzipien und Fertigungstechniken entsprechende realisiert werden kann.\nBackground and aims\nA novel concept for a dedicated CT of the breast tracked at the Institute of Medical Physics shows in simulations and experiments for the first time the required diagnostic and screnning performance values with respect to image quality and dose on the basis of new and optimized imaging components. Thus, the concept seems suitable for solving the problems of mammography, as a reduced sensitivity in dense breast tissue, superposition effects or even the often as painful perceived compression of the breast during the examination. In order to implement the concept of the Breast-CT-System, two alternative rotating structural concepts for the imaging unit are favored, which both for power, signal and data transmission require a so called rotary joint.\nThe aim of this work is to research and develop different transmission principles for power, signal and data transmission across the rotary joint. The requirements of the specifications of the Breast-CT-System must be respected and implemented.\nMethods\nAt first, the two structural concepts and the specification parameters of the Breast-CT-System have been described, clarified and summarized.\nAfter the definition of rotary joints of the prior art, the transmission principles for power, signal and data transmission were explained. The transfer technologies were studied and a resume was given for the advantages and disadvantages. As a result, those transfer principles were selected and further studied in detail, which appeared to suit best for a rotary joint for one of the two structural concepts. Basic considerations and simulations were carried out. In addition experimental built ups were made to verify the practicality of the transmission principles. This was followed by a final selection of the transfer technologies best qualified for alternative rotary joint setups of the Breast-CT. The rotary joint with the best appropriateness for the Breast-CT was built up as a prototype unit.\nResults and Observations\nIt turns out that the imaging components represent the greatest challenge for the rotary joint.\nThe X-ray tube will require either a voltage of 60 kV at a power of 3 kW, if the tube generator does not rotate, or, if the generator rotates, a voltage of 400 V and a power of 5 KW. These voltages and power can be transferred with contacting graphite brushes or contacting gold spring wires. A solution for a direct contactless transfer of tube voltage of 60 kV could not be found or is not implementable as part of the total cost.\nThe data transfer rate of projection data of the x-ray detector of 28.6 Gbit / s is easily transferable directly with the developed and patent pending single-channel optical rotary joint placed on the rotation axis. With a wavelength multiplexing the transfer of the projection data will be also possible, if three 10 Gbit Ethernet channels are used. But this is too expensive for the specified financial budget. For data transmission with a rotary joint, which has a free inner bore, there is under the given specification no solution.\nFurthermore, it was found that for a Breast-CT-System, a rotary joint with contacting gold spring wires for power and control data transmission is the most suitable.\nPractical conclusions\nThe investigations carried out showed that the specified parameters for the Breast-CT can be implemented in the developed rotary joint with an U-arrangement of the structural concept and also with the prior art available transmission principles and manufacturing techniques.\n2014-11-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5484\nurn:nbn:de:bvb:29-opus4-54843\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-54843\nhttps://opus4.kobv.de/opus4-fau/files/5484/HarrySchillingDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5503\n2018-11-12\ndoc-type:masterThesis\nbibliography:false\nddc\nddc:306\nddc:708\nccs\nccs:A.1\nccs:A.m\npacs\npacs:01.40.-d\nmsc\nmsc:00-02\nmsc:00Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Phil_Paedagogik\n\"Erfahrung Kunst\" - Eine qualitative Studie zur besucherorientierten Kunstvermittlung: Kunstgespr\u00e4che aus retrospektiver Sicht \u00e4lterer Museumsbesucher\nKiefer (geb. Kerner), Melanie\nAltenbildung\nBildbetrachtung\nBesucherforschung\nBesucherf\u00fchrung\nBesucher\nDrittes Lebensalter\nEmpirische P\u00e4dagogische Forschung\nEvaluation\nGermanisches Nationalmuseum\nInterview\nKunstbetrachtung\nKunstvermittlung\nKulturvermittlung\nKulturp\u00e4dagogik\nKunstkurs\nKunstgespr\u00e4ch\nKunst- und Kulturp\u00e4dagogisches Zentrum der Museen <N\u00fcrnberg>\nKPZ\nMuseum\nKunstmuseum\nMuseumsp\u00e4dagogik\nQualitative Inhaltsanalyse\nSenioren\nStaedtler Stiftung\nddc:306\nddc:708\nIn der Museumsp\u00e4dagogik ist der Trend zu besucherorientierten Angeboten erkennbar, wodurch auch die Besucherforschung zunehmend an Bedeutung gewinnt. Studien im museumsp\u00e4dagogischen Bereich beruhen vorwiegend auf quantitativen Erhebungen, die statistische Aussagen erm\u00f6glichen. Gezielte qualitative Untersuchungen zu museumsp\u00e4dagogischen Angeboten, in welchen spezifische Formate der Vermittlung aus der Sicht der jeweiligen Zielgruppe erforscht werden, sind selten, obwohl das Wissen um diese und deren Bed\u00fcrfnisse meist l\u00fcckenhaft ist. Dies betrifft auch Besuchergruppen, die zum sogenannten Stammklientel geh\u00f6ren, wie beispielsweise die Senioren,\ndie im Zuge des demografischen Wandels verst\u00e4rkt in den Fokus r\u00fccken. Didaktische Methoden zur Arbeit mit \u00c4lteren wurden anhand praktischer Erfahrungen entwickelt, welche es wissenschaftlich zu untermauern gilt. Die vorliegende Masterarbeit soll hierzu einen Beitrag leisten, indem der Frage nachgegangen wird: Inwiefern stellt das Kunstgespr\u00e4ch, im Sinne der Besucherorientierung, ein geeignetes Format der Kunstvermittlung f\u00fcr Museumsbesucher im dritten Lebensalter dar? Um eine Antwort auf diese Frage zu finden, werden die Interviews mit vier Senioren untersucht, die an einem Gespr\u00e4chskurs des Kunst- und Kulturp\u00e4dagogischen Zentrums der Museen in N\u00fcrnberg (KPZ), bestehend aus zehn Kunstgespr\u00e4chen, teilgenommen haben, welcher im Rahmen des Staedtler-Projektes \u201ePers\u00f6nlichkeitsbildung durch k\u00fcnstlerisches Gestalten\u201c durchgef\u00fchrt wurde. Die Kursteilnehmer gaben in leitfadengest\u00fctzten Interviews Auskunft zu ihren Erwartungen an den Kunstkurs, zu ihren Bed\u00fcrfnissen im Hinblick auf die Kunstvermittlung (bez\u00fcgl. d. Rahmenbedingungen, Methode, Kursgruppe, Kursleitung, Kursinhalte) und zu den subjektiv wahrgenommenen Wirkungen des Kursbesuchs. Die Interviews wurden nach der inhaltlich strukturierenden qualitativen Inhaltsanalyse ausgewertet (vgl. Udo Kuckartz 2012), wobei eine QDA-Software unterst\u00fctzend eingesetzt wurde. Die Interpretation der Ergebnisse erlaubt insgesamt eine Beantwortung der Forschungsfrage.\n2014-11-29\nmasterthesis\ndoc-type:masterThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5503\nurn:nbn:de:bvb:29-opus4-55036\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-55036\nhttps://opus4.kobv.de/opus4-fau/files/5503/KernerMelanieMasterarbeit.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-sa/3.0/de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5529\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:003\nddc:610\nccs\nccs:I.5\nmsc\nmsc:62G99\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nStatistische Modellierung, prototypische Implementierung und Evaluierung eines Systems zur Sturzrisikoprognose f\u00fcr Senioren\nStatistical modelling, prototypical implementation and evaluation of a system for fall-risk prognosis for elderly people\nBienk, Stefan\nInferenzstatistik\nMaschinelles Lernen\nTelemedizin\nddc:003\nddc:610\nVor dem Hintergrund intensiver einschl\u00e4giger Forschungsarbeiten wird in dieser Arbeit ein neuartiger Ansatz zum Sturzrisikomonitoring f\u00fcr Senioren entwickelt. Dabei werden pers\u00f6nliche mittelfristige Sturzrisikoprognosen anhand permanent im Alltag der Patienten erhobener Daten eines einzigen, in eine herk\u00f6mmliche Hausalarmuhr integrierten Akzelerometers automatisch mittels Software erstellt. Mit den damit einhergehenden pragmatischen Vorteilen gegen\u00fcber etablierten Ans\u00e4tzen ist jedoch ein erh\u00f6hter Schwierigkeitsgrad des Prognoseproblems verbunden: Einerseits erfolgt die Erhebung unter naturalistischen (d.h. unkontrollierten) Bedingungen, andererseits wird eine indirekte Messung vorgenommen, d.h. insbesondere ohne Sensorik an den unteren Extremit\u00e4ten. Diese Herausforderungen werden durch ein mehrstufiges Verfahren adressiert: Dieses berechnet (nach einer f\u00fcr jeden Patienten einmalig durchzuf\u00fchrenden supervisionierten Trainingsphase) regelm\u00e4\u00dfig einen bereits validierten Sturzrisikoindikator, die sog. Gangvariabilit\u00e4t. Zun\u00e4chst werden hierzu anhand der Beschleunigungsdaten Zeitintervalle ermittelt, w\u00e4hrend derer eine Person Gangstrecken absolvierte. F\u00fcr diese Intervalle werden dann die Schrittzeitpunkte rekonstruiert und hieraus die Gangvariabilit\u00e4t als Eingangsgr\u00f6\u00dfe f\u00fcr die Sturzprognose berechnet.\nHierbei kommen Methoden der Aktivit\u00e4tsklassifizierung und der Wavelet-Theorie zum Einsatz. W\u00e4hrend die letztere lediglich zur Anwendung gebracht wird, besteht ein Schwerpunkt der Arbeit in der theoretischen Analyse und Weiterentwicklung der etablierten heuristischen Methoden zur Aktivit\u00e4tsklassifizierung. Dies umfasst insbesondere die Untersuchung von technischen Fragen der Messbarkeit wie auch die Untersuchung der statistischen G\u00fcte der Inferenzmethoden. Durch die Einf\u00fchrung des abstrakten Konzepts des Pr\u00e4diktionsproblems mit allgemeinen separablen metrischen Zielr\u00e4umen wird dabei die blo\u00dfe \u00dcbertragung analoger Begriffe und Resultate aus der etablierten Klassifikationstheorie vermieden.\nDie Beschreibung der technischen Umsetzung dieser Ideen sowie eine erste Evaluierung bilden schlie\u00dflich den Abschluss der Arbeit.\nBased on extensive related work, in this thesis an alternative approach to fall-risk\nmonitoring of elderly persons is developed. Software components are provided\nwhich automatically establish personal middle-term risk prognostics, using as input\nonly data recorded in patients' everyday life employing one single accelerometer on board of\na common wristwatch (originally featuring only an emergency call function). Along with numerous pragmatic achievements compared to existing assessment tools, this\nentails a higher complexity of the prediction task: Data acquisition takes place in a\nnaturalistic (i.e. uncontrolled) setting and the measurement is of indirect nature, as\nno sensors are attached to the patients' lower extremities. These challenges are addressed\nby a multiple-stage procedure: After a supervised training to be performed\none time for each patient in order to personalize the system, an already established\nfall-risk indicator, the so called gait variability, can be periodically reconstructed by\nthe system. Firstly, time intervals during which monitored persons were walking are\ndetermined from the permanently recorded motion data. Secondly, for these intervals, gait\nevents are reconstructed and gait variability is calculated, serving as input for the\nfinal computation of fall-risk.\nOn the modelling level, methods from activity classification and the theory of wavelets\nare utilized. While the latter is simply applied, as a major contribution a\ntheoretical analysis and improvement of heuristic methods for activity classification\nis provided. This includes rather technical measurability issues as well as the\nanalysis of statistical notions of quality of the inferential methods. By introducing\nan abstract concept called \"prediction problem\", pure reproduction of analogous notions\nand results from established classification theory is avoided.\nThe thesis is completed by a brief sketch of the technical implementation of these\nideas and a first evaluation.\n2014-12-05\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5529\nurn:nbn:de:bvb:29-opus4-55293\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-55293\nhttps://opus4.kobv.de/opus4-fau/files/5529/StefanBienkDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5579\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\nmsc\nmsc:49-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nExistence and regularity results for solutions of spectral problems\nMazzoleni, Dario\nShape Optimization\nDirichlet Laplacian\nEigenvalues\nddc:519\nThis Thesis is devoted to the study of some shape optimization problems for eigenvalues of the Dirichlet Laplacian. In particular we prove that there exists an optimal set for the kth eigenvalue of the Dirichlet Laplacian among quasi open sets of R^N and that this set admits an eigenfunction, corresponding to lambda_k, which is Lipschitz continuous in R^N.\n2014-12-15\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5579\nurn:nbn:de:bvb:29-opus4-55798\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-55798\nhttps://opus4.kobv.de/opus4-fau/files/5579/MazzoleniDarioDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5619\n2018-11-12\ndoc-type:MovingImage\nbibliography:false\nddc\nddc:006\nccs\nccs:I.6.4\npacs\npacs:45.70.Mg\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nCrystallization in a Granular Channel Flow Past an Obstacle\nPreclik, Tobias\nGranul\u00e4rer Stoff\nDirekte numerische Simulation\nKristallisation\nDreidimensionale Str\u00f6mung\nKanal\nHochleistungsrechnen\nParallelisierung\nddc:006\nThe video shows a granular flow through a channel of length 30cm, width 6.3cm and height 2.1cm. Gravitational acceleration acts towards the bottom. On the left end spherical particles of 1mm diameter are generated with an inflow velocity of 2m/s. An obstacle obstructs the flow 3cm downstream. It has a D-shaped cross section, is 2cm long and wide, extends to the full height of the channel, and is placed directly in the middle of the channel. The setup was simulated for 5s simulation time with time steps of length 10us. In each time step 10 iterations of a parallel non-smooth contact dynamics (NSCD) method were performed. The simulation ran for 2 hours on 64 nodes of the Emmy cluster at the regional computing centre in Erlangen (RRZE). To properly resolve the dynamics visual scene descriptions were written to disk with a rate of 600 frames per second. The final video is slowed down by a factor of 20. The formation of a dead zone behind the obstacle with a triangular cross section can be observed as well as the formation of crystalline clusters with occasional defects.\n2014-12-22\nmovingimage\ndoc-type:MovingImage\nvideo/mp4\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5619\nurn:nbn:de:bvb:29-opus4-56192\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56192\nhttps://opus4.kobv.de/opus4-fau/files/5619/5619_crystalline-channelflow.mp4\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5621\n2018-11-12\ndoc-type:MovingImage\nbibliography:false\nddc\nddc:006\nccs\nccs:I.6.4\npacs\npacs:45.70.Mg\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nGranular Channel Flow Past an Obstacle\nPreclik, Tobias\nGranul\u00e4rer Stoff\nHochleistungsrechnen\nDirekte numerische Simulation\nKanal\nDreidimensionale Str\u00f6mung\nParallelisierung\nStation\u00e4re Str\u00f6mung\nddc:006\nThe video shows a granular flow through a channel of length 15cm, width 6.3cm and height 2.1cm. Gravitational acceleration acts towards the bottom. On the left end spherical particles of 1mm diameter are generated with an inflow velocity of 1m/s. An obstacle obstructs the flow 3cm downstream. It has a D-shaped cross section, is 2cm long and wide, extends to the full height of the channel, and is placed directly in the middle of the channel. The setup was simulated for 10s simulation time with time steps of length 10us. In each time step 10 iterations of a parallel non-smooth contact dynamics (NSCD) method were performed. The simulation ran for 50 minutes on 32 nodes of the Emmy cluster at the regional computing centre in Erlangen (RRZE). To properly resolve the dynamics visual scene descriptions were written to disk with a rate of 300 frames per second. The final video is slowed down by a factor of 10. The average maximum penetration in 50 samples was 1.8% of the particle diameter. The formation of a steady-state flow with minor local fluctuations as well as the formation of recirculation regions behind the obstacle can be observed.\n2014-12-22\nmovingimage\ndoc-type:MovingImage\nvideo/mp4\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5621\nurn:nbn:de:bvb:29-opus4-56215\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56215\nhttps://opus4.kobv.de/opus4-fau/files/5621/5621_channelflow.mp4\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5622\n2018-11-12\ndoc-type:MovingImage\nbibliography:false\nddc\nddc:006\nccs\nccs:I.6.4\npacs\npacs:45.70.Mg\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nSize Segregation of Sharp-edged Granular Matter in a Horizontal Shaker\nPreclik, Tobias\nGranul\u00e4rer Stoff\nHochleistungsrechnen\nDirekte numerische Simulation\nParallelisierung\nEntmischung\nInstation\u00e4re Str\u00f6mung\nddc:006\nThe video shows a large-scale horizontal shaker filled with 864000 sharp-edged particles ranging in size between 0.25mm to 2mm. The shaking frequency is set to 4 Hz and the amplitude to 3cm. The size of the particles is color-coded. To properly resolve the dynamics visual scene descriptions were written to disk with a rate of 1000 frames per second. The final video is slowed down by a factor of 10. The simulation ran for 27 hours on 64 nodes of the Emmy clusters located at the regional computing centre in Erlangen (RRZE). The setup was simulated for 2.8s simulation time with time steps of length 10us. In each time step 10 iterations of a parallel non-smooth contact dynamics (NSCD) method were performed. An emerging size segregation can be observed: The smallest particles with beige color collect at the bottom right below the larger bluish particles.\n2014-12-22\nmovingimage\ndoc-type:MovingImage\nvideo/mp4\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5622\nurn:nbn:de:bvb:29-opus4-56220\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56220\nhttps://opus4.kobv.de/opus4-fau/files/5622/shaker.mp4\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5623\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nccs\nccs:I.6\npacs\npacs:45.70.-n\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nModels and Algorithms for Ultrascale Simulations of Non-smooth Granular Dynamics\nModelle und Algorithmen f\u00fcr ultraskalige Simulationen nichtglatter granularer Dynamiken\nPreclik, Tobias\nGranul\u00e4rer Stoff\nHochleistungsrechnen\nDirekte numerische Simulation\nParallelisierung\nNichtglatte Mechanik\nKontaktmechanik\nRegularisierung\nddc:000\nThe macroscopic behaviour of granular matter in contrast to fluids and solids is often insufficiently understood not least because poor visual accessibility hinders the analysis. Computer simulations are capable of revealing the internal dynamics not directly accessible in the experiment but are computationally extremely expensive since no unified model equations exist homogenizing the dynamics of the individual particles. Unfortunately, the time and length scales of relevant problems are quickly beyond the reach of today's simulation tools. This thesis aims to address these issues by providing models and algorithms efficiently harnessing the power of today's supercomputers and at the same time extending the reachable scales by hiding the collision micro-dynamics and improving the convergence rate of the iterative solver and time coherence of the solutions. To hide the collision micro-dynamics hard contact models are employed leading to non-smooth trajectories and discontinuous velocities of the particles. The convergence rate and time coherence are impeded by the non-uniqueness of the contact reactions. Hence, a new friction model is introduced that can be viewed as a regularization of the Coulomb friction model exhibiting unique one-contact solutions. Another regularization, stiffening contact compliances in multi-contact problems, identifies the contact reactions of weighted minimum norm as physically meaningful unique solutions. Finally, an iterative solver based on the non-smooth contact dynamics method is parallelized for distributed-memory architectures. The parallelization uses a sophisticated protocol supporting the processes in deciding robustly upon responsibilities such as contact treatment and position integration while minimizing communication overhead. The parallel efficiency is assessed in strong- and weak-scaling experiments on three clusters including Juqueen and SuperMUC, two of today's largest supercomputers. The simulations are of unprecedented scale: in the order of 10 billion non-spherical particles and contacts. Aside from scaling experiments the versatility is demonstrated using the examples of a large-scale horizontal shaker filled with sharp-edged granular matter and rapid granular channel flows. Though the limits of non-smooth granular matter simulations are pushed considerably, the dependence of the convergence rate of the non-smooth contact dynamics method on the number of unknowns remains an open problem.\nDas makroskopische Verhalten granularer Materie ist - im Gegensatz zu dem von Fluiden und Festk\u00f6rpern - h\u00e4ufig nur unzureichend verstanden. Dies r\u00fchrt nicht zuletzt daher, dass die Analyse durch die schlechte visuelle Zug\u00e4nglichkeit erschwert wird. Computersimulationen verm\u00f6gen die im Experiment nicht direkt zug\u00e4ngliche interne Dynamik preiszugeben, sind aber extrem rechenaufwendig, da keine allgemeing\u00fcltigen Modellgleichungen existieren, welche die Dynamik der einzelnen Partikel homogenisieren. Allerdings liegen die Zeit- und L\u00e4ngenskalen relevanter Probleme schnell au\u00dferhalb der Erreichbarkeit heutiger Simulationsprogramme. Ziel der Arbeit ist es, diese Aspekte durch die Entwicklung von Modellen und Algorithmen anzugehen, welche die Rechenleistung heutiger Supercomputer nutzbar machen und zugleich die erreichbaren Skalen erweitern. Dies soll realisiert werden, indem die Mikrodynamik der Kollisionen ausgeblendet wird und die Konvergenzrate des iterativen L\u00f6sers sowie die zeitliche Koh\u00e4renz der L\u00f6sungen verbessert werden. Um die Mikrodynamik der Kollisionen auszublenden, werden harte Kontaktmodelle eingesetzt, welche auf nichtglatte Trajektorien und unstetige Geschwindigkeiten der Partikel f\u00fchren. Konvergenzrate und zeitliche Koh\u00e4renz werden durch die Uneindeutigkeit der Kontaktreaktionen verschlechtert. Daher wird ein neues Reibungsmodell eingef\u00fchrt, welches als Regularisierung der Coulombschen Reibung betrachtet werden kann und eindeutige Einkontaktl\u00f6sungen aufweist. Eine weitere Regularisierung, welche Kontaktnachgiebigkeiten in Mehrkontaktproblemen verfestigt, identifiziert die Kontaktreaktion mit minimaler gewichteter Norm als physikalisch bedeutsame eindeutige L\u00f6sung. Abschlie\u00dfend wird ein iteratives L\u00f6sungsverfahren, welches auf der \"non-smooth contact dynamics\"-Methode basiert, f\u00fcr Architekturen mit verteiltem Speicher parallelisiert. Die Parallelisierung verwendet ein durchdachtes Protokoll, welches die Prozesse darin unterst\u00fctzt, Zust\u00e4ndigkeiten - wie beispielsweise Kontaktbehandlung oder Positionsintegration - robust zu entscheiden bei gleichzeitiger Reduktion des Kommunikationsaufwandes. Die parallele Effizienz wird in starken und schwachen Skalierungsexperimenten auf drei Clustern - einschlie\u00dflich Juqueen und SuperMUC, zwei der gr\u00f6\u00dften heutigen Supercomputer - ausgewertet. Die Gr\u00f6\u00dfenordnung der Simulationen erreicht ein bislang ungekanntes Ausma\u00df: 10 Milliarden nicht-sph\u00e4rische Partikel und Kontakte. Abgesehen von den Skalierungsexperimenten wird die Vielseitigkeit anhand eines gro\u00dfskaligen horizontalen R\u00fcttlers demonstriert, welcher mit scharfkantiger granularer Materie gef\u00fcllt ist und anhand rapider granularer Kanalstr\u00f6mungen. Obwohl die Simulationsm\u00f6glichkeiten nichtglatter granularer Materie deutlich erweitert werden, bleibt die Abh\u00e4ngigkeit der Konvergenzrate der \"non-smooth contact dynamics\"-Methode von der Anzahl der Unbekannten ein ungel\u00f6stes Problem.\n2014-12-22\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5623\nurn:nbn:de:bvb:29-opus4-56232\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56232\nhttps://opus4.kobv.de/opus4-fau/files/5623/dissertation_preclik_tobias.pdf\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5697\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:92-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nDie Modulation immunologisch relevanter endogener Antigene und ihre prognostische Bedeutung in Speicheldr\u00fcsenkarzinomen\nMueller, Maximilian\nSpeicheldr\u00fcsenkarzinome\nddc:610\nA Hintergrund und Ziele\nDamit ein Tumor entstehen, wachsen und \u00fcberleben kann, sind molekulare Dys-regulationen notwendig, die einerseits die zellul\u00e4re Proliferation und Differenzierung be-einflussen und andererseits eine Immunantwort des K\u00f6rpers unterdr\u00fccken. Den Protei-nen \u03b22-Mikroglobulin, Calnexin, Calreticulin, ERp57, HLA-A, LMP-2, LMP-7, TAP-1, TAP-2, Tapasin und HC-10 kommt beim Entkommen der Immunabwehr (\u201eimmune escape\u201c) eine entscheidende Rolle zu, da sie die Fremdantigenexpression an Zellober-fl\u00e4chen und die anschlie\u00dfende Erkennung durch T-Zellen steuern. Ziel der vorliegenden Arbeit war die Untersuchung des Expressionsprofils dieser Proteine in Speicheldr\u00fcsen-karzinomen, um zu analysieren, ob \u201eImmun escape\u201c-Mechanismen eine Rolle spielen, es innerhalb der verschiedenen Entit\u00e4ten diesbez\u00fcglich Unterschiede gibt und ob eine prognostische Bedeutung abzuleiten ist.\nB Methoden\nF\u00fcr die Untersuchung wurden Proben von paraffineingebetteten Tumor- und korres-pondierenden Normalgeweben der Speicheldr\u00fcse aus einem Kollektiv von 288 Patien-ten, die seit 1984 in den Universit\u00e4tskliniken Regensburg, Erlangen und am Klinikum N\u00fcrnberg behandelt worden waren, herangezogen. Die klinischen Daten wurden durch das Erlanger Speicheldr\u00fcsenregister (Leiter: Prof. Dr. H. Iro) in Zusammenarbeit mit den Tumorzentren Regensburg und Erlangen-N\u00fcrnberg erhoben und zur Verf\u00fcgung gestellt. Durch eine besondere Stanztechnik wurden Multi-Gewebe-Bl\u00f6cke (\u201eTissue microarrays\u201c) erstellt, die eine immunhistochemische Untersuchung des Kollektivs er-m\u00f6glichten. Mit Hilfe einer Polymerkonjugatf\u00e4rbemethode (EnVisionTM) wurden die immunhistochemischen Objekttr\u00e4ger zu den oben genannten Markern angefertigt. Die weitere statistische Analyse (SPSS 18.0) st\u00fctzte sich auf Punktewerte, in die F\u00e4rbean-teil und \u2013intensit\u00e4t einflossen. Dadurch konnten Abweichungen zwischen Normal- und Tumorgewebe (mit dem t-Test f\u00fcr unabh\u00e4ngige Variablen mit p<0,05) und diesbez\u00fcgliche \u00dcberlebensunterschiede der Patienten retrospektiv analysiert werden (univariat mit der Kaplan-Meier-Analyse mit p<0,05, multivariat mit Hilfe einer r\u00fcckw\u00e4rts bedingten Cox-Regression mit schrittweiser Elimination bei p<0,05 und Konfidenzintervall 95 %).\nC Ergebnisse und Beobachtungen\nHinsichtlich des Geschlechts der Patienten, der Tumorlokalisation und der Karzinom-typen (Entit\u00e4ten) konnte ein, verglichen mit den von der Weltgesundheitsorganisation ver\u00f6ffentlichten Daten, repr\u00e4sentatives Fallkollektiv zusammengestellt werden. Bei den h\u00e4ufigsten Karzinomen lie\u00dfen sich statistisch signifikante \u00dcber- und Unterexpressionen der untersuchten Marker dokumentieren. Dabei zeigen sich unter Ber\u00fccksichtigung der Tumorhistogenese Parallelen der Expressionsmuster bei Karzinomen mit derselben Entstehungslokalisation. Weiterhin wurde die \u00c4nderung der Expression einzelner immu-nologisch relevanter Parameter in ihrem Einfluss auf das \u00dcberleben der Patienten un-tersucht und im Kontext etablierter klinischer Parameter multivariat analysiert. In der univariaten Analyse aller 288 Tumoren waren Tumoren mit Unterexpressionen von nukle\u00e4rem LMP-7 (p=0,005), \u00dcberexpressionen von \u03b22-Mikroglobulin (p=0,024), HLA-A (p<0,001), TAP-1 (p=0,01) und Tapasin (p<0,001) mit einem schlechteren \u00dcberleben der Patienten assoziiert. Bezogen auf einzelne Entit\u00e4ten lie\u00dfen sich hier hingegen Ab-weichungen dokumentieren. Hinsichtlich klinischer Parameter zeigten Patienten mit einem Alter \u00fcber 60 Jahre (p<0,001), m\u00e4nnlichem Geschlecht (p=0,002), einer Lokali-sation des Tumors in der Gl. parotis (p=0,005) und hinsichtlich histopathologischer Fak-toren wie einem hohen pT/N- und UICC-Stadium (p<0,001) sowie mit einem hohen Malignit\u00e4tsgrad G2/3 (p<0,001), Residualtumoren R1/2 (p<0,001) und innerhalb einzel-ner Entit\u00e4ten ein schlechteres \u00dcberleben. In der multivariaten Analyse beeinflussten ein hoher Malignit\u00e4tsgrad (Hazard Ratio, HR 4,1), hohes Tumorstadium T3/4 (HR 1,46) sowie Fernmetastasierung (HR 1,95), eine \u00dcberexpressionen von Tapasin (HR 1,51) sowie von HLA-A (HR 1,95) negativ das \u00dcberleben, eine \u00dcberexpression von LMP-7 in der nukle\u00e4ren Auswertung (HR 0,58) stellte einen positiven \u00dcberlebensfaktor dar.\nD Praktische Schlussfolgerung\nDie Expression immunologisch bedeutsamer Proteine differiert zwar bei den biologisch sehr unterschiedlichen Karzinomtypen der Speicheldr\u00fcse erheblich, dennoch lie\u00dfen sich einzelne Marker (HLA-A, LMP-7 und Tapasin) als auch etablierter klinisch-pathologische Parameter als prognostisch bedeutsam herausstellen. Diese Erkenntnis-se k\u00f6nnten gegenw\u00e4rtig als zus\u00e4tzliche Entscheidungshilfen bei der Festlegung der indi-viduellen Tumortherapie in der Adjuvanz (Strahlentherapie, Chemotherapie) fungieren, ob auf der Basis der Untersuchungen zuk\u00fcnftig auch immunologische Therapieans\u00e4tze verfolgt werden k\u00f6nnen, ist hingegen auch von funktionellen Studien abh\u00e4ngig.\nA Background and Aims\nFor a tumor to develop, grow and survive, molecular dysregulations which not only af-fect cellular proliferation and differentiation but also suppress an immunologic response of the organism are considered a prerequisite. With the proteins \u03b22-Microglobulin, Calnexin, Calreticulin, ERp57, HLA-A, LMP-2, LMP-7, TAP-1, TAP-2, Tapasin and HC-10 regulating antigen presentation on cell surfaces and their subsequent discovery by T-cells, these molecules play a decisive role in immune escape. In this context, the purpose of this investigation was to study the expression profile of these proteins in salivary gland carcinomas and to determine, whether immune escape mechanism ap-pear in these carcinomas, if there are differences between subtypes and to analyse a prognostic impact.\nB Methods\nParaffin embedded samples of tumor and corresponding healthy salivary gland tissue obtained from 288 patients who had been treated at the universities of Regensburg and Erlangen as well as at Nuremberg hospital served as a basis for this investigation. Clinical data were obtained and provided by the salivary gland registry Erlangen (Head: Prof. Dr. H. Iro) in cooperation with the cancer centers Erlangen-Nuremberg and Regensburg. Applying a special punch technique, tissue microarrays were fabricated and subsequently stained by means of a polymer conjugate staining method (EnVi-sionTM) for visualizing the above mentioned markers. Point scores based on the degree of staining and staining intensity served as a basis for statistical analysis (SPSS 18.0). This approach allowed for a retrospective analysis of both, differences between regular tissue and tumor tissue (t-test for independent variables \u03b1 = 0.05) and expres-sion profile related differences in survival rate of the patients (univariate Kaplan-Meier analysis, \u03b1 = 0.05; multivariate backwards Cox-regression with stepwise elimination at \u03b1 = 0.05 and confidence interval, CI 95%).\nC Results and observations\nIn terms of patient gender, tumor localisation and tumor entity, a representative patient population comparable to the data reported by the world health organization could be compiled. In the most frequent carcinomas statistically significant above und below average expressions of the markers examined were found. Taking into account the histogenesis of the tumors, carcinomas from identical localisations showed comparable expression profiles. Furthermore, the effect of changes in the expression of specific immunologically relevant parameters on patient survival was studied and subject to multivariate analysis in context with already established clinical parameters. Univariate analysis of all of the 288 tumors revealed that tumors with below average expression of LMP-7 in nuclear analysis (p=0.005) as well as above average expression of \u03b22-Microglobulin (p=0.024) and HLA-A (p<0.001), TAP-1 (p=0.01) and Tapasin (p<0.001) were associated with a lower survival rate. However, in some entities, deviations from these findings were found. In terms of clinical parameters, patients aged above 60 years (p<0.001), male patients (p=0.002), patients with tumors in the parotid gland (p=0.005) as well as patients with histopathologic factors such as high pT/N- and UICC-stage (p<0.001) high level of malignity G2/3 (p<0.001), residual tumors R1/2 (p<0.001) and entities among each other showed lower survival rates. Multivariate analysis revealed that a high level of malignity (Hazard Ratio, HR 4.1), advanced staging of local tumour (HR 1.3) or distant metastasis (HR 1.95), above average expression of Tapasin (HR 1.7) and HLA-A (HR 1.95) had a negative effect on the relative risk of death, a below average expression of LMP-7 in nuclear analysis seems to be a positiv factor of survival (HR 0.58).\nD Conclusion\nAlthough the expression of immunologically important proteins differs greatly between the biologically different types of salivary gland carcinomas, the prognostic value of specific markers (HLA-A, LMP-7 and Tapasin), also in context with established clinical and pathologic parameters, could be shown. Currently, these findings may be used as additional determinants in developing a patient specific adjuvant tumortherapy (radiation therapy, chemotherapy). Functional studies should be conducted to clarify whether immunologic therapies are feasible based on the current findings.\n2015-01-10\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5697\nurn:nbn:de:bvb:29-opus4-56979\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56979\nhttps://opus4.kobv.de/opus4-fau/files/5697/Korrigiert%20Abschluss%20Erlangen%20Bibliothek.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5953\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\nmsc\nmsc:60J25\nmsc:60K35\nmsc:92D25\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Mathematik\nThe historical process of the spatial Moran model with selection and mutation\nDer historische Prozess des r\u00e4umlichen Moran-Modells mit Selektion und Mutation\nSeidel, Peter\nWahrscheinlichkeitstheorie\nPopulationsgenetik\nStochastisches Teilchensystem\nddc:519\nWe consider a spatial, locally finite and multitype population model, the Moran model, in which each individual of the population inhabits a site in geographic space and has a genetic type. The evolution of the population is given by migration, type-dependent mutation, resampling and selection, and the large population limit leads to a collection of spatially interacting Fleming-Viot processes.\nThis thesis is dedicated to investigate the evolution of genealogical information of the Moran model forward in time. More precisely, we construct a path-valued Markov process, the historical process of the Moran model, in which we assign to each individual alive at time $t$ its extended ancestral line. The collection of extended ancestral lines contains the ancestral line of each individual and the genealogical distance of each pair of individuals alive at time $t$. We analytically characterize the historical process of the Moran model by means of a well-posed martingale problem, where we have to use a refined Liggett-Spitzer space to describe the evolution of genealogical information of a locally finite Moran population.\nThe main tool is a Feynman-Kac duality for the historical process of the Moran model. The dual process, which we call historical backward process, in a natural way generates the extended ancestral lines backward in time, where it is driven by new kinds of mechanisms compared to the known duals for the Moran model. It is the huge amount of genealogical information contained in the extended ancestral lines that gives rise to this historical backward process, however a functional of the historical backward process evolves as a (site,type)-marked coalescent if the initial site-type distribution in the Moran model is nice and there is no selection.\nThe main result is a stochastic representation for the conditional law of the extended ancestral lines of a subpopulation alive at a fixed time $T$ given the site-type information of the subpopulation at this time $T$ in terms of a transformation of the historical backward process arising by a special change of measure, where the transformed backward process is a time-inhomogeneous Markov process which turns out to be time-homogeneous if the site-type information of the Moran model is in equilibrium. In addition, our special change of measure is a new way to transform a general Markov process and in a special case it is the compensated $h$-transform of such a Markov process.\nAs an application we obtain that the law of the extended ancestral lines of a subpopulation alive at time $T$ converges in the limit $T \\to \\infty$ if the law of the site-type information of the population alive at time $T$ converges. In addition, we give an explicit representation for the expected fixation time of a $d$-dimensional Fisher-Wright diffusion and determine for the case where the population is located at one site and there is neither mutation nor selection the distribution function for the conditional law of the genealogical distance of two individuals alive at time $T$ given the types of these two individuals at this time $T$.\nWir betrachten ein r\u00e4umliches und lokal endliches Populationsmodell, das Moran-Modell, in welchem ein jedes der Individuen der Population einen geographischen Ort besiedelt und einen von endlich vielen verschiedenen genetischen Typen hat. Die Evolution der Population ist durch Migration, typenabh\u00e4ngige Mutation, Resampling und Selektion gegeben. Der Diffusionslimes f\u00fchrt zu einer Kollektion von r\u00e4umlich interagierenden Fleming-Viot-Prozessen.\nIn dieser Dissertation widmen wir uns der Untersuchung der Evolution genealogischer Information vorw\u00e4rts in der Zeit. Wir konstruieren explizit einen pfadwertigen Markovprozess, den historischen Prozess des Moran-Modells, wobei wir jedem Individuum, welches zur Zeit $t$ lebt, seine erweiterte Ahnenlinie zuweisen. Die Kollektion von erweiterten Ahnenlinien enth\u00e4lt sowohl die Ahnenlinie eines jeden Individuums als auch die genealogische Distanz eines jeden Paares von Individuen, die zur Zeit $t$ leben. Wir charakterisieren den historischen Prozess des Moran-Modells analytisch mit Hilfe eines gut gestellten Martingalproblems, wobei wir einen weiterentwickelten Liggett-Spitzer-Raum verwenden, um die Evolution genealogischer Information einer lokal endlichen Population zu beschreiben.\nDas wichtigste Instrument ist eine Feynman-Kac-Dualit\u00e4t f\u00fcr den historischen Prozess des Moran-Modells. Der duale Prozess, welchen wir als historischen R\u00fcckw\u00e4rtsprozess bezeichnen, erzeugt auf nat\u00fcrliche Weise die erweiterten Ahnenlinien r\u00fcckw\u00e4rts in der Zeit. Dabei wird der historischen R\u00fcckw\u00e4rtsprozess, verglichen mit den bekannten dualen Prozessen des Moran-Modells, von ganz neuen Mechanismen gesteuert. Die von den erweiterten Ahnenlinien beschriebene enorme Menge an genealogischer Information ist der Grund f\u00fcr diesen historischen R\u00fcckw\u00e4rtsprozess, jedoch evolviert ein Funktional des historischen R\u00fcckw\u00e4rtsprozesses wie ein (Ort,Typ)-markierter Koaleszent, wenn die Orte und die Typen im Moran-Modell am Anfang sch\u00f6n verteilt sind und keine Selektion vorliegt.\nDas Hauptresultat ist eine stochastische Darstellung der bedingten Verteilung der erweiterten Ahnenlinien einer Teilpopulation, die zu einer festen Zeit $T$ lebt, gegeben die Ort-Typ Information der Teilpopulation zu dieser Zeit $T$ mittels einer Transformation des historischen R\u00fcckw\u00e4rtsprozesses, die durch einen speziellen Ma\u00dfwechsel zustande kommt. Dabei ist der transformierte R\u00fcckw\u00e4rtsprozess ein im Allgemeinen zeitinhomogener Markovprozess, welcher aber zeithomogen ist, falls die Ort-Typ Information des Moran-Modells im Equilibrium ist. Zudem ist unser spezieller Ma\u00dfwechsel, der in einem Spezialfall die kompensierte $h$-Transformierte eines Markovprozesses ist, eine neue Art und Weise einen allgemeinen Markovprozess zu transformieren.\nAls Anwendung erhalten wir, dass die Verteilung der erweiterten Ahnenlinien einer Teilpopulation, die zur Zeit $T$ lebt, im Limes $T \\to \\infty$ konvergiert, falls die Verteilung der Ort-Typ Information der Population zur Zeit $T$ konvergiert. Zudem geben wir eine explizite Darstellung f\u00fcr die erwartete Fixationszeit einer $d$-dimensionalen Fisher-Wright Diffusion und bestimmen f\u00fcr den Fall, dass die Population an einem Ort lebt und es weder Mutation noch Selektion gibt, die bedingte Verteilung der genealogischen Distanz zweier Individuen, die zur Zeit $T$ leben, gegeben die Typen dieser zwei Individuen zur Zeit $T$.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5953\nurn:nbn:de:bvb:29-opus4-59538\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-59538\nhttps://opus4.kobv.de/opus4-fau/files/5953/PeterSeidelDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6042\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:E.\npacs\npacs:00.00.00\nmsc\nmsc:28-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nPr\u00e4diktoren des Bildrauschens bei einer CT-Koronarangiographie\nPatient-specific Predictors of Image Noise\nSchaefer, Marcella\nBildrauschen\nddc:610\nPatientenspezifische Vorwert, der eine Aussage dar\u00fcber macht, ob man ein Niedrig-Dosen-Protokoll verwenden kann\nwhich parameter may be the optimal patient-specific discriminator to determine the use of reduced tube voltage\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6042\nurn:nbn:de:bvb:29-opus4-60422\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60422\nhttps://opus4.kobv.de/opus4-fau/files/6042/Pr%C3%A4diktoren%20des%20Bildrauchens%20letzteVersion09.2013.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6098\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nmsc\nmsc:55U10\nmsc:62-09\nmsc:62B10\nmsc:94A17\nmsc:94C15\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Mathematik\nSpektrale Hypergraphen Partitionierung und Relative Entropie\nSpectral Hypergraph Partitioning and Relative Entropy\nGmeiner, Peter\nHypergraph\nEntropie <Informationstheorie>\nExponentialfamilie\nIsoperimetrische Ungleichung\nSimplizialer Komplex\nUngerichteter Graph\nddc:510\nDie vorliegende Arbeit handelt vom informationstheoretischen Unterschied zwischen\nHypergraphen und ungerichteten Graphen, der sich bei spektraler Bisektion dieser\nObjekte zeigt. Es werden auch die Auswirkungen allgemeinerer L\u00f6sch- und Kontraktionsoperationen\nvon Hyperkanten in einem Hypergraphen von verschiedenen\nanderen Blickwinkeln aus betrachtet. In der Arbeit schr\u00e4nken wir uns auf eine Klasse\nvon Hypergraphen ein, die strukturell etwas reichhaltiger ist: Simplizialkomplexe.\nDiese erm\u00f6glichen es h\u00f6herdimensionale Laplace-Operatoren zu definieren, welche in\nnat\u00fcrlicher Weise Laplace-Operatoren auf ungerichteten Graphen verallgemeinern.\nEbenso k\u00f6nnen isoperimetrische Gr\u00f6\u00dfen f\u00fcr Graphen - kombinatorische Cheeger-\nKonstanten - auf Simplizialkomplexe erweitert werden. Es wird eine neue isoperimetrische\nGr\u00f6\u00dfe f\u00fcr Simplizialkomplexe, basierend auf minimalem Interaktionsverlust,\nvorgeschlagen und eine entsprechende isoperimetrische Ungleichung f\u00fcr diese\nGr\u00f6\u00dfe gezeigt. Die Kullback-Leibler Divergenz, auch bekannt als relative Entropie,\nwird verwendet um den Unterschied zwischen Simplizialkomplexen und ungerichteten\nGraphen aus informationstheoretischer Sicht zu betrachten. Mittels hierarchischer\nExponentialfamilien, die in nat\u00fcrlicher Weise von Simplizialkomplexen induziert\nwerden, definieren wir eine relative Entropie zwischen Simplizialkomplexen.\nZur systematischen Untersuchung dieser relativen Entropie betrachten wir deren\nVerhalten bei der Ausf\u00fchrung bestimmter Operationen, wie L\u00f6sch- und Kontraktionsoperationen\nvon Simplizes, auf Simplizialkomplexen. Durch eine genaue Analyse\nvon marginalisierten Exponentialfamilien und der Verwendung von bereits bekannten\nZerlegungsformeln f\u00fcr die Kullback-Leibler Divergenz, gelingt es eine Klasse von\nOperationen auf Simplizialkomplexen zu identifizieren, f\u00fcr welche sich zahlreiche exakte\nAusdr\u00fccke und Ungleichungen der relativen Entropie herleiten lassen. F\u00fcr allgemeinere\nOperationen auf Simplizialkomplexen wird eine obere Schranke der relativen\nEntropie gezeigt. Mittels einer expliziten Konstruktion von Wahrscheinlichkeitsverteilungen\nlassen sich untere Schranken der relativen Entropie zwischen getrennten\nund nicht getrennten Simplizialkomplexen angeben. Basierend auf der Beobachtung\naus der spektralen Graphentheorie, in welcher der kleinste nichttriviale Eigenwert\ndes Laplace-Operators mit einer kombinatorischen Cheeger-Konstante in Relation\ngebracht wird, zeigen wir einen Zusammenhang dieses Eigenwertes mit skalierten relativen\nEntropiegr\u00f6\u00dfen. Dazu wird eine informationstheoretische Cheeger-Konstante\neingef\u00fchrt, welche wir mit einer kombinatorischen Cheeger-Konstante in Verbindung\nbringen. Es lassen sich dann informationstheoretische Cheeger-Ungleichungen\nf\u00fcr ungerichtete Graphen und f\u00fcr Simplizialkomplexe herleiten. Insbesondere ist es\nm\u00f6glich durch eine sogenannte Entropiel\u00fccke den informationstheoretischen Unterschied\nzwischen Simplizialkomplexen und ungerichteten Graphen zu sehen.\nThis thesis is about the information-theoretic difference between hypergraphs and\nundirected graphs, which arises in spectral bisections of these objects. We also\ninvestigate the effect of general deletion and contraction operations of hyperedges\nin a hypergraph from different points of view. We restrict ourselves to a wellstructured\nclass of hypergraphs, namely, simplicial complexes. This allows us to\ndefine higher-dimensional Laplace operators, which generalize graph Laplacians in\na natural way. Similarly, isoperimetric numbers for graphs - combinatorial Cheeger\nconstants - can be extended to simplicial complexes. We suggest a new isoperimetric\nquantity for simplicial complexes, based on minimal interaction loss, and show\nan isoperimetric inequality for them. The Kullback-Leibler divergence, also known\nas relative entropy, is used to consider the difference between simplicial complexes\nand undirected graphs from an information-theoretic point of view. By means of\nhierarchical exponential families, which are naturally induced by simplicial complexes,\nwe define a relative entropy between simplicial complexes. For a systematic\ninvestigation of this relative entropy we consider its effect under certain operations,\nlike deletion and contraction of simplices in a simplicial complex. With a detailed\nanalysis of marginalized exponential families and with already known decomposition\nformulas for the Kullback-Leibler divergence, we succeed to identify a class of\noperations on simplicial complexes for which we can deduce many exact expressions\nand inequalities for the relative entropy. For more general operations on simplicial\ncomplexes we can show an upper bound for the relative entropy. Through an\nexplicit construction of probability distributions we are able to state lower bounds\nof the relative entropy between separated and non separated simplicial complexes.\nBased on the observation from spectral graph theory, in which the smallest nontrivial\neigenvalue of the Laplacian is related to a combinatorial Cheeger constant,\nwe show a connection of this eigenvalue to scaled relative entropies. For this purpose,\nan information-theoretic Cheeger constant is introduced, which is related to\na combinatorial Cheeger constant. This allows us to derive information-theoretic\nCheeger inequalities for undirected graphs and simplicial complexes. In particular,\nit is possible to see the information-theoretic difference between undirected graphs\nand simplicial complexes by means of an entropy gap.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6098\nurn:nbn:de:bvb:29-opus4-60985\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60985\nhttps://opus4.kobv.de/opus4-fau/files/6098/PeterGmeinerDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6197\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:530\nddc:600\nccs\nccs:I.6.4\npacs\npacs:89.20.Bb\nmsc\nmsc:65Z05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nSimulation and Modeling of Silicon Carbide Devices\nSimulation und Modellierung von Siliziumkarbid-Bauelementen\nUhnevionak, Viktoryia\nWide-gap-Halbleiter, Siliciumcarbid, Simulation, n-Kanal-FET, Hall-Beweglichkeit, Haftstelle\nddc:530\nddc:600\nIn den letzten Jahren wurde Siliziumkarbid (SiC) ein attraktives Material f\u00fcr die Leistungselektronik und \u00f6ffnete wegen seiner \u00fcberlegenen Materialeigenschaften neue Perspektiven auf diesem Gebiet. Der hohe Bandabstand, die hohe thermische Leitf\u00e4higkeit und\ndie hohe Durchbruchfeldst\u00e4rke machen SiC zum Material der Wahl f\u00fcr Leistungs-MOSFETs.Die Verwendung von SiC MOSFETs in Leistungswandlern erlaubt zum Beispiel eine Verringerung deren Gewicht und Gr\u00f6\u00dfe. Das kann ein gro\u00dfer Vorteil f\u00fcr viele Anwendungen inklusive Elektroautos sein. Die F\u00e4higkeit von SiC Bauelementen, auch bei hohen Temperaturen zu funktionieren, vereinfacht das W\u00e4rmemanagement von elektrischen Systemen. Die kommerzielle Nutzung von SiC MOSFETs ist derzeit jedoch durch technologische Probleme\nbegrenzt, die sich in Form von niedrigen Kanalbeweglichkeiten und hohen Einschaltspannungen manifestieren.\nDer Zweck dieser Doktorarbeit war, durch numerische Simulationen den Mechanismus zu verstehen und zu erkl\u00e4ren, der in SiC MOSFETs die Kanalbeweglichkeit bestimmt und eine selbstkonsistente Simulationsmethodologie zur Beschreibung der elektrischen Eigenschaften von\nSiC MOSFETs zu entwickeln. F\u00fcr den technologischen Fortschritt, sowie f\u00fcr die Entwicklung und Optimierung von Halbleiterbauelementen ist der rechnergest\u00fctzte Entwurf von elektronischen Bauelementen und ihrer Herstellung (TCAD \u2013 Technology Computer Aided\nDesign) zu einem zunehmend wichtigen Untersuchungswerkzeug geworden. TCAD-Simulationen f\u00fcr SiC-Bauelemente sind aktuell jedoch eine gro\u00dfe Herausforderung. Die meisten Simulationsmodelle wurden f\u00fcr Silizium entwickelt und k\u00f6nnen deshalb die Transporteigenschaften von SiC-Bauelementen nicht ad\u00e4quat beschreiben. Dar\u00fcber hinaus ist die Grenzschicht zwischen Siliziumkarbid und Gateoxiden durch eine hohe Konzentration von Haftstellen charakterisiert, die die Kanalbeweglichkeit in SiC-MOSFETs stark degradieren.Deshalb ist ein genaues Modell f\u00fcr die Haftstellen an der Grenzschicht von vorrangiger Bedeutung f\u00fcr die Simulation.\nIm Rahmen des Projekts MobiSiC (Mobility Engineering for SiC Devices) wurden laterale n-Kanal 4H-SiC MOSFETs hergestellt und elektrisch durch Strom-Spannungs-und Halleffektmessungen charakterisiert. Die Effekte von Temperatur und Substratdotierung auf die\nTransporteigenschaften im Kanal von SiC MOSFETs wurden untersucht. Die Interpretation sowohl der Strom-Spannungskennlinen (ID(VG)) als auch der aus den Halleffektmessungen abgeleiteten Schichtladungstr\u00e4gerkonzentrationen und Kanalbeweglichkeiten (ninv(VG),\u03bc(VG)) wurden in dieser Arbeit mit Hilfe numerischer Simulationen mit Sentaurus Device von Synopsys durchgef\u00fchrt.\nZur genauen Analyse der Halleffektmessungen wurde eine neue Methode der Berechnung des Hallfaktors entwickelt. Sie beruht auf der Tatsache, dass sowohl der Hallfaktor als auch die Beweglichkeit von denselben Mechanismen bestimmt werden, durch die die Ladungstr\u00e4ger\ngestreut werden. Die Berechnungsmethode ber\u00fccksichtigt alle Streumechanismen im aktiven Bereich der Bauelemente. Auf diese Weise ist es zum ersten Mal m\u00f6glich, einen genauen Wert f\u00fcr den Hallfaktor f\u00fcr den Kanal von MOSFETs zu berechnen und f\u00fcr die Korrektur der Halleffektmessungen zu verwenden.\nExperimentelle Daten, z. B. von Halleffektmessungen, werden oft zur Charakterisierung der Dichte von Haftstellen an der Grenzschicht verwendet. In dieser Arbeit wird eine neue Methode vorgeschlagen, die eine genauere Charakterisierung erlaubt. In einem ersten Schritt werden Haftstellendichten als Funktion der Energie (DIT(ET)) von Halleffekt- und Kapazit\u00e4ts-Spannungsmessungen auf konventionelle Art extrahiert. Danach werden sie in Sentaurus Device eingegeben und numerisch optimiert um die Abweichungen zwischen den simulierten Kennlinien (ID(VG), ninv(VG) und \u03bc(VG)) und den Messungen zu minimieren. Die numerische Simulation erlaubt, Effekte wie z.B. die Potentialverteilung zwischen Source und Drain sowie die Fermi-Dirac-Verteilung der Elektronen zu ber\u00fccksichtigen. Solche Effekte bleiben bei der konventionellen Extraktion der Haftstellendichte unber\u00fccksichtigt. Es ist deshalb zu erwarten, dass die neue Methode physikalisch schl\u00fcssigere Ergebnisse f\u00fcr Haftstellendichte DIT(ET)liefert.Basierend auf den experimentellen Ergebnissen und den Simulationen werden Ursprung und Natur der Grenzfl\u00e4chenhaftstellen diskutiert.\nDie Simulationsmethodologie, in die die Methode der Berechnung von Hallfaktoren und die Methode der Extraktion der Haftstellendichte\neingingen, konnte konsistent die Temperaturabh\u00e4ngigkeit sowie die Konzentrationsabh\u00e4ngigkeit der Transporteigenschaften der\nin dieser Arbeit betrachteten SiC MOSFETs beschreiben. Auf der Basis der guten \u00dcbereinstimmung zwischen Simulationen und Messungen konnten die Streumechanismen im Kanal von SiC MOSFETs mit unterschiedlichen Dotieratomkonzentrationen und bei unterschiedlichen Temperaturen umfassend interpretiert werden. Eine der Haupterkenntnisse dieser Arbeit ist, dass eine Verringerung der Grenzfl\u00e4chenhaftstellendichte nicht der einzige Faktor zur Verbesserung der Eigenschaften von SiC MOSFETs ist. Ihre Eigenschaften k\u00f6nnen z.B. auch durch eine Verringerung der Kanaldotierung erheblich verbessert werden. Weiterhin wurde gefunden, dass die Konzentration der Kanaldotierung die Temperaturabh\u00e4ngigkeit der Kanalbeweglichkeit beeinflusst: Bei hochdotierten MOSFETs steigt sie mit der Temperatur w\u00e4hrend sie sich bei niedrig dotierten MOSFETs verringert.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6197\nurn:nbn:de:bvb:29-opus4-61975\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-61975\nhttps://opus4.kobv.de/opus4-fau/files/6197/Thesis-UhnevionakViktoryia.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6217\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:539\nccs\nccs:A.\npacs\npacs:04.60.Pp\nmsc\nmsc:81Q20\nmsc:83C45\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nOn the relation of canonical and covariant formulations of Loop Quantum Gravity\nZipfel, Antonia\nQuantengravitation\nQuasiklassische N\u00e4herung\nddc:539\nLoop Quantum Gravity (LQG) is a background independent approach towards a quantum theory of gravity that splits into a canonical and a covariant branch the latter of which is also often called spin foam model. The spin foam model can only be derived formally from a constrained BF-theory that is discretized prior to quantization so that the resulting quantum theory is not continuous while canonical LQG rests on a true representation of the continuum canonical commutation relations at the kinematical level. The covariant approach therefore raises a lot of questions, for example, the faith of Dirac observables is unknown to a large extend and it is neither settled whether all constraints are implemented in the spin foam model nor how to take the continuum limit. In contrast, it is possible to even solve the quantum dynamics of deparametrized models coupled to a scalar field within the canonical approach. However, the full physical Hilbert space of canonical LQG can still not be determined satisfactorily even though a non-anomalous quantization of the Hamiltonian constraint is known. It is widely believed although not proven that spin foams could be a useful tool in order to solve this issue. A comparison of the two approaches could therefore give valuable insights for both branches of LQG.\nSemiclassical techniques provide an important tool to check the consistency of a model in the absence of experimental data. In canonical LQG such a limit is realized through so-called complexifier coherent states. These states could for example be used in the context of deparametrized models, e.g. dust models, where the Hamiltonian is not a constraint but defines a true evolution, in order to compare the semiclassical dynamics of the canonical theory with the prediction of a (to-be-defined) spin foam model coupled to dust. Since spin foams should already incorporate the dynamics it is crucial for such a comparison that the coherent states defined in the canonical theory stay (approximately) coherent throughout the evolution. In the first part of the thesis a stability criterion for finite systems is derived an analyzed. Although some important insights can be gained, the formalism developed turns out to be too restrictive to be applied to LQG. Despite some promising hints a further investigation of these issues would go beyond the scope of this thesis.\nInstead it is examined whether spin foam techniques can be directly applied to construct a projector onto the physical Hilbert space of the canonical theory. The fundaments for this ansatz where laid in a seminal work by Kaminski, Kisielowski and Lewandowski [1\u20133] who extended the definition of the (Euclidean) spin foam model to arbitrary boundary graphs. This finally allows to make contact to the canonical formulation whose Hilbert space contains all these graphs. The KKL-model used in the thrid part of the dissertation to build a spin foam operator Z[\u03ba] based on abstract 2-complexes \u03ba that acts on the kinematical Hilbert space H0 of LQG by identifying the spin nets induced on the boundary graph of \u03ba with states in H0. Following a common heuristic argument, a rigging map is then postulated by summing over all possible spin foams \u03ba including a possible weight that is designed such that it does not violate a certain gluing property. In the analysis of the resulting object it is possible to identify a spin foam transfer matrix that allows to generate any finite foam as a finite power of the transfer matrix.\nThat the would-be rigging map has the chance to define a projector is demonstrated in a simplified situation where the class of foams is restricted to a certain type with only one internal vertex and where the weight and Barbero-Immirzi parameter are set to one. The so-obtained sum indeed annihilates the Euclidean constraint whose matrix elements are calculated here as well.\nDespite these certainly encouraging results, the further analysis transpires that the full sum over all \u03ba, as written, does not define a projector into the physical Hilbert space. This statement is independent of the concrete spin foam model and Hamiltonian constraint. However, the transfer matrix potentially contains the necessary ingredient in order to construct a proper rigging map in terms of a modified transfer matrix.\nThere are several hints that the failure of the sum to define a rigging map is caused by the fact that the simplicity constraint used in current spin foam models does not only admit gravitational solutions but includes all Plebanski sectors. A similar effect also causes problems in the asymptotic expansion of the Euclidean 4-simplex amplitude but can be cured by an additional constraint as shown in [4\u20137]. These results are extended to the Lorentzian spin foam model in the last part.\nSchleifenquantengravitation (SQG) ist ein hintergrundunabha\u0308ngiger Zugang zur Quantengravitation und teilt sich in zwei Ansa\u0308tze, den kanonischen und den kovarianten. Letzterer wird auch als Spin- schaummodell bezeichnet. Wa\u0308hrend die kanonische SQG auf einer korrekten Darstellung der kontinuier- lichen kanonischen Vertauschungsrelationen aufbaut, kann das Spinschaummodell nur formal von einer BF-Wirkung, deren Name sich von ihrer Form ableitet, mit zusa\u0308tzlichen Zwangsbedingungen hergeleitet werden. Dabei wird bereits die klassische Theorie diskretisiert, so dass die daraus resultierende Quan- tentheorie im Gegensatz zur kanonischen SQG keine kontinuierlichen Freiheitsgrade aufweist. Des Weiteren ist zum Beispiel die Behandlung von Dirac Observablen im kovarianten Ansatz weitgehend unverstanden und es ist nicht bewiesen, ob alle Zwangsbedingungen tatsa\u0308chlich implementiert sind. In der kanonischen SQG kann hingegen sogar die Quantendynamik im Rahmen deparametrisierter Modelle mit gekoppelten Skalarfeldern gelo\u0308st werden. Jedoch ist der physikalische Hilbertraum der vollen Theorie bisher nur unzureichend bestimmt, obwohl seit La\u0308ngerem eine anomaliefreie Quantisierung der Hamiltonschen Zwangsbedingung bekannt ist. Es wird ha\u0308ufig angenomen, das Spinschaummodell ko\u0308nnte ein nu\u0308tzliches Mittel darstellen, dieses Problem zu lo\u0308sen. Ein Vergleich der kovarianten und kanonischen SQG kann daher wertvolle Erkenntnisse fu\u0308r beide Ansa\u0308tze liefern.\nDa bisher experimentelle Daten fehlen, ist eine semiklassische Analyse gut geeignet, die Konsistenz der Theorien zu pru\u0308fen. Im kanonischen Ansatz wird ein solcher Limes mit Hilfe spezieller koha\u0308renter Zusta\u0308nde definiert. Solche Zusta\u0308nde ko\u0308nnen zum Beispiel dazu benutzt werden die semiklassische Dynamik deparametrisierter Modelle wie z.B. Staubmodelle, in welchen die Hamiltonfunktion eine echte Evolution generiert und keine Zwangsbedingung ist, mit den Vorhersagen eines hypothetische Staub-Spinschaummodells zu vergleichen. Weil Spinschaumamplituden definitionsgema\u0308\u00df die Dynamik implementieren sollten, ist es fu\u0308r einen Vergleich der beiden Ansa\u0308tze wichtig, dass die koha\u0308renten Zusta\u0308nde in der kanonischen Theorie (approximativ) stabil unter der Zeitentwicklung sind. Im ersten Teil der Ar- beit wird ein Stabilita\u0308tskriterium fu\u0308r endlich dimensionale Systeme hergeleitet und untersucht. Obwohl einige wichtige Erkenntnisse gewonnen werden ko\u0308nnen, ist der hier entwickelte Formalismus zu restriktiv um in der SQG angewendet werden zu ko\u0308nnen. Trotz vielversprechender Hinweise kann die obige Problematik hier leider nicht weiterverfolgt werden, da sie zu weit vom Thema der Arbeit abfu\u0308hren wu\u0308rde.\nAnstatt dessen wird im Hauptteil der Dissertation untersucht, ob sich Spinschaumtechniken nutzen lassen, um einen Projektor auf den physikalischen Hilbertraum der kanonischen Theorie zu bauen. Der Grundstein hierfu\u0308r wurde in der wegweisenden Arbeit von Kaminski, Kisielowski und Lewandowski [1\u20133] gelegt, da es das KKL-Modell erlaubt auch Spinscha\u0308ume mit allgemeineren Randgraphen, wie sie in der kanonischen Theorie vorkommen, zu definieren. Indem die auf dem Rand eines 2-Komplexes \u03ba induzierten Spinnetze mit Elementen im kinematischen Hilbertraum H0 der kanonischen Theorie iden- tifiziert werden, kann im dritten Teil ein Spinschaumoperator Z[\u03ba] auf H0 konstruiert werden. Einer gela\u0308ufigen heuristischen Argumentation folgend, wird anschlie\u00dfend eine sogenannte \u2018Riggingabbildung\u2019 postuliert, indem alle mo\u0308glichen Spinschaumoperatoren aufsummiert werden. Die einzelnen Summanden Z[\u03ba] ko\u0308nnen dabei auch unterschiedlich gewichtet werden, um die Summe zu regularisieren, solange das Gewicht nicht eine gewisse Klebeeigenschaft verletzt. In der weiteren Analyse des resultierenden Objekts ist es schlie\u00dflich mo\u0308glich eine spezifische Spinschaumtransfermatrix zu identifizieren, mit der jeder beliebige Spinschaum generiert werden kann.\nDass die postulierte Riggingabbildung tatsa\u0308chlich ein Projektor auf den physikalischen Hilbertraum sein ko\u0308nnte, wird anhand eines vereinfachenden Beispiels gezeigt. Wenn die Klasse der 2-Komplexe auf spezielle Komplexe mit nur einem internen Vertex eingeschra\u0308nkt wird und das Gewicht und der Barbero- Immirzi Parameter gleich eins gewa\u0308hlt werden, dann anihiliert das resultierende Objekt tatsa\u0308chlich die Euklidische Hamiltonsche Zwangsbedingung, deren Matrixelemente im zweiten Teil berechnet werden.\nTrotz dieses ermutigenden Resultats stellt sich im weiteren Verlauf heraus, dass die vollsta\u0308ndige Summe u\u0308ber alle 2-Komplexe keinen Projektor auf den physikalischen Hilbertraum liefert. Diese Tatsache ist unabha\u0308ngig von den gewa\u0308hlten Parametern innerhalb des Spinschaummodells und der konkreten Quantisierung der Hamiltonschen Bedingung. Jedoch ko\u0308nnten diese Probleme eventuell durch eine modifizierte Spinschaumtransfermatrix behoben werden.\nEs gibt mehrere Hinweise, dass das Fehlverhalten der postulierten Riggingabbildung darauf zuru\u0308ckzufu\u0308hren ist, dass die Simplizita\u0308tsbedingungen, die momentan im Spinschaummodell implementiert werden, alle sogenannten Plebanskisektoren mit einschliessen. Ein a\u0308hnliches Problem taucht auch im asymptotischen Limes der Euklidischen 4-Simplexamplitude auf und kann durch eine zusa\u0308tzliche Bedingung vermieden werden, wie in [4\u20137] gezeigt wurde. Dieses Resultat wird im letzten Teil auf das Lorentzsche Modell erweitert.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6217\nurn:nbn:de:bvb:29-opus4-62172\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62172\nhttps://opus4.kobv.de/opus4-fau/files/6217/Phdthesis.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6275\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:536\nddc:608\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nPhysical assessment of a novel concept for computed tomography with a stationary source ring of fixed X-ray anodes for medical imaging\nKellermeier, Markus\nCT physics Fact Fixed anode CT Thermal load capacity X-ray anode\nddc:536\nddc:608\nBackground and Aims:\nThe potential performance of current technology for computed tomography (CT) is basically limited by the thermal load capacity of the X-ray anode. In addition, the gantry rotation time is limited, because the centrifugal force can become destructive. Focusing primarily on the thermal load capacity, this work intends to show a novel CT with fixed anodes (Fixed Anodes Computed Tomography: FACT), which has basically no mechanical constraints. For comparison, a common clinical CT was used as Reference CT.\nMaterial and Methods:\nMonte Carlo simulations, with the free software combination GAMOS/GEANT4, were used to determine the energy deposition of accelerated electrons in the so-called thermal focal spot of an X-ray anode. The thermal energy thus obtained was used to determine the temperature distribution, using the finite element method, where the commercial software COMSOL Multiphysics was used. Based on the typical sizes of the anode, a basic simulation model has been developed for fixed anodes, which comprises a thin tungsten layer embedded in a copper block, and is transmitted for comparison to a model for a rotating anode. The operating temperature of the X-ray focal spot in the tungsten target of the anode was set at a maximum permissible temperature of 2500 K. For cooling, a heat bath at 300 K has been defined at the distal copper block end. Simulations for the fixed anode model were performed under the assumption of a short thermal load till up to the beginning of long thermal load (1 \u03bcs \u2013 1 s) at the respective performances of the maximum thermal focal spots. Subsequently, numerous parameters were changed for carrying out the studies of the thermal model of the fixed anode in order to investigate the validity of modelling. Long-term exposure of a single fixed anode was examined for continuous pulse operation at any FACT. A virtual model for FACT has been created in order to analyse different temporal sequences in the X-ray source ring, forming a circular array of 1160 single fixed anodes. Under the assumption of known detector characteristics, which have been transferred to the much lower integration times at FACT, the image quality was determined using the CT image reconstruction library ROTLib at the Institute of Medical Physics in Erlangen, Germany. With the definition of a Relative Performance (RP ), the relation to the Reference CT was made.\nResults:\nSimulations for thermal focal spot power density (P_th/A) of anodes were in good agreement with literature results and vendor data with regard to their absolute values and trends. Studies on continuous pulse operation showed that with fixed anodes, the FACT can be operated, in principle, in an unlimited manner. The virtual model for FACT showed that around 60 gantry rotations per second are required to achieve the reference CT, according to the X-ray power (at RP = 1). Un- der this condition, an equivalent image quality in FACT can be obtained by superposition. The optimal duration of each projection direction was 10 \u03bcs. With a beam pause of 1 \u03bcs between the projections 78.4 rounds per second at successive X-ray source activities were possible, resulting in an RP of 1.3 at the same focal spot size, in comparison with the Reference CT.\nConclusions:\nAs a stationary system with fixed anodes, FACT has no focal spot blurring of the X-ray source during projection. With an RP > 1, a shorter scan time can be achieved, while maintaining radiation exposure and image quality. Based on the high number of rounds at each low dose, we can conclude that FACT can support a high image frame rate and very thin slices, which could be of significant advantage in a wide range of medical diagnostic as well as technical applications.\nHintergrund und Ziele:\nDie potentiale Leistungsf\u00e4higkeit eines aktuellen Computertomographen (CT) ist im Wesentlichen durch die thermische Belastbarkeit der R\u00f6ntgenanode begrenzt. Zus\u00e4tzlich ist die Gantry-Umlaufzeit limitiert, da mit wachsender Zentrifugalkraft eine destruktive Wirkung einhergeht. Mit Blick auf die thermische Belastbarkeit ist in dieser Arbeit ein neuartiges CT mit Festanoden (engl.: Fixed Anodes Computed Tomography: FACT) vorgestellt worden, was vom grundlegenden Konzept aus keine mechanische Einschr\u00e4nkung aufweist. Zum Vergleich wurde ein g\u00e4ngiges klinisches CT als Referenz-CT herangezogen.\nMaterial und Methoden:\nMithilfe der freien Softwarekombination GAMOS/GEANT4 wurden Monte-Carlo-Simulationen durchgef\u00fchrt, um die Energiedeposition von beschleunigten Elektronen in dem sogenannten thermischen Brennfleck einer R\u00f6ntgenanode zu bestimmen. Die so erhaltene thermische Energie diente zur Ermittlung der Temperaturverteilung unter Anwendung der Finite\u2013Elemente-Methode, wobei die kommerzielle Software COMSOL Multiphysics zum Einsatz kam.\nBasierend auf typische Abmessungen f\u00fcr Anoden wurde ein grundlegendes Simulationsmodell f\u00fcr Festanoden, die aus einer d\u00fcnnen Wolframschicht, eingebettet in einen Kupferblock, aufgebaut sind, entwickelt und zum Vergleich auf ein Modell zur Drehanode \u00fcbertragen. Die Betriebstemperatur des R\u00f6ntgenanodenbrennflecks im Wolframtarget wurde auf eine maximal zul\u00e4ssige Temperatur von 2500 K festgelegt. Zur K\u00fchlung wurde ein W\u00e4rmebad mit 300 K am entfernten Kupferblockende definiert. Simulationen zum Festanodenmodell wurden unter der Annahme einer kurzen bis in den Beginn einer langen thermischen Belastung (1 \u03bcs \u2013 1 s) bei der jeweiligen maximalen thermischen Brennfleckleistung durchgef\u00fchrt. Im Anschluss wurden zahlreiche Parameter zum thermischen Studienmodell der Festanode ver\u00e4ndert, um die Modellierung auf seine G\u00fcltigkeit untersuchen zu k\u00f6nnen. Die Langzeitbelastung einer einzelnen Festanode wurde hinsichtlich eines kontinuierlichen Impulsbetriebs bei einem FACT untersucht. Es wurde ein virtuelles Modell zur FACT erstellt, um verschiedene zeitliche Abl\u00e4ufe im R\u00f6ntgenquellring, der eine kreisf\u00f6rmige Anordnung von 1160 einzelnen Festanoden bildet, analysieren zu k\u00f6nnen. Unter der Annahme von bekannten Detektoreigenschaften, die auf die deutlich kleineren Integrationszeiten beim FACT \u00fcbertragen worden sind, wurde die Bildqualit\u00e4t mithilfe der CT-Bildrekonstruktionsbibliothek ROTLib aus dem Institut f\u00fcr Medizinische Physik in Erlangen, Deutschland, ermittelt. Mit der Definition einer Relative Performance (RP) wurde ein Bezug zum Referenz-CT hergestellt.\nErgebnisse:\nSimulationsstudien mit den definierten Anodenmodellen zur thermischen Brennfleck-Leistungs- dichte (P_th/A) waren in guter \u00dcbereinstimmung in den absoluten Werten sowie deren Trends zu Literatur- und Hersteller-Angaben. Studien zum kontinuierlichen Impulsbetriebs zeigten, dass mit Festanoden das FACT prinzipiell unbegrenzt betrieben werden kann. Aus dem virtuellen Modell zum FACT ging hervor, dass rund 60 Gantry-Uml\u00e4ufe pro Sekunde erforderlich sind, um die R\u00f6ntgenleistung (bei RP = 1) entsprechend dem Referenz-CT zu erreichen. Unter dieser Bedingung konnte eine \u00e4quivalente Bildqualit\u00e4t beim FACT durch Superposition erzielt werden. Die optimale Projektionsdauer aus jeder Projektionsrichtung lag bei 10 \u03bcs. Mit einer Strahl-Pause von 1 \u03bcs zwischen den Projektionen waren 78,4 Uml\u00e4ufe pro Sekunde bei aufeinander folgenden R\u00f6ntgenquellenaktivit\u00e4ten m\u00f6glich, was in einer RP von 1,3 resultierte bei gleicher Brennfleckabmessung im Vergleich zum Referenz-CT.\nSchlussfolgerungen:\nAls station\u00e4res System mit Festanoden zeigt FACT keine Brennfleck-Verschmierung w\u00e4hrend der Projektion. Mit einer RP >1 kann eine k\u00fcrzere Aufnahmezeit erreicht werden, bei gew\u00f6hnlicher Strahlenexposition und Bildqualit\u00e4t. Basierend auf der hohen Anzahl von Uml\u00e4ufen bei jeweils niedriger Dosis kann mit einem FACT eine hohe Bildwiederholungsrate und sehr d\u00fcnne Bildschichten erzielt werden, was f\u00fcr ein breites Spektrum von medizinisch-diagnostischen sowie technischen Anwendungen einen deutlichen Fortschritt darstellen k\u00f6nnte.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6275\nurn:nbn:de:bvb:29-opus4-62751\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62751\nhttps://opus4.kobv.de/opus4-fau/files/6275/dissertation_200dpi.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6288\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:006\npacs\npacs:31.15.es\nmsc\nmsc:35-02\nmsc:65-02\ninstitutes\ninstitutes:Tech_Informatik\nMultilevel Adaptive Techniques for Higher-Order Finite Differences in Elliptic Problems with Interfaces\nAdaptive Multilevel-Techniken f\u00fcr Finite Differenzen h\u00f6herer Ordnung in Elliptischen Problemen mit Grenzfl\u00e4chen\nRitter, Daniel\nmultigrid\nadaptive\nmolecular dynamics\nddc:006\nHermitian methods, also known as Mehrstellenverfahren (MSV), are a class of finite-difference (FD)discretization schemes for partial differential equations (PDEs) that have fourth\nor higher error order and lead to compact stencil operators. Their high order is achieved by forcing the fulfillment of the governing PDE also at neighboring locations of the current grid point, i.e., by averaging the right-hand side of the PDE.\nMultigrid (MG) methods are efficient, iterative solvers for sparse linear systems, such as those resulting from a PDE that is discretized with an MSV. Based on the combination of\nlocal smoothing methods, e.g. Gauss-Seidel iterations, with a coarse-grid error correction, MG methods achieve efficient error elimination over all frequencies.\nThe focus of this thesis is on the derivation of MSV schemes and their application to elliptic PDEs. Out of the various fields where these PDEs occur, examples from quantum electro-chemistry are chosen. Here, interfaces are of special interest: These can be found in the model equation, leading to a transition in the conductivity coefficient of the PDE that can be either modeled by a smooth variation or by a jump, as well as in the discretization, if hierarchical refined grids are coupled. For both cases, the combination of MSV schemes with MG solvers leads to an efficient and precise solution of the PDE.\nFor interfaces in the model equation, different MSV schemes are developed for both the smooth and the discontinuous coefficient, and experiments are performed to compare these approaches. A systematic stencil generation algorithm is introduced, based on Taylor expansions and least-squares optimization. This algorithm can produce both variants.\nBesides a comparison of the MSV scheme with linear finite elements, enhanced by Tau-extrapolation, the smooth and discontinuous schemes are compared before different variants of MSVs are integrated into the RSDFT software. Within that framework they are applied for the solution of the potential equation and for the calculation of electron-electron interactions. The resulting systems of equations are solved by MG, in particular, by applying V-cycles. Convergence properties, run times of different solver configurations, and errors in the potential and energy terms are analyzed for different MSV approaches.\nA staggered coefficient MSV variant that is very efficient to establish competes with the aligned coefficient MSV in all means except the error in the potential.\nFor interfaces between grids, the fast adaptive composite (FAC) grid approach is used to simulate open boundary conditions in the potential equation. Hence, grids of different mesh widths are coupled, i.e., coarser grids expand the original domain. Error analysis shows that the\noriginal FAC coupling does not preserve the error order of the discretization method, if the latter is not adapted at the interfaces between two grids. Therefore, a slight change in the algorithm is made, introducing higher-order interpolation as the prolongation operator.\nThis approach can restore the original error order, demonstrated for a seven-point FD as well as\nfor the corresponding MSV discretization.\nHermitesche Methoden bzw. Mehrstellenverfahren (MSV) bezeichnen eine Klasse von Diskretisierungen f\u00fcr partielle Differentialgleichungen (PDEs) mittels finiter Differenzen (FD), die vierte oder h\u00f6here Fehlerordnung besitzen und zu kompakten Stencil-Operatoren f\u00fchren. Die hohe Ordnung wird dadurch erreicht, dass die Erf\u00fcllung der PDE auch an benachbarten Gitterpunkten erzwungen wird, indem die rechte Seite der PDE gemittelt wird. Mehrgittermethoden (MG) z\u00e4hlen zu den effizientesten iterativen L\u00f6sungsverfahren f\u00fcr\nschwachbesetzte lineare Systeme, wie sie bei der Diskretisierung einer PDE mit MSV entstehen. Durch die Kombination eines lokalen Gl\u00e4tters, wie z.B. der Gau\u00df-Seidel-Iteration, mit der Fehlerkorrektur auf einem gr\u00f6beren Gitter k\u00f6nnen MG Fehler \u00fcber alle Frequenzen eliminieren.\nIm Mittelpunkt dieser Arbeit steht die Herleitung von MSV-Stencils sowie deren Anwendung auf elliptische PDEs. Es werden Beispiele aus der Quanten-Elektrochemie betrachtet, einer von vielen Disziplinen, in denen diese Differentialgleichungen Verwendung finden.\nGrenzfl\u00e4chen, bzw. Interfaces, sind hier von herausragender Bedeutung: Sie k\u00f6nnen sowohl in den Modellgleichungen selbst auftreten, wobei sie zu einem variablen Koeffizienten f\u00fchren,\nder entweder durch einen glatte Ver\u00e4nderung oder einen Sprung modelliert werden kann, als auch in der Diskretisierung zwischen gekoppelten, hierarchisch verfeinerten Gittern vorkommen. In beiden F\u00e4llen k\u00f6nnen MSV mit MG kombiniert werden, um PDEs effizient und pr\u00e4zise\nzu l\u00f6sen.\nF\u00fcr Grenzfl\u00e4chen in der Modellgleichung werden MSV sowohl f\u00fcr glatte als auch f\u00fcr springende Koeffizienten entwickelt und anschlie\u00dfend experimentell miteinander verglichen.\nBasierend auf Taylor-Entwicklungen und der Methode der kleinsten Quadrate wird ein Algorithmus zur systematischen Synthese von MSV-Stencils vorgestellt, der f\u00fcr beide Varianten verwendet werden kann.\nDas Fehlerverhalten des MSV wird mit dem der Tau-Extrapolation verglichen. Dar\u00fcber hinaus werden die Schemata f\u00fcr glatte und springenden Koeffizienten einander gegen\u00fcber gestellt, bevor die Integration verschiedener Varianten von MSV in die Software RSDFT beschrieben wird.\nHier werden die Diskretisierungen f\u00fcr die L\u00f6sung der Potentialgleichung und somit f\u00fcr die Berechnung von Elektron-Elektron-Wechselwirkungen verwendet. Die resultierenden linearen Gleichungssysteme werden dann mit einem MG gel\u00f6st, indem V-Zyklen ausgef\u00fchrt werden. F\u00fcr die verschiedenen MSV werden Konvergenzraten, Laufzeiten f\u00fcr verschiedene Konfigurationen des L\u00f6sers sowie die Fehler im Potential und der Energie miteinander verglichen. Es zeigt sich, dass sich ein effizient zu berechnendes MSV mit gestaffelten (staggered) Koeffizienten sich, bis auf den Fehler des Potentials, genauso verh\u00e4lt wie die aufw\u00e4ndigere Variante, in der die Koeffizienten auf einem ausgerichteten (aligned) Gitter diskretisiert sind.\nF\u00fcr Grenzfl\u00e4chen bei der Kopplung von Gittern wird der sogenannte Fast Adaptive Composite (FAC) Ansatz verwendet, um offene Randbedingungen in der Potentialgleichung zu simulieren. Daf\u00fcr w\u00e4hlt man eine Gitterhierarchie, in der die gr\u00f6beren Gitter das Simulationsgebiet vergr\u00f6\u00dfern, und koppelt diese miteinander. Die Fehleranalyse zeigt, dass die Fehlerordnung einer Diskretisierung mit dem urspr\u00fcnglichen FAC ohne eine Anpassung der Diskretisierungsoperatoren an den Grenzfl\u00e4chen zwischen zwei Gittern nicht erhalten werden kann. Jedoch kann unter Verwendung von Interpolationsverfahren h\u00f6herer Ordnung zur Prolongation die\nurspr\u00fcngliche Fehlerordnung wiederhergestellt werden. Dies wird in Tests sowohl f\u00fcr den Sieben-Punkt-Stern als auch f\u00fcr das entsprechende MSV gezeigt.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6288\nurn:nbn:de:bvb:29-opus4-62886\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62886\nhttps://opus4.kobv.de/opus4-fau/files/6288/DissDanielRitter.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6348\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:92-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nGelenkfunktion nach Bikondyl\u00e4rer Knie-Endoprothese: Prospektiv vergleichende Studie bei Patienten mit Gonarthrose und Rheumatoider Arthritis\nStolle, Jeska\nKniegelenksendoprothese\nrheumatoide Arthritis\nGonarthrose\nddc:617\n1.1 Hintergrund\nM\u00f6gliche Unterschiede zwischen Patienten mit Rheumatoider Arthritis (RA) und Gonarthrose (OA) bez\u00fcglich der rein subjektiven Einsch\u00e4tzung der Gelenkfunktion nach Knie-TEP sind bis dato nicht untersucht.\n1.2 Material und Methoden\nProspektive klinische Studie mit 128 konsekutiv erfassten Patienten (OA n=92; RA n=36) mit der Indikation zur hybriden bikondyl\u00e4ren Knie-TEP. Die Kniefunktion wurde pr\u00e4operativ (T0) und nach sechs Monaten (T1) mittels Knee Injury and Osteoarthritis Outcome Score (KOOS) und Oxford Knee Score (OKS) erfasst.\n1.3 Ergebnisse\nOKS und KOOS zeigen eine statistisch signifikante Verbesserung f\u00fcr die OA- und RA-Patientenkohorte bei T1 (6 Monate) verglichen mit T0 (pr\u00e4operativ).\n1.4 Schlussfolgerung\nSowohl bei OA-Patienten als auch bei RA-Patienten wurde sechs Monate nach Implantation eine Besserung der Knie-Funktion beobachtet. Offen bleibt, ob die postoperative Datenauswertung nach 12 Monaten und mehr deutliche Unterschiede zwischen den Subgruppen OA und RA aufzeigen kann.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6348\nurn:nbn:de:bvb:29-opus4-63480\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63480\nhttps://opus4.kobv.de/opus4-fau/files/6348/Promotion_JeskaStolle.VersionBib.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6382\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\nmsc\nmsc:34Fxx\nmsc:37Exx\nmsc:60-XX\nmsc:81-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nAnalysis of certain random operators related to solid state physics\nAnalyse gewisser zuf\u00e4lliger Operatoren mit Bezug zur Festk\u00f6rperphysik\nDrabkin, Maxim\nDynamisches System\nLjapunov-Exponent\nElektronentransport\nSchr\u00f6dinger-Gleichung\nTopologischer Isolator\nChern-Zahl\nddc:519\nThe presented thesis is about the random Kronig-Penney model and other related quantum mechanical models. The main objects in consideration are random mostly one dimensional and discrete Schr\u00f6dinger operators and their spectral properties. From the physical point of view the most interesting objects in these models are conductivity and charge transport in disordered solid media. These show different behavior than the ordered systems. For the Kronig-Penney model lower bounds on the growth of the time-averaged q-th moment of the position operator X are obtained, as well as the perturbative analysis of the Lyapunov exponent and the integrated density of states. On the technical level the theory of the products of random matrices is used. It is known, that the products of random matrices exhibit Gaussian fluctuations around almost surely convergent Lyapunov exponents. For the 2x2 matrices the variance is calculated perturbatively. Furthermore for the random Bogoliubov-de Gennes model operators the localization in the spectral gap is proven.\nIn der vorliegenden Arbeit werden das zuf\u00e4llige Kronig-Penney-Modell sowie andere quantenmechanische Modelle behandelt. Dabei werden \u00fcberwiegend eindimensionale diskrete zuf\u00e4llige Operatoren und deren spektrale Eigenschaften betrachtet. Die physikalischen Fragestellungen beziehen sich auf die Leitf\u00e4higkeit und den Elektronentransport in ungeordneten Festk\u00f6rpern. Diese zeigen ein anderes Verhalten auf als geordnete. F\u00fcr das zuf\u00e4llige Kronig-Penney-Modell wird eine untere Schranke des Zeitmittels des q-ten Moments vom Ortsoperator X angegeben. Der Lyapunov-Exponent und die integrierte Zustandsdichte werden st\u00f6rungstheoretisch ausgerechnet. Dies geschieht unter Zuhilfenahme der Theorie von Produkten zuf\u00e4lliger Matrizen. F\u00fcr letztere existiert eine Version des zentralen Grenzwertsatzes. F\u00fcr 2x2 Matrizen wird die Varianz st\u00f6rungstheoretisch berechnet. Ferner wird die Anderson-Lokalisierung f\u00fcr zuf\u00e4llige Bogoliubov-de Gennes Operatoren in der spektralen L\u00fccke gezeigt.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6382\nurn:nbn:de:bvb:29-opus4-63825\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63825\nhttps://opus4.kobv.de/opus4-fau/files/6382/MaximDrabkinDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6399\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:616\nccs\nccs:E.0\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nDer Einfluss der H\u00e4ufigkeit von Papillenrandblutungen auf die Progressionsrate bei chronischem Offenwinkelglaukom\nThe influence of the frequency of peripapillary bleedings on the progression of chronic open angle glaucoma\nBilger, Angelika\nhttp://d-nb.info/gnd/4021210-5Link\nddc:616\nHintergrund und Ziele\nDas Glaukom ist eine Erkrankung, die meist in Zusammenhang mit einem erh\u00f6hten Augeninnendruck auftritt und progredient aufgrund einer Sehnervenzerst\u00f6rung zu einer Sehverschlechterung f\u00fchrt.\nIm Zusammenhang mit dem Glaukom kann man im Laufe der Erkrankung h\u00e4ufig Papillenrandblutungen beobachten. Diese sind sehr spezifisch f\u00fcr das Glaukom, da sie bei fast keinen anderen Erkrankungen auftreten. Allerdings treten sie nicht bei jedem Patienten, der an einem Glaukom leidet, auf. Oft sind Papillenrandblutungen ein Zeichen einer fortschreitenden Optikusatrophie im Rahmen eines Offenwinkelglaukoms und gehen meistens im Verlauf der Erkrankung mit einer Sehverschlechterung einher.\nDa Papillenrandblutungen im Allgemeinen mit einer Sehverschlechterung assoziiert sind, soll in dieser Arbeit untersucht werden, ob die H\u00e4ufigkeit von Papillenrandblutungen Einfluss auf den Verlauf nimmt. Es wird in dieser medizinischen Dissertation untersucht, ob das geh\u00e4ufte Autreten von Papillenrandblutungen im Vergleich zu einer einmaligen Blutung, zu einer schnelleren Progression der Erkrankung f\u00fchrt oder ob auch beim nur einmaligen Auftreten einer Papillenrandblutung die Erkrankung gleich schnell fortschreitet.\nPatienten und Methoden\nRetrospektiv wurden 50 Patienten beziehungsweise 60 Augen von Patienten, die an einem Glaukom leiden und bei denen im Verlauf der Erkrankung Papillenrandblutungen auftraten, untersucht.\nDie Patienten wurden in 2 Gruppen unterteilt. Bei Gruppe 1 wurde nur eine Papillenrandblutung beobachtet und bei Gruppe 2 konnte mehr als eine Blutung festgestellt werden. Die Patienten wurden nach dem Auftreten der ersten dokumentierten Butung beobachtet. Die Patienten wurden w\u00e4hrend des Beobachtungszeitraums j\u00e4hrlich zur Kontrolle einbestellt, sodass durchschnittlich \u00fcber 10 Jahre im Zeitraum von 1992 bis 2011 der Verlauf der Erkrankung beobachtet werden konnte. Bei einigen Patienten konnte auch der Gesichtsfeldverlauf vor der ersten Blutung untersucht werden.\nZur Beurteilung der Krankheitsprogression wurden fundoskopische Bilder und perimetrische Befunde genutzt. Der Zeitpunkt, zu dem man einen Beginn einer Progression beobachtete, wurde dokumentiert und mittels Kaplan-Maier-Kurve und Log-Rank-Test f\u00fcr die unterschiedlichen Progressionsmethoden die Signifikanz ermittelt und zwischen Gruppe 1 und 2 verglichen.\nMittels eines Gesichtsfeldbefundungsprogrammes, PeriData, konnte die Progression im gesamten Gesichtsfeld in dB/Jahr trendbasiert nach Auftreten der ersten Blutung gemittelt werden. Bei einigen Patienten war dies auch vor Auftreten der ersten Blutung m\u00f6glich. Ebenso wurde trendbasiert in dB/Jahr die Progression im betroffenen Sektor der ersten Papillenrandblutung vor und nach Auftreten der Blutung ausgewertet. Zudem wurde eine globale eventbasierte Progressionsanalyse durchgef\u00fchrt. Eine globale Progression wurde dann dokumentiert, wenn perimetrisch 3 nebeneinander liegende Punkte ermittelt werden konnten, die sich signifikant (p=0,05) verschlechtert hatten. Zudem musste dies in 3 Folgeuntersuchungen best\u00e4tigt werden. Mithilfe von fundoskopischen Bildern wurden Nervenfaserrandsaumverluste ausfindig gemacht und als Progression gewertet.\nErgebnisse und Beobachtungen\nInnerhalb der Gruppen 1 und 2 konnte ein Unterschied in der Progression festgestellt werden. In dieser konnte Arbeit gezeigt werden, dass bei Gruppe 2 eine schnellere Progression als bei Gruppe 1 auftritt. Allerdings erwies sich dieser Unterschied in der statistischen Auswertung nicht als signifikant.\nBei der Beurteilung von Nervenfaserrandsaumverlusten zeigt sich in der Kaplan-Meier-Kurve graphisch eine schnellere Verschlechterung bei Gruppe 2, was jedoch im Logrank-Test nicht signifikant ist (p=0,191). Bei der globalen eventbasierten Progressionsanalyse zeigt sich graphisch eine schnellere Verschlechterung bei Gruppe 2, was im Logrank-Test wiederum nicht signifikant ist (p=0,06). Bei der trendbasierten Auswertung im gesamten Gesichtsfeld in dB/Jahr zeigt sich vor der ersten Blutung bei Gruppe 1 eine Verschlechterung um -0,144 dB/Jahr und bei Gruppe 2 eine Verbesserung um +0,233 dB/Jahr. Im unabh\u00e4ngigen t-Test ist dieser Unterschied jedoch nicht signifikant (p=0,237). Bei der trendbasierten Auswertung im gesamten Gesichtsfeld nach der ersten Blutung konnte bei Gruppe 1 eine mittlere Verschlechterung um -0,2dB/Jahr und bei Gruppe 2 eine Verschlechterung um -0,465 dB/Jahr ermittelt werden. Auch hier ist der Unterschied im unabh\u00e4ngigen t-Test nicht signifikant (p=0,096). Bei der Progression im Sektor der ersten Blutung zeigt sich nach dem Auftreten der ersten Blutung bei Gruppe 1 eine Verschlechterung um -0,222 dB/Jahr und bei Gruppe 2 eine Verschlechterung um -0,520 dB/Jahr. Im unabh\u00e4ngigen t-Test ist dieser Unterschied nicht signifikant (p=0,121).\nPraktische Schlussfolgerungen\nSowohl bei Gruppe 1 als auch bei Gruppe 2 verl\u00e4uft die Erkrankung chronisch progredient.\nEine schnellere Sehverschlechterung bei Patienten der Gruppe 2, bei denen mehr als eine Papillenrandblutung auftrat, spricht f\u00fcr einen Zusammenhang multipler Papillenrandblutungen als Risikofaktor f\u00fcr eine schnellere Progression. Bei allen unseren Untersuchungen zeigte sich nach dem Auftreten der ersten Papillenrandblutung eine schnellere Sehverschlechterung bei Patienten der Gruppe 2. Nur bei der trendbasierten Auswertung im gesamten Gesichtsfeld in dB/Jahr vor der ersten Papillenrandblutung konnten bei Gruppe 2 bessere Werte im Vergleich zu Gruppe 1 ermittelt werden. Allerdings konnten wir in keiner unserer Untersuchungen das Signifikanzniveau (p = 0,05) erreichen, sodass weitere Studien notwendig sind, um den Zusammenhang n\u00e4her zu untersuchen.\nDa, wie bereits erw\u00e4hnt bei beiden Gruppen die Erkrankung chronisch progredient verl\u00e4uft, ist es notwendig, dass weiterhin bei beiden Patientengruppen regelm\u00e4\u00dfige Kontrolluntersuchungen stattfinden und die Patienten ad\u00e4quat therapiert werden.\nNur aufgrund der Tatsache, dass nur eine Blutung auftrat, darf keine Herabsetzung der Anzahl der Kontrollbesuche erfolgen. Diese sollten in gleicher Anzahl stattfinden wie bei Patienten der Gruppe 2, die mehr Blutungen aufweisen.\nBackground and Objectives\nGlaucoma is a disease which is correlated with a higher intraocular pressure. It is a chronical progressive disease leading to an irreversible damage of the optic nerve and a detoriation of the vision.\nIn glaucoma eyes peripapillary bleedings could be observed very often. They are very specific for glaucoma and are rarely seen in other diseases. Peripapillary bleedings do not occur with every eye suffering glaucoma, though. If they are seen in patients with glaucoma it is often a sign for progression of atrophy of nervus opticus and a sign of detoriation of the visus.\nAs it is already known that peripapillary bleedings in general could be a sign of detoriation of the visus, in this dissertation it is analysed if multiple peripapillary bleedings lead to a quicker progression of the disease in comparison with a single bleeding.\nPatients and Methods\nThis retrospective study has the aim to observe the occurring of peripapillary bleedings by comparing and analyzing 60 eyes of 50 patients suffering from eye glaucoma.\nPatients were divided into two groups. Group 1 had only one single peripapillary bleeding and in group 2 several bleedings could be seen. We started the investigation of each patient when the first bleeding was found. Patients were seen annually after their first bleeding and we had a follow-up of around 10 years.\nTo evaluate the disease`s progression, we used fundoscopic photographs and perimetric findings. The time of progression was documented and analyzed with Kaplan-Meier curve and Log-Rank-test. Consequently we could test the level of significance between the two groups.\nThe time of progression was analyzed with several methods. We used a programme called PeriData to average the progression trend-based in dB/year within the whole visual field before and after the first bleeding could be registered. In addition, the sector of the visual field in which the first bleeding occured was tested in the same way. Another method to analyze the progression was to look for 3 points next to each other on the visual field which became worse in a significance level of p = 0,05 and could be confirmed by three following examinations. In fundoscopic photographs a loss of the nerve fiber layer was considered as progression.\nResults\nA difference in progression could be seen. Group 2 had a faster progression than group 1. The significance niveau (p=0,05) could not be reached in any of our analysis, though.\nA faster loss of the nerve fiber layer could be seen in group 2 but there was no significant difference (p=0,191). In the global eventbased progression analysis measured with three points next to each other on the visual field, which became worse in a significance level of p=0,05, a faster progression could be seen in group 2. The Log-Rank-test was not significant, eather (p=0,06). Before the first bleeding occured a detoriation of -0,144dB/year could be observed in the average trend-based progression in dB/year in group 1. Group 2 showed an amelioration of +0,233 dB/year. In the t-test there was no significant difference (p=0,237). In the average trend-based progression after the first bleeding there was a mean detoriation of -0,2dB/year and a mean detoriation of -0,465 dB/year in group 2. The t-test was not significant (p=0,096). In the sector of the visual field in which the first bleeding occured a progression of -0,222dB/year could be observed in group 1 and a progression of -0,520 dB/year in group 2. This difference is not significant when tested by the t-test (p=0,121).\nConclusions\nIn both groups a chronical progression of the disease was seen.\nThe faster progression in group 2 shows that multiple bleedings could be a risk factor for a worse course of disease. As the significance niveau couldn`t be reached further studies are warranted to analyze this correlation.\nAs glaucoma is a chronical progressive disease regular check-ups are needed for every patient to optimize therapy.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6399\nurn:nbn:de:bvb:29-opus4-63998\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63998\nhttps://opus4.kobv.de/opus4-fau/files/6399/Abgabe.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6456\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nKlinische Ergebnisse nach laparoskopischer Operation bei Morbus Crohn mit und ohne Komplikationen\nClinical results after laparoscopic surgery in Crohn's disease with and without complications\nF\u00f6rtsch, Line\nMorbus Crohn\nddc:617\nZusammenfassung\n1. Hintergrund und Ziele\nDer Morbus Crohn geh\u00f6rt zu den chronisch-entz\u00fcndlichen Darmerkrankungen und kann den gesamten Magen-Darm-Trakt befallen. Die Ursache der Erkrankung ist sehr komplex und noch nicht vollst\u00e4ndig gekl\u00e4rt. Die Erkrankung ist langj\u00e4hrig und rezidivierend. Sie kann milde oder mit diversen Komplikationen wie Fisteln, Abszessen oder Perforationen verlaufen, was zu unterschiedlicher Beschwerdesymptomatik f\u00fchrt. Da es bisher keine Heilung gibt, ist das Ziel der Therapie die Reduktion der Entz\u00fcndung und die Symptomlinderung. Die Therapie besteht einerseits aus einer immunsupprimierenden und antiinflammatorischen Medikation, andererseits stehen operative Verfahren zur Verf\u00fcgung, die vornehmlich bei konservativem Therapieversagen oder schweren intestinalen Komplikationen Anwendung finden. Besondere Bedeutung gewinnt hierbei die Laparoskopie.\nZiel dieser Arbeit war es herauszufinden, ob es einen Unterschied im klinischen Verlauf bei laparoskopisch operierten Patienten, die sich pr\u00e4operativ bez\u00fcglich Komplikationen unterschieden, gibt. Es soll in vorliegender Studie nachgegangen werden, ob Voroperationen oder Komplikationen wie Fisteln, Abszesse, Perforationen und Peritonitis zu vermehrten intra- und postoperativen Komplikationen, erh\u00f6hter Mortalit\u00e4tsrate und einem l\u00e4ngeren postoperativen Krankenhausaufenthalt f\u00fchren.\n2. Methoden\nDer Beobachtungszeitraum dieser Arbeit erstreckt sich vom Jahr 1999 \u2013 2012. Insgesamt wurden 79 Patienten mit Morbus Crohn laparoskopisch operiert. Dieses Patientenkollektiv wurde in \u201eunkomplizierte\u201c und \u201ekomplizierte\u201c F\u00e4lle unterteilt. Unkomplizierte F\u00e4lle wiesen lediglich Stenosen und Entz\u00fcndung auf und die komplizierten F\u00e4lle hatten zus\u00e4tzliche Komplikationen wie Fisteln, Abszesse, Perforationen oder eine Peritonitis. Die Daten wurden anhand eines Erhebungsbogens retrospektiv erfasst und mit Hilfe des Statistikprogramms SPSS statistisch ausgewertet.\n3. Ergebnisse und Schlussfolgerung\nBeide Gruppen zeigten keine signifikanten Unterschiede in der Verteilung der Patienteneigenschaften. Sie unterschieden sich nicht im M\u00e4nner-Frauen-Verh\u00e4ltnis, im Alter zum Zeitpunkt der Erstdiagnose bzw. der OP, in der Anamnesedauer, in Laborparametern, im BMI oder in der ASA-Klassifikation. Ausserdem lagen in beiden Gruppen \u00e4hnlich viele Rezidive und Erstdiagnosen des Morbus Crohn vor.\nIn der Gruppe der unkomplizierten F\u00e4lle gaben Patienten wesentlich h\u00e4ufiger Schmerzen an als Patienten der komplizierten F\u00e4lle. Ebenso war in der unkomplizierten Gruppe das Ileum signifikant h\u00e4ufiger betroffen und Stenosen traten in der Gruppe der unkomplizierten F\u00e4lle \u00f6fter auf als in der Gruppe der komplizierten F\u00e4lle. Bez\u00fcglich der Stenosenlokalisation war der D\u00fcnndarm bzw. der \u00dcbergang von D\u00fcnndarm zum Dickdarm bei unkomplizierten Patienten signifikant h\u00e4ufiger befallen \u2013 mit konsekutiv signifikant mehr Ileoz\u00f6kalresektionen - und Steroide wurden von unkomplizierten Patienten vermehrt eignenommen.\nBez\u00fcglich intraoperativer Komplikationen zeigte sich in dieser Arbeit kein signifikanter Unterschied zwischen beiden Gruppen. Ebenso war keine Signifikanz hinsichtlich postoperativer Komplikationen und der postoperativen Krankenhausverweildauer erkennbar.\nZusammenfassend kann diese Arbeit darlegen, dass es keinen signifikanten Unterschied bez\u00fcglich des klinischen Verlaufes zwischen pr\u00e4operativ komplizierten und komplikationslosen Patienten bei laparoskopischer Operationsmethode gibt.\nSummary\n1.Background and objectives\nCrohn's disease is an inflammatory bowel disease and can affect the entire gastrointestinal tract. The cause of the disease is very complex and not yet fully understood. The disease is long-standing and recurrent. There are mild symptoms as well as severe ones with various complications such as: fistulas, abscesses or perforations, which then lead to different symptomatology. Since there is no cure yet, the objective of the therapy is to reduce the inflammation and to relieve the symptoms. On one hand, the therapy consists of an immunosuppressive and anti-inflammatory medication, on the other hand there are surgical procedures available, which are mainly used when conservative treatment fails or severe intestinal complications occur. In this case, laparoscopy is particularly important.\nThe objective of this dissertation was to find out whether there is a difference in the clinical symptoms of patients undergoing laparoscopic resection, which differed with respect to preoperative complications. In the study presented here, an investigation will be made into understanding if previous surgeries or complications such as fistulas, abscesses, perforations and peritonitis have led to increased intra-and postoperative complications, increased mortality rate and a longer postoperative hospital stay.\n2. Methods\nThe observation period of this study extends from 1999 \u2013 2012. A total of 79 patients with Crohn's disease underwent laparoscopic surgery. This patient population was divided into \"uncomplicated\" and \"complicated\" cases. Uncomplicated cases showed only stenosis and inflammation whereas complicated ones had additional difficulties such as fistulas, abscesses, perforation or peritonitis. The data were recorded retrospectively using a written questionnaire and were then statistically analyzed using the statistics software SPSS.\n3 Results and Conclusions\nBoth groups showed no significant differences in patient characteristics. They did not differ in the male-female ratio, age at diagnosis or surgery, in the duration of anamnesis, in laboratory parameters, in BMI or in the ASA classification. In addition, there was a similar amount of recurrences and initial diagnoses of Crohn's disease in both groups.\nIn the group of uncomplicated cases patients reported pain significantly more frequently than patients in the complicated cases. Likewise, in the group of uncomplicated cases, the ileum was affected significantly more often and stenoses occurred far more frequently than in the group of complicated cases. Regarding the location of stenosis the small intestine or the transition from the small intestine to the colon was affected significantly more often in the group of uncomplicated patients \u2013 consecutively with significantly more ileo-caecal-resections, \u2013 also steroids were taken more often in this group.\nRegarding intraoperative complications, no significant difference between the two groups was found. Also no significance in regards to postoperative complications and postoperative hospital stay was apparent.\nIn conclusion, this study demonstrates that there is no significant difference in the clinical course between preoperatively complicated and uncomplicated patients, who had laparoscopic surgery.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6456\nurn:nbn:de:bvb:29-opus4-64565\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-64565\nhttps://opus4.kobv.de/opus4-fau/files/6456/Klinische%20Ergebnisse%20nach%20laparoskopischer%20Operation%20bei%20Morbus%20Crohn%20mit%20und%20ohne%20Komplikationen.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6552\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nMultimodale Bildgebungsstrategien zur optimierten Tumorabgrenzung am experimentellen Gliommodell\nImaging strategies glioma\nPitann, Patrick\nGliom\nGlioblastom\nddc:610\nHinsichtlich der Tumorabgrenzung haben sich die DCE - bzw. die DTI -Sequenzen als \u00e4u\u00dferst n\u00fctzlich erwiesen. Tierexperimentell w\u00fcrde sich durch die Definition von Grenzwerten, unter Ber\u00fccksichtigung der Grenzwertvariation, eine schnelle (automatisierte) Darstellung des Glioms mit seiner diffusen Infiltration erreichen lassen. Insbesondere die, aus den FA - und KTrans - Karten berechneten Grenzwerte und die sich daraus ergebenden Volumina zeigten eine hohe \u00dcbereinstimmung mit der Histologie als Goldstandard.\nLetztlich k\u00f6nnen mit den oben genannten Sequenzen pr\u00e4zisere Aussagen hinsichtlich malignisierter Areale und insbesondere eine Abgrenzung des Tumors bez\u00fcglich seiner R\u00e4nder, anhand der, aus den berechneten FA - und KTrans - Karten definierten Grenzwerten, gemacht werden. Somit kann in Bezug auf die Malignit\u00e4t des Tumors ein Surrogat - Parameter erstellt werden, an dem sich ein m\u00f6gliches Ansprechen auf eine Therapie zeigen l\u00e4sst. Ebenfalls von Vorteil w\u00e4re diese Methode bei der chirurgischen Versorgung des Glioblastoms. Hier h\u00e4tte der Neurochirurg die M\u00f6glichkeit, unter Verwendung der genannten Technik, eine exaktere pr\u00e4operative Planung durchzuf\u00fchren [84].\nZusammenfassend l\u00e4sst sich tierexperimentell, mit Hilfe des funktionellen MRTs und unter Verwendung definierter Grenzwerte, in guter \u00dcbereinstimmung zur Histologie eine pr\u00e4zise Abgrenzung des Glioblastoms erzielen.\nWith regard to tumor differentiation, the DCE have - or DTI sequences proved extremely useful. In animal experiments would be attained by the definition of limit values, taking into account the variation limit, a fast (automated) Presentation of glioma with its diffuse infiltration. In particular, from the FA - and Ktrans - calculated card limits and volumes resulting showed high correlation with histology as the gold standard.\nUltimately, with the sequences above precise statements regarding malignisierter areas and in particular a definition of the tumor with respect to its edges, on the basis of, from the calculated FA - be defined card limits - and Ktrans. Thus, a surrogate can with respect to the malignancy of the tumor - parameters are created at which a possible response can point to a therapy. Also of advantage would this method in the surgical treatment of glioblastoma. Here the neurosurgeon would have the ability to carry out using the above technique, a more accurate preoperative planning [84].\nIn summary, in animal experiments, achieve using the functional MRI and using defined limits, in good agreement for histology precise delineation of glioblastoma\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6552\nurn:nbn:de:bvb:29-opus4-65528\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-65528\nhttps://opus4.kobv.de/opus4-fau/files/6552/PatrickPitanndissertation.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6589\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:518\nmsc\nmsc:65M60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nTime discretization for capillary problems\nZeitdiskretisierung f\u00fcr kapillare Probleme\nWeller, Stephan\nNumerische Str\u00f6mungssimulation\nMehrphasenstr\u00f6mung\nFinite-Elemente-Methode\nNavier-Stokes-Gleichung\nddc:518\nDiese Arbeit vergleicht eine Reihe von Zeitdiskretisierungstechniken f\u00fcr kapillare Str\u00f6mungen,\nd.h. Str\u00f6mungen mit freier Oberfl\u00e4che und Zweiphasenstr\u00f6mungen. Der Schwerpunkt liegt\ndabei auf der Entwicklung von Verfahren die a) von h\u00f6herer Ordnung (d.h. mindestens zweiter\nOrdnung) sind, b) absolut stabil sind und c) wenig numerische Dissipativit\u00e4t aufweisen.\nZun\u00e4chst wird die mathematische Modellierung des Problems beschrieben und eine va-\nriationelle, dimensionslose Beschreibung erarbeitet, die sowohl f\u00fcr einphasige als auch f\u00fcr\nzweiphasige kapillare Str\u00f6mungen geeignet ist. Dabei werden allgemeine Lagrange-Euler (ALE)\nKoordinaten verwendet um ein dem Problem angepasstes, mitbewegtes Gitter benutzen zu\nk\u00f6nnen. Mit der Finite-Elemente-Methode (FEM) werden die Gleichungen im Ort diskretisiert\nund in eine differential-algebraische Matrix-Vektor-Formulierung umgeschrieben.\nMehrere voll implizite und linear implizite Zeitdiskretisierungsverfahren werden vorgestellt,\nihre Stabilit\u00e4ts-, Konvergenz- und Dissipativit\u00e4tseigenschaften werden diskutiert. Diese Verfah-\nren werden dann auf die ortsdiskreten Gleichungen angewendet und es werden voll diskrete,\nf\u00fcr die Computer-Simulation geeignete Gleichungssysteme hergeleitet.\nWeiterhin wird ein Raum-Zeit-Galerkin-Verfahren vorgestellt, auch hier werden wieder die\nStabilit\u00e4ts-, Konvergenz- und Dissipativit\u00e4tseigenschaften diskutiert. Dieses Verfahren wird\ndann auf die variationelle Formulierung der kapillaren Str\u00f6mungen angewendet und auch hier\nein voll diskretes System hergeleitet. F\u00fcr das Raum-Zeit-Galerkin-Verfahren wird weiterhin\neine Energieabsch\u00e4tzung bewiesen, welche die unbedingte Stabilit\u00e4t des Verfahrens analytisch\netabliert.\nIn einem Anwendungsteil werden sodann mehrere ein- und zweiphasige Beispielprobleme\nvorgestellt und mit den beschriebenen Verfahren gel\u00f6st. Mittels der entsprechenden Finite-\nElemente-Simulationen werden die Konvergenz- und Dissipativit\u00e4tseigenschaften numerisch\nuntersucht.\nEin abschlie\u00dfender Vergleich stellt die Vor- und Nachteile der verglichenen Verfahren in\n\u00fcbersichtlicher Form da.\nIn this work several time discretization techniques for capillary flows, i.e. either one-phase free\nsurface flow or two-phase flow, are compared. The focus is the development of methods that\nare a) of higher order (i.e. at least second order), b) unconditionally stable and c) do not suffer\nfrom much numerical dissipativity.\nThe mathematical model of the problem is initially described and a variational, dimensionless\nformulation is given that can be used for both one- and two-phase capillary flows. The formula-\ntion is in Arbitrary Lagrangian-Eulerian (ALE) coordinates so a problem-adapted moving mesh\ncan be used. Utilizing the Finite Element Method (FEM) the equations are discretized in space\nand a differential algebraic matrix-vector formulation is derived.\nSeveral fully implicit and linearly implicit time discretization techniques are introduced, their\nproperties with respect to stability, convergence, and dissipativity are discussed. These methods\nare then applied to discretize the equations in time. Thus fully discrete equations are derived\nthat can be used for computer simulations.\nFurthermore a space-time Galerkin approach is presented, its stability, convergence and\ndissipativity properties are discussed as well. This method is then applied to the variational\nformulation of the capillary flow problems and a fully discrete system is derived. For the\nspace-time approach an energy estimate is proved that establishes the unconditional stability\nof the method by analytical means.\nIn an application chapter several one- and two-phase exemplary problems are introduced\nand solved with the described time discretization techniques. Using Finite Element simulations\nthe convergence and dissipativity properties are numerically investigated.\nA concluding comparison of the methods presents the advantages and drawbacks of all\nmethods that were compared.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6589\nurn:nbn:de:bvb:29-opus4-65894\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-65894\nhttps://opus4.kobv.de/opus4-fau/files/6589/DissertationStephanWeller.pdf\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6593\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:B.m\npacs\npacs:78.67.Ch\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nPolymer-Selected Carbon Nanotubes for Light-Emitting Field-Effect Transistors\nPolymerselektierte Kohlenstoffnanor\u00f6hren f\u00fcr lichtemittierende Feldeffekttransistoren\nJakubka, Florian\nSWNT\nFET\nddc:620\nCarbon nanotubes hold remarkable optoelectronic properties. Beside their high conductivity\nand field-effect mobility, which predetermines their use in future electronic applications,\nthey feature a direct bandgap that enables luminescence in the near-infrared. Yet, carbon\nnanotubes still suffer from heterogeneous source material that commonly consists of various\nnanotube species with different metallicity, diameter and emission wavelength. This dissertation\ninvestigates the selective sorting of carbon nanotubes by polyfluorene-polymers as\nwell as the application of these polymer/nanotube dispersions in light-emitting field-effect\ntransistors.\nSome polyfluorene-polymers show a striking selectivity for the dispersion of certain nanotube\nspecies that is not yet completely understood. In this work, the influence of the solvent\ntype and the polymer molecular weight on the selective dispersion behavior is analyzed, to\nproduce nanotube/polymer dispersions with tailored composition and yield. It was found\nthat meta- or non-stable nanotube species are stabilized by a low diffusion constant. Therefore\na low dispersion viscosity leads to high selectiveness, however at at the expense of\noverall yield.\nCarbon nanotube dispersions can be used with high excess of polymer as semiconducting\npolymer layers with improved injection behavior. The application of these films in lightemitting\nfield-effect transistors reveals an improved charge injection from the metal source\nand drain contacts. As shown in electrostatic simulations, the one-dimensionally confined\nstructure of carbon nanotubes leads to an enhancement of the applied electric field at the\ntube-tips. The nanotube doping reduces threshold voltages for both electron and holes and\nincreases ambipolar currents and light emission. Hence, it represents an easy-to-achieve\nperformance improvement for polymer transistors where charge injection might be an issue.\nA further purification of the dispersion and removal of the excess polymer leads to solution\nprocessable nanotube inks with near-monochiral distribution. Their performance\nin nanotube-network field-effect transistors with polymer dielectrics and electrolyte-gated\ntransistors has been evaluated. While monochiral and narrowband excitonic light emission\nwas obtained from transistors with polymer dielectric, the high charge carrier densities in\nion-gel gated devices revealed electrically stimulated trion emission and low voltage operation\nwith the nanotube network films. The dependence of exciton and trion emission\non charge carrier density has been shown, effectively creating a photoluminescence emitter\nwith voltage-controllable emission wavelength.. Defined emission in the near-infrared\nand good electrical performance promotes the use of polymer-selected carbon nanotubes as\na bridge between electric and optical signal transmission, especially for upcoming generations\nof printable and flexible electronics.\nKohlenstoffnanor\u00f6hren besitzen bemerkenswerte optoelektronische Eigenschaften.\nNeben hoher Leitf\u00e4higkeit und Feldeffekt-Mobilit\u00e4t erm\u00f6glicht eine direkte Bandl\u00fccke Lumineszenz\nim nahen infraroten Spektrum. Das heterogene Ausgangsmaterial der Kohlenstoffnanor\u00f6hrenherstellung\nerweist sich allerdings als Nachteil f\u00fcr optoelektronische Anwendungen,\nda es in der Regel verschiedene Spezies mit leitenden oder halbleitenden Eigenschaften\nsowie unterschiedlichen Durchmessern und Emissionwellenl\u00e4ngen enth\u00e4lt. Diese\nDissertation behandelt das selektive Dispergieren von Nanor\u00f6hren mit Polyfluoren-Polymeren\nund die Anwendung dieser Dispersionen in lichtemittierenden Feldeffekttransistoren.\nBestimmte Polyfluorene zeigen eine noch nicht eindeutig verstandene Selektivit\u00e4t f\u00fcr spezifische\nNanor\u00f6hren-Arten. Diese Arbeit analysiert hierbei den Einfluss des L\u00f6sungsmittels\nund des Polymer-Molekulargewichts auf das selektive Dispergierverhalten, mit dem Ziel,\nma\u00dfgeschneiderte Nanor\u00f6hren-Zusammensetzungen herzustellen. Die Ergebnisse zeigen\neine Stabilisierung von halb- und instabilen R\u00f6hren durch niedrige Diffusionskonstanten.\nNiedrigviskose Dispersionen zeigen deshalb eine h\u00f6here Selektivit\u00e4t, allerdings verbunden\nmit einer niedrigeren Gesamtausbeute.\nErzeugte Nanor\u00f6hren-Dispersionen k\u00f6nnen mit hohem Polymer\u00fcberschuss als halbleitende\nPolymerfilme benutzt werden. Die Anwendung dieser Schichten zeigt ein verbessertes\nInjektionsverhalten der Ladungstr\u00e4ger. Elektrostatische Simulationen deuten auf eine\nErh\u00f6hung der angelegten elektrischen Felder um die R\u00f6hrenspitzen, verursacht durch die\neindimensionale Struktur der Kohlenstoffnanor\u00f6hren, hin. Dies erleichtert das Tunneln von\nLadungstr\u00e4gern durch die Schottkybarrieren an Source- und Drain-Kontakten, was zu einer\nVerringerung der Schwellspannung und erh\u00f6hten ambipolaren Str\u00f6men f\u00fchrt.\nMit einem weiteren Zentrifugierschritt kann die selektive Dispersion von \u00fcbersch\u00fcssigem\nPolymer gereinigt werden. Dies erzeugt l\u00f6sungsprozessierbare Nanor\u00f6hren-Formulierungen\nmit fast monochiralen Zusammensetzungen. Diese wurden in Nanor\u00f6hrennetzwerken\nals Halbleiter f\u00fcr Feldeffekttransistoren getestet. W\u00e4hrend mit Polymerdielektrikas\neine exzitonische und schmalbandige Emission erreicht werden kann, zeigen Elektrolytkontaktierte\nTransitoren weitere Emmisionsb\u00e4nder bei niedrigerer Energie, die der Emission\nvon Trionen zugeordent werden kann. Es wird die Abh\u00e4ngigkeit bei Exzitonen- und\nTrionenbildung von der Ladungstr\u00e4gerdichte aufgezeigt, was effektiv zu einem spannungsgesteuerten\nPhotolumineszenzemitter mit wechselbarer Wellenl\u00e4nge f\u00fchrt. Die schmalbandige\nInfrarotlumineszenz und die guten elektrischen Eigenschaften erm\u00f6glichen den\nEinsatz der Polymer-selektierten Nanor\u00f6hren als Br\u00fccke zwischen elektrischer und optischer\nSignal\u00fcbermittlung, insbesondere in neuen Anwendungsfeldern wie druckbarer und\nflexibler Elektronik.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6593\nurn:nbn:de:bvb:29-opus4-65938\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-65938\nhttps://opus4.kobv.de/opus4-fau/files/6593/Thesis_Florian_Jakubka_2015.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6683\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\nmsc\nmsc:74A05\nmsc:74A10\nmsc:90C11\nmsc:90C25\nmsc:90C26\nmsc:90C27\nmsc:90C35\nmsc:90C57\nmsc:90C90\ninstitutes\ninstitutes:Nat_Mathematik\nDiscrete Approaches for Optimal Routing of High Pressure Pipes\nDiskrete Verfahren zur optimalen Trassierung von Hochdruck-Rohrleitungen\nSchelbert, Jakob\nGemischt-ganzzahlige Optimierung\nSteiner-Baum\nTimoshenko-Balken\nNichtkonvexe Optimierung\nKonvexe Optimierung\nDekomposition\nDampfleitung\nddc:519\nThe planning of a whole power plant involves diversivied specializations in engineering and can take up to several years.\nWithin the planning procedure the routing of the high pressure pipes transporting the hot steam from the main boiler to the steam turbines exerts a strong influence on efficiency of the power plant.\nThe design of pipe routing is usually done by a specially trained and experienced engineer with the aid of CAE software.\nThe starting point of the problem is given by a rough outline of the design space and specified points where steam is produced and expended.\nIn addition to the routing, placement of hangers has to be specified to hold the pipe into place and meet technical restrictions.\nEven with the help of engineering software the planning procedure often involves trial and error, which makes the design phase cumbersome - especially if unforeseen obstacles have to be incorporated in a late stage of the project.\nThis thesis tries to lay a first foundation of an algorithmic engineering aid to help speed up the routing of the pipe and suggest a placing for the hangers.\nAfter discretizing the design space, the problem brings together two different main ingredients.\nOn the one hand a very well-known combinatorial problem of finding a shortest path or a Steiner tree in a graph has to be considered as we want to obtain a solution that resembles the shape of a pipe.\nOn the other hand we have to deal with the mechanics of the pipe that has to withstand its self-weight and forces due to the heated steam.\nBringing together all necessary prerequisites we start by deriving a mixed-integer nonlinear model (MINLP) that captures all combinatorial and nonlinear aspects of our problem.\nTo solve real-world instances we test several approaches: solving the non-convex MINLP, linearizing the problem, reformulating to a MISOCP, and decomposing the problem.\nThe last technique yields the fastest results and can be further accelerated by employing an infeasibility check.\nThe test can be further extended to specially tailored cutting planes that help to provide a better bound for the subproblem.\nMoreover, we prove hardness results for some occurring problems and models of our application.\nWe test the proposed algorithms and formulations on real-world instances and explain some interesting insights into the solutions of the procedures.\nDie Planung eines Kraftwerks beinhaltet zahlreiche verschiedene Ingenieursdisziplinen und ben\u00f6tigt mehrere Jahre bevor mit dem eigentlichen Bau begonnen werden kann.\nEiner der einflussreichsten Faktoren auf den Wirkungsgrad des Kraftwerks l\u00e4sst sich im Verlauf der Hochdruck-Rohrleitung festmachen, die den hei\u00dfen Dampf vom Dampfkessel zur Turbine bef\u00f6rdert.\nDie Konzeption des Rohrverlaufs ist ein h\u00e4ndischer Prozess, der von Fachexperten mit mehrj\u00e4hriger Erfahrung unter Zuhilfenahme von CAE-Software durchgef\u00fchrt wird.\nAusgehend vom Dampfkessel wird unter Ber\u00fccksichtigung der zul\u00e4ssigen Bereiche im Kraftwerksgeb\u00e4ude ein erster Verlauf der Rohrleitung zur Turbine festgelegt.\nIn weiteren Schritten m\u00fcssen die notwendigen H\u00e4nger platziert werden, die neben dem Gewicht der Rohrleitung auch deren Zusatzkr\u00e4fte aus beispielsweise W\u00e4rmeausdehnung aufnehmen.\nSelbst mit moderner Software beinhaltet dieser Planungsprozess h\u00e4ufig noch h\u00e4ndisches Ausprobieren, was die Planung besonders bei sp\u00e4teren Umplanungen aufgrund von nachtr\u00e4glich ge\u00e4nderten Rahmenbedingungen umst\u00e4ndlich gestaltet.\nDie vorliegende Arbeit legt die Grundlage f\u00fcr eine algorithmischen Planungshilfe, die den Entwurf der Rohrleitung beschleunigt und einen Vorschlag f\u00fcr die H\u00e4ngerplatzierung gibt.\nNach der Diskretisierung des Bauraums vereinigt das Problem zwei verschiedene Hauptbestandteile.\nAuf der einen Seite findet sich eine bekannte kombinatorische Fragestellung wieder: Die Berechnung eines k\u00fcrzesten Pfades oder - im komplizierteren Fall - das Steinerbaum-Problem.\nDies fu\u00dft auf der Vorgabe, dass die L\u00f6sung des Problems einen Rohrleitungsverlauf beschreiben soll, der sich nicht beliebig aufspalten darf.\nAndererseits muss die Mechanik der Rohrleitung ber\u00fccksichtigt werden, welche sich haupts\u00e4chlich im Eigengewicht der Rohrleitung und Ausdehnungskr\u00e4fte aufgrund der W\u00e4rme widerspiegelt.\nNach Einf\u00fchrung der wichtigsten Voraussetzungen leiten wir ein gemischt-ganzzahlig-nichtlineares Modell (MINLP) her, das alle kombinatorischen und nichtlinearen Aspekte des Problems ber\u00fccksichtigt.\nUm reale Instanzen berechnen zu k\u00f6nnen werden verschiedene Ans\u00e4tze untersucht: Das L\u00f6sen des nicht-konvexen MINLP-Modells, Linearisierungsstrategien, Umformulierung des Problems zu einem MISOCP-Modells und schlie\u00dflich eine Dekomposition des Problems.\nLetztgenannte Technik erweist sich in der Praxis als am vorteilhaftesten, wobei sie sich durch den geschickten Einsatz eines Unzul\u00e4ssigkeitstests noch weiter verbessern l\u00e4sst.\nDieser Test kann weiterhin genutzt werden um spezielle Schnittebenen herzuleiten, die zu besseren unteren Schranken in den Subproblemen f\u00fchren.\nDes Weiteren beweisen wir, dass das Problem auch von komplexit\u00e4tstheoretischer Seite her \"schwer\" zu l\u00f6sen ist.\nAlle entwickelten Algorithmen und Modelle werden an akademischen und Realinstanzen getestet, wobei sich interessante Einsichten in die Vor- und Nachteile der einzelnen Vorgehen ergeben.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6683\nurn:nbn:de:bvb:29-opus4-66837\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-66837\n978-3-7375-6373-4\nhttps://opus4.kobv.de/opus4-fau/files/6683/Thesis.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6684\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\npacs\npacs:87.61.Tg\nmsc\nmsc:97M60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nReconstruction Techniques for Dynamic Radial MRI\nRekonstruktionstechniken f\u00fcr dynamische, radiale MR-Bildgebung\nGrimm, Robert\nMRT\n3D-Rekonstruktion\nMedizinische Bildgebung\nPositronen-Emissions-Tomographie\nOnkologie\nddc:600\nToday, magnetic resonance imaging (MRI) is an essential clinical imaging modality and routinely used for orthopedic, neurological, cardiovascular, and oncological diagnosis.\nThe relatively long scan times lead to two limitations in oncological MRI.\nFirstly, in dynamic contrast-enhanced MRI (DCE-MRI), spatial and temporal resolution have to be traded off against each other.\nSecondly, conventional acquisition techniques are highly susceptible to motion artifacts. As an example, in DCE-MRI of the liver, the imaging volume spans the whole abdomen and the scan must take place within a breath-hold to avoid respiratory motion. Dynamic imaging is achieved by performing multiple breath-hold scans before and after the injection of contrast agent. In practice, this requires patient cooperation, exact timing of the contrast agent injection, and limits the temporal resolution to about 10 seconds.\nThis thesis addresses both challenges by combining a radial k-space sampling technique with advanced reconstruction algorithms for higher temporal resolution and improved respiratory motion management.\nA novel reconstruction technique, golden-angle radial sparse parallel MRI (GRASP),\nenables performing DCE-MRI at simultaneously high spatial and temporal resolution.\nIterative gradient-based and alternating optimization techniques were implemented and evaluated.\nGRASP is based on a single, continuous scan during free breathing, allowing for a simplified and more patient-friendly examination workflow.\nThe technique is augmented by an automatic detection of the contrast agent bolus arrival and by incorporating variable temporal resolution.\nThese proposed extensions reduce the number of generated image volumes, resulting in faster reconstruction and post-processing.\nThe radial trajectory also allows to extract a respiratory signal directly from the scan data. This self-gating property can be used for dynamic imaging in such a way that different, time-averaged phases of respiration are retrospectively reconstructed from a free-breathing scan. Automated algorithms for deriving, processing, and applying the self-gating signal were developed.\nThe clinical relevance of self-gating was demonstrated by generating a motion model to correct for respiratory motion in a simultaneous positron emission tomography (PET) examination on hybrid PET/MRI scanners. This approach reduces the motion blur and, thus, improves tracer uptake quantification in moving lesions, while avoiding an increased noise level as it would be the case for conventional gating techniques.\nIn conclusion, the presented advanced reconstruction techniques help to improve the spatio-temporal resolution as well as the robustness with respect to motion of dynamic radial MRI. The effectiveness of the proposed methods was supported by numerous studies in patient settings, showing that non-Cartesian k-space sampling can be advantageous in a variety of applications.\nDie Magnetresonanztomographie (MRT) ist heute eine unverzichtbare klinische Bildgebungsmodalit\u00e4t und wird in der Routine zur Diagnose von orthop\u00e4dischen, neurologischen, kardiovaskul\u00e4ren und onkologischen Fragestellungen angewendet.\nIn der Onkologie f\u00fchrt die relativ lange Untersuchungsdauer zu zwei Einschr\u00e4nkungen.\nZum einen m\u00fcssen, besonders in der dynamischen kontrastmittelgest\u00fctzten MRT (DCE-MRI), r\u00e4umliche und zeitliche Aufl\u00f6sung gegeneinander abgewogen werden.\nZum anderen sind konventionelle Abtastverfahren des k-Raums anf\u00e4llig gegen\u00fcber Bewegungsartefakten.\nBeispielsweise muss das Bildgebungsvolumen bei DCE-MRI der Leber das gesamte Abdomen abdecken und die Messung muss dennoch innerhalb eines Atemstopps erfolgen, um Atembewegung zu vermeiden. Dynamische Bildgebung wird durch mehrere derartige Messungen vor und nach der Injektion des Kontrastmittels erzielt.\nIn der Praxis ist bei diesem Vorgehen eine Zeitaufl\u00f6sung von maximal etwa 10 Sekunden erreichbar, w\u00e4hrend die Kooperation des Patienten und genaue zeitliche Abstimmung der Kontrastmittelgabe unabdingbar sind.\nIn der vorliegenden Arbeit werden beide Aspekte durch die Kombination von radialer k-Raum-Abtastung und erweiterten Rekonstruktionstechniken adressiert.\nEine neue Rekonstruktionstechnik f\u00fcr radiale Perfusionsbildgebung, GRASP, erlaubt es, gleichzeitig sowohl hohe Orts- als auch Zeitaufl\u00f6sungen zu erreichen.\nIterative gradientenbasierte und alternierende Optimierungsverfahren wurden f\u00fcr GRASP implementiert und evaluiert.\nBei GRASP basiert die Akquisition auf einer einzigen, zusammenh\u00e4ngenden Messung bei freier Atmung, was die Untersuchung vereinfacht und auch patientenfreundlicher gestaltet.\nDas Verfahren wird durch eine neuartige automatische Detektion der Kontrastmittelanflutung und durch die M\u00f6glichkeit einer variablen Zeitaufl\u00f6sung erg\u00e4nzt.\nDiese Erweiterungen reduzieren die Anzahl der erzeugten Volumina und erlauben dadurch schnellere Rekonstruktion und Nachverarbeitung der Bilder.\nDas radiale Abtastschema erlaubt es au\u00dferdem, ein Atemsignal aus den k-Raum-Rohdaten abzuleiten.\nDiese auch als Self-Gating bezeichnete Eigenschaft kann zur dynamischen Bildgebung genutzt werden, indem unterschiedliche, zeitlich gemittelte Atemphasen retrospektiv aus einer Messung in freier Atmung abgebildet werden. In dieser Arbeit werden automatische Algorithmen pr\u00e4sentiert, um das Self-Gating-Signal zu extrahieren, zu verarbeiten und anzuwenden. Der praktische Nutzen wird durch die Erzeugung eines Bewegungsmodells demonstriert, das die Kompensation von Bewegungsartefakten einer zeitgleich ausgef\u00fchrten Positronen-Emissions-Tomographie (PET) bei hybriden PET/MRT-Ger\u00e4ten erm\u00f6glicht. Dieser Ansatz reduziert Bewegungsunsch\u00e4rfe und f\u00fchrt dadurch zu einer genaueren Quantifizierung der Tracer-Aufnahme in beweglichen L\u00e4sionen und vermeidet dabei aber ein erh\u00f6htes Rauschniveau, wie es durch konventionelle Gating-Technik herbeigef\u00fchrt w\u00fcrde.\nDie pr\u00e4sentierten erweiterten Bildgebungstechniken helfen so, die Orts- und Zeitaufl\u00f6sung sowie die Bewegungsunempfindlichkeit von dynamischer, radialer MRT zu verbessern. Die Wirksamkeit der vorgeschlagenen Verfahren wird durch mehrere Studien mit klinischen Patienten gest\u00fctzt und zeigt, dass unterschiedliche Applikationen von den Vorteilen nicht-kartesischer k-Raum-Abtastung profitieren k\u00f6nnen.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6684\nurn:nbn:de:bvb:29-opus4-66842\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-66842\nhttps://opus4.kobv.de/opus4-fau/files/6684/Diss_Grimm_FINAL.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6724\n2018-07-11\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:H.\npacs\npacs:00.00.00\nmsc\nmsc:68-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nEntwicklung und Anwendung eines Modells zur Untersuchung soziotechnischer Faktoren bei der Einf\u00fchrung neuer Informationssysteme im klinischen Bereich\nA framework to evaluate human and technical concerns when introducing IT systems to healthcare clinics\nVollmer, Anne-Maria\nMensch-Computer-Interaktion\nSoziotechnik\nddc:004\nHintergrund und Ziele\nViele gro\u00dfe IT-Implementierungen im klinischen Bereich scheitern, es wird von einer Rate von bis zu 70% ausgegangen [273]. Die oftmals kommerziellen Systeme enthalten formale und standardisierte Konzepte, welche nicht der Arbeit im spezifischen klinischen Umfeld entsprechen. Zum Teil sind die Systeme f\u00fcr andere organisatorische Rahmenbedingungen, Anwendungsf\u00e4lle und Gesetzgebungen entwickelt und entsprechen nicht der Organisation-/einheit in der sie um Einsatz kommen. Als Folge k\u00f6nnen diese Systeme schwer implementiert werden, die klinischen Anwender wie \u00c4rzte und Pflegekr\u00e4fte lehnen die Nutzung ab. Die vorliegende Arbeit entwickelt ein soziotechnisches Evaluationsmodell um die zugrunde liegenden technischen, organisatorischen und anwenderbezogenen Einflussfaktoren schon w\u00e4hrend der IT-Implementierungen zu identifizieren. Mithilfe der konstanten Evaluation w\u00e4hrend der Einf\u00fchrung sollen Erfolg bzw. Misserfolg der Implementierung erkl\u00e4rt und valide Daten f\u00fcr die Weiterentwicklung der IT sowie der Organisationsstrukturen geliefert werden. Das Modell wurde in einer empirischen Studie zur Evaluation der Umstellung auf eine elektronische Pflegedokumentation am Universit\u00e4tsklinikum (UK) Erlangen eingesetzt.\nMethoden\nDas Modell wurde aufbauend auf einer Literaturstudie entwickelt. Die gefundene Literatur wurde in drei Kategorien klassifiziert: Theorien, welche beschreiben wie Technologie im klinischen Bereich implementiert werden, Studien, welche die Anforderungen f\u00fcr erfolgreiche Implementierungen und potentielle Hindernisse aufzeigen und Studien, welche Methoden f\u00fcr die Evaluation von IT-Implementierungen liefern. Die verschiedenen Aspekte wurden in einem neuen Evaluationsmodell zusammengef\u00fchrt, welches Prinzipien f\u00fcr die prozessbegleitende Evaluation erschlie\u00dft. Das Modell wurde eingesetzt, um den Ver\u00e4nderungsprozess von der papiergest\u00fctzten hin zu einer elektronisch gef\u00fchrten Pflegedokumentation am UK Erlangen zu untersuchen. Die Evaluation erfolgte \u00fcber einen Zeitraum von zwei Jahren auf den f\u00fcnf beteiligten Pilotstationen und im Rahmen der Projektarbeit mithilfe qualitativer und quantitativer Methoden.\nErgebnisse\nDas entwickelte Modell beantwortet die Fragen des \u201dWann soll evaluiert werden?\u201d, \u201dWelche Faktoren k\u00f6nnen potenziell Einfluss nehmen und untersucht werden?\u201d, \u201dWelche Sichtweisen k\u00f6nnen evaluiert werden?\u201d und \u201dWelche Methoden stehen zur Verf\u00fcgung?\u201d. Die empirische Studie am UK Erlangen hat gezeigt, dass der Erfolg oder Misserfolg von neuer IT im klinischen Bereich sich nicht mit wenigen Faktoren erkl\u00e4ren l\u00e4sst. Was die Einf\u00fchrung misslingen oder gelingen l\u00e4sst, wird von multiplen Faktoren bestimmt, die sich w\u00e4hrend der Einf\u00fchrung \u00e4ndern: Einstellungen und Erwartungen der Pflegekr\u00e4fte, die Konzeption der Benutzeroberfl\u00e4che, fehlende Funktionalit\u00e4ten,unzureichende Umsetzung von Anwenderanforderungen, fachspezifische Abl\u00e4ufe und viele andere. Die im Modell vorgesehene breite Untersuchung des Zusammenspiels von Anwender, Arbeitsorganisation und IT, die kontinuierliche Evaluation, sowie der Einsatz der methodischen Triangulation haben sich in der Studie bewiesen, um das Nutzungsverhalten der Anwender zu erkl\u00e4ren.\nPraktische Schlussfolgerungen\nIn der Vergangenheiten haben viele Forschungsarbeiten versucht, den Erfolg bzw. die Nutzung von klinischen IT-Systemen mit einem definierten Set an Einflussfaktoren vorauszusagen. Es wurde beispielsweise die \u201dAngst Fehler zu machen\u201d als Einflussfaktor untersucht und quantifiziert,\nwie stark dieser einzelne Faktor auf die Systemakzeptanz wirkt. Nach den Erfahrungen in unserer Studie ist dieses Vorgehen nicht zielf\u00fchrend. Wir empfehlen die umfassende Analyse der Arbeitsorganisation mit einem wie von uns entwickelten Rahmenwerk. Die Einflussfaktoren sind zu vielf\u00e4ltig, kontextspezifisch und unvorhersebar. Nach der Untersuchung anhand des entwickelten Rahmenwerkes, welches die wirkenden Einflussfaktoren offen legt, k\u00f6nnen diese Faktoren im weiteren Studienverlauf auf ihre Einflussst\u00e4rke hin untersucht werden. Als Schlussfolgerung sehen wir zudem, dass die prozessbegleitende Evaluation ausreichend Personalressourcen oder sich weniger aufwendigen Forschungsmethodiken bedienen muss, um ein zeitnahes Feedback zu gew\u00e4hrleisten.\nDie Evaluation der technischen Produktentwicklung sehen wir als essenziell. Das eingekaufte Softwareprodukt wurde in unserer Studie stark hausintern angepasst. Nach dieser Erfahrung muss zur Interpretation des Erfolgs und des Nutzungsverhaltens die tats\u00e4chliche Ausgestaltung des Produkts bekannt sein. In der Studie der elektronischen Pflegedokumentation wurde deutlich, dass die Dokumentation flexibel gestaltet sein muss. Die Dokumentation der verschiedenen Schritte muss an die Patientenbed\u00fcrfnisse und die Expertise der Pflegekraft anpassbar sein. Das System\nbeispielsweise sieht eine Planung von allen pflegerischen Ma\u00dfnahmen vor, dies entspricht oft nicht der pflegerischen Wirklichkeit. Bekommt ein Patient pl\u00f6tzlich Fieber, m\u00fcssen Ma\u00dfnahmen auch ungeplant dokumentiert werden. Die angebotenen Inhalte m\u00fcssen fachbereichsspezifisch oder sogar\nstationsspezifsch angepasst sein und die Benutzeroberfl\u00e4che muss den Tagesablauf der Pflege widerspiegeln, um die Benutzerfreundlichkeit zu gew\u00e4hrleisten.\nBackground and Objectives\nNearly 70% of major IT implementations in clinics fail [273]. These mostly commercial IT systems are built on requirements that are distanced from practical scenarios in a clinic. They are often developed for different organizational contexts and use cases, or are beholden to laws that do not apply to the organization in which they are deployed. In addition to being difficult to implement,\nphysicians and nurses often do not use them. We develop a socio-technical model that evaluates technical, organizational, and user factors during an IT deployment. Through constant evaluation, it can detect early problems and risks, and glean data to explain an eventually successful or unsuccessful deployment. The model has been validated with this objective in an empirical study in evaluating how nurses document processes electronically at Erlangen University Hospital.\nMethods\nWe created the model by classifying literature into three categories: studies showing how technology is implemented in the clinic, studies that show the requirements for successful deployment and possible pitfalls, and studies that show methods for evaluating a deployment\u2019s outcome. The model is based on aspects found in the literature which describe the process and methods of evaluating IT deployments. We used the model to evaluate how nurses shifted towards documenting their processes electronically. Our work was conducted at Erlangen University Hospital over two years at five participating wards, using qualitative and quantitative methods.\nResults\nThe model we created is grounded in extensive questions: when to evaluate a process, what methods to use in evaluation, which of a myriad of factors to evaluate, and whose perspectives to consider about a new process. Through the Erlangen study, we found that it is impossible to determine why an IT deployment succeeds or fails in a clinic when only considering a few factors. What makes or breaks a deployment varies wildly: attitudes and expectations of nurses, poor user interfaces, malfunctioning software, an inability to implement user requirements, and a fear of and lack of support from the IT department, among others. In the Erlangen study, our methodology allowed us to predict how nurses would use the system.\nPractical Conclusions\nIn the past, researchers have attempted to describe why IT deployments in the clinic fail or succeed by hypothesizing about a handful of factors. They might choose \u201cfear of making mistakes,\u201d as one factor, and take measurements to quantify how much it contributes or detracts from a deployment\u2019s success. They restrict themselves to be able to compare results with other studies scientifically. We believe this approach is ineffective; instead, we recommend a catch-all evaluative framework like we have developed. Influencing factors are simply too numerous and unpredictable to prune before a deployment. After the model has been used to show which factors are and are not important, a finer research study can delve into how much the now-identified factors contribute to a project\u2019s success.\nStaff resources should be increased, so evaluators can iterate quickly on factors that prove themselves useful or useless during the evaluation. During deployment, software grows dissimilar from its original state as it conforms to user requirements, so describing the transformation is essential to interpreting the results. In the Erlangen study, we found that nursing process documentation must be flexible. Which steps to document depends on the nurse\u2019s expertise and the specific patient. IT systems often force nurses into a planned day of care for a patient, but this contradicts reality. A patient may suddenly catch fever, and an unplanned event like this must also be documented. The user interface must likewise reflect the daily care routine. Finally, documentation catalogs should be pared down by department, as the verbosity is otherwise overwhelming to nurses.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6724\nurn:nbn:de:bvb:29-opus4-67243\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67243\nhttps://opus4.kobv.de/opus4-fau/files/6724/Dissertation_final_Vollmer.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6753\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:629\npacs\npacs:47.11.Df\nmsc\nmsc:76-04\ninstitutes\ninstitutes:Tech_Chemieing\nEntwicklung und Anwendung eines semi-impliziten Kopplungsverfahrens zur numerischen Berechnung der Fluid-Struktur-Wechselwirkung in turbulenten Str\u00f6mungen mittels Large-Eddy Simulationen\nDevelopment and application of a semi-implicit coupling approach for the numerical computation of fluid-structure interaction in turbulent flows using large-eddy simulations\nM\u00fcnsch, Manuel\nFluid-Struktur-Wechselwirkung\nddc:629\nIn vielen Bereichen der Ingenieurwissenschaften ist das Mehrfeldproblem der Fluid-Struktur-Wechselwirkung von gro\u00dfer Bedeutung, wobei die\nStr\u00f6mungsfelder zumeist von turbulenter Natur sind. Zur Berechnung dieser Str\u00f6mungsfelder ist das Verfahren der Large-Eddy\nSimulation in Verbindung mit einem expliziten Pr\u00e4diktor-Korrektor-Verfahren ein zu bevorzugender Ansatz, welcher\ngeeignet mit dem Strukturfeld zu koppeln ist. Das \u00fcbergeordnete Ziel der vorliegenden Arbeit ist die Entwicklung\neines neuartigen Kopplungsansatzes zur effektiven Berechnung der Fluid-Struktur-Wechselwirkung in turbulenten Str\u00f6mungen mittels\nLarge-Eddy Simulationen (LES).\nIn der vorliegenden Arbeit erfolgt zun\u00e4chst eine Einf\u00fchrung in die Thematik der Fluid-Struktur-Wechselwirkung. Dazu werden die grundlegenden Anregemechanismen\nanhand von Fallbeispielen beschrieben und ein Literatur\u00fcberblick \u00fcber Modellierungs- und Berechnungsans\u00e4tze gegeben.\nAusgehend von den Erhaltungsgleichungen der Kontinuumsmechanik erfolgt eine Vorstellung der hier ber\u00fccksichtigten Erhaltungsgleichungen\nf\u00fcr jedes der Teilfelder, d.h. der Str\u00f6mungs- und Strukturmechanik. Hinsichtlich der Str\u00f6mungsmechanik wird in\ndiesem Zusammenhang das Verfahren der Large-Eddy Simulation basierend auf einer impliziten Filterung detailliert vorgestellt.\nEin wesentlicher Gesichtspunkt ist die Vereinbarkeit der zugrundeliegenden ALE-Formulierung\n(Arbitrary Lagrangian-Eulerian) bzw. eines bewegten Rechengitters und dem Verfahren der LES. Die Bewegung des\nRechengitters induziert eine r\u00e4umlich und zeitliche Variation der Filterweite, was zu zus\u00e4tzlichen\nCommutation-Fehlern und einer Reduktion der Qualit\u00e4t der Str\u00f6mungsvorhersage f\u00fchrt und im weiteren Verlauf der\nArbeit am Beispiel eines Testfalls untersucht wird.\nDie Berechnung der Str\u00f6mung basiert hierbei auf dem Str\u00f6mungsl\u00f6ser FASTEST-3D.\nDer Str\u00f6mungsl\u00f6ser basiert auf der dreidimensionalen Finite-Volumen-Methode und wird im Rahmen dieser Arbeit f\u00fcr den Fall der\nexpliziten Zeitdiskretisierung um eine ALE-Formulierung der Erhaltungsgleichung unter Ber\u00fccksichtigung\ndes Raumerhaltungsgesetzes erweitert. Im Rahmen des Pr\u00e4diktor-Korrektor-Verfahrens zweiter Ordnung wird die\nPr\u00e4diktion einer intermedi\u00e4ren Str\u00f6mungsgeschwindigkeit \u00fcber ein explizites, low-storage Runge-Kutta-Verfahren durchgef\u00fchrt,\nwelches die Impulserhaltungsgleichung in der Zeit integriert. Die Korrektur\nder intermedi\u00e4ren Str\u00f6mungsgeschwindigkeit und des Druckes zur Erf\u00fcllung der Massenerhaltung erfolgt \u00fcber eine Poisson-Gleichung,\nwas im Detail in der vorliegenden Arbeit beschrieben wird.\nZur Berechnung des Strukturfeldes wird auf die Methode der Finite-Elemente f\u00fcr die Berechnung einer flexiblen Struktur zur\u00fcckgegriffen. Die eigentliche Berechnung\nwird \u00fcber das Programmpaket Carat (LS Statik, TU M\u00fcnchen) durchgef\u00fchrt. Die zeitliche Integration erfolgt in diesem Fall\n\u00fcber eine Formulierung des Newmark-Verfahrens f\u00fcr nicht-lineare F\u00e4lle.\nEinige der anvisierten Testf\u00e4lle lassen sich auf Seiten der Struktur auf Starrk\u00f6rperbewegungen reduzieren. Die zeitliche Integration\nwird \u00fcber ein explizites Runge-Kutta-Verfahren oder ebenso ein nicht-lineares Newmark-Verfahren realisiert. Die im Rahmen dieser Arbeit\nimplementierten L\u00f6sungsans\u00e4tze werden anhand einer Modellgleichung validiert.\nZur Kopplung von Str\u00f6mungs- und Strukturfeld werden zun\u00e4chst die Kopplungsbedingungen und m\u00f6gliche L\u00f6sungsans\u00e4tze\ndiskutiert. Der Hauptgesichtspunkt ist hierbei die Entwicklung eines neuartigen partitionierten Kopplungsalgorithmus,\nwelcher die Vorteile eines expliziten Pr\u00e4diktor-Korrektor-Verfahrens f\u00fcr das Fluidfeld mit den\nStabilit\u00e4tseigenschaften impliziter Kopplungsans\u00e4tze zur Berechnung der Fluid-Struktur-Wechselwirkung verbindet. Das resultierende semi-implizite\nKopplungsverfahren ist dadurch charakterisiert, dass der Korrektor-Schritt, d.h. die Poisson-Gleichung, des Fluidfeldes\nim Rahmen einer Subiterationsschleife mit dem Strukturl\u00f6ser gekoppelt wird. Hierbei wird so lange iteriert, bis ein vorgeschriebenes\nKonvergenzkriterium unterschritten wird. Hierdurch wird die Anforderung nach einem expliziten Zeitschrittverfahren im Kontext der LES\nund einem stabilen Kopplungsalgorithmus erf\u00fcllt. Weitere Aspekte der Kopplung sind hierbei die Pr\u00e4diktion und Unterrelaxation der Verschiebungen zur\nStabilisierung und Beschleunigung der Berechnung und die die verwendeten Konvergenzkriterien. Aufgrund unterschiedlicher r\u00e4umlicher\nDiskretisierungen der Teilfelder sind des Weiteren Lasten und Verschiebungen an der Strukturoberfl\u00e4che zwischen den jeweiligen\nRechengittern zu interpolieren. Ein weiterer diskutierter Gesichtspunkt stellt\ndie informationstechnische Realisierung der Kopplung zwischen Str\u00f6mung und Struktur bzw. zwischen zwei entsprechenden\nBerechnungsprogrammen, hier FASTEST-3D und Carat, dar. Der Datentransfer und die Dateninterpolation wird\nhier \u00fcber die Kopplungsschnittstelle CoMA (LS Statik, TU M\u00fcnchen) realisiert, welche im Kern auf eine parallele\nProzessierung mittels MPI (Message Passing Interface) aufbaut. Dies erm\u00f6glicht die Nutzung von Mehrkern-Architekturen oder HPC-Systemen.\nNachfolgend befasst sich die Arbeit mit der Berechnung von Testf\u00e4llen. Zun\u00e4chst richtet sich hierbei der Fokus auf die Qualit\u00e4t der\nLES auf bewegten Rechengittern, wie bereits oben beschrieben. Der Einfluss des induzierten Commutation-Fehlers wird hierzu\nam Testfall einer Kanalstr\u00f6mung bei einer auf die Wandschubspannungsgeschwindigkeit bezogenen Reynolds-Zahl von 590 untersucht. Zun\u00e4chst\nwerden hierzu Berechnungen ohne eine aufgepr\u00e4gte Gitterbewegung durchgef\u00fchrt. F\u00fcr die untersuchten LES-Feinstrukturmodelle zeigt\nsich eine gute \u00dcbereinstimmung hinsichtlich der Str\u00f6mungsgeschwindigkeit, der Wandschubspannungsgeschwindigkeit und den Statistiken\nzweiter Ordnung mit DNS- und LES-Referenzdaten, was als erfolgreiche Validierung des Pr\u00e4diktor-Korrektor-Verfahrens und der\nimplementierten LES-Feinstrukturmodelle zu sehen ist. Bei einer aufgepr\u00e4gten, sinusf\u00f6rmigen Gitterbewegung mit vorgegebener Amplitude\nund Periode zeigt sich, dass der induzierte Gesamtfehler hinsichtlich der gemittelten Str\u00f6mungsgeschwindigkeit und der Statistiken zweiter Ordnung f\u00fcr\nzunehmende Amplituden und kleiner werdende Perioden im Vergleich zum unbewegten Referenzfall zunimmt. F\u00fcr moderate Gitterbewegungen liegen\ndie Abweichungen in einem tolerablen Bereich, was somit grunds\u00e4tzlich die Anwendung der LES im Kontext der Fluid-Struktur-Wechselwirkung unter Ber\u00fccksichtigung\nder ALE-Formulierung der Erhaltungsgleichungen erm\u00f6glicht. Ausgehend von dieser Feststellung, wird der entwickelte semi-implizite Kopplungsansatz im\nRahmen von drei numerischen und zwei experimentellen Testf\u00e4llen f\u00fcr laminare und\nturbulente Str\u00f6mungsregime untersucht und validiert. Die Reynolds-Zahl reicht hierbei von 200 bis 68000. Die Fluid-Struktur-Wechselwirkung\nist f\u00fcr die Testf\u00e4lle durch den Mechanismus der bewegungs-induzierten oder der instabilit\u00e4ts-induzierten Anregung dominiert.\nF\u00fcr den semi-impliziten Kopplungsalgorithmus zeigen sich insbesondere an den Ergebnissen zum Benchmark-Testfall FSI3\nder DFG Forscherguppe 493 sehr gute \u00dcbereinstimmungen mit den Referenzwerten hinsichtlich der ermittelten Schwingungsamplituden und Frequenzen der\nStruktur. Am Beispiel experimenteller Testf\u00e4lle von Gomes et al. werden des Weiteren Parameterstudien zum Einfluss der Pr\u00e4diktion der Verschiebungen im Rahmen der Kopplung,\nder LES-Feinstrukturmodelle, der Modellkonstanten und der Randbedingungen im Fluidfeld auf die L\u00f6sung des Mehrfeldproblems durchgef\u00fchrt.\nHinsichtlich des wichtigen Aspektes der Feinstrukturmodellierung wird gezeigt, dass Frequenz und Amplitude der resultierenden Strukturbewegung von\nden LES-Feinstrukturmodelle bzw. der Wahl der Modellkonstanten abh\u00e4ngen. Dieses Verhalten wird urs\u00e4chlich\nauf die Beeinflussung des Umverteilungsprozesses der Wirbelst\u00e4rke zur\u00fcckgef\u00fchrt, was die St\u00e4rke der Druckmaxima und -minima, induziert durch\nan der Struktur abschwimmende Wirbel, beeinflusst und somit Auswirkungen auf die resultierende Strukturbewegung hat.\nFluid-structure interaction (FSI) plays an important role in many fields of engineering\nsciences. In addition, most of the technically relevant flows are dominated by turbulence.\nFor the prediction of a turbulent flow field in the context of FSI, large-eddy\nsimulation (LES) is a preferential method in combination with an\nexplicit predictor-corrector scheme. The overall objective of the current thesis is the\ndevelopment of a novel coupling approach to enable the efficient prediction of FSI in turbulent\nflows via LES.\nStarting point of the present thesis is an introduction to the fluid-structure interaction phenomenon\ndescribing the excitation mechanisms and giving a literature overview on modeling and numerical\napproaches. Based on the conservation equations of continuum mechanics the formulation of the\ngoverning equations for each particular field, here the fluid and the structure, is presented.\nRegarding the simulation of turbulent flows the concept of large-eddy\nsimulation by the use of implicit filtering is introduced. The underlying ALE (Arbitrary Lagrangian-Eulerian)\nformulation of the governing equations is an important issue for LES relying on implicit filtering.\nMoving grids are causing a variation of the filter width in time and space leading to additional commutation\nerrors. This issue is addressed by a test case described later on.\nFor the computation of the flow field the in-house code FASTEST-3D is used. The code relies on a three-dimensional\nfinite-volume formulation for the spatial discretization of the governing equations\nwhich is extended towards an ALE formulation in the case of explicit time marching\nwithin the present developments. A predictor-corrector scheme of second order is basically used\nto compute the flow field. Within the predictor step an explicit low storage Runge-Kutta scheme\npredicts intermediate flow velocities by advancing the momentum equation in time. The corrector-step\nupdates the intermediate flow velocities and the pressure via a Poisson equation to fulfill mass conservation,\nwhich is described in detail in the current thesis.\nAlso the numerical approaches to solve the governing equations for the structure are described in detail.\nSpatial discretization via finite-elements is used to compute the behaviour of a complex, i.e. a flexible\nstructure. Here, the temporal integration is performed via a non-linear formulation of the Newmark scheme.\nThe computation is done by using the highly developed finite-element solver Carat (Chair of Structural Analysis, TU Munich).\nFor particular test cases the structure can be described by a rigid body approach. The underlying equations are integrated\nin time via an explicit Runge-Kutta scheme or a non-linear implementation of the Newmark scheme. The implemented methods\nare validated against the results of a model equation.\nFor the coupling of the fluid and the structure coupling conditions and solution methods are presented. Here,\nthe main objective is the development of a novel partitioned coupling approach which combines the features of an\nexplicit predictor-corrector scheme for the estimation of the fluid field with the stability properties known\nfrom fully implicit coupling approaches. The resulting semi-implicit coupling approach relies on a coupling\nthe corrector-step of the fluid field and the governing equations for the structure within a sub-iteration loop.\nHere, sub-iterations are performed until a prescribed convergence criterion is fulfilled. Thus, the requirements\nof explicit time-marching favorable for LES on the one hand side and stable coupling on the other hand are met.\nFurther coupling related aspects tackled in this context are named as the prediction and underrelaxation of displacements\nto stabilize and accelerate convergence and the applied convergence criterion. Furthermore interpolation of loads\nand displacements is required since both fields are spatially discretized by different approaches (FEM versus FVM) leading\nto non-matching meshes on the surface of the structure. In addition, the technological realization and implementation of the\ncoupling of two codes, here FASTEST-3D and Carat, has to be considered. For that purpose the coupling interface\nCoMA (Chair of Structural Analysis, TU Munich) is used whereas the data transfer is done via MPI\n(Message Passing Interface). Thus, multi-core or high-performance computing is enabled to solve the multi-field problem.\nIn the following chapter test cases are presented and discussed in detail. The impact of moving grids on the quality of\nLES predictions is investigated by considering a turbulent channel test case at friction velocity based Reynolds number of 590.\nInitial computations of the classical test case without any grid movement deliver satisfactory results for the implemented\nLES models and the implemented predictor-corrector scheme in comparison with DNS and LES reference data sets.\nThe impact of moving grids is addressed by forcing a sinusoidal displacement of the grid points with specific amplitudes\nand periods. With increasing amplitudes and decreasing periods the induced deviations of the mean velocity and the second-order\nmoments increase compared to the fixed grid case. For moderate grid movement the impact on the flow field turns out to be\nless significant. Thus, the application of LES in the context of FSI and ALE formulation of the governing equations\nis possible for moderate grid movements. Starting from this results, the behaviour of the developed semi-implicit coupling\napproach is evaluated basing on three numerical and two experimental test cases\nin laminar as well as in turbulent flow regimes. The Reynolds number for this cases ranges from 200 up to 68000 and the\nFSI is dominated by the moving induced or instability induced excitation mechanism.\nThe semi-implicit coupling algorithm delivers satisfactory results, especially for the benchmark test case FSI3\nof DFG Research Group 493, regarding oscillation amplitudes and frequencies of the structures compared to reference\nvalues. The impact of the FSI predictor within the coupling scenario, the LES models and\nmodel constants and the boundary conditions on the solution of the multi-field problem\nis investigated basing on experimental test cases of Gomes et al..\nA core aspect of the current thesis is the LES modeling in the framework of FSI. Basing on the present investigations LES models and model constants\nhave a strong influence on the resulting oscillation amplitudes and frequencies of the structure. The behaviour is explained due to the\nrelation between LES modeling and the process of vorticity redistribution caused by vortex tilting. This influences the induced pressure\nzones of separating vortices. These pressure zones are in turn responsible for the movement of the structure.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6753\nurn:nbn:de:bvb:29-opus4-67533\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67533\nhttps://opus4.kobv.de/opus4-fau/files/6753/150521_DissMT.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6762\n2021-11-19\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nmsc\nmsc:74-05\nmsc:74Qxx\nmsc:74Rxx\nmsc:74Sxx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nCharakterisierung und Modellierung des quasi-statischen Verhaltens und der Erm\u00fcdung eines zellularen Verbundwerkstoffes\nCharacterization and Modeling of Quasi-Static and Fatigue Behavior of a Cellular Composite\nDiel, Sergej\nVerbundwerkstoff\nSchaumglas\nEpoxidharz\nMaterialerm\u00fcdung\nModellierung\nKontinuumssch\u00e4digungsmechanik\nHomogenisierung\nWerkstoffpr\u00fcfung\nddc:620\nZellulare Werkstoffe leisten im Rahmen des Stoff- und Verbundleichtbaus einen wichtigen Beitrag zur Gewichtsreduktion. Dabei haben die syntaktischen Sch\u00e4ume als Vertreter der zellularen Werkstoffe vor allem im Bereich der Tiefseeanwendungen eine gro\u00dfe Verbreitung erfahren. Alternativ zu den \u00fcblicherweise verwendeten Mikrohohlkugeln kann Glasschaumgranulat aus Recyclingglas verwendet werden. Dieses weist gute mechanische Eigenschaften wie Steifigkeit und Druckfestigkeit bei deutlich geringeren Kosten auf. Syntaktische Sch\u00e4ume mit zellularem Granulat als Platzhalter sowie Metall- oder Polymermatrizen werden zellulare Verbundwerkstoffe (ZVW) genannt.\nF\u00fcr einen erfolgreichen Einsatz des ZVW als Konstruktionswerkstoff ist die Kenntnis der mechanischen Eigenschaften unerl\u00e4sslich. In der vorliegenden Arbeit wird das quasi-statische Verhalten und die Erm\u00fcdung eines zellularen Verbundwerkstoffes mit Epoxidharzmatrix und Glasschaumgranulat experimentell untersucht und modelliert. Zur Ermittlung der Schubeigenschaften wird eine neuartige Schubversuchsvorrichtung entwickelt und vorgestellt. Mit Hilfe einer numerischen Homogenisierungsmethode k\u00f6nnen die effektiven elastischen Werkstoffeigenschaften mit hoher Genauigkeit und ohne aufw\u00e4ndige Versuche am ZVW aus den Eigenschaften der Verbundwerkstoffkomponenten berechnet werden. Anhand von zwei ausgew\u00e4hlten Anwendungsbeispielen wird das hohe Leichtbaupotenzial des ZVW demonstriert.\nEs wird erstmals das Erm\u00fcdungsverhalten eines zellularen Verbundwerkstoffes anhand einachsiger statischer Zug- und Druckversuche sowie zyklischer Versuche bei unterschiedlichen Spannungsverh\u00e4ltnissen untersucht. Der Sch\u00e4digungsprozess bei zyklischer Beanspruchung stellt eine Interaktion zwischen statischer und zyklischer Sch\u00e4digung dar. Die Werkstoffsch\u00e4digung wird mit der Abnahme des makroskopischen Elastizit\u00e4tsmoduls assoziiert. Die Analyse der Sch\u00e4digungsevolution erfolgt mit Hilfe von licht- und rasterelektronenmikroskopischen Aufnahmen. Zur Vorhersage der Lebensdauer in Abh\u00e4ngigkeit von der Frequenz und der Signalform werden eindimensionale kontinuumsmechanische Sch\u00e4digungsmodelle f\u00fcr statische bzw. zyklische Sch\u00e4digung verwendet. Die Parameterabstimmung wird basierend auf Versuchen mit rein statischer sowie rein zyklischer Sch\u00e4digung durchgef\u00fchrt. Die Sch\u00e4digungsmodelle ber\u00fccksichtigen eine nichtlineare Sch\u00e4digungsevolution sowie -akkumulation, womit der Reihenfolgeeinfluss abgebildet werden kann.\nEs wird ein spezieller Algorithmus entwickelt, welcher eine sehr schnelle Berechnung der Sch\u00e4digungsevolution f\u00fcr beliebige Frequenzen erm\u00f6glicht. Die mit dem Sch\u00e4digungsmodell f\u00fcr unterschiedliche Frequenzen, Signalformen und Lastreihenfolgen berechneten Lebensdauern stimmen sehr gut mit den experimentellen Ergebnissen \u00fcberein.\nCellular solids are very important in lightweight design due to their good mechanical and physical properties such as high weight-specific stiffness, good energy-absorbing behavior and good thermal and acoustic isolation. Syntactic foams, a class of closed-cell foams, are extensively used e.g. in deep submergence buoyancy where the high compressive strength as well as their low water absorption are required. Cellular composites are syntactic foams with metals or polymers as matrix material combined with cellular glass, mineral, or metal place holders. Cellular composites made with cellular glass granules from recycled glass offer high stiffness and compressive strength at low costs.\nWhen designing components made from cellular composites, the knowledge of their mechanical properties is of vital importance. In this work, the quasi-static and the fatigue behavior of a cellular composite with epoxy matrix and glass foam granules will be studied. The shear properties of the cellular composite are investigated using a new shear test fixture which is particularly suitable for testing of cellular solids. A numerical homogenization method for cellular composites will be introduced. Using this method, the effective elastic properties can be calculated in a very effective manner from the properties of constituent materials without tests on the composite itself. Two examples of practical applications demonstrate the high lightweight capability of the cellular composite.\nThe fatigue behavior of the cellular composite will be investigated for the first time in this work. The experimental tests comprise uniaxial static and cyclic tension, compression, and reversal loading. It is found that the damage process under cyclic loading is an interaction between static and cyclic damage. The material damage is described in terms of macroscopic stiffness loss and the damage behavior is studied using optical and scanning electron microscopy. Continuum damage mechanics models for static and cyclic damage are used to model the damage evolution of the cellular composite. For calibrating the model parameters, pure static and pure cyclic damage tests are performed. The models incorporate nonlinear accumulation and nonlinear interaction of damage and are therefore capable to represent load sequence effects.\nA cycle jumping procedure is developed which allows a very fast calculation of damage evolution for arbitrary load frequencies. The calculated lifetimes for different load frequencies, waveforms, and load sequences are in very good agreement with experiments.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6762\nurn:nbn:de:bvb:29-opus4-67627\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67627\nhttps://opus4.kobv.de/opus4-fau/files/6762/SergejDielDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6771\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:E.\npacs\npacs:00.00.00\nmsc\nmsc:00Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nKnochenneubildung und -remodelling bei der Verwendung von biofunktionalisierten Implantatoberfl\u00e4chen\nKnochenneubildung und -remodelling bei der Verwendung von biofunktionalisierten Implantatoberfl\u00e4chen\nKranich, Lena\nbiofunctional\nddc:617\n1 Zusammenfassung/ Abstract\n1.1 Zusammenfassung\n1.1.1 Hintergrund und Ziele\nZiel biofunktionalisierter Implantatoberfl\u00e4chen ist es, sich beschleunigend auf die Einheilphase der Implantate auszuwirken. In dieser Studie wurden Hydroxylapatit (HA)-beschichtete und durch Korundbestrahlung und Hochtemperatur-\u00c4tzen mikro-strukturell ver\u00e4nderte Titanimplantate in Bezug auf die Osseointegration und ihren Einfluss auf die Ausrichtung der Kollagenfasern untersucht.\n1.1.2 Material und Methode\nAufgrund des dem Menschen vergleichbaren Knochenstoffwechsels wurde f\u00fcr diese Studie das adulte Hausschwein als Versuchstier gew\u00e4hlt. Es wurden insgesamt 540 Implantate untersucht. Hierf\u00fcr wurden 45 Tieren jeweils 12 Implantate in das Os frontale inseriert. Verwendet wurden hierbei modifizierte Ankylos-A8-Implantate mit einer FRIADENT\u00ae-plus-Oberfl\u00e4che, sowohl unbeschichtet als auch mit einer bio-funktionalisierten Implantatoberfl\u00e4che. Als Opferungszeitpunkte wurden 7, 14 und 30 Tage gew\u00e4hlt. Um Aussagen \u00fcber die Osseointegration treffen zu k\u00f6nnen, wurden die gewonnenen Knochenproben histomorphometrisch und immunhistochemisch untersucht. Da viele Quellen einen Zusammenhang zwischen Kraftvektor und Kol-lagenfaserorientierung beschreiben, wurde in dieser Studie die Ausrichtung dieser Fasern entlang unbelasteter Implantate analysiert.\n1.1.3 Ergebnisse\nDie Implantate mit Hydroxylapatitbeschichtung erreichten in Bezug auf die Osteozy-tendichte, Kollagenfaserorientierung und Osteoidbildung die h\u00f6chsten Werte. Ledig-lich die Neoangiogenese konnte durch die Verwendung der FRIADENT\u00ae-plus-Oberfl\u00e4che positiv beeinflusst werden. Die unbelasteten Implantate zeigten \u00fcberwie-gend transversale Kollagenfaserverl\u00e4ufe (61,96%- 100%).\n1.1.4 Schlussfolgerung\nDie Osseointegration von Implantaten kann durch die Verwendung einer Hydroxyla-patitbeschichtung verbessert werden. Die Biofunktionalisierung von Implantaten stellt somit eine zukunftsweisende Ma\u00dfnahme zur Optimierung von Einheilvorg\u00e4ngen dar. Ob diese histologisch ermittelten Ergebnisse allerdings eine klinische Relevanz besit-zen, ist jedoch fraglich und sollte in weiteren Studien in einem geeigneten Tiermodell \u00fcberpr\u00fcft werden.\n1.2 Abstract\n1.2.1 Background and objectives\nBiofunctionalized implant surfaces aim to accelerate the initial healing period follow-ing implant placement. In this study, hydroxyapatite-coated titanium implants, the microstructure of which was changed by corundum blasting and high-temperature etching, were examined with respect to osseointegration and with respect to their effect on the orientation of the collagen fibers.\n1.2.2 Material and method\nFor this study, the adult domestic pig was selected as the experimental animal be-cause the bone metabolism of the domestic pig is comparable to that of human be-ings. A total of 540 implants were examined. To that end, 12 implants were inserted in the os frontale of 45 animals. The implants used were modified Ankylos-A8 im-plants with a FRIADENT\u00ae plus surface, both uncoated as well as provided with a biofunctionalized implant surface. The selected examination times were 7, 14 and 30 days. The bone samples were examined histomorphometrically and immunohisto-chemically so as to be able to draw conclusions regarding the osseointegration. As many sources describe the correlation between force vector and collagen fiber ori-entation, in this study the direction of these fibers along unloaded implants was ana-lysed.\n1.2.3 Results\nThe implants with a hydroxyapatite coating obtained the highest values with respect to osteocyte density, collagen fiber orientation, and osteoid formation. The use of the FRIADENT\u00ae plus surface merely had a positive effect on the neoangiogenesis. The unloaded implants primarily displayed transverse collagen fiber directions (61,96%- 100%).\n1.2.4 Conclusion\nThe osseointegration of implants can be improved by applying a hydroxyapatite coating. The biofunctionalization of implants thus constitutes an advanced measure for optimizing the initial healing process. However, whether these histologically de-termined results actually have a clinical relevance is questionable and should be veri-fied through further studies, using a suitable animal model.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6771\nurn:nbn:de:bvb:29-opus4-67715\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67715\nhttps://opus4.kobv.de/opus4-fau/files/6771/LenaKranichDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6775\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:519\nmsc\nmsc:60J80\nmsc:60K35\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Mathematik\nThe Shape of Population Profiles for a Class of Interacting Particle Systems\nDie Form von Populationsprofilen f\u00fcr eine Klasse von Interagierenden Teilchensystemen\nSchirmeier, Frank\nStochastisches Teilchensystem\nMarkov-Verzweigungsprozess\nGau\u00df-Funktion\nWahrscheinlichkeitstheorie\nddc:519\nIn the simplest geographic setting, an interacting particle system is a $\\{0,1\\}^{\\Z}$ or $\\N_{0}^{\\Z}$-valued continuous-time stochastic process $(X_{t})_{t\\ge0}$, with the interpretation that $X_{t}(\\xi)$ denotes the number of particles at the geographic location $\\xi\\in\\Z$ at time $t\\ge0$. The term 'particle' is a placeholder whose meaning varies depending on the context. An example is the $\\{0,1\\}^{\\Z}$-valued contact process, which models the spread of an infection in a linearly arranged population: the positions of the 1s in the state $X_{t}$ indicate where the particles (=\u2019infections\u2019) are located, i.e. which individuals are infected at time t. In the course of time, the number and the locations of the infections change according to certain stochastic rules after random times.\nThe goal of this thesis is to examine the overall shape of a given interacting particle system that has been started with a single particle at the origin, at a given finite time t. The starting point is a result of L. Gray (1991) and L. Gray and E. Andjel (2014): For the $\\{0,1\\}^{\\Z}$-valued contact process with nearest neighbour infections, and for fixed t, the map\n$\\Z \\to [0,1]$, $\\xi \\mapsto E [ X_{t}(\\xi) ]$\nis 'bell-shaped': it is symmetric around the origin and is, when restricted to $\\N_{0}$, monotonically decreasing. Gray calls this map a 'population profile'.\nThis thesis proves similar shape results for a class of $\\N_{0}^{\\Z}$-valued systems that we call branching random walks with interactions. The base process is called branching random walks process, which is a simple spatial population model in which the particles can be interpreted as \u2018individuals\u2019: Particles perform independent continuous-time random walks on $\\Z$, and additionally, after certain random times, a given particle either dies or branches. In the latter case, the particle is replaced by two identical copies that behave independently henceforth. From this base process, we derive multilevel branching random walks (in which particles are grouped into clans which themselves undergo certain dynamics), coupled branching random walks (in which additionally -- at certain spatial locations and certain times -- local catastrophes diminish the local population), and coalescing branching random walks (in which the death rate of a given particle is higher if many other particles are around, which can be interpreted in terms of a competition for scarce resources).\nBroadly speaking, we prove that suitable generalizations of Gray's population profile are 'bell-shaped' for our class of particle systems at any given finite time, given that the respective system is initialized with one particle at the origin, and every interaction kernel is itself 'bell-shaped' in a suitable sense. As a corollary, we obtain a criterion whose verification would allow to remove the nearest-neighbour assumption that Gray needed to impose on the contact process.\nRegarding the proof techniques, the focus on finite time horizons comes with the challenge to find novel combinatorial arguments. Further proof concepts are to distinguish particles via labels, which leads to the notion of tree-indexed random walks; to exploit that the 'bell-shape' is preserved by elementary operations such as convolutions, and by less elementary operations such as so-called tree-indexed convolutions; and to invoke a function-valued duality.\nWe briefly indicate how to transfer our results to other geographical spaces rather than $\\Z$, most notably to the hierarchical group $\\Omega_{N}$ and to the hypercube $H_{N}$ (N=2,3,...). Finally, also a side-project concerning local survival of particle systems on Cayley graphs, initiated by J. Swart, is discussed.\nEin interagierendes Teilchensystem ist -- im einfachsten r\u00e4umlichen Setting -- ein $\\{0,1\\}^{\\Z}$ oder $\\N_{0}^{\\Z}$-wertiger zeitstetiger stochastischer Prozess $(X_{t})_{t\\ge0}$, wobei $X_{t}(\\xi)$ als die Anzahl der Teilchen am Ort $\\xi\\in\\Z$ zur Zeit $t\\ge0$ interpretiert wird. Der Begriff Teilchen ist ein Platzhalter, dessen Bedeutung vom Kontext abh\u00e4ngt. Ein Beispiel ist der $\\{0,1\\}^{\\Z}$-wertige 'contact process', der die Ausbreitung einer Infektion in einer linear angeordneten Population modelliert: die 1er im Zustand $X_{t}$ entsprechen den Orten der Teilchen (=Infektionen) zur Zeit t. Diese Orte und deren Anzahl \u00e4ndern sich nach zuf\u00e4lligen Zeitschritten, basierend auf gewissen zugrundeliegenden stochastischen Regeln.\nDas Ziel dieser Arbeit ist, die generelle Form eines gegebenen interagierenden Teilchensystems zu einer endlichen Zeit t zu untersuchen, unter der Annahme, dass das System mit einem einzigen Teilchen am Ursprung gestartet wurde. Der Ausgangspunkt ist ein Resultat von L. Gray (1991) und L. Gray und E. Andjel (2014): F\u00fcr den $\\{0,1\\}^{\\Z}$-wertigen 'contact process' mit n\u00e4chste-Nachbarn-Infektionen ist -- f\u00fcr fixiertes t -- die Abbildung\n$\\Z \\to [0,1]$, $\\xi \\mapsto E [ X_{t}(\\xi) ]$\n'glockenf\u00f6rmig', d.h. symmetrisch um den Ursprung und, eingeschr\u00e4nkt auf $\\N_{0}$, monoton fallend. Gray nennt diese Abbildung ein Populationsprofil.\nDiese Arbeit zeigt vergleichbare Ergebnisse f\u00fcr eine Klasse von $\\N_{0}^{\\Z}$ -wertigen Systemen, die wir 'branching random walks with interactions' nennen. Der zugrundeliegende Prozess 'branching random walks' ist ein einfaches r\u00e4umliches Populationsmodell, in dem Teilchen als 'Individuen' interpretiert werden k\u00f6nnen: Zu gewissen zuf\u00e4lligen Zeiten kann ein gegebenes Teilchen entweder sterben oder verzweigen; im letzteren Fall wird das Teilchen durch zwei identische Kopien ersetzt, die von nun an unabh\u00e4ngig voneinander evolvieren. Zwischen solchen Verzweigungsereignissen werden unabh\u00e4ngige zeitstetige Irrfahrten auf $\\Z$ ausgef\u00fchrt.\nAusgehend von diesem zugrundeliegenden Prozess werden weitere Prozesse abgeleitet: 'multilevel branching random walks' (worin Teilchen zu V\u00f6lkern gruppiert sind, die ihrerseits gewissen Ereignissen ausgesetzt sind), 'coupled branching random walks' (worin zus\u00e4tzlich -- zu gewissen Zeiten -- sogenannte Katastrophen die an einem gegebenen Ort vorhandene Bev\u00f6lkerung dezimieren), und 'coalescing branching random walks' (worin die Todesrate eines gegeben Teilches h\u00f6her ist, wenn viele andere Teilchen in der N\u00e4he sind, was als Kampf um knappe Ressourcen interpretiert werden kann).\nWir beweisen, dass gewisse Verallgemeinerungen von Grays Populationsprofil zu jeder beliebigen gegebenen Zeit 'glockenf\u00f6rmig' sind, vorausgesetzt, dass das jeweilige System mit einem Teilchen am Ursprung startet, und dass alle Interaktionskerne in einem geeigneten Sinne selbst 'glockenf\u00f6rmig' sind. Aus unseren Ergebnissen kann ein Kriterium abgeleitet werden, dessen Verifikation erlauben w\u00fcrde, die n\u00e4chste-Nachbar-Annahme zu entfernen, welche Gray ben\u00f6tigte f\u00fcr seine Untersuchung des 'contact process'.\nDa der Fokus auf einem endlichen Zeithorizont liegt, stehen zum Beweis der Resultate die \u00fcblichen Grenzwertmethoden nicht zur Verf\u00fcgung. Stattdessen m\u00fcssen neue, zum Teil kombinatorische Beweismethoden entwickelt werden. Dar\u00fcberhinaus basieren die Beweise auf einem Hilfsprozess, genannt 'tree-indexed random walks', worin die per se ununterscheidbaren Teilchen mit Hilfe von eindeutigen Labels unterschieden werden. Faltung spielt eine zentrale Rolle, da die 'Glockenform' unter Faltung erhalten bleibt; die Kombination dieser Ideen f\u00fchrt zu einer sogenannten 'tree-indexed convolution'. Um einen neuen Blickwinkel auf die Prozesse zu erhalten, wird schlie\u00dflich eine Dualit\u00e4t ausgewertet, wobei der duale Prozess funktionswertig ist.\nEs wird kurz erkl\u00e4rt, wie unsere Resultate auf andere R\u00e4ume als $\\Z$ ausgeweitet werden k\u00f6nnen, insbesondere auf die hierarchische Gruppe $\\Omega_{N}$ und auf den Hypercube $H_N$ (N=2,3,...). Schlie\u00dflich wird ein von J. Swart angesto\u00dfenes Nebenprojekt besprochen, welches das lokale \u00dcberleben von Teilchensystemen auf Cayley-Graphen zum Thema hat.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6775\nurn:nbn:de:bvb:29-opus4-67751\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67751\nhttps://opus4.kobv.de/opus4-fau/files/6775/schirmeierdiss.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6836\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:006\nccs\nccs:G.1\nmsc\nmsc:65N30\nmsc:65N50\nmsc:65N55\nmsc:65Y05\nmsc:68W10\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nData Structures and Algorithms for the Optimization of Hierarchical Hybrid Multigrid Methods\nDatenstrukturen und Algorithmen zur Optimierung hierarchisch-hybrider Mehrgittermethoden\nGradl, Tobias\nMehrgitterverfahren\nGitterverfeinerung\nFinite-Elemente-Methode\nTetraedrische finite Elemente\nHochleistungsrechnen\nBranch-and-Bound-Methode\nddc:006\nMultigrid methods are among the theoretically most efficient\nalgorithms in numerical simulation. They solve certain classes of\nequations - e. g., those arising from finite element (FE)\ndiscretizations - with optimal complexity. Practically relevant for\nlarge-scale simulations, however, are only algorithms that\nexploit the massive parallelism that characterizes today\u2019s\nhigh-performance computing landscape. Implementing multigrid\nmethods efficiently on massively parallel computers is\nchallenging, because for some of the core algorithms the\ndistribution of the numerical operations to many processors is\nnot straightforward.\nBergen et al. proved with the Hierarchical Hybrid Grids (HHG)\nsoftware framework that it is possible to solve FE simulations\nefficiently with multigrid methods on supercomputers. The\ncentral concept of HHG is to discretize the simulated domains\ninto patch-wise structured meshes. It facilitates the\ndistribution of the computational work to many processors, but it\nalso restricts HHG\u2019s flexibility regarding the types of numerical\nproblems it can be applied to.\nThis thesis presents performance studies for FE simulations with\nup to 3 \u00d7 10\u00b9\u00b9 degrees of freedom that demonstrate short time to\nsolution and good scalability of HHG on up to 16384 processor\ncores. We describe the modifications to the initial version of\nHHG - e. g., in the build system and the performance measurement\nmethods - that were necessary in order to execute and study HHG\non systems of this size.\nThe central chapter of the thesis is dedicated to adaptive mesh\nrefinement (AMR). The technique makes HHG applicable to a new\nclass of problems, which is characterized by a strong variance in\nthe required mesh resolution across the domain, e. g., the\nsimulation of room acoustics or turbulent flows. AMR allows for\nthe FE mesh to be tailored flexibly to the simulation\u2019s\ncharacteristics. The multigrid solver can thus spend the\ncomputer\u2019s resources - memory and processor cycles - on areas\nwhere the simulation requires a high resolution. In consequence,\nthe time to solution decreases and the problem size that can be\nhandled increases. When implementing AMR for HHG, it was\nimportant to maintain the numerical and software engineering\nconcepts that are crucial for HHG\u2019s performance and\nscalability. We describe how the algorithms and data structures\nwere extended in order to achieve this goal.\nThere are many other techniques for optimizing the distribution\nof computational resources in multigrid algorithms. As a contrast\nto AMR, we present a technique that was developed in joint work\nwith Thekale et al. A branch and bound search is used to find the\noptimal number of V-cycles on each level of a full multigrid\nalgorithm. By performing V-cycles on the levels where they yield\nthe best ratio between error reduction and cost, the time to\nsolution of a full multigrid run in a realistic scenario was\nreduced by 35%.\nMehrgittermethoden geh\u00f6ren zumindest theoretisch zu den effizientesten\nAlgorithmen in der Numerischen Simulation. Einige Klassen von Gleichungen,\nz.B. die bei der Disktretisierung mit Finiten Elementen (FE) entstehenden\nGleichungssysteme, sind damit unter bestimmten Voraussetzungen in optimaler\nKomplexit\u00e4t l\u00f6sbar. F\u00fcr die Anwendung im High-Performance-Computing sind jedoch\nnur Methoden relevant, die den extremen Parallelismus aktueller Supercomputer\nausnutzen k\u00f6nnen. Mehrgittermethoden effizient f\u00fcr Parallelrechner zu\nimplementieren ist eine Herausforderung, weil f\u00fcr einige der zentralen\nAlgorithmen das Verteilen der numerischen Operationen auf viele Prozessoren\nnicht trivial ist.\nMit dem Software-Framework Hierarchical Hybrid Grids (HHG) zeigten Bergen et\nal., da\u00df effiziente FE-Simulationen mit Mehrgittermethoden auf Supercomputern\nm\u00f6glich sind. Das zentrale Konzept von HHG ist die Diskretisierung des\nsimulierten Gebiets in abschnittsweise strukturierte Gitter. Das erleichtert\ndas Verteilen der Rechenoperationen auf viele Prozessoren, schr\u00e4nkt allerdings\nauch die Anwendbarkeit von HHG auf bestimmte numerische Probleme ein.\nDiese Arbeit demonstriert mit Performance-Studien auf bis zu 16384\nProzessorkernen und FE-Simulationen mit bis zu 3 \u00d7 10\u00b9\u00b9 Freiheitsgraden die\nhohe Effizienz und Skalierbarkeit von HHG. Um HHG auf Systemen dieser Gr\u00f6\u00dfe\nausf\u00fchren und analysieren zu k\u00f6nnen, waren \u00c4nderungen an der urspr\u00fcnglichen\nHHG-Version n\u00f6tig, z.B. am Build-System und an den Methoden zur\nPerformance-Messung.\nDas zentrale Kapitel der Arbeit widmet sich der adaptiven Gitterverfeinerung\n(AMR, von adatpive mesh refinement). Diese Technik erweitert den\nAnwendungsbereich von HHG auf Probleme mit starker r\u00e4umlicher Varianz in der\nben\u00f6tigten Gitterweite, z.B. die Simulation von Raumakustik oder von\nturbulenten Str\u00f6mungen. AMR erm\u00f6glicht eine flexible Anpassung des FE-Gitters\nan die Simulationscharakteristika. So k\u00f6nnen die Ressourcen des Computers \u2013\nSpeicher und Prozessorzyklen \u2013 gezielt dort eingesetzt werden, wo eine hohe\nGitteraufl\u00f6sung n\u00f6tig ist, und damit die Rechenzeit verringert und die l\u00f6sbare\nProblemgr\u00f6\u00dfe erh\u00f6ht werden. Bei der Implementierung von AMR in HHG war es\nwichtig, die Konzepte aus der Numerik und aus dem Software-Engineering, die f\u00fcr\ndie Performance und Skalierbarkeit von HHG entscheidend sind, zu erhalten. Die\nArbeit beschreibt, wie die Algorithmen und Datenstrukturen erweitert wurden, um\ndieses Ziel zu erreichen.\nEine weitere Methode zur Optimierung von Mehrgittermethoden wurde in\nZusammenarbeit mit Thekale et al. entwickelt. Mit einer Branch-And-Bound-Suche\nwird die Verteilung von V-Zyklen im Full-Multigrid-Algorithmus\noptimiert. V-Zyklen werden gezielt auf den Leveln ausgef\u00fchrt, wo sie das beste\nVerh\u00e4ltnis aus Fehlerreduktion und Kosten erzielen. Mit dieser Methode wurde in\neinem realistischen Szenario eine Verringerung der Laufzeit von Full Multigrid\num 35% erreicht.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6836\nurn:nbn:de:bvb:29-opus4-68368\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-68368\nhttps://opus4.kobv.de/opus4-fau/files/6836/Gradl_Dissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nd/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6867\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:001\nccs\nccs:B.\nccs:D.\npacs\npacs:00.00.00\nmsc\nmsc:68-XX\nmsc:94-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nCode Generation for Tightly Coupled Processor Arrays\nCodegenerierung f\u00fcr eng gekoppelte Prozessorfelder\nBoppu, Srinivas\nhardware accelerators\ncompilers\nhardware/software co-design\nprocessor arrays\nhighlevel synthesis\nddc:001\nIn this dissertation, we consider techniques for automatic code generation and\ncode optimization of loop programs for programmable tightly coupled processor\narray targets. These consist of interconnected small light-weight very\nlong instruction word cores, which can exploit both loop-level parallelism and\ninstruction-level parallelism. These arrays are well suited for executing computeintensive\nnested loop applications, often providing a higher power and area\nefficiency compared with commercial off-the-shelf processors. They are ideal\ncandidates for accelerating the computation of nested loop programs in future\nheterogeneous systems, where energy efficiency is one of the most important\ndesign goals for overall system-on-chip design. In order to harness the full compute\npotential of such an array, we need efficient compiler techniques which can\nautomatically map nested loop programs onto them. Such a compiler framework\nis essential for increasing the productivity of designers as well as for shortening\ndevelopment cycles. In this context, this dissertation proposes a novel code\ngeneration and compaction approach which generates the assembly-level codes\nfor all the processing elements in an array from a scheduled loop nest. The\ncode generation approach itself is independent of the array size, preserves the\ngiven schedule, and is independent of the problem size. As part of this compiler\nframework, we also present a scalable interconnect generation approach where\nthe connections among different processing elements are automatically generated\nfrom the same scheduled loop program. Furthermore, we consider the\nintegration of a tightly coupled processor array into a multi-processor systemon-\nchip: Here, we propose the design of new hardware components such as a\nglobal controller, which generates control signals to orchestrate (synchronize)\nthe programs running on the different processing elements, and address generators,\nwhich are required to generate the address as well as enable signals for\na set of reconfigurable I/O buffers surrounding the processor array. We propose\na fully programmable design of these required hardware components and\nadd the required compiler support to generate the configuration data from the\nsame scheduled loop program as well. In summary, the major contributions of\nthis dissertation enable and ease the fully automated mapping of nested loop\nprograms onto tightly coupled processor arrays.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6867\nurn:nbn:de:bvb:29-opus4-68678\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-68678\nhttps://opus4.kobv.de/opus4-fau/files/6867/SrinvasBoppuDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6885\n2018-07-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:518\nddc:535\npacs\npacs:02.60.Lj\npacs:02.60.Nm\npacs:02.70.Jn\npacs:03.50.De\npacs:03.65.Nk\npacs:07.60.Ly\npacs:11.55.-m\npacs:41.20.-q\npacs:42.15.Dp\npacs:42.25.Fx\npacs:42.40.Jv\npacs:42.40.Lx\npacs:42.79.Dj\nmsc\nmsc:34B05\nmsc:35F15\nmsc:45B05\nmsc:65L04\nmsc:65L10\nmsc:65N22\nmsc:78Mxx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nRigorous Fourier Methods Based on Numerical Integration for the Calculation of Diffractive Optical Systems\nIff, Wolfgang\nElektromagnetische Welle\nElektromagnetisches Feld\nMaxwellsche Theorie\nElektromagnetische Streuung / Berechnung\nBeugungstheorie\nDifferentialgleichung\nRandwertproblem\nFredholm-Integralgleichung\nFredholm-Integralgleichung / Art 2\nddc:518\nddc:535\nRigorous Fourier methods are methods for the rigorous calculation of the scattering of waves at gratings, which are based on Fourier expansions of the field and material distribution in the direction(s) of periodicity; they are of importance for the calculation of optical systems containing diffractive elements \u2013 e.g. interferometers. In the direction(s) of periodicity, the time-independent Maxwell equations are projected onto a Fourier basis. The remaining ordinary differential equation defines together with the boundary conditions at the homogeneous medium of incidence and transmission the boundary value problem, the subject of this thesis. Its solution is given by the scattering-matrix (S-matrix) to be calculated.\nThe fulfillment of the boundary conditions is possible in different ways: The Conventional Differential Method is based on the Shooting Method. Its disadvantage is numerical integration along exponentially increasing functions (anti-evanescent waves). As naturally stable pendant to this, the Direct S-matrix Integration has been developed in the context of this thesis: It integrates the S-matrix directly by a differential equation derived for this. Both methods have O(N\u00b3)-complexity, with N the number of harmonics.\nRecently, alternative rigorous Fourier methods of merely O(N\u00b7ln(N))-complexity based on integral equations are emerging. Improvable points here are the number of iterations for the solution of the integral equation, the memory consumption and a lack of flexibility. One remedy to this developed in the context of this thesis is the S-vector algorithm.\nThe presented methods are combined with polarization ray tracing as well as the angular spectrum of plane waves outside gratings. The specified examples show the need, applicability and merit of rigorous Fourier methods.\n2016\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6885\nurn:nbn:de:bvb:29-opus4-68857\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-68857\nhttps://opus4.kobv.de/opus4-fau/files/6885/WolfgangIffDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6889\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:509\nccs\nccs:A.\npacs\npacs:10.00.00\nmsc\nmsc:94-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nFault-Tolerant Self-Calibrating Pseudolite System\nFehlertolerantes selbstkalibrierendes Pseudolite-System\nPatino Studencki, Lucila\nSachbegriff\nddc:509\nIn this work a new and flexible pseudolite concept is investigated. The main objective of this work is to prove that the proposed system consisting of simple, inexpensive and miniaturized transmitters with free-running clocks is capable of achieving sub-decimeter accuracy through the use of software-based synchronization and a robust and fault tolerant automatic algorithm for locating the transmitters.\nIn dieser Arbeit wird ein neues und flexibles Pseudolitekonzept untersucht. Das Hauptziel der Arbeit ist zu beweisen, dass das vorgeschlagene System, das aus einfachen, kosteng\u00fcnstigen und miniaturisierten Sendern (mit freilaufenden Uhren) besteht, in der Lage ist sub-dezimeter Genauigkeit zu erreichen durch die Verwendung einer softwarebasierten Synchronisation und einer robusten und fehlertoleranten automatische Lokalisierung der Sender.\n2016\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6889\nurn:nbn:de:bvb:29-opus4-68898\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-68898\n978-3-8396-0962-0\nhttps://opus4.kobv.de/opus4-fau/files/6889/thesis_opus.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6900\n2018-07-11\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:535\nddc:537\nddc:621\nccs\nccs:B.7.3\nccs:B.8.m\npacs\npacs:41.20.-q\npacs:42.00.00\npacs:73.00.00\npacs:78.00.00\npacs:85.00.00\nmsc\nmsc:78-05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Elektrotechnik\nEntwicklung und Herstellung von neuartigen Sonden f\u00fcr die elektrische und optische Rastersondenmikroskopie\nDevelopment and fabrication of novel probes for electrical and optical scanning probe microscopy\nJambreck, Joachim Dietmar\nElektrische Messtechnik\nOptische Messtechnik\nRastersondenmikroskop\nRastersondenmikroskopie\nSensor\nSonde\nMikroelektronik\nHalbleiter\nddc:535\nddc:537\nddc:621\nDie Bedeutung der Nanowissenschaft und deren Umsetzung in der Nanotechnologie, welche ihren Ursprung im Bereich der Mikroelektronik hat, nimmt laufend zu und f\u00fchrt so in vielen Bereichen von Wissenschaft und Technik zu wichtigen neuen Erkenntnissen und Entwicklungen. Die analytischen Verfahren mit Aufl\u00f6sungen im Nanometer-Bereich sind wesentlicher Bestandteil dieses Erfolgs. Einen wichtigen Anteil daran haben die Methoden der Rastersondenmikroskopie (SPM, englisch Scanning Probe Microscopy). Diese bieten eine hohe r\u00e4umliche Aufl\u00f6sung und zus\u00e4tzlich die M\u00f6glichkeit, elektrische, optische und andere physikalische, chemische oder biologische Eigenschaften von Proben zu analysieren. Alle Methoden der SPM nutzen als wesentliche Elemente zur Analyse der Proben spezielle Sonden mit feinen Spitzen. Im Rahmen dieser Arbeit wurden Sonden f\u00fcr die wichtigen Methoden der elektrischen und der optischen SPM untersucht. Dabei wurden insbesondere neuartige Sonden entwickelt und hergestellt. Als Erstes wurden Sonden f\u00fcr die elektrische SPM untersucht, deren Spitzen einheitlich aus elektrisch leitf\u00e4higem Kompositmaterial bestehen, eine optimierte Form aufweisen sowie auf relativ g\u00fcnstige Weise hergestellt werden k\u00f6nnen. Es wurde ein neuartiger Ansatz zu ihrer Herstellung entwickelt, der auf der Fertigung von Spitzen mittels UV-Nanoimprintlithographie beruht. Dazu wurden spezielle Pr\u00e4geformen durch Bearbeitung mit dem fokussierten Ionenstrahl hergestellt, dann Spitzen gefertigt und einer Nachbehandlung unterzogen. Weiterhin wurden ausgehend von den Spitzen Demonstratorsonden hergestellt, charakterisiert und f\u00fcr elektrische SPM-Messungen angewendet, wobei sich gegen\u00fcber speziellen konventionellen Sonden ein vergleichsweise geringer elektrischer Widerstand sowie eine Verbesserung der erreichbaren Aufl\u00f6sung zeigte. Als Zweites wurden Verbesserungen im Bereich der elektrischen SPM hinsichtlich einer Reduktion der parasit\u00e4ren Kapazit\u00e4ten und der Transienteneffekte sowie hinsichtlich der Vergleichbarkeit der Messungen an verschiedenen Positionen auf der Probe untersucht. Dazu wurden elektrische Modelle gebildet und davon ausgehend Kapazit\u00e4ten berechnet. Weiterhin wurden neue geschirmte Sonden, spezielle Halter und eine Fertigungsvorrichtung zur Kontaktierung entwickelt und hergestellt. Die Sonden wurden mikroskopisch und elektrisch charakterisiert sowie f\u00fcr Messungen der lokalen Kapazit\u00e4t und lokaler Strom-Spannungs-Kennlinien angewendet. Dabei wurde eine Reduktion der parasit\u00e4ren Kapazit\u00e4t und des Verschiebungsstroms um bis zu 80 % erreicht. Als Drittes wurden neuartige Sonden im Bereich der optischen SPM untersucht. Zum ersten Mal wurden Sonden mit speziellen Bragg-Strukturen entwickelt, die eine Adaption der zur Untersuchung genutzten elektromagnetischen Felder erlauben. Die Sonden wurden entworfen, hergestellt und anschlie\u00dfend mikroskopisch inspiziert sowie optisch r\u00e4umlich und spektral aufgel\u00f6st analysiert. Dabei zeigte sich im Vergleich zu routinem\u00e4\u00dfig verwendeten Sonden eine bessere Lokalisierung der elektromagnetischen Felder und eine Verbesserung des Kontrastes bis zu einem Faktor 3,3.\nThe role of nanoscience and its implementation in nanotechnology, which originally has its roots in microelectronics, is constantly growing and, therefore, leads to new discoveries and evolution in diverse areas of science and technology. Essential parts of this success are analytical techniques with resolutions in the nanometer scale. Great share in this can be attributed to scanning probe microscopy (SPM) techniques. They combine high spatial resolution with the additional possibility of measuring electrical, optical, other physical, chemical or biological properties of the samples. All SPM methods use special probes with sharp tips as essential components to analyze the samples. Within the scope of this work probes for important electrical and optical SPM techniques were investigated. In doing so, particularly novel probes could be developed and fabricated. Firstly, probes for electrical SPM including tips consisting uniformly of one composite material, having an optimized shape, and also offering the possibility of relatively cheap production were investigated. For this, a new approach for the fabrication of probes for electrical SPM was developed, based on the manufacturing of tips using UV-nanoimprint lithography. Therefore, special molds were fabricated by focused ion beam structuring. Then tips were fabricated and special post-treatment was applied to the tips. After that, demonstrator probes containing the tips were fabricated, characterized, and applied for electrical SPM measurements, which showed relatively low electrical resistance and improvements of the imaging resolution compared to special conventional probes. Secondly, improvements in the field of electrical SPM measurements regarding reduction of parasitic capacitances and transient effects as well as regarding comparability of measurements taken at different positions were investigated. For that appropriate modeling was performed and capacitances were calculated. Furthermore, new shielded probes, as well as special holders, and a manufacturing apparatus for attaching electrical contacts were developed and fabricated. The probes were characterized microscopically and electrically. Additionally they were used for local capacitance measurements, as well as for local measurements of current-voltage-characteristics. Thereby, a clear reduction of the parasitic capacitance and of the displacement current of up to 80 % was achieved. Thirdly, novel probes in the field of optical SPM were investigated. For the first time, probes including special Bragg-structures were developed, which allow advanced control of the electromagnetic fields used for investigation of the sample. The probes were designed, fabricated and after that they were analyzed microscopically and optically including raster scans of the probes and spectral measurements. The results showed a clear improvement in contrast up to a factor of 3.3 and a better localization of the electromagnetic fields compared to probes used routinely.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6900\nurn:nbn:de:bvb:29-opus4-69006\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-69006\nhttps://opus4.kobv.de/opus4-fau/files/6900/DissertationJoachimJambreck.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6997\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:520\nccs\nccs:I.\npacs\npacs:90.00.00\nmsc\nmsc:85-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nModel-independent search for neutrino sources with the ANTARES neutrino telescope\nGei\u00dfels\u00f6der, Stefan\nAstronomy\nddc:520\nThe origin of high energetic cosmic rays has been puzzling since their discovery.\nMany theories about the sources of these cosmic rays also predict a flux of high\nenergetic cosmic neutrinos. Recently, the existence of such a high energetic neutrino flux has been confirmed, but the location and nature of its sources remains\nunknown. The ANTARES neutrino telescope was built in the Mediterranean\nSea, 40 km off the French coast near Toulon in a depth of 2475 meters to help\nanswer this and other questions. It consists of a three dimensional array of 885\nphotomultiplier tubes that detect the Cherenkov light emitted by secondary\nparticles, which are produced in interactions between neutrinos and nuclei in\nthe water.\nThe identification and reconstruction of the observed neutrino events constitute challenging tasks. Parts of this thesis deal with algorithmic approaches\nto improve these tasks using pattern recognition. The first application is the\nsuppression of undesired background by a classification algorithm. The second\napproach is the selection of the best available direction reconstruction for each\nneutrino.\nThe main focus of this thesis lies on a new method to evaluate the spatial\ndistribution of the observed neutrinos. While most approaches test one specific\nhypothesis for a specific source, derived from theory or other measurements,\nthis search refrains from optimizing for individual source hypotheses and tries\nto detect the most pronounced density fluctuation in the spatial distribution,\nregardless of its specific position, size, shape or internal distribution as unbiased\nas possible. To achieve this, the statistical likelihood for the observed neutrino\ndensity is evaluated in multiple scales up to distances between events of 180\u00b0. To\nrecognize a potential cosmic neutrino signal, regions with the most pronounced\ndeviations are identified and compared to the expectations from a random background hypothesis. The strength of such a flexible, model-independent search\nis not the sensitivity for a specific source hypothesis, but instead to detect also\nunexpected hypotheses that can then be analyzed in more detail.\nIn the data recorded from 2007 to 2012 this search found a very large structure close to the direction of the center of our galaxy with a post-trial significance of 2.52\u03c3. It can therefore be explained best by a statistical fluctuation.\nAs a simple crosscheck this method has been applied to a publicly available\ndata sample recorded independently by the neutrino telescope IceCube. This\nevaluation also resulted in an overfluctuation at the location where the most\nsignificant structure from ANTARES data overlaps with the field of view of\nIceCube. With the devised analysis method the found structure in the IC40\ndata has a significance of 2.14\u03c3.\nWhile this is intriguing, ultimately, a dedicated follow-up analysis that is\noptimized for the derived hypothesis is necessary to find unambiguous evidence\nfor its true nature.\nSince, despite further studies, no unambiguous explanation could be found\nfor the obtained results, a follow-up analysis is recommended, that can be\nadapted specifically to the results and therefore has a higher chance to provide\nunambiguous insights.\nNevertheless this result constitutes the most significant spatially resolved\nhypothesis for the sources of high energetic astrophysical neutrinos so far.\n2016\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6997\nurn:nbn:de:bvb:29-opus4-69975\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-69975\nhttps://opus4.kobv.de/opus4-fau/files/6997/dissertation_geisselsoeder.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\n169836139100000", "language": null, "image": null, "pagetype": null, "links": []}