{"title": "PDF", "author": "PDF", "url": "https://www.tcs.uni-luebeck.de/de/mitarbeiter/tantau/lehre/lectures/Algorithmendesign-2016.pdf", "hostname": "PDF", "description": "PDF", "sitename": "PDF", "date": "PDF", "id": "PDF", "license": "PDF", "body": "PDF", "comments": "PDF", "commentsbody": "PDF", "raw_text": "PDF", "text": "Vorlesungsskript\nAlgorithmendesign\nCS3000, Wintersemester 2016/2017\nFassung vom 18. November 2016\nTill Tantau\nProblemstellungen\nProblemstellungenEntwurfsmethoden\nEntwurfsmethodenAlgorithmen\nAlgorithmenDatenstrukturen\nDatenstrukturenAnalysemethoden\nAnalysemethodenKlassifikationen\nKlassifikationenA L G O R I T H M E N D E S I G N\nProblemstellungen\nProblemstellungenArten der Eingabe\nDatentypen\nZahlen\nArrays\nMatrizen\nGeometrische Objekte\nGraphen\nDatenbanken\nWorte / Texte\nTemporale Eigenschaften\nOnline\nOffline\nOrdnungseigenschaften\nLinear geordnete Daten\nHashbare Daten\nGeometrisch geordnete Daten\nKonkrete Problemstellungen\nArithmetische Probleme\nZahlmultiplikation\nMatrixmultiplikation\nSubset-Sum-Problem\nPartition-Problem\nM\u00fcnzr\u00fcckgabe-Problem\nRucksack-Problem\nScheduling\nOhne / mit Deadlines\nOhne / mit Unterbrechungen\nEine / mehrere Maschinen\nCaches\nSuchen in Daten\nKompression\nGraphprobleme\nZusammenhang\nMaximaler Fluss\nMatching\nHamilton- und Eulerkreise\nVertex-Cover\nF\u00e4rbbarkeit\nIndependent-Set und Clique\nArten der Problemstellungen\nZiel der Berechnung\nEntscheidungsprobleme\nFunktionsprobleme\nOptimierungsprobleme\nKontinuit\u00e4t\nDiskrete Problemstellungen\nNumerische Problemstellungen\nKodierungen der Eingabe\nZahlen\nUn\u00e4r\nBin\u00e4r\nGleitkomma\nGraphen\nAdjazenzlisten\nAdjazenzmatrizen\nKomprimierte Eingaben\nRedundanz in EingabenEntwurfsmethoden\nEntwurfsmethodenReduktion auf bekannte Probleme\nReduktion auf kombinatorische Probleme\nReduktion auf numerische Probleme\nDarstellung als Lineares Programm\nDarstellung als lineares Ganzzahlproblem\nRandomisierung\nPseudo-Zufallszahlen\nZufallszahlen\nHashing\nRekursive Ans\u00e4tze\nTeile-und-Herrsche\nBacktracking\nBranch-and-Bound\nIterative Ans\u00e4tze\nGreedy-Verfahren\nDynamische Tabellen\nZunehmende Wege\nKompressionsmethoden\nEntropiebasierte Kodierung\nW\u00f6rtbuchtechniken\nTransformationen\nAns\u00e4tze f\u00fcr Online-Probleme\nH\u00e4ufigkeitsbasierte Strategien\nDauerbasierte Strategien\nHeuristische Ans\u00e4tze\nLokale Suche\nEvolution\u00e4re Algorithmen\nSimulated-Annealing\nApproximative Verfahren\nFixed-Parameter-Verfahren\nParameterwahl\nSuchbaum-Verfahren\nVorverarbeitung\nKernelisierung\nIndizierungAlgorithmen\nAlgorithmenHashing-Algorithmen\nLineares Hashing\nKuckucks-Hashing\nArithmetische Algorithmen\nKaratsuba-Algorithmus\nStrassen-Algorithmus\nGau\u00df-Algorithmus f\u00fcr Determinanten\nAlgorithmen auf Graphen\nBrute-Force\nFlussalgorithmen\nFord-Fulkerson\nMatching und das Heiratsproblem\nApproximationsalgorithmen\nf\u00fcr Vertex-Cover\nf\u00fcr das Rucksack-Problem\nFixed-Parameter-Algorithmen\nf\u00fcr Vertex-Cover\nTextverarbeitung\nKompressionsverfahren\nHuffman\nLempel-Ziv\nBurrows-Wheeler-Transformation\nDC/three.oldstyle-Algorithmus\nGeometrische Algorithmen\nk-Means\nk-Server-Algorithmen\nScheduling-Algorithmen\nOffline-Algorithmen\nEarliest-Finish-First\nStart-Time-Ordering\nEarliest-Deadline-First\nOnline Caching-AlgorithmenDatenstrukturen\nDatenstrukturenIndex-Strukturen\nTries\nParticia-B\u00e4ume\nSuffix-Indizes\nSuffix-Trie\nSuffix-Tree\nSuffix-Array\nMap-Strukturen\nBalanzierte B\u00e4ume\nAVL-B\u00e4ume\n/two.oldstyle-/three.oldstyle-B\u00e4ume\nRot-Schwarz-B\u00e4ume\nGeometrische B\u00e4ume\nQuad- und Oct-B\u00e4ume\nMehrdimensionale Suchb\u00e4ume\nHash-Tabellen\nOffenes Hashing\nGeschlossenes Hashing\nPriorit\u00e4ts-Warteschlangen\nEinfache Heaps\nFibbonacci-Heaps\nUnion-Find-Strukturen\nDynamische TabellenAnalysemethoden\nAnalysemethodenKorrektheit\nHoare-Kalk\u00fcl\nRekursionsgleichungen\nMaster-Theorem\nUntere Schranken\nH\u00e4rte mittels Reduktionen\nKonstruktion von Worst-Case-Eingaben\nAdversary-Argumente\nExchange-Argumente\nObere Schranken\nAbsch\u00e4tzung der ben\u00f6tigten Ressourcen\nLaufzeit\nSpeicherplatz\nTiefe\nAmortisierte Analyse\nPotentialmethode\nFehlerwahrscheinlichkeit\nEingabeverteilung\nWahrscheinlichkeitsverteilungen\nWorst-Case\nAverage-Case\nSmoothed-Case\nBest-Case\nBeschr\u00e4nkung von Eingabe-ParameternKlassifikationen\nKlassifikationenAsymptotik von Funktionen\nO-Klassen und o-Klassen\n\u0398-Klassen\n\u2126-Klassen und \u03c9-Klassen\nIgnorierte Faktoren\nKonstante Faktoren\nLogarithmische Faktoren\nPolynomielle Faktoren\nKlassifikation von Datenstrukturen\nPlatzverbrauch von Datenstrukturen\nO(n)\nO(n/two.oldstyle)\nO(/two.oldstylen)\nTiefe von Datenstrukturen\nO(logn)\nO(n)\nKlassifikation der Laufzeit\nTypische Zeitschranken\nO(/one.oldstyle)\nO(logn)\nO(n)\nO(n\u03b1(n))\nO(nlogn)\nO(n/two.oldstyle\u0000\u03b4)\nO(n/two.oldstyle)\nO(n/two.oldstyle+\u03b4)\nO(n/three.oldstyle)\nO(nc)\nO(f(k)nc)\nO(/two.oldstylen)\nKleine Zeitklassen\nP\nP-vollst\u00e4ndig\nZPP\nRP\ncoRP\nBPP\nFP (functional P)\nFPT (fixed parameter tractable)\nGro\u00dfe Zeitklassen\nNP\nNP-vollst\u00e4ndig\nEXP\nEXP-vollst\u00e4ndig\nKlassifikation des Platzverbrauchs\nKleine Platzklassen\nL\nNL\nNL-vollst\u00e4ndig\nFL\nGro\u00dfe Platzklassen\nPSPACE\nPSPACE-vollst\u00e4ndig\nRaten\nKompetitive Rate\nApproximationsrate\nKompressionsrateVeranstaltungskarte\nCS/three.oldstyle/zero.oldstyle/zero.oldstyle/zero.oldstyle AlgorithmendesignVeranstaltungsziele\n\u2013Vertrautheit mit algorithmischen Entwurfsprinzipien\n\u2013neue komplexe Algorithmen durch Anwendung dieser Prinzipien entwickeln k\u00f6nnen\n\u2013Erfahrung beim algorithmischen Probleml\u00f6sen\n/one.oldstyle/seven.oldstyle\n/one.oldstyle/eight.oldstyle\n/one.oldstyle/nine.oldstyle\n/two.oldstyle/zero.oldstyle\n/two.oldstyle/one.oldstyle\n/two.oldstyle/two.oldstyle\n/two.oldstyle/three.oldstyle\n/two.oldstyle/four.oldstyle\n/two.oldstyle/five.oldstyle\n/two.oldstyle/six.oldstyle\n/two.oldstyle/seven.oldstyle\n/two.oldstyle/eight.oldstyle\n/two.oldstyle/nine.oldstyle\n/three.oldstyle/zero.oldstyle\n/three.oldstyle/one.oldstyle\nNovember /two.oldstyle/zero.oldstyle/one.oldstyle/six.oldstyle\n/one.oldstyle\n/two.oldstyle\n/three.oldstyle\n/four.oldstyle\n/five.oldstyle\n/six.oldstyle\n/seven.oldstyle\n/eight.oldstyle\n/nine.oldstyle\n/one.oldstyle/zero.oldstyle\n/one.oldstyle/one.oldstyle\n/one.oldstyle/two.oldstyle\n/one.oldstyle/three.oldstyle\n/one.oldstyle/four.oldstyle\n/one.oldstyle/five.oldstyle\n/one.oldstyle/six.oldstyle\n/one.oldstyle/seven.oldstyle\n/one.oldstyle/eight.oldstyle\n/one.oldstyle/nine.oldstyle\n/two.oldstyle/zero.oldstyle\n/two.oldstyle/one.oldstyle\n/two.oldstyle/two.oldstyle\n/two.oldstyle/three.oldstyle\n/two.oldstyle/four.oldstyle\n/two.oldstyle/five.oldstyle\n/two.oldstyle/six.oldstyle\n/two.oldstyle/seven.oldstyle\n/two.oldstyle/eight.oldstyle\n/two.oldstyle/nine.oldstyle\n/three.oldstyle/zero.oldstyle\nDezember /two.oldstyle/zero.oldstyle/one.oldstyle/six.oldstyle\n/one.oldstyle\n/two.oldstyle\n/three.oldstyle\n/four.oldstyle\n/five.oldstyle\n/six.oldstyle\n/seven.oldstyle\n/eight.oldstyle\n/nine.oldstyle\n/one.oldstyle/zero.oldstyle\n/one.oldstyle/one.oldstyle\n/one.oldstyle/two.oldstyle\n/one.oldstyle/three.oldstyle\n/one.oldstyle/four.oldstyle\n/one.oldstyle/five.oldstyle\n/one.oldstyle/six.oldstyle\n/one.oldstyle/seven.oldstyle\n/one.oldstyle/eight.oldstyle\n/one.oldstyle/nine.oldstyle\n/two.oldstyle/zero.oldstyle\n/two.oldstyle/one.oldstyle\n/two.oldstyle/two.oldstyle\n/two.oldstyle/three.oldstyle\n/two.oldstyle/four.oldstyle\n/two.oldstyle/five.oldstyle\n/two.oldstyle/six.oldstyle\n/two.oldstyle/seven.oldstyle\n/two.oldstyle/eight.oldstyle\n/two.oldstyle/nine.oldstyle\n/three.oldstyle/zero.oldstyle\n/three.oldstyle/one.oldstyle\nJanuar /two.oldstyle/zero.oldstyle/one.oldstyle/seven.oldstyle\n/one.oldstyle\n/two.oldstyle\n/three.oldstyle\n/four.oldstyle\n/five.oldstyle\n/six.oldstyle\n/seven.oldstyle\n/eight.oldstyle\n/nine.oldstyle\n/one.oldstyle/zero.oldstyle\n/one.oldstyle/one.oldstyle\n/one.oldstyle/two.oldstyle\n/one.oldstyle/three.oldstyle\n/one.oldstyle/four.oldstyle\n/one.oldstyle/five.oldstyle\n/one.oldstyle/six.oldstyle\n/one.oldstyle/seven.oldstyle\n/one.oldstyle/eight.oldstyle\n/one.oldstyle/nine.oldstyle\n/two.oldstyle/zero.oldstyle\n/two.oldstyle/one.oldstyle\n/two.oldstyle/two.oldstyle\n/two.oldstyle/three.oldstyle\n/two.oldstyle/four.oldstyle\n/two.oldstyle/five.oldstyle\n/two.oldstyle/six.oldstyle\n/two.oldstyle/seven.oldstyle\n/two.oldstyle/eight.oldstyle\n/two.oldstyle/nine.oldstyle\n/three.oldstyle/zero.oldstyle\n/three.oldstyle/one.oldstyle\nFebruar /two.oldstyle/zero.oldstyle/one.oldstyle/seven.oldstyle\n/one.oldstyle\n/two.oldstyle\n/three.oldstyle\n/four.oldstyle\n/five.oldstyle\n/six.oldstyle\n/seven.oldstyle\n/eight.oldstyle\n/nine.oldstyle\n/one.oldstyle/zero.oldstyle\n/one.oldstyle/one.oldstyle\n/one.oldstyle/two.oldstyle\n/one.oldstyle/three.oldstyleEinf\u00fchrung, OrganisatorischesOffline-Probleme /one.oldstyle: Zahlen Offline-Probleme /two.oldstyle: Texte Online-Probleme Randomisierung Schwere Probleme und ihre L\u00f6sungEntwurfsmethode: Teile-und-Herrsche /one.oldstyle Entwurfsmethode: Teile-und-Herrsche\n\u2013Die Schritte des Algorithmendesigns (Problem,\nEntwurf, Algorithmus und Datenstruktur, Analyse,\nKlassifikation) anhand von Zahl-Problemen\nnachvollziehen\n\u2013Den Karatsuba- und den Strassen-Algorithmus kennen\n\u2013Das Master-Theorem kennen und anwenden k\u00f6nnen\nEntwurfsmethode: Dynamische Tabellen /two.oldstyle Entwurfsmethode: Dynamische Tabellen\n\u2013Beispiele von Problemstellungen kennen, die sich mit\ndynamischen Tabellen l\u00f6sen lassen\n\u2013Konzept der dynamischen Tabelle in eigenen\nAnwendungen einsetzen k\u00f6nnen\n\u2013Auf dynamischen Tabellen aufbauende Algorithmen\nanalysieren k\u00f6nnen\nA&D: Indizieren mit B\u00e4umen /three.oldstyle A&D: Indizieren mit B\u00e4umen\n\u2013Die Datenstruktur des Tries kennen und\nimplementieren k\u00f6nnen\n\u2013Die Datenstruktur des Suffix-Trees kennen\n\u2013Methoden zur Suche in Texten kennen, die auf diesen\nDatenstrukturen aufbauen\nA&D: Indizieren mit Arrays /four.oldstyle A&D: Indizieren mit Arrays\n\u2013Die Datenstruktur des Suffix-Arrays kennen\n\u2013Den dc/three.oldstyle-Algorithmus verstehen\nA&D: Textkompression /five.oldstyle A&D: Textkompression\n\u2013Zeichenbasierte Kompressionsalgorithmen kennen\n\u2013Burrows-Wheeler-basierte Kompressionsalgorithmen\nkennen\nKlassifikation: Kompetitive Rate /six.oldstyle Klassifikation: Kompetitive Rate\n\u2013Varianten von Scheduling- und Caching-Problemen\nkennen\n\u2013Greedy-Verfahren zu deren L\u00f6sung kennen\n\u2013Konzept des Online-Algorithmus verstehen\n\u2013Konzept der kompetitiven Rate verstehen\n\u2013Kompetitive Raten von Algorithmen bestimmen\nk\u00f6nnen\nAnalysemethode: Amortisierung /seven.oldstyle Analysemethode: Amortisierung\n\u2013Konzept der amortisierten Analyse verstehen\n\u2013Die Potential-Methode einsetzen k\u00f6nnen\nA&D: Union-Find /eight.oldstyle A&D: Union-Find\n\u2013Den optimalen Algorithmus zur Verwaltung\ndisjunkter Mengen erkl\u00e4ren k\u00f6nnen.\n\u2013Die amortisierte Analyse des Algorithmus verstehen.\nA&D: Suchen in hashbaren Daten /nine.oldstyle A&D: Suchen in hashbaren Daten\n\u2013Ideen und Methodik des Hashings auffrischen\n\u2013Hash-Tabellen mit ver\u00e4nderlicher Gr\u00f6\u00dfe analysieren\nk\u00f6nnen\nA&D: Perfektes Hashing /one.oldstyle/zero.oldstyle A&D: Perfektes Hashing\n\u2013Idee des perfekten Hashings und einfache perfekte\nHash-Verfahren kennen\n\u2013Kuckucks-Hash-Tabellen verstehen und\nimplementieren k\u00f6nnen\nEntwurfsmethoden: Zufall /one.oldstyle/one.oldstyle Entwurfsmethoden: Zufall\n\u2013Die Konzepte \u00bbzuf\u00e4llige Eingabe\u00ab, \u00bbzuf\u00e4llige\nAusgabe\u00ab und \u00bbzuf\u00e4llige Entscheidung\u00ab\nunterscheiden k\u00f6nnen\n\u2013Einsch\u00e4tzen k\u00f6nnen, wie verl\u00e4sslich\nZufallsalgorithmen sind\n\u2013Wichtige Klassen von Zufallsalgorithmen erl\u00e4utern\nk\u00f6nnen\n\u2013Einfache Beispiele von Zufallsalgorithmen\nimplementieren k\u00f6nnen\nEntwurfsmethode: Approximation /one.oldstyle/two.oldstyle Entwurfsmethode: Approximation\n\u2013Die Entwurfsmethode \u00bbApproximation\u00ab kennen und\nApproximationsalgorithmen entwerfen k\u00f6nnen\n\u2013Wichtige Approximationsalgorithmen f\u00fcr mehrere\nschwere Probleme kennen\nEntwurfsmethode: Fixed-Parameter /one.oldstyle/three.oldstyle Entwurfsmethode: Fixed-Parameter\n\u2013Konzept des Fixed-Parameter-Algorithmus verstehen\n\u2013Fixed-Parameter-Algorithmus f\u00fcr das\nVertex-Cover-Problem kennen\n\u2013Konzept der Kernelisierung verstehen\n\u2013Kernelisierungs-Algorithmus f\u00fcr das\nVertex-Cover-Problem kennenT I L L T A N T A U\nW W W . T C S . U N I - L U E B E C K . D E\n/one.oldstyle /one.oldstyle . O K T O B E R /two.oldstyle /zero.oldstyle /one.oldstyle /six.oldstyle\n/one.oldstyleInhaltsverzeichnisiii\nInhaltsverzeichnis\nVorwort . . . . . . . . . . . . . . . . . . . . . . . 1\nTeil I\nOffline-Probleme 1: Zahlen\n1 Entwurfsmethode:\nTeile-und-Herrsche\n1.1 Was ist Algorithmendesign? 5\n1.2 Wiederholung: O-Klassen, W-Klassen und\nQ-Klassen 6\n1.3 Die Schritte des Algorithmendesigns 8\n1.3.1 Die Probleme: Multiplikation . . . . . . 8\n1.3.2 Die Entwurfsmethode:\nTeilen-und-Herrschen . . . . . . . . . . 8\n1.3.3 Die Algorithmen: Karatsuba und Strassen 9\n1.3.4 Die Analysemethode: Das\nMaster-Theorem . . . . . . . . . . . . . 11\n1.3.5 Die Klassi\ufb01kation . . . . . . . . . . . . 13\n\u00dcbungen zu diesem Kapitel 15\n2 Entwurfsmethode:\nDynamische Tabellen\n2.1 Der Weg zur \u00bbdynamischen Tabelle\u00ab 17\n2.1.1 Von der Rekursion... . . . . . . . . . . 17\n2.1.2 ...\u00fcber Memoization... . . . . . . . . . 18\n2.1.3 ...zur Iteration . . . . . . . . . . . . . . 19\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme 20\n2.2.1 Das Subset-Sum-Problem . . . . . . . . 20\n2.2.2 Das Rucksack-Problem . . . . . . . . . 22\u00dcbungen zu diesem Kapitel 25\nTeil II\nOffline-Probleme 2: Texte\n3 A&D: Indizieren mit B\u00e4umen\n3.1 Einf\u00fchrung 28\n3.1.1 Fallbeispiel I: Bibliometrie . . . . . . . 28\n3.1.2 Fallbeispiel II: Virusdatenbanken . . . . 28\n3.2 Tries 29\n3.2.1 Die Idee . . . . . . . . . . . . . . . . . 29\n3.2.2 Grundoperationen . . . . . . . . . . . . 29\n3.2.3 Pfad-Kompression: Patricia-B\u00e4ume . . . 31\n3.2.4 Implementation . . . . . . . . . . . . . 32\n3.3 Suffix-Tries und -Trees 33\n3.3.1 Su\ufb03x-Tries . . . . . . . . . . . . . . . . 33\n3.3.2 Su\ufb03x-Trees . . . . . . . . . . . . . . . 34\n\u00dcbungen zu diesem Kapitel 35\n4 A&D: Indizieren mit Arrays\n4.1 Suffix-Arrays 37\n4.1.1 Su\ufb03x-B\u00e4ume sind gut... . . . . . . . . 37\n4.1.2 ...aber nicht gut genug . . . . . . . . . 37\n4.1.3 Die Idee . . . . . . . . . . . . . . . . . 37\n4.1.4 Vergleich von Su\ufb03x-Arrays und\nSu\ufb03x-B\u00e4umen . . . . . . . . . . . . . . 38ivInhaltsverzeichnis\n4.2 Der DC3-Algorithmus 38\n4.2.1 Das Ziel: ein Linearzeitalgorithmus . . . 38\n4.2.2 Der Algorithmus im \u00dcberblick . . . . . 39\n4.2.3 Schritt 1: Sortierung der taktlosen Su\ufb03xe 40\n4.2.4 Schritt 2: Sortierung der taktvollen Su\ufb03xe 41\n4.2.5 Schritt 3: Verschmelzung . . . . . . . . 43\n5 A&D: Textkompression\n5.1 Einf\u00fchrung 47\n5.2 Zeichenweise Kompression 48\n5.2.1 Die Idee . . . . . . . . . . . . . . . . . 48\n5.2.2 Hu\ufb00man-Kodierung . . . . . . . . . . . 48\n5.2.3 Move-To-Front-Kodierung . . . . . . . . 49\n5.3 Kompression von Zeichenwiederholungen 50\n5.3.1 Die Idee . . . . . . . . . . . . . . . . . 50\n5.3.2 Run-Length-Kompression . . . . . . . . 50\n5.4 Suffixbasierte Kompression 51\n5.4.1 Die Idee . . . . . . . . . . . . . . . . . 51\n5.4.2 Burrows-Wheeler-Transformation . . . . 51\n5.4.3 Seward-Kompression . . . . . . . . . . 52\n5.5 *W\u00f6rterbuchbasierte Kompression 53\n5.5.1 Die Idee . . . . . . . . . . . . . . . . . 53\n5.5.2 Lempel-Ziv-Welch-Kompression . . . . 53\n\u00dcbungen zu diesem Kapitel 55\nTeil III\nOnline-Probleme\n6 Klassifikation: Kompetitive Rate\n6.1 Einf\u00fchrung zu Online-Algorithmen 58\n6.1.1 Das Ski-Leihen-Problem . . . . . . . . . 58\n6.1.2 Klassi\ufb01kation: Kompetitive Rate . . . . 59\n6.2 Caching 60\n6.2.1 O\ufb04ine-Strategien . . . . . . . . . . . . 61\n6.2.2 Online-Strategien . . . . . . . . . . . . 63\n6.2.3 Kompetitive Rate . . . . . . . . . . . . 64\n6.3 Scheduling 64\n6.3.1 O\ufb04ine-Strategien . . . . . . . . . . . . 65\n6.3.2 Online-Strategien . . . . . . . . . . . . 65\n6.3.3 Kompetitive Rate . . . . . . . . . . . . 66\u00dcbungen zu diesem Kapitel 68\n7 Analysemethode:\nAmortisierung\n7.1 Einf\u00fchrung zur Amortisierung 70\n7.1.1 Fallbeispiel I: Stacks . . . . . . . . . . . 70\n7.1.2 Fallbeispiel II: Z\u00e4hler . . . . . . . . . . 70\n7.1.3 Worst-Case-Kosten . . . . . . . . . . . 71\n7.1.4 Amortisierte Kosten I . . . . . . . . . . 72\n7.2 Die Potential-Methode 73\n7.2.1 Die Idee der R\u00fcckstellung . . . . . . . . 73\n7.2.2 Amortisierte Kosten II . . . . . . . . . . 74\n7.3 Fallbeispiel III: Das Listen-Zugriffsproblem 75\n7.3.1 Online-Algorithmen . . . . . . . . . . . 76\n7.3.2 Kompetitive Rate . . . . . . . . . . . . 76\n\u00dcbungen zu diesem Kapitel 78\n8 A&D: Union-Find\n8.1 Die Verwaltung disjunkter Menge 81\n8.1.1 Die Problemstellung . . . . . . . . . . . 81\n8.1.2 Die Datenstruktur . . . . . . . . . . . . 82\n8.2 Analyse 85\n8.2.1 Begri\ufb00e: Eimer und M\u00fcnzen . . . . . . 85\n8.2.2 Regeln: Ein- und Auszahlung . . . . . . 85\n8.2.3 Absch\u00e4tzung der amortisierten Kosten . 87\n8.2.4 Absch\u00e4tzung des maximalen Levels . . . 88\nTeil IV\nZufall als Entwurfsmethode\n9 A&D: Suchen in hashbaren\nDaten\n9.1 Grundlagen zu Hash-Tabellen 93\n9.1.1 Die Idee . . . . . . . . . . . . . . . . . 94\n9.1.2 Verkettung . . . . . . . . . . . . . . . . 95\n9.1.3 Lineares Sondieren . . . . . . . . . . . . 95\n9.1.4 Hash-Funktionen . . . . . . . . . . . . . 96\n9.2 Dynamische Gr\u00f6\u00dfenanpassung 96\n9.2.1 Die Verdoppelung-Halbierungs-Strategie 96\n9.2.2 Amortisierte Analyse . . . . . . . . . . 97Inhaltsverzeichnisv\n\u00dcbungen zu diesem Kapitel 99\n10 A&D: Perfektes Hashing\n10.1 Perfektes Hashing: Die Anforderung 101\n10.2 Statische perfekte Hash-Tabellen 101\n10.2.1 Geburtstagstabellen . . . . . . . . . . . 101\n10.2.2 Analyse der statischen perfekten\nHash-Tabellen . . . . . . . . . . . . . . 103\n10.3 Kuckucks-Hashing 104\n10.3.1 Die Idee . . . . . . . . . . . . . . . . . 104\n10.3.2 Die Implementation . . . . . . . . . . . 104\n10.3.3 Analyse der dynamischen perfekten\nHash-Tabellen . . . . . . . . . . . . . . 107\n\u00dcbungen zu diesem Kapitel 111\n11 Entwurfsmethoden: Zufall\n11.1 Arten des Zufalls 113\n11.1.1 Zuf\u00e4llige Eingaben . . . . . . . . . . . . 113\n11.1.2 Zuf\u00e4llige Ausgaben . . . . . . . . . . . 114\n11.1.3 Zuf\u00e4llige Entscheidungen . . . . . . . . 115\n11.2 Zufallsalgorithmen: Beispiele 115\n11.2.1 Term\u00e4quivalenz pr\u00fcfen . . . . . . . . . . 115\n11.2.2 Das\nSchwarz-Zippel-DeMillo-Lipton-Lemma 117\n11.2.3 Perfekte Matchings pr\u00fcfen . . . . . . . . 118\n11.3 Zufallsalgorithmen: Arten 120\n11.4 Zuf\u00e4llig und trotzdem zuverl\u00e4ssig? 120\n11.4.1 Wahrscheinlichkeitsverst\u00e4rkung . . . . . 120\n11.4.2 Sehr unwahrscheinlich = nie . . . . . . . 121\n\u00dcbungen zu diesem Kapitel 122\nTeil V\nEntwurfsmethoden f\u00fcr schwere\nProbleme12 Entwurfsmethode:\nApproximation\n12.1 Optimierungsprobleme 125\n12.1.1 Das Konzept . . . . . . . . . . . . . . . 125\n12.1.2 Ma\u00df und G\u00fcte . . . . . . . . . . . . . . 125\n12.2 Approximationsalgorithmen 126\n12.2.1 Das Konzept . . . . . . . . . . . . . . . 126\n12.2.2 Handelsreisender in der Ebene . . . . . . 127\n12.2.3 Bin-Packing . . . . . . . . . . . . . . . 130\n12.2.4 Vertex-Cover . . . . . . . . . . . . . . . 131\n12.3 *Approximationsschemata 132\n12.3.1 Das Konzept . . . . . . . . . . . . . . . 132\n12.3.2 Rucksack-Problem . . . . . . . . . . . . 132\n\u00dcbungen zu diesem Kapitel 135\n13 Entwurfsmethode:\nFixed-Parameter\n13.1 NP-Vollst\u00e4ndig hei\u00dft nicht \u00bbunl\u00f6sbar\u00ab 138\n13.1.1 Sind \u00bbschwere\u00ab Probleme \u00bbschwer\u00ab? . . 138\n13.1.2 Fallbeispiel: Vertex-Cover . . . . . . . . 139\n13.1.3 Ein ehrgeiziges Ziel . . . . . . . . . . . 139\n13.2 Der Fixed-Parameter-Ansatz 140\n13.2.1 Der Pakt mit dem Teufel . . . . . . . . . 140\n13.2.2 Eine brillante Idee . . . . . . . . . . . . 141\n13.2.3 Verfeinerungen der Idee . . . . . . . . . 142\n13.2.4 Das allgemeine Konzept . . . . . . . . . 144\n13.3 Kernelisierung 144viInhaltsverzeichnisVorwort1\nVorwort\nSie fangen als frisch gebackener Informatiker in einer Firma f\u00fcr Sensornetztechnik an und\nschon nach wenigen Tagen kommt Ihre jung-dynamische Che\ufb01n an und l\u00e4dt folgendes Pro-\nblem auf Ihrem Schreibtisch ab: \u00bbDer Kunde hat uns gebeten, noch Bewegungssensoren f\u00fcr\ndas Geb\u00e4ude zu einzuplanen, so dass jeder Quadratzentimeter Korridor von mindestens ei-\nnem Sensor erfasst wird. Ich schicke dir gleich noch die Datei mit den Geb\u00e4udepl\u00e4nen. Ach\nja, es sollen nat\u00fcrlich m\u00f6glichst wenige Sensoren verbaut werden, die Dinger kosten ein\nHeidengeld. Scha\ufb00st du das bis morgen?\u00ab Bevor Sie verneinen k\u00f6nnen, ist sie auch schon\nwieder weg.\nIn dieser Veranstaltung werden Sie lernen, wie Sie ein solches Problem angehen k\u00f6nnen.\nAlgorithmendesign dreht sich genau darum, ein unbekanntes Problem \u00bbanzugehen\u00ab, um am\nEnde einen Algorithmus (und zugeh\u00f6rige Datenstrukturen) zu dessen L\u00f6sung zu bekom-\nmen. Die verschiedenen Arten, wie man ein Problem \u00bbangehen\u00ab kann, nennt man vornehmer\nEntwurfsmethoden oder ganz genau Algorithmen-Entwurfsmethoden , um sie von den Me-\nthoden der Softwaretechnik zum Entwurf von gro\u00dfen Softwaresystemen etwas abzugrenzen.\nDas wohl bekanntestes Beispiel einer solchen Entwurfsmethode ist das Teile-und-Herrsche-\nPrinzip, das Sie schon kennen (sollten), andere Beispiele sind Reduktionstechniken, Appro-\nximationsmethoden, Heuristiken, Randomisierung oder auch Fixed-Parameter-Methoden.\n(Wenn Ihnen diese Begri\ufb00e jetzt noch nichts sagen, dann ist das gut so, sonst k\u00f6nnten Sie\nsich die Veranstaltung auch sparen.)\nMit dem Entwurf und sogar mit der Implementation eines neuen Algorithmus ist es aber\nnicht getan: Die Analyse von Algorithmen ist (fast) genauso wichtig wie deren Entwick-\nlung. Der Grund ist ganz einfach: Was n\u00fctzt mir der ra\ufb03nierteste Algorithmus, wenn er\nbei meinen Eingaben nicht zu meinen Lebzeiten fertig wird? Schlimmer noch: Selbst wenn\ner bei meinen Eingaben in der Praxis schnell ist, was passiert, wenn er bei den Eingaben\ndesKunden pl\u00f6tzlich versagt? Ein klassisches Beispiel ist das Programm Word von Micro-\nsoft: Es eignet sich hervorragend, einen Brief von zwei Seiten zu schreiben. Wer aber schon\neinmal versucht hat, eine Arbeit von mehreren hundert Seiten damit zu bearbeiten, wei\u00df,\ndass bei Microsoft o\ufb00enbar noch nie jemand so lange Texte geschrieben hat (oder Microsoft\nbenutzt selbst gar nicht Word, was einiges erkl\u00e4ren w\u00fcrde).\nDas zentrale Ziel der Analyse ist es, das Verhalten von Algorithmen bei beliebigen (oder zu-\nmindest bei sehr wahrscheinlichen) Eingaben vorherzusagen. Diesen Aspekt der Algorithmen-\nAnalyse werden wir in dieser Veranstaltung auch eingehend betrachten. Wieder kennen Sie\nbereits Verfahren aus fr\u00fcheren Veranstaltungen, beispielsweise die Ermittlung der O-Klasse\nder Laufzeit eines Algorithmus. Und wieder werden Sie neue Verfahren kennen lernen, mit\ndenen sich leichter oder besser Aussagen \u00fcber das Verhalten von Algorithmen und Daten-\nstrukturen machen lassen. Solche \u00bbAussagen \u00fcber das Verhalten\u00ab nennen wir dann Klassi-\n\ufb01kationen, das letzte Glied in der Kette Problem \u2013 Entwurf \u2013 Algorithmus / Datenstruktur \u2013\nAnalyse \u2013 Klassi\ufb01kation .\nIm Rahmen dieser Veranstaltung werden wie die Kette f\u00fcr immer neue Probleme immer\nwieder neu durchlaufen. Wir werden exemplarisch Probleme der unterschiedlichsten Arten\nbetrachten: Probleme auf Zahlen, Probleme auf Texten, Problem auf Graphen, Probleme auf\ngeometrischen Objekten, Probleme, bei denen die Eingaben am Anfang gar nicht vollst\u00e4ndig\nvorliegen, oder auch Probleme, bei denen manche Eingaben wahrscheinlicher sind als an-\ndere. Der Fokus wird oft, aber nicht immer, auf der L\u00f6sung der Probleme liegen; manchmal\ngeht es eher darum, eine Entwurfs- oder Analysemethode genauer zu betrachten.\nJedes Kapitel dieses Skripts entspricht grob einer Vorlesungsdoppelstunde und mit jedem\nKapitel verfolge ich gewisse Ziele, welche Sie am Anfang des jeweiligen Kapitels genannt\n\ufb01nden. Neben diesen etwas kleinteiligen Zielen gibt es auch folgende zentralen \u00bbo\ufb03ziellen\u00ab\nVeranstaltungsziele, wie sie auch im Modulhandbuch zu \ufb01nden sind:2Vorwort\n1.Vertrautheit mit algorithmischen Entwurfsprinzipien.\n2.Neue komplexe Algorithmen durch Anwendung dieser Prinzipien entwickeln k\u00f6nnen.\n3.Erfahrung beim algorithmischen Probleml\u00f6sen.\nDas W\u00f6rtchen \u00bbk\u00f6nnen\u00ab taucht bei den Veranstaltungszielen mehrfach auf. Um etwas wirk-\nlich zu k\u00f6nnen, reicht es nicht, davon geh\u00f6rt zu haben oder davon gelesen zu haben. Man\nmuss es auch wirklich getan haben: Sie k\u00f6nnen sich tausend Pokerspiele im Fernsehen an-\nschauen, sie sind deshalb noch nicht mit Poker reich werden; sie k\u00f6nnen tausend Stunden\nWorld of Warcraft spielen, sie werden deshalb trotzdem keine Feuerkugeln auf Ihren Pro-\nfessor geschleudert bekommen.\nDeshalb steht bei dieser Veranstaltung der \u00dcbungsbetrieb mindestens gleichberechtigt ne-\nben der Vorlesung. Der Ablauf ist dabei folgender: In der Vorlesung werde ich Ihnen die\nThematik vorstellen und Sie k\u00f6nnen schon mit dem \u00dcben im Rahmen kleiner Mini\u00fcbungen\nw\u00e4hrend der Vorlesung beginnen. Alle zwei Wochen gibt es ein \u00dcbungsblatt, das inhaltlich\nzu den Vorlesung geh\u00f6rt. Sie m\u00fcssen sich die \u00dcbungsbl\u00e4tter aber nicht \u00bballeine erk\u00e4mpfen\u00ab.\nVielmehr gibt es Tutorien, in denen Sie Aufgaben \u00fcben werden, die \u00bbso \u00e4hnlich\u00ab wie die\nAufgaben auf den \u00dcbungsbl\u00e4ttern sind. Sie werden feststellen, dass die als \u00bbleicht\u00ab und in\nder Regel auch die als \u00bbmittel\u00ab eingestuften Aufgaben mit der Vorbereitung im Tutorium in\nder Tat mit vertretbarem Aufwand scha\ufb00bar sind. Ist eine Aufgabe \u00bbschwer\u00ab, so ist es kein\nUngl\u00fcck, wenn Sie diese nicht scha\ufb00en \u2013 probieren sollten Sie es aber trotzdem.\nIch w\u00fcnsche Ihnen viel Spa\u00df mit dieser Veranstaltung.\nTill TantauTeil I\nOffline-Probleme 1: Zahlen3\nTeil I\nOffline-Probleme 1: Zahlen\nWomit lie\u00dfe sich tre\ufb04icher eine Veranstaltung \u00fcber Algorithmen beginnen als mit der vor-\nnehmsten Aufgabe eines jeden Computers, dem Rechnen ? Auch wenn man es heute kaum\nnoch glauben mag: Computer sind urspr\u00fcnglich nicht daf\u00fcr entworfen worden, unser Leben\nin Timelines zu dokumentieren, sondern ganz banal f\u00fcr das Rechnen \u2013 wie ja der Name\n\u00bbComputer\u00ab auch schon nahelegt.\nDie Welt der Zahl-Probleme ist vielf\u00e4ltig und reich, wir werden nur einen sehr kleinen Aus-\nschnitt betrachten k\u00f6nnen. Beginnen m\u00f6chte ich mit der Multiplikation, allerdings nicht nur\nder von einfachen Zahlen, sondern auch gleich von ganzen Matrizen. Man sollte meinen,\ndass dieses Problem wie man so sch\u00f6n sagt \u00bbwohl untersucht\u00ab sein sollte, jedoch ist es v\u00f6l-\nlig o\ufb00en, wie schnell man zwei Matrizen nun wirklich multiplizieren kann; es gibt also selbst\nbei den scheinbar einfachen Problemen noch viel zu tun f\u00fcr angehende Algorithmendesigne-\nrinnen und -designer. Wir werden uns dann auch an nachweisbar schwere Probleme machen\nwie das Subset-Sum-Problem und diese mit Hilfe von dynamischen Tabellen angehen.\nIn diesem, wie in allen folgenden Teilen, stehen die eigentlichen Rechen-Probleme nicht im-\nmer im Vordergrund. Vielmehr geht es um den Prozess, wie man von einer Problemstellung,\nund sei es so eine \u00bbeinfache\u00ab wie die Multiplikation von zwei Zahlen, zu einem korrekten,\ne\ufb03zienten Algorithmus kommt.41 Entwurfsmethode: Teile-und-Herrsche\n1-1 1-1\nKapitel 1\nEntwurfsmethode: Teile-und-Herrsche\n\u00bbGaussian Elimination is not Optimal\u00ab\n1-2 1-2Lernziele dieses Kapitels\n1.Die Schritte des Algorithmendesigns (Problem,\nEntwurf, Algorithmus und Datenstruktur, Analyse,\nKlassi\ufb01kation) anhand von Zahl-Problemen\nnachvollziehen\n2.Den Karatsuba- und den Strassen-Algorithmus\nkennen\n3.Das Master-Theorem kennen und anwenden k\u00f6nnenInhalte dieses Kapitels\n1.1 Was ist Algorithmendesign? 5\n1.2 Wiederholung: O-Klassen, W-Klassen und\nQ-Klassen 6\n1.3 Die Schritte des Algorithmendesigns 8\n1.3.1 Die Probleme: Multiplikation . . . . . . 8\n1.3.2 Die Entwurfsmethode:\nTeilen-und-Herrschen . . . . . . . . . . 8\n1.3.3 Die Algorithmen: Karatsuba und Strassen 9\n1.3.4 Die Analysemethode: Das\nMaster-Theorem . . . . . . . . . . . . . 11\n1.3.5 Die Klassi\ufb01kation . . . . . . . . . . . . 13\n\u00dcbungen zu diesem Kapitel 15\nWorum\nes heute\ngehtWorum\nes heute\ngehtWissenschaft ist immer dann am spannendsten, wenn scheinbar o\ufb00ensichtliche Wahrheiten\n\u00fcber den Haufen gesto\u00dfen werden. Eine solche o\ufb00ensichtliche Wahrheit war, dass man zwei\nMatrizen, jede der Gr\u00f6\u00dfe n\u0002n, nicht schneller als in Zeit O(n3)multiplizieren k\u00f6nne. In der\nTat: Die Produktmatrix besteht hat n2viele Eintr\u00e4ge und jeder dieser Eintr\u00e4ge ist die Summe\nvonnProdukten von je zwei Zahlen in den Eingabematrizen. Es ist schwer vorstellbar, wie\nman schneller als in Zeit n3gerade n2Mal eine Summer von je nZahlen bestimmen soll.\nAuthor: David Eppstein, Creative Commons Attribution Licence, Volker Strassen gibt die\n2008 Knuth-Preis-Vorlesung beim 20th ACM-SIAM Symposium on Discrete AlgorithmsEs war deshalb ein ziemlicher Paukenschlag, als Volker Strassen /one.taboldstyle/nine.taboldstyle/six.taboldstyle/eight.taboldstyle ein dreiseitiges Papier\nbei der Zeitschrift Numerische Mathematik einreichte mit dem schlichten Titel \u00bbGaussian\nElimination is not Optimal\u00ab, in dem gezeigt wird, wie man zwei Matrizen schneller als\nin Zeit n3multipliziert. Die entscheidende Idee dabei ist, einen Teile-und-Herrsche-Ansatz\nzu verfolgen, wobei man sich allerdings recht geschickt anstellen muss; einfach drauf-los-\nteilen-und-herrschen f\u00fchrt nicht zum Ziel.\nEs ist nicht sonderlich schwierig einzusehen, dass Strassens Algorithmus korrekt arbeitet.\nSchwieriger ist es hingegen, seine Laufzeit zu analysieren: Der Algorithmus scha\ufb00t es, in\njeder Rekursion nur sieben statt acht rekursive Aufrufe zu machen \u2013 was hei\u00dft das nun aber\nf\u00fcr die Laufzeit? Da solche Fragen h\u00e4u\ufb01ger auftauchen, werden wir uns mit Hilfe des so ge-\nnannten Master-Theorems ein Werkzeug zurechtlegen, mit dem sie leicht beantwortet wer-\nden k\u00f6nnen.\nSeit Strassens Beitrag ist die Laufzeit f\u00fcr Algorithmen zur Matrixmultiplikation immer wei-\nter verbessert worden, weshalb mittlerweile die Vermutung kursiert, dass man Matrizen so-\ngar in Zeit O(n2)multiplizieren kann; zeigen konnte dies allerdings noch niemand.1 Entwurfsmethode: Teile-und-Herrsche\n1.2 Was ist Algorithmendesign?5\n1.1 Was ist Algorithmendesign?\n1-4 1-4 Zur Einstimmung: ein Zahlproblem aus Theorie undPraxis\nDas Primzahlproblem\nEingabe Eine ganze Zahl in Bin\u00e4rdarstellung mit nBits.\nFrage Ist die Zahl prim? (Hat sie genau zwei Teiler?)\nDieses Problem ist uralt, es wird bereits bei Euklid behandelt.\nPapyrus von Euklid\nSeine L\u00f6sung wird in g\u00e4ngigen Public-Key-Kryptosystemen eingesetzt.\nOffizielles Werk des Bundes.\n.Zur Diskussion\nWie w\u00fcrden Sie das Problem l\u00f6sen? Wie schnell w\u00e4re Ihre L\u00f6sung bei Eingaben mit 1024\nBit? Ist das in der Praxis schnell genug?\nZum Vergleich: Der Miller-Rabin-Test beantwortet die Frage innerhalb weniger Millisekun-\nden.\n1-5 1-5 Worum geht es beim Algorithmendesign?\nAlgorithmendesign bietet Methoden , um Probleme systematisch zu l\u00f6sen:\n1.Ausgangspunkt ist eine Problemstellung .\nBeispiel: Primzahltest\nHeute: Multiplikation von Zahlen und Matrizen.\n2.Auf diese wendet man eine Entwurfsmethode an.\nBeispiel: Randomisierung\nHeute: Teile-und-Herrsche\n3.Im Ergebnis erh\u00e4lt man Algorithmen undDatenstrukturen .\nBeispiel: Miller-Rabin-Test\nHeute: Karatsuba- und Strassen-Algorithmen\n4.Die \u00bbQualit\u00e4t\u00ab der L\u00f6sung bestimmt man mittels Analysemethoden .\nBeispiel: Average-Case-Analyse\nHeute: Rekursionsgleichungen und Master-Theorem\n5.Im Ergebnis erh\u00e4lt man eine Klassi\ufb01kation .\nBeispiel: Erwartete polynomielle Laufzeit\nHeute: O-Klassen61 Entwurfsmethode: Teile-und-Herrsche\n1.2 Wiederholung: O-Klassen, W-Klassen und Q-Klassen\n1.2 Wiederholung: O-Klassen, W-Klassen und Q-Klassen\n1-6 1-6 Wiederholung: Wie kann man die Rechenzeit von Algorithmen vergleichen?\nSollte man lieber Insertion-Sort oder Merge-Sort benutzen? Laufzeitmessungen ergeben fol-\ngende Werte:\nnzuf\u00e4llige Zahlen Insertion-Sort Merge-Sort\nn=3 0;056ms 0;183ms\nn=10 0;360ms 1;080ms\nn=1000 1:250ms 190ms\nn=100:000 12:122:000ms 35:000ms\nMoral\n1.Ein guter Algorithmus schl\u00e4gt einen schlechten Algorithmus, selbst wenn der gute Al-\ngorithmus schlecht implementiert wird und der schlechte gut implementiert wird.\n2.Die Terme nundlognin Laufzeiten sind in der Regel wichtiger\n\u2013als die verwendete Hardware,\n\u2013als das Geschick der Programmierer.\n1-7 1-7 O-Klassen = h\u00f6chstens.\nIDefinition: Gro\u00df-O-Klasse\nSeig:N!R+\n0eine Funktion. Dann ist die O-Klasse O(g)die Menge aller Funktionen\nf:N!R+\n0, f\u00fcr die\n\u2013es eine Konstante cgibt und\n\u2013es eine Konstante n0gibt, so dass\n\u2013f\u00fcr alle n>n0giltf(n)\u0014c\u0001g(n).\nMerke\n\u00bbf2O(g)\u00ab bedeutet, dass f\u00fcr gen\u00fcgend gro\u00dfe nund einen gen\u00fcgend gro\u00dfen Faktor cgilt,\ndassc\u0001g(n)immer gr\u00f6\u00dfer als f(n)ist.\n1-8 1-8 Veranschaulichung von f2O(g)\nnf(n),g(n),c\u0001g(n)\nf(n)muss in\ndiesem Bereich\nliegen\nn0\n1-9 1-9 Beispiele zu O-Klassen.\nBeispiel\nSeif(n) =3+17n2undg(n) =n2. Dann ist f2O(g).\n(W\u00e4hle c=1000 undn0=1000 . Dann ist sicherlich 3+17n2\u0014cn2.)\nBeispiel\nSeig(n) =1f\u00fcr alle n. Dann enth\u00e4lt die Klasse O(g)alle beschr\u00e4nkten Funktionen.1 Entwurfsmethode: Teile-und-Herrsche\n1.3 Wiederholung: O-Klassen, W-Klassen und Q-Klassen7\nBeispiel\nSeif(n) =n2undg(n) =n. Dann ist f=2O(g).\nBeispiel\nSeif(n) =log2(n2)undg(n) =log2n. Dann ist f2O(g).\n(Es gilt log2(n2) =2log2n.)\n1-10 1-10 Zur Schreibweise von O-Klassen.\n\u2013Man schreibt einfach \u00bb O(n2)\u00ab f\u00fcr \u00bb O(g), wobei gdie Funktion g(n) =n2ist\u00ab.\n\u2013Man schreibt auch \u00bb f(n) =O(n2)\u00ab statt \u00bb f2O(n2)\u00ab.\n\u2013Man schreibt auch Dinge wie \u00bb 2O(n)\u00ab und meint damit eigentlich \u00bbEine Funktion, die\nf\u00fcr hinreichend gro\u00dfe nund eine hinreichend gro\u00dfe Konstante cdurch 2c\u0001nnach oben\nbeschr\u00e4nkt ist.\u00ab\n\u2013Das ist mathematisch alles Quatsch \u2013 aber das st\u00f6rt niemanden.\n1-11 1-11 O-Klassen = h\u00f6chstens. W-Klassen = mindestens. Q-Klassen = genau.\n\u2013Eine O-Klasse enth\u00e4lt alle Funktionen, die h\u00f6chstens soundso schnell wachsen.\n\u2013EineW-Klasse enth\u00e4lt alle Funktionen, die mindestens soundso schnell wachsen.\n\u2013EineQ-Klasse enth\u00e4lt alle Funktionen, die genau soundso schnell wachsen.\nIDefinition: Gro\u00df-Omega-Klassen\nEs gilt f2W(g)genau dann, wenn g2O(f).\nIDefinition: Theta-Klassen\nQ(g) =O(g)\\W(g).\nBeispiel: Maximumsbestimmung\nEin Programm soll das Maximum einer Liste von Zahlen bestimmen. Dann muss es je-\nde Zahl mindestens einmal untersuchen (sonst k\u00f6nnte man n\u00e4mlich eine Eingabe bauen,\nbei der das Programm eine falsche Ausgabe macht). Der Aufwand des Programms ist also\nmindestens linear , das hei\u00dft W(n). Da er andererseits auch h\u00f6chstens linear ist, liegt der\nZeitaufwand sogar in Q(n).\nBeispiel: Sortieren\nEin Programm soll eine Liste von Objekten sortieren. Man kann zeigen, dass man hierzu\nmindestens nlog2n\neVergleiche ben\u00f6tigt. Der Aufwand eines solchen Programms ist also\nW(nlogn).\n1-12 1-12 .Zur \u00dcbung\nGeben Sie f\u00fcr folgende fundgan, ob f2O(g), obf2W(g)und ob f2Q(g)gelten:\n1.f(n) =pnundg(n) =n.\n2.f(n) =n4;00001undg(n) =n4.\n3.f(n) =n5undg(n) =n4(logn)7.\n4.f(n) =log2nundg(n) =1.\n5.f(n) =log3nundg(n) =log2n.\n6.f(n) =2nundg(n) =3n.\nF\u00fcr noch mehr Klassen und eine ausf\u00fchrlichere Einf\u00fchrung siehe das Kapitel zur O-Notation\nim Skript zur Theoretischen Informatik /two.taboldstyle/zero.taboldstyle/zero.taboldstyle/nine.taboldstyle .81 Entwurfsmethode: Teile-und-Herrsche\n1.3 Die Schritte des Algorithmendesigns\n1.3 Die Schritte des Algorithmendesigns\n1.3.1 Die Probleme: Multiplikation\n1-13 1-13 Zwei scheinbar einfache Problemstellungen.\nDas Multiplikationsproblem\nEingabe Zwei Zahlen xundyin Bin\u00e4rdarstellung mit je nBits.\nAusgabe Die Bin\u00e4rdarstellung des Produkts xy.\nDas Multiplikationsproblem f\u00fcr Matrizen\nEingabe Zwei n\u0002nMatrizen XundYmit Zahleintr\u00e4gen (sinnvoll kodiert).\nAusgabe Das Matrix-Produkt XY(sinnvoll kodiert).\nBekannterma\u00dfen gilt: Mittels der Schulmethode zur schriftlichen Multiplikation kann man\nzwei n-Bit-Zahlen in Q(n2)Zeitschritten multiplizieren. Mittels der Schulmethode zur Ma-\ntrixmultiplikation kann man zwei n\u0002nMatrizen mit Q(n3)Zahl-Multiplikationen multipli-\nzieren.\n1.3.2 Die Entwurfsmethode: Teilen-und-Herrschen\n1-14 1-14 Die Grundidee des Teile-und-Herrschen-Ansatz.\nEingabe der L\u00e4nge n\nL\u00e4nge n=b L\u00e4nge n=b L\u00e4nge n=b\nTeill\u00f6sung Teill\u00f6sung Teill\u00f6sung\nL\u00f6sungAufteilen in\naTeile in\nZeit f(n)\nRekursion\nZusammensetzen\nin Zeit g(n)\n1.Aus der Eingabe der L\u00e4nge nwerden kleinere Eingaben jeweils der L\u00e4nge n=berrech-\nnet.Merke: \u00bb bwie Bruchteil\u00ab.\n2.Die Anzahl ader kleineren Eingaben ist oft gleich b, aber nicht immer. Merke: \u00bb awie\nAnzahl neuer Probleme\u00ab.\n1-15 1-15 .Zur \u00dcbung\nWie lautet die Anzahl ader Teile, die Bruchzahl b, die Laufzeitschranke f(n)f\u00fcr das Auf-\nteilen und g(n)f\u00fcr das Zusammensetzen f\u00fcr folgende Algorithmen:\n1.Merge-Sort,\n2.Quick-Sort,\n3.Bin\u00e4re Suche?1 Entwurfsmethode: Teile-und-Herrsche\n1.3 Die Schritte des Algorithmendesigns9\n1.3.3 Die Algorithmen: Karatsuba und Strassen\n1-16 1-16 Multiplikation per Teilen-und-Herrschen.\nErster Versuch.\nWir wollen zwei n-Bit-Zahlen xundymultiplizieren. Wendet man den Teilen-und-Herrschen-\nAnsatz an, so liegt es nahe, jede Zahl in zwei Teile zu spalten. Schreiben wir also xals\nx=a\u00012n=2+bundy=c\u00012n=2+d, wobei a,b,cunddjeweils n=2Bits haben. Das Produkt\nxyist dann gleich ac\u00012n+ (ad+bc)2n=2+bd. Statt xundyzu multiplizieren, k\u00f6nnen wir\nalso die vierProdukte ac,ad,bcundbdberechnen.\na b c d\u0001\nad bd\nac bc\nac ad+bc bd\n.Zur Diskussion\nWie schnell ist der Algorithmus, der entsteht, wenn man wie oben angedeutet die vier Pro-\ndukte ihrerseits jeweils rekursiv berechnet?\n1-17 1-17 Multiplikation per Teilen-und-Herrschen.\nKaratsubas Algorithmus.\nEine geniale Beoachtung\nad+bc= (a+b)(c+d)\u0000ac\u0000bd.\n1function Karatsuba (x,y)\n2 n Anzahl Bits von x und y //nmuss eineZweierpotenz sein\n3 ifn= 1then\n4 return x\u0001y\n5 else\n6 a ersten n=2Bits von x\n7 b letzten n=2Bits von x\n8 c ersten n=2Bits von y\n9 d letzten n=2Bits von y\n10 ac Karatsuba (a,c)\n11 bd Karatsuba (b,d)\n12 z Karatsuba (a+b;c+d)\u0000ac\u0000bd\n13 return ac\u00012n+z\u00012n=2+bd\nDieser Algorithmus kann zwei n-Bit-Zahlen mit dreirekursiven Aufrufen f\u00fcrn\n2-Bit-Zahlen\nberechnen.\n1-18 1-18 Matrix-Multiplikation per Teilen-und-Herrschen.\nErster Versuch.\nWir wollen nun zwei n\u0002nMatrizen XundYper Teilen-und-Herrschen multiplizieren. Es\nliegt nahe, diese Matrizen in je vier Teile aufzuspalten:\nX=\u0012A B\nC D\u0013\n;\nY=\u0012E F\nG H\u0013\n:\nDann gilt:\nXY=\u0012A B\nC D\u0013\u0012E F\nG H\u0013\n=\u0012AE+BG AF +BH\nCE+DG CF +DH\u0013101 Entwurfsmethode: Teile-und-Herrsche\n1.3 Die Schritte des Algorithmendesigns\nWir k\u00f6nnen so die Multiplikation von zwei n\u0002nMatrizen auf die Multiplikation von acht\nn\n2\u0002n\n2Matrizen rekursiv reduzieren.\n1-19 1-19 Professor Howard und sein Daemon.\nK\u00fcrzlich unterhielt sich Professor Howard vom Institute for Theoretical Computational De-\nmonology ( /i.sc/t.sc/c.sc/d.sc) mit seinem Daemon:\nHoward Ich habe etwas \u00fcber diesen Teile-und-Herrschen-Ansatz f\u00fcr das Matrizenpro-\nblem nachgedacht.\nDaemon Ja, Meister. Und?\nHoward Wir m\u00fcssen doch vier Zahlen berechnen: AE+BG,AF+BH,CE+DGund\nCF+DH.\nDaemon Ja, Meister. Ihr seid weise, Meister.\nHoward Ich frage mich nun, ob das auch mit nur sieben Multiplikationen geht.\nDaemon \u00d6h, Meister? Da sind acht Produkte zu berechnen.\nHoward Ja, schon. Aber Karatsuba hat es auch gescha\ufb00t, drei Zahlen, die vier Produkte\nenthalten, mit nur drei Multiplikationen zu berechnen.\nDaemon Dem hat bestimmt sein Daemon geholfen, Meister.\nHoward Gute Idee! Bei den M\u00e4chten der Informatik, beschw\u00f6re ich dich, mir eine L\u00f6sung\nf\u00fcr dieses Problem zu \ufb01nden!\nDie M\u00e4chte der Informatik ergreifen vom Daemon Besitz. Rauch steigt auf.\nDaemon\nP1= (A+D)(E+H);\nP2= (C+D)E;\nP3=A(F+H);\nP4=D(G\u0000E);\nP5= (A+B)H;\nP6= (C\u0000A)(E+F);\nP7= (B\u0000D)(G+H);\nAE+BG=P1+P4\u0000P5+P7;\nAF+BH=P3+P5;\nCE+DG=P2+P4;\nCF+DH=P1\u0000P2+P3+P6:\nHoward So ein Daemon ist schon praktisch...\n1-20 1-20 Matrix-Multiplikation per Teilen-und-Herrschen.\nStrassens Algorithmus.\n1function Strassen (X,Y)\n2 //XundYsindn\u0002nMatrizen ,nisteineZweierpotenz\n3 ifn= 1then\n4 return X\u0001Y\n5 else\n6 A bis D die vier Quadranten von X\n7 E bis F die vier Quadranten von Y\n8 P1 Strassen (A+D;E+H)\n9 ...//Gem\u00e4\u00dfdenAngaben desDaemons\n10 P7 Strassen (B\u0000D;G+H)\n11 return\u0010\nP1+P4\u0000P5+P7 P3+P5\nP2+P4 P1\u0000P2+P3+P6\u0011\nDieser Algorithmus kann zwei n\u0002nMatrizen mit sieben rekursiven Aufrufen berechnen.1 Entwurfsmethode: Teile-und-Herrsche\n1.3 Die Schritte des Algorithmendesigns11\n1.3.4 Die Analysemethode: Das Master-Theorem\n1-21 1-21 Rekursionsgleichungen: Das Analyse-Werkzeug bei Teilen-und-Herrschen\nSchritte beim Teilen-und-Herrschen bei Eingaben der L\u00e4nge n:\n1.Bilde in Zeit f(n)irgendwie aneue Eingaben der Gr\u00f6\u00dfe n=b.\n2.Wende den Algorithmus auf jede dieser aTeileingaben rekursiv an.\n3.Setze in Zeit g(n)die Ergebnisse irgendwie zusammen.\nIstT(n)die Laufzeit des Algorithmus bei Eingaben der L\u00e4nge n, so ergibt sich folgende\nRekursionsgleichung f\u00fcr die Laufzeit:\nT(n) =f(n) +a\u0001T(n=b) +g(n):\nBemerkungen: Ist nso klein, dass keine Rekursion mehr statt\ufb01ndet, so gilt die Formel\nnat\u00fcrlich nicht mehr, sondern T(n) =O(1). Das Problem, dass nnicht durch bglatt teilbar\nist, kann man ignorieren.\n1-22 1-22 Ein Satz, mit dem sich Rekursionsgleichungen bequem l\u00f6sen lassen.\nISatz: Master-Theorem\nDie Funktion T:R!Rgen\u00fcge folgender Rekursion:\nT(n) = a\u0001T(n=b)|{z}\nHerrschen-Aufwand+Q(ntlogdn)|{z}\nTeilen-Aufwand\nmita\u00151,b>1undt\u00150sowie, falls t=0, auch d\u00150.\nWir nennen tden Teilungsexponenten und h=logbaden Herrschaftsexponenten. Dann gilt:\n1.Teilungsexponent gr\u00f6\u00dfer: Teilen-Aufwand \u00fcberwiegt\nFalls h<t, so gilt T(n) =Q(ntlogdn).\n2.Exponenten gleich: Logarithmischer Anstieg\nFalls h=t, so gilt T(n) =Q(ntlogd+1n).\n3.Herrschaftsexponent gr\u00f6\u00dfer: Herrschen-Aufwand \u00fcberwiegt\nFalls h>t, so gilt T(n) =Q(nh).\nBeachte: hTtgilt genau dann, wenn a=btT1.\n1-23 1-23 Beweis des Master-Theorems\nEin allgemeines Lemma\nILemma\nSeiT(n) =a\u0001T(n=b) +Q(ntlogdn). Dann gilt\nT(n) =nhT(1) +logbnX\ni=0Q\u0010\u0000a\nbt\u0001intlogdn\nbi\u0011\n:\nBeweis. Wir setzen einfach immer wieder die Rekursionsformel an:\nT(n) =aT(n=b) +Q(ntlogdn)\n=a2T(n=b2) +Q(ant=btlogdn=b) +Q(ntlogdn)\n=a3T(n=b3) +Q(a2nt=b2tlogdn=b2)\n+Q(ant=btlogdn=b) +Q(ntlogdn)\n=:::\n=alogbn\n|{z}\n=nhT(1) +logbnX\ni=0Q(aint=btilogdn=bi):121 Entwurfsmethode: Teile-und-Herrsche\n1.3 Die Schritte des Algorithmendesigns\n1-24 1-24 Beweis des Master-Theorems\nDer Fall eines gro\u00dfen Teilungsexponenten.\nILemma\nFalls h<t, so gilt T(n) =Q(ntlogdn).\nBeweis. Es galt T(n) =nhT(1) +Plogbn\ni=0Q\u0010\u0000a\nbt\u0001intlogdn\nbi\u0011\n.\n1.Wegen h<twird der Term nhT(1)vonntlogdnf\u00fcri=0\u00bbaufgefressen\u00ab.\n2.Wegen h<tgilt weiter x:=a=bt<1und somit\nlogbnX\ni=0Q\u0010\u0000a\nbt\u0001intlogdn\nbi\u0011\n\u0014logbnX\ni=0xiQ(ntlogdn)\n=Q(ntlogdn)Plogbn\ni=0xi\n|{z}\n<1:\n1-25 1-25 Beweis des Master-Theorems\nDer Fall gleicher Exponenten.\nILemma\nFalls h=t, so gilt T(n) =Q(ntlogd+1n).\nBeweis. Es galt T(n) =nhT(1) +Plogbn\ni=0Q\u0010\u0000a\nbt\u0001intlogdn\nbi\u0011\n.\nDer Term nhT(1)wird immer noch von ntlogdnf\u00fcri=0\u00bbaufgefressen\u00ab.\nJohannes FaulhaberWegen h=tgilta=bt=1und somit l\u00e4sst sich T(n)schreiben als\nlogbnX\ni=0Q\u0000\nntlogd\nbn\nbi\u0001\n=Q(nt)logbnX\ni=0logd\nbn\nbi\n=Q(nt)logbnX\ni=0(logbn\u0000ilogbb)d:\nSetzt man x=logbn, so ist die letzte Summe\n\u2013gerade die Summe der ersten xZahlen (f\u00fcr d=1),\n\u2013der ersten xQuadratzahlen (f\u00fcr d=2),\n\u2013der ersten xKubikzahlen (f\u00fcr d=3) und so weiter.\nHierf\u00fcr gilt nach der Formel von Faulhaber, dass diese Summe in Q(xd+1)liegt.\n1-26 1-26 Beweis des Master-Theorems\nDer Fall eines gro\u00dfen Herrschaftsexponenten.\nILemma\nFalls h>t, so gilt T(n) =Q(nh).\nBeweis. Es galt T(n) =nhT(1) +Plogbn\ni=0Q\u0010\u0000a\nbt\u0001intlogd\nbn\nbi\u0011\n.\nMity=a=btl\u00e4sst sich die Summe umschreiben zu\nQ(nt)logbnX\ni=0yi(logbn\u0000i)d=Q(ntylogbn)logbnX\ni=0yi\u0000logbn(logbn\u0000i)d\nSetzt man k=logbn, so kann man die Summe schreiben alsPk\ni=0id=yi, was wegen y>1\nkonvergiert, also beschr\u00e4nkt ist. Nun gilt\nntylogbn=ntalogbn=btlogbn=ntnlogba=nt=nh:1 Entwurfsmethode: Teile-und-Herrsche\nZusammenfassung dieses Kapitels13\n1.3.5 Die Klassifikation\n1-27 1-27 Teilen-und-Herrschen + Master-Theorem = Klassifikation.\nISatz\nDer Karatsuba-Algorithmus l\u00e4uft in Zeit Q(nlog23).\nBeweis. Die Laufzeit gen\u00fcgt der Rekursionsformel T(n) =3T(n=2)+O(n). Der Herrschafts-\nexponent ist somit h=log23und der Teilungsexponent ist t=1. Nach dem Fall \u00bbgr\u00f6\u00dferer\nHerrschaftsexponent\u00ab des Master-Theorems folgt die Behauptung.\nISatz\nDer Strassen-Algorithmus ben\u00f6tigt Q(nlog27)Zahl-Multiplikationen zur Multiplikation zwei-\ner Matrizen.\nBeweis. Die Anzahl der Zahl-Multiplikationen gen\u00fcgt der Formel T(n) =7T(n=2)+O(n2).\nDer Herrschaftsexponent ist somit h=log27und der Teilungsexponent ist t=2. Nach dem\nFall \u00bbgr\u00f6\u00dferer Herrschaftsexponent\u00ab des Master-Theorems folgt die Behauptung.\nZusammenfassung dieses Kapitels\n1-28 1-28 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationMultiplikation von Zahlen\nMultiplikation von Matrizen\nTeile-und-Herrsche\nKaratsuba-Algorithmus\nStrassen-Algorithmus\nMaster-Theorem\nLaufzeit O(nlog23)\nLaufzeit O(nlog27)\nIDas Klassifikationswerkzeug \u00bb O-Klassen\u00ab\nSind fundgFunktionen, so entspricht bis auf Faktoren und kleine Werte grob:\nf2O(g) \u0019 f\u0014g;\nf2Q(g) \u0019 f=g;\nf2W(g) \u0019 f\u0015g:\nDie wichtigsten O-Klassen und ihre Inklusionsbeziehungen:\nO(1)(O(logn)(O(pn)\n(O(n)(O(nlogn)(O(nlog2n)\n(O(n2)(O(n3)\n(O(2n)(O(3n):141 Entwurfsmethode: Teile-und-Herrsche\nZusammenfassung dieses Kapitels\nIDas Entwurfsprinzip \u00bbTeilen-und-Herrschen\u00ab\nEingabe der L\u00e4nge n\nL\u00e4nge n=b L\u00e4nge n=b L\u00e4nge n=b\nTeill\u00f6sung Teill\u00f6sung Teill\u00f6sung\nL\u00f6sungAufteilen in\naTeile in\nZeit f(n)\nRekursion\nZusammensetzen\nin Zeit g(n)\nDies f\u00fchrt zur Rekursionsformel T(n) =f(n) +aT(n=b) +g(n).\nIDas Analysewerkzeug \u00bbMaster-Theorem\u00ab\nSei\nT(n) = a\u0001T(n=b)|{z}\nHerrschen-Aufwand+Q(ntlogdn)|{z}\nTeilen-Aufwand:\nundh=logbaderHerrschaftsexponenten . Dann gilt:\n1.Teilungsexponent gr\u00f6\u00dfer: Teilen-Aufwand \u00fcberwiegt\nFalls h<t, so gilt T(n) =Q(ntlogdn).\n2.Exponenten gleich: Logarithmischer Anstieg\nFalls h=t, so gilt T(n) =Q(ntlogd+1n).\n3.Herrschaftsexponent gr\u00f6\u00dfer: Herrschen-Aufwand \u00fcberwiegt\nFalls h>t, so gilt T(n) =Q(nh).\nIDie Algorithmen\n1.Der Karatsuba-Algorithmus multipliziert Zahlen in Zeit Q(nlog23).\n2.Der Strassen-Algorithmus multipliziert Matrizen in Zeit Q(nlog27).\nZum Weiterlesen\n[1]Martin F\u00fcrer. Faster Integer Multiplication, Proceedings of the 39th Annual Symposi-\num on Theory of Computing, /a.sc/c.sc/m.sc, 57\u201366, 2007.\n\u00dcber das Problem, zwei Zahlen zu multiplizieren, kann man auch heute noch auf einer der wich-\ntigsten Konferenzen der Informatik Artikel vorstellen (Theoretiker sind immer sehr gl\u00fccklich,\nwenn sie bei /s.sc/t.sc/o.sc/c.sc oder /f.sc/o.sc/c.sc/s.sc einen Artikel akzeptiert bekommen). In diesem Artikel stellt F\u00fcrer\nden derzeit schnellsten Algorithmus zur Multiplikation von Zahlen vor.\n[2]Volker Strassen. Gaussian Elimination is not Optimal, Numerische Mathematik, 13:\n354\u2013356, 1969.\nDieser Artikel hat eine L\u00e4nge von ganzen drei Seiten; in der K\u00fcrze liegt die W\u00fcrze. Bis heute\nr\u00e4tseln Forscher, wie Strassen auf die in dem Artikel angegebenen Gleichungen gekommen ist,\nsie sind dort \u2013 anders als in diesem Kapitel \u2013 ohne motivierendes Ged\u00f6ns einfach angegeben.1 Entwurfsmethode: Teile-und-Herrsche\n1.4 \u00dcbungen zu diesem Kapitel15\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 1.1 Schnelle Polynommultiplikation, schwer\nEin Polynom vom Grad nkann man durch n+1Koe\ufb03zienten (Zahlen) beschreiben: Beispielsweise ist\ndas Polynom p(x) =x2\u00002vom Grad 2und wenn man es als p(x) =1\u0001x2+0\u0001x1+ (\u00002)\u0001x0schreibt,\nso sieht man, dass das Polynom durch die Koe\ufb03zientenfolge (1;0;\u00002)eindeutig beschrieben ist.\nGeben Sie einen Algorithmus mit an, der die Koe\ufb03zienten von zwei Polynomen pundqjeweils vom\nGrad nals Eingabe erh\u00e4lt und der die Koe\ufb03zienten ihres Produkts pqberechnet (das Produkt rvon\nzwei Polynomen pundqist auf die nahe liegende Art de\ufb01niert durch die Vorschrift r(x) =p(x)\u0001q(x)).\nIhr Algorithmus darf zur Berechnung des Produkts maximal O(nlog23)Multiplikationen von Zahlen\ndurchf\u00fchren.\nErl\u00e4utern Sie Ihren Algorithmus kurz und f\u00fchren Sie eine Analyse der Laufzeit durch.\nTipp: Schreiben Sie ein Polynom p(x)alsp1(x)xn=2+p2(x), wobei beide piden Grad n=2haben.\n\u00dcbung 1.2 Transitivit\u00e4t der O-Relation beweisen, leicht\nWenden Sie die De\ufb01nition der O-Relation aus der Vorlesung an, um zu zeigen, dass f\u00fcr alle Funktionen\nf;g;h:N!Nmitf2O(g)undg2O(h)auch f2O(h)gilt. Was folgt hieraus f\u00fcr die W- undQ-\nRelationen?\n\u00dcbung 1.3 Probleme und Algorithmen f\u00fcr typische Wachstumsklassen finden, leicht\nFinden Sie zu jeder der folgenden typischen Wachstumsklassen ein Problem, welches sich durch ei-\nnen Algorithmus l\u00f6sen l\u00e4sst, dessen Laufzeit oder Platzbedarf durch eine Funktion aus der Klasse\nbeschr\u00e4nkt ist:\nO(1)(O(logn)(O(pn)(O(n)(O(nlogn)(O(n2)(O(n3)(O(2n)(O(n!)(O(nn):\nDie Probleme und Algorithmen sollten dabei leicht verst\u00e4ndlich und kurz erkl\u00e4rbar oder aus dem\nStudium schon bekannt sein. Geben Sie f\u00fcr jedes Problem an, wof\u00fcr nsteht.\n\u00dcbung 1.4 Algorithmus von Karatsuba anwenden, leicht\nWenden Sie den Algorithmus von Karatsuba an, um das Produkt 7421\u00011211 zu berechnen.\n\u00dcbung 1.5 Master-Theorem zur Analyse von Algorithmen verwenden, leicht\nIn diesem Kapitel wurden die Laufzeiten der Algorithmen Merge-Sort, Quick-Sort und Bin\u00e4re Suche\ndurch Rekursionsgleichungen abgesch\u00e4tzt.\nAuf welche Gleichungen l\u00e4sst sich das Master-Theorem anwenden? Ermitteln Sie f\u00fcr Algorithmen,\nauf die sich das Theorem anwenden l\u00e4sst, eine asymptotische Absch\u00e4tzung der Laufzeit.\n\u00dcbung 1.6 Master-Theorem anwenden, leicht\nVerwenden Sie das Master-Theorem, um das Wachstum der Funktion T(n) =2T(n=4) +gi(n)f\u00fcr\nfolgende gi:N!Nabzusch\u00e4tzen: g1(n) =1,g2(n) =logn,g3(n) =pn,g4(n) =pn\u0001logn,g5(n) =n\nundg6(n) =n2.162 Entwurfsmethode: Dynamische Tabellen\n2-1 2-1\nKapitel 2\nEntwurfsmethode: Dynamische Tabellen\nDer Rekursionwolf im Iterationspelz\n2-2 2-2Lernziele dieses Kapitels\n1.Beispiele von Problemstellungen kennen, die sich\nmit dynamischen Tabellen l\u00f6sen lassen\n2.Konzept der dynamischen Tabelle in eigenen\nAnwendungen einsetzen k\u00f6nnen\n3.Auf dynamischen Tabellen aufbauende Algorithmen\nanalysieren k\u00f6nnenInhalte dieses Kapitels\n2.1 Der Weg zur \u00bbdynamischen Tabelle\u00ab 17\n2.1.1 Von der Rekursion... . . . . . . . . . . 17\n2.1.2 ...\u00fcber Memoization... . . . . . . . . . 18\n2.1.3 ...zur Iteration . . . . . . . . . . . . . . 19\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme 20\n2.2.1 Das Subset-Sum-Problem . . . . . . . . 20\n2.2.2 Das Rucksack-Problem . . . . . . . . . 22\n\u00dcbungen zu diesem Kapitel 25\nWorum\nes heute\ngehtWorum\nes heute\ngehtW\u00e4re Hamlet Informatik-Student gewesen und nicht ein wankelm\u00fctiger von Geistern geleite-\nter d\u00e4nischer Prinz, so h\u00e4tte er wohl seinen ber\u00fchmtesten Monolog eingeleitet mit den Wor-\nten \u00bbTo recurse or not to recurse, that is the question.\u00ab W\u00e4re das Gretchen eine Informatik-\nStudentin gewesen, so h\u00e4tte sie ihren an der Wissenschaft im Allgemeinen zweifelnden und\nsich deshalb der Esoterik hingebenden Lover Faust wohl gefragt \u00bbNun sag, wie hast du\u2019s mit\nder Rekursion? Du bist ein herzlich guter Mann, allein ich glaub, du h\u00e4ltst nicht viel davon.\u00ab\nUm kaum eine Frage wird und wurde in der Informatik so gerungen wie um die Frage: Lie-\nber rekursiv oder iterativ? Die Beantwortung dieser Frage durch Informatikprofessorinnen\nund -professoren (die es ja eigentlich wissen sollten) \u00e4ndert sich alle paar Jahre: Als die In-\nformatik noch jung und knackig war, war Speicherplatz kostbar und Call-Stacks hatten Platz\nf\u00fcr vielleicht sechzehn Eintr\u00e4ge. Folglich f\u00fchrten Rekursionen schnell zu \u00bbStack-Over\ufb02ows\u00ab\nund die Lehrb\u00fccher f\u00fcllten sich mit Verfahren, wie man Rekursion durch Iterationen erset-\nzen kann. Als dann mit der funktionalen Programmierung die reine Lehre mehr en vogue\nkam und die Sprachen Call-Stacks mit Tausenden oder gar Millionen Eintr\u00e4gen zulie\u00dfen,\nwaren rekursive Ans\u00e4tze pl\u00f6tzlich sehr schick. Man konnte in funktionalen Sprachen \u00fcber-\nhaupt nicht mehr iterative Programmieren \u2013 und die Lehrb\u00fccher f\u00fcllten sich mit Verfahren,\nwie man For-Schleifen geschickt in Tail-Recursions umbasteln kann. Heute hat sich die La-\nge etwas entspannt: Moderne Sprachen bieten einerseits sowohl Rekursion wie Iteration als\ngut nutzbare syntaktische Konstrukte an und die Call-Stacks sind so gro\u00df, dass auch Rekur-\nsionen mit einigen Hundert Rekursionsstufen verkraftet werden.\nIst es also egal, ob man nun rekursiv oder iterativ vorgeht? Sollte man immer dem elegan-\nteren Konstrukt den Vortritt lassen?\nNein, sollte man nicht.2 Entwurfsmethode: Dynamische Tabellen\n2.1 Der Weg zur \u00bbdynamischen Tabelle\u00ab17\nGNU Free Documentation LicenceEs gibt F\u00e4lle, bei denen rekursive Ans\u00e4tze elegant sind, aber kl\u00e4glich scheitern. Der promi-\nnenteste Fall ist die Fibonacci-Folge:\nf(n) =(\n1 f\u00fcrn=1und f\u00fcr n=2und\nf(n\u00001) +f(n\u00002)f\u00fcrn\u00153.\nEleganter kann man es kaum noch ausdr\u00fccken. Wandelt man dies aber direkt in Java-, C-,\nScala-, Lua- oder Was-auch-immer-Code um, so erh\u00e4lt man ein irrsinnig langsames Pro-\ngramm. Der triviale iterative Ansatz (in einer Schleife den jeweils n\u00e4chsten Wert aufgrund\nder beiden vorherigen ausrechnen) f\u00fchrt hingegen zu einem schnellen Programm (es geht\nallerdings auch noch viel schneller).\nWoran liegt dies? Was macht den iterativen Ansatz schnell? Ist es der vermiedene Aufwand\ndurch Funktionsaufrufe? Liegt es an weniger Caching-Misses, da weniger Speicher ben\u00f6tigt\nwird? Der Grund liegt tiefer: Der iterative Ansatz vermeidet doppelte Berechnungen dersel-\nben Werte . Das rekursive Programm \u00bbmerkt\u00ab nicht, wenn es beispielsweise zur Berechnung\nvonf(7)den Wert f(6)ausrechnet, dass es dies gerade schon einmal getan hat, als es n\u00e4m-\nlich f\u00fcr die Berechnung von f(8)auch schon f(6)ausgerechnet hat.\nMan k\u00f6nnte die rekursive Berechnung der Fibonacci-Folge unglaublich beschleunigen, ver-\nmiede man dieses Neuberechnen schon einmal berechneter Werte. Genau an dieser Stelle\nsetzen die dynamischen Tabellen an, um die es heute gehen soll: Eine dynamische Tabelle\nist im Prinzip nichts anderes als eine Tabelle, in der Werte so gespeichert sind, dass man bei\nrekursiven Aufrufen nichts doppelt berechnet. F\u00fcr die Fibonacci-Folge bedeutet dies kon-\nkret Folgendes: Sobald wir in der Rekursion einen Wert f\u00fcr ein Folgenglied errechnet haben,\nspeichern wir dies in einem Array. Immer wenn nun ein rekursiver Aufruf erfolgen soll, so\nschauen wir zun\u00e4chst nach, ob wir diesen Wert nicht schon in der Tabelle stehen haben.\nWenn ja, so rechnen wir einfach mit diesem weiter und sparen uns die Rekursion.\nGemeinfreiMan sieht leicht, dass man im Resultat einen Algorithmus erh\u00e4lt, der genauso schnell ist\nwie der iterative Algorithmus. Eine dynamische Tabelle ist letztendlich ein Rekursionswolf\nim Iterationspelz. Es sei erw\u00e4hnt, dass man sich in der besten aller Welten gar nicht um die\nErstellung der dynamischen Tabellen verdient machen braucht: Soll sich doch der Compiler\ndarum k\u00fcmmern! Wenn man n\u00e4mlich sein Programm in einer reinen funktionalen Sprache\nschreibt, so kann sich der Compiler, wenn er denn will, die Ergebnisse einer Funktionsaus-\nwertung einfach selber in einer Tabelle merken (man spricht von Memoization ). In diesem\nFall beschreibt man das Problem so wie oben rekursiv in vollendeter Eleganz und erh\u00e4lt ein\nProgramm, das so schnell ist wie der iterative Ansatz.\nWomit wir wieder beim Ausgangspunkt w\u00e4ren und die Schlacht zwischen Rekursion und\nIteration in eine neue Runde geht.\n2.1 Der Weg zur \u00bbdynamischen Tabelle\u00ab\n2.1.1 Von der Rekursion. . .\n2-4 2-4 Der Klassiker unter den Folgen.\nAuthor Rainer Kn\u00e4pper, Creative Commons Attribution Sharealike LicenceDieFibonacci-Folge ist rekursiv wie folgt de\ufb01niert:\nf(n) =(\n1 f\u00fcrn\u00142,\nf(n\u00001) +f(n\u00002)f\u00fcrn\u00153.\nDies l\u00e4sst sich leicht in Java implementieren:\nint fib(int n)\n{\nif(n<=2)\nreturn 1;\nelse\nreturn fib(n-1) + fib(n-2);\n}182 Entwurfsmethode: Dynamische Tabellen\n2.1 Der Weg zur \u00bbdynamischen Tabelle\u00ab\n.Zur Diskussion\nWie verh\u00e4lt sich diese Laufzeit im Vergleich zu einer iterativen L\u00f6sung?\n2-5 2-5 Ein zweites Beispiel: L\u00f6sen von Rekursionsgleichungen.\nWir wollen eine Rekursionsgleichung l\u00f6sen wie die Folgende:\nT(1) =23;\nT(2) =42;\nT(n) =T(dn=3e) +8T(bn=3c) +12n2+3:\nMit dem Master-Theorem l\u00e4sst sich diese Rekursion leicht l\u00f6sen :\n1.Wir schreiben dies zun\u00e4chst um als T(n) =9T(n=3) +Q(n2).\n2.Der Herrschaftsexponent ist h=log39=2.\n3.Der Teilungsexponent ist t=2.\n4.Also liegt die L\u00f6sung in Q(n2logn).\nWill man aber nicht nur die Theta-Klasse bestimmen, sondern die Funktion genau ausrech-\nnen, so kann man dies leicht rekursiv machen.\nint T(int n)\n{\nif(n<=1) return 23;\nif(n==2) return 42;\nreturn T(n%3 == 0 ? n/3:n/3+1) + 8 *T(n/3) + 12 *n*n+ 3;\n}\n.Zur Diskussion\nWie verh\u00e4lt sich diese Laufzeit im Vergleich zu einer iterativen L\u00f6sung?\n2.1.2 . . . \u00fcber Memoization. . .\n2-6 2-6 Warum ist die rekursive L\u00f6sung so langsam?\nDas zentrale Problem der Rekursion ist, dass die gleichen Werte immer wieder berechnet\nwerden .\n8\n5\n3\n2\n1 112\n1 13\n2\n1 11\n2-7 2-7 Dies kann man vermeiden durch \u00bbCaching\u00ab .\nBig Idea\nImmer, wenn man einen Funktionswert berechnet hat, speichert man diesen in einer Tabelle .\nWenn der Funktionswert erneut ben\u00f6tigt wird, so spart man sich die Berechnung und gibt\nden Wert in der Tabelle zur\u00fcck.\nboolean []cached ;\nint[] table ;\nint fib(int n)\n{\nif(!cached [n]) {\nif(n<=2) table [n] = 1;\nelse table [n] = fib(n-1) + fib(n-2);2 Entwurfsmethode: Dynamische Tabellen\n2.1 Der Weg zur \u00bbdynamischen Tabelle\u00ab19\ncached [n] = true ;\n}\nreturn table [n];\n}\nDiesen Trick nennt man auch Memoization (\u00bbErinnerung\u00ab).\nboolean []cached ;\nint[] table ;\nint T(int n)\n{\nif(!cached [n]) {\nif(n<=1) table [n] = 23;\nelse if(n==2) table [n] = 42;\nelse table [n] =\nT(n%3 == 0 ? n/3:n/3+1) + 8 *T(n/3) + 12 *n*n+ 3;\ncached [n] = true ;\n}\nreturn table [n];\n}\n2.1.3 . . . zur Iteration\n2-8 2-8 Bottom-Up versus Top-Down.\nEine Beobachtung zur Berechnung\nBetrachtet man Memoization genauer, so geht die Rekursion zwar \u00bbtop-down\u00ab vor, die Ta-\nbelle f\u00fcllt sich aber \u00bbbottom-up\u00ab (da die kleinsten Werte als erstes vorliegen).\nDynamische Tabellen: Iterationen, die mal Rekursionen waren\nWenn sich die Tabelle \u00bbsowieso\u00ab von unten nach oben f\u00fcllt, dann ist es oft schneller , diese\nin einer Schleife von unten nach oben zu f\u00fcllen. Das resultierende iterative Programm nennt\nman (komischerweise) ein \u00bbdynamisches Programm\u00ab und die Tabelle eine \u00bbdynamische Ta-\nbelle\u00ab .\n2-9 2-9 Die Fibonacci-Folge mittels einer dynamischen Tabelle.\nint fib(int n)\n{\nint[]table =new int[n+1];\nfor (int i= 1; i<=n;i++) {\nif(i<=2) table [i] = 1;\nelse table [i] = table [i-1] + table [i-2];\n}\nreturn table [n];\n}\n(Der Pro\ufb01 \u00bbsieht\u00ab nat\u00fcrlich, dass man gar nicht die komplette Tabelle speichern muss, da\nimmer nur die letzten beiden Element ben\u00f6tigt werden.)202 Entwurfsmethode: Dynamische Tabellen\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme\n2-10 2-10 .Zur \u00dcbung\nSchreiben Sie den Java-Code eines dynamischen Programms auf, das die Funktion Tbe-\nrechnet:\nint T(int n)\n{\nint[]table =new int[n+1];\nfor (int i= 1; i<=n;i++) {\n//Ihr Code\n}\nreturn table [n];\n}\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme\n2.2.1 Das Subset-Sum-Problem\n2-11 2-11 Zwei \u00bbeinfache\u00ab Probleme\nAuf einer Tafel m\u00f6gen ein paar Zahlen stehen:\n3170 9794 932 3889 8001 2113 987 8867 345\n9347 2690 7669 1092 4790 3049 433 7235\n8647 8663 986 4116 9044 7031 3538 1218\n4935 5953 6131 1791 5720 8828 1882 7229\n1242 4424 6540 3541 8915 7081 2211 4261\n8084 3635 6752 4158 5002 3491 3277 4662\n5578 6689 3708 3722 1896 5900 3814 9271\n3069 23 1455 3231 6416 5014 2175 1489 364\n1616 1982 9954 122\nZwei Aufgaben\n1.Unterstreichen Sie m\u00f6glichst wenige Zahlen, so dass deren Summe mindestens 100.000\nergibt.\n2.Unterstreichen Sie m\u00f6glichst wenige Zahlen, so dass deren Summe genau 100.000 er-\ngibt.\n2-12 2-12 Das formale Subset-Sum-Problem\nDie formale Problemstellung\nEingabe Eine Folge (a1;:::;an)von nat\u00fcrlichen Zahlen und eine Zahl b.\nFrage Gibt es eine Teilmenge I\u0012f1;:::;ng, so dassP\ni2Iai=b.\nDie ganz formale Problemstellung\n/s.sc/u.sc/b.sc/s.sc/e.sc/t.sc/s.sc/u.sc/m.sc =\b\nbin(a1)#:::#bin (an)##bin (b)\f\f\n9I\u0012f1;:::;ng\u0000P\ni2Iai=b\u0001\t\n:2 Entwurfsmethode: Dynamische Tabellen\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme21\n2-13 2-13 Anwendung der Entwurfsmethode \u00bbdynamische Tabelle\u00ab auf das Subset-Sum-Problem.\nSchritt 1: Das rekursive Programm\nDas Problem l\u00e4sst sich rekursiv sehr leicht l\u00f6sen:\n1function subsetsum (a1;:::;an;b):boolean\n2 ifn=0then\n3 return b=0\n4 else\n5 return subsetsum (a1;:::;an\u00001;b)_subsetsum (a1;:::;an\u00001;b\u0000an)\nIn Java:\nboolean subsetsum (int[]a,int n,int b)\n{\nif(b< 0) return false ;\nif(n== 0)\nreturn b==0;\nelse\nreturn subsetsum (a,n-1,b) || subsetsum (a,n-1,b-a[n]);\n}\n2-14 2-14 Anwendung der Entwurfsmethode \u00bbdynamische Tabelle\u00ab auf das Subset-Sum-Problem.\nSchritt 2: Memoization\nboolean [][] cached ;\nboolean [][] table ;\nboolean subsetsum (int[]a,int n,int b)\n{\nif(b< 0) return false ;\nif(!cached [b][n]) {\nif(n== 0)\ntable [b][n] = b==0;\nelse\ntable [b][n] = subsetsum (a,n-1,b)\n||subsetsum (a,n-1,b-a[n]);\ncached [b][n] = true ;\n}\nreturn table [b][n];\n}\n2-15 2-15 Anwendung der Entwurfsmethode \u00bbdynamische Tabelle\u00ab auf das Subset-Sum-Problem.\nSchritt 3: Umschreiben als Iteration\nboolean subsetsum (int[]a,int n,int b)\n{\nboolean [][] table =new boolean [b+1][ n+1];\nfor (int k= 0; k<=n;k++) {\nfor (int i= 0; i<=b;i++) {\nif(k== 0)\ntable [i][k] = i==0;\nelse\ntable [i][k] = table [i][k-1] ||\n(i>=a[k] ? table [i-a[k]][k-1] : false );\n}\n}\nreturn table [b][n];\n}\nWieder kann man Platz sparen, wenn man beachtet, dass immer nur auf Tabelleneintr\u00e4gen\nf\u00fcrk\u00001zugegri\ufb00en wird. Es reicht also ein eindimensionaler Array.222 Entwurfsmethode: Dynamische Tabellen\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme\n2-16 2-16 Die Laufzeit des Dynamische-Tabellen-Algorithmus f\u00fcr das Subset-Sum-Problem.\nISatz\nDer auf einer dynamischen Tabelle basierende Algorithmus l\u00f6st das Subset-Sum-Problem\nin Zeit O(n\u0001b).\nBeweis. Der Algorithmus iteriert \u00fcber zwei verschachtelte Schleifen, mit n+1beziehungs-\nweise b+1Iterationen.\nBemerkungen:\n\u2013Dies bedeutet nicht, dass /s.sc/u.sc/b.sc/s.sc/e.sc/t.sc/s.sc/u.sc/m.sc2P. Warum nicht?\n\u2013Algorithmen mit Laufzeiten, die polynomiell in den Werten der vorkommenden Zah-\nlen sind (nicht aber unbedingt in der L\u00e4nge der Kodierung der Eingabe), nennt man\npseudopolynomiell .\n2.2.2 Das Rucksack-Problem\n2-17 2-17 Das Dilemma des Diebes.\nGNU Public LicenceDer Dieb und sein Rucksack.\nEin Dieb knackt einen Tresor, in dem sich Gold, Juwelen und Edelsteine be\ufb01nden. In seinem\nRucksack kann er aber nur maximal 50kg tragen.\nWelche Gegenst\u00e4nde sollte er einpacken, um deren Gesamtwert zu maximieren?\nDie formale Problemstellung\nEingabe Eine Folge (w1;:::;wn)vonWerten und eine Folge (g1;:::;gn)vonGewichten\nund ein Maximalgewicht m.\nAusgabe Der maximale Wert vonP\ni2Iwi\u00fcber alle Teilmengen I\u0012f1;:::;ng, so dassP\ni2Igi\u0014m.\n2-18 2-18 Anwendung der Entwurfsmethode \u00bbdynamische Tabelle\u00ab auf das Rucksack-Problem.\nSchritt 1: Das rekursive Programm\n1function rucksack (w1;:::;wn;g1;:::;gn;m):int\n2 ifn=0then\n3 return 0\n4 else\n5 ifm<gnthen //Passt nicht ?\n6 return rucksack (w1;:::;wn\u00001;g1;:::;gn\u00001;m)\n7 else//Passt\n8 return max\b\nrucksack (w1;:::;wn\u00001;g1;:::;gn\u00001;m);\n9 wn+rucksack (w1;:::;wn\u00001;g1;:::;gn\u00001;m\u0000gn)\t\nIn Java:\nint rucksack (int[]w,int[]g,int n,int m)\n{\nif(n== 0)\nreturn 0;\nelse if(m<g[n])\nreturn rucksack (w,g,n-1,m);\nelse\nreturn Math .max(rucksack (w,g,n-1,m),\nw[n]+rucksack (w,g,n-1,m-g[n]));\n}2 Entwurfsmethode: Dynamische Tabellen\n2.2 Dynamische Tabellen f\u00fcr Zahl-Probleme23\n2-19 2-19 Anwendung der Entwurfsmethode \u00bbdynamische Tabelle\u00ab auf das Rucksack-Problem.\nSchritt 2: Memoization\nboolean [][] cached ;\nint[][] table ;\nint rucksack (int[]w,int[]g,int n,int m)\n{\nif(!cached [m][n]) {\nif(n== 0)\ntable [m][n] = 0;\nelse if(m<g[n])\ntable [m][n] = rucksack (w,g,n-1,m);\nelse\ntable [m][n] = Math .max(rucksack (w,g,n-1,m),\nw[n]+rucksack (w,g,n-1,m-g[n]));\ncached [m][n] = true ;\n}\nreturn table [m][n];\n}\n2-20 2-20 Anwendung der Entwurfsmethode \u00bbdynamische Tabelle\u00ab auf das Rucksack-Problem.\nSchritt 3: Als Iteration\nint rucksack (int[]w,int[]g,int n,int m)\n{\nint[][] table =new int[m+1][ n+1];\nfor (int k=0; k<=n;k++) {\nfor (int i=0; i<=m;i++) {\nif(k== 0)\ntable [i][k] = 0;\nelse if(i<g[k])\ntable [i][k] = table [i][k-1];\nelse\ntable [i][k] = Math .max(table [i][k-1],\nw[k] + table [i-g[k]][k-1]);\n}\n}\nreturn table [m][n];\n}\nISatz\nDas Rucksack-Problem kann in Zeit O(mn)gel\u00f6st werden.242 Entwurfsmethode: Dynamische Tabellen\nZusammenfassung dieses Kapitels\nZusammenfassung dieses Kapitels\n2-21 2-21 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationSubset-Sum-Problem\nKnapsack\nDynamische-Tabellen-Methode /\nMemoization\nDynamische Tabelle\nEinfache Schleifenanalyse\nPseudopolynomielle Laufzeit\nIDie Entwurfsmethode: Dynamische Tabellen\nDynamische Tabellen entstehen, wenn man\n1.auf ein rekursives Programm\n2.die Technik der Memoization anwendet und\n3.daraus wiederum ein iteratives Programm macht.\nWer darin ge\u00fcbt ist, kann nat\u00fcrlich auch gleich das iterative Programm hinschreiben.\nMerke\nDynamische Tabellen bringen nur dann etwas, wenn ein rekursives Programm die immer\ngleichen Werte in verschiedenen Rekursions\u00e4sten berechnen w\u00fcrde.\nIDie Klassifikation\nEs gelten die folgenden S\u00e4tze:\nISatz\nDas Subset-Sum-Problem kann mittels dynamischer Tabellen in Zeit O(n\u0001b)und Platz O(b)\ngel\u00f6st werden.\nISatz\nDas Rucksack-Problem kann mittels dynamischer Tabellen in Zeit O(n\u0001m)und Platz O(m)\ngel\u00f6st werden.\nAchtung\nMan kann zeigen, dass beide Probleme NP-vollst\u00e4ndig sind!\nZum Weiterlesen\n[1]Daniel Lokshtanov, Jesper Nederlof, Saving space by algebraization. Proceedings of\nthe 42nd ACM Symposium on Theory of Computing ( /s.sc/t.sc/o.sc/c.sc),321\u2013330, 2010.\nWir haben in diesem Kapitel gesehen, dass man das Subset-Sum-Problem in pseudopolynomi-\neller Zeit und pseudopolynomiellem Platz l\u00f6sen kann. Es ist auch leicht zu sehen, dass man das\nProblem in polynomiellem Platz l\u00f6sen kann. In diesem Paper wird nun gezeigt, dass man durch\n\u00bbAlgebraisierung\u00ab einen Algorithmus erh\u00e4lt, der das Problem gleichzeitig in pseudopolynomi-\neller Zeit und polynomiellem Platz l\u00f6st.\n[2]Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Cli\ufb00ord Stein, Intro-\nduction to Algorithms, zweite Au\ufb02age, MIT Press, 2001, Kapitel 15 \u00bbDynamic Pro-\ngramming\u00ab.\nIn diesem Kapitel wird das Konzept der dynamischen Tabellen sehr sch\u00f6n erkl\u00e4rt mit vielen\ninteressanten Beispielen.2 Entwurfsmethode: Dynamische Tabellen\n2.3 \u00dcbungen zu diesem Kapitel25\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 2.1 Planende Diebe, mittel, mit L\u00f6sung\nMeisterhacker A. Nonymous plant seinen n\u00e4chsten gro\u00dfen Coup: Er wird mal wieder jede Menge\nDaten der Firma S. Ony klauen und diese dann verkaufen. Er hat es bereits gescha\ufb00t, sich vom Server\nder Firma S. Ony ein Verzeichnis der Daten zu bescha\ufb00en, die er stehlen k\u00f6nnte: Er kennt f\u00fcr jede\nDatei deren Gr\u00f6\u00dfe in Byte und aufgrund ihrer Namen wei\u00df er auch, wie viel Euro jede Datei wert sein\nwird.\nA. Nonymous hat vor, Dateien im Wert von genau einer Million Euro zu klauen. Da der Diebstahl um\nso schwieriger wird, je mehr Daten er vom Server \u00fcbertr\u00e4gt, fragt er sich, wie viele Byte er mindestens\n\u00fcbertragen muss, um Daten im Wert von genau einer Million Euro zu bekommen.\nHelfen Sie A. Nonymous! Geben Sie dazu zun\u00e4chst einen rekursiven Algorithmus an, der zwei Arrays\ngundwjeweils der L\u00e4nge nals Eingabe bekommt, wobei gdie Gr\u00f6\u00dfen der Dateien enth\u00e4lt und wihre\nWerte, sowie eine Zahl r(im konkreten Fall eine Million). Ergebnis soll der kleinste WertP\ni2Ig[i]\nsein \u00fcber alle I\u0012f1;:::;ngso dassP\ni2Iw[i] =r.\n\u00dcbung 2.2 Alternatives dynamisches Programm f\u00fcr das Rucksackproblem, mittel\nGeben Sie ein Programm an, das das Rucksackproblem in Zeit O(nW)l\u00f6st, wobei Wdie Summe aller\nWerte von Gegenst\u00e4nden im Tresor ist, also W=Pn\ni=1wi.\nTipp: Wandeln Sie den Algorithmus aus \u00dcbung 2.1 zun\u00e4chst in ein dynamisches Programm um. Er-\nmitteln Sie dann das gr\u00f6\u00dfte r, so dass im Resultatarray an der Stelle rder Eintrag die Rucksackgr\u00f6\u00dfe\nmnicht \u00fcberschreitet.\n\u00dcbung 2.3 Unabh\u00e4ngige Mengen berechnen, mittel\nF\u00fcr einen Graphen G= (V;E)ist eine unabh\u00e4ngige Knotenmenge eine Teilmenge der Knoten U\u0012V,\nzwischen denen es keine Kanten in Ggibt (das hei\u00dft, f\u00fcr alle Paare von Knoten v;w2Ugiltfv;wg=2E.)\nF\u00fcr die in dieser Aufgabe betrachteten Graphen hat jeder Knoten v2Vzudem einen Wert w(v)2N.\nIm Folgenden soll die Entwurfsmethode der dynamischen Tabellen verwendet werden, um m\u00f6glichst\nwertvolle unabh\u00e4ngige Mengen zu \ufb01nden. Das hei\u00dft, f\u00fcr einen gegebenen Graphen G= (V;E)ist eine\nunabh\u00e4ngige Menge U\u0012Vgesucht, die die SummeP\nv2Uw(v)maximiert.\n1.Entwickeln Sie einen Algorithmus, der m\u00f6glichst wertvolle unabh\u00e4ngige Mengen in Pfad-Graphen\n\ufb01ndet.\n2.Erweitern Sie ihn, um m\u00f6glichst wertvolle unabh\u00e4ngige Mengen in B\u00e4umen zu \ufb01nden.\n\u00dcbung 2.4 Dynamische Programmierans\u00e4tze f\u00fcr das Rucksackproblem implementieren, mittel\nDas Ziel dieser Aufgabe ist es, ein Programm zu schreiben, welches das Rucksackproblem l\u00f6st. Ihr\nProgramm sollte dabei so e\ufb03zient sein, dass es die Testeingaben von der Webseite der Veranstaltung\nin kurzer Zeit l\u00f6sen kann. Um alle Testeingaben l\u00f6sen zu k\u00f6nnen, emp\ufb01ehlt es sich, auf verschiedene\nL\u00f6sungsans\u00e4tze f\u00fcr das Rucksackproblem zur\u00fcckzugreifen und f\u00fcr eine gegebene Eingabe jeweils die\npassendste auszuw\u00e4hlen: An L\u00f6sungsans\u00e4tzen kennen sie bereits\n1.die dynamische Tabelle \u00fcber das Gewicht des Rucksackinhaltes aus der Vorlesung,\n2.die dynamische Tabelle \u00fcber den Wert des Rucksackinhaltes aus Aufgabe 2.2 und\n3.die rekursive Methode zum L\u00f6sen des Rucksackproblems, die alle m\u00f6glichen Rucksackinhalte\ndurchgeht.\nKombinieren Sie diese und gegebenenfalls weitere Ans\u00e4tze, um die Testeingaben m\u00f6glichst schnell zu\nl\u00f6sen.\nJede Eingabedatei hat die folgende Form, wobei in der ersten Zeile die Gr\u00f6\u00dfe mdes Rucksacks und in\nden weiteren Zeilen jeweils zuerst das Gewicht giund dann der Wert wieines Gegenstandes steht.\n3\n1 1\n2 2\n2 3\n1 2\nIhr Programm soll f\u00fcr eine solche Datei den gr\u00f6\u00dftm\u00f6glichen Wert eines Rucksackinhaltes berechnen.\nF\u00fcr die Instanz aus der obigen Datei ist dieser Wert zum Beispiel 5.\nHinweis: Sie d\u00fcrfen davon ausgehen, dass alle Zahlen in ein Java long passen.26Teil II\nOffline-Probleme 2: Texte\nTeil II\nOffline-Probleme 2: Texte\nWie weit waren Sie in Ihrem Leben jemals von mit Text bedruckten oder beschriebenen\nFl\u00e4chen entfernt?\nVermutlich werden Sie es kaum auf zehn Meter gescha\ufb00t haben. Es f\u00e4ngt schon damit an,\ndass Ihre Kleidung voll ist mit kleinen Hinweisschildern und Texten; selbst eine Badehose\nhat noch eine Kochanleitung eingedruckt. Praktisch die einzige Chance, einen gr\u00f6\u00dferen Ab-\nstand von Texten zu gewinnen, ist, nackt in einem Gebirgssee zu schwimmen. Ohne Boote\nin der N\u00e4he.\nNichts durchdringt unsere Zivilisation so sehr wie Symbolfolgen; sie sind im wahrsten Sinne\ndes Wortes allgegenw\u00e4rtig.\nEs ist daher auch kein Wunder, dass in der Informatik Texte beziehungsweise \u00bbStrings\u00ab zu\nden wichtigsten Konzepten \u00fcberhaupt geh\u00f6ren. Auf der praktischen Seite ist es in so ziem-\nlich jeder Programmiersprache m\u00f6glich, Strings direkt einzugeben, sie zu manipulieren und\ne\ufb03zient zu verarbeiten. Auf der theoretischen Seite sind Zeichenketten dasmathematische\nGrundkonstrukt, auf dem beispielsweise die gesamte Komplexit\u00e4tstheorie aufgebaut ist.\nMan k\u00f6nnte im Rahmen des Algorithmendesigns den Zeichenketten ganze Vorlesungsreihen\nwidmen; da wir aber auch noch andere spannende Gebiete betrachten wollen, will ich nur\nzwei Themen herauspicken und anrei\u00dfen: Das Indizieren von Texten und das Komprimieren\nvon Texten.3 A&D: Indizieren mit B\u00e4umen27\n3-1 3-1\nKapitel 3\nA&D: Indizieren mit B\u00e4umen\nVon der Kunst, einen Index zu erstellen\n3-2 3-2Lernziele dieses Kapitels\n1.Die Datenstruktur des Tries kennen und\nimplementieren k\u00f6nnen\n2.Die Datenstruktur des Su\ufb03x-Trees kennen\n3.Methoden zur Suche in Texten kennen, die auf\ndiesen Datenstrukturen aufbauenInhalte dieses Kapitels\n3.1 Einf\u00fchrung 28\n3.1.1 Fallbeispiel I: Bibliometrie . . . . . . . 28\n3.1.2 Fallbeispiel II: Virusdatenbanken . . . . 28\n3.2 Tries 29\n3.2.1 Die Idee . . . . . . . . . . . . . . . . . 29\n3.2.2 Grundoperationen . . . . . . . . . . . . 29\n3.2.3 Pfad-Kompression: Patricia-B\u00e4ume . . . 31\n3.2.4 Implementation . . . . . . . . . . . . . 32\n3.3 Suffix-Tries und -Trees 33\n3.3.1 Su\ufb03x-Tries . . . . . . . . . . . . . . . . 33\n3.3.2 Su\ufb03x-Trees . . . . . . . . . . . . . . . 34\n\u00dcbungen zu diesem Kapitel 35\nWorum\nes heute\ngehtWorum\nes heute\ngehtDie Suche in Texten geh\u00f6rt zum Brot-und-Butter-Gesch\u00e4ft moderner Rechner und es ist nicht\nimmer der Mensch (neudeutsch User), der sich nicht mehr erinnern kann, von wem die Mail\nzum Thema \u00bbExtrem wichtige Besprechung\u00ab kam. St\u00e4ndig wollen auch Programme in lan-\ngen Texten irgendetwas suchen, seien es Schl\u00fcsselw\u00f6rter, /x.sc/m.sc/l.sc-Tags oder Variablennamen.\nWenn man lediglich einmal in einem Text etwas suchen m\u00f6chte, so gibt es eine Reihe von\nmehr (Boyer-Moore) oder weniger (naive) ra\ufb03nierten Suchalgorithmen. Oft kommt es aber\nauch vor, dass man im selben Text viele Male suchen m\u00f6chte; das gilt f\u00fcr W\u00f6rterb\u00fccher\nebenso wie f\u00fcr die Basensequenz von Viren. In diesem Fall liegt es nahe, einen speziellen\nIndex zu erstellen, der die Suche etwas vereinfacht.\nDie Idee, einen Index zu erstellen, um Suchanfragen zu beschleunigen, ist nicht neu. Ein\nTelefonbuch ist beispielsweise ein Index, auch wenn Telefonb\u00fccher akut vom Aussterben\nbedroht sind. Der Grund daf\u00fcr ist aber nur, dass Computer eben die Indizierung f\u00fcr uns\n\u00fcbernehmen.\nIn diesem Kapitel geht es um die Frage, wie sich schnell ein Index f\u00fcr einen langen Text\nerstellen l\u00e4sst. Die im Laufe des Kapitels entwickelte Datenstruktur der \u00bbSu\ufb03x-Trees\u00ab kann\nsich sehen lassen: F\u00fcr einen beliebig langen Text kann man einen Index erstellen, der\n1.genauso gro\u00df ist wie der Text selber und\n2.in dem man jedes beliebige Suchwort in einer Laufzeit \ufb01ndet, die linear von der L\u00e4nge\nder zu suchenden Wortes abh\u00e4ngt und v\u00f6llig unabh\u00e4ngig ist von der L\u00e4nge des Textes.\nSucht man also mittels eines Su\ufb03x-Tree im menschlichen Genom (3 Milliarden Basepaare)\nnach einer Basesequenz der L\u00e4nge 50, so ben\u00f6tigt dies nur 50 Zeitschritte.283 A&D: Indizieren mit B\u00e4umen\n3.2 Einf\u00fchrung\n3.1 Einf\u00fchrung\n3.1.1 Fallbeispiel I: Bibliometrie\n3-4 3-4 Welche Autoren schreiben \u00fcber dasselbe Thema?\nIn der Bibliometrie geht es darum, Ma\u00dfe f\u00fcr Ver\u00f6\ufb00entlichungen zu bestimmen. Beispiels-\nweise kann man versuchen zu messen, wie \u00bb\u00e4hnlich\u00ab die Ver\u00f6\ufb00entlichungen von zwei Auto-\nren sind. Hierdurch kann man \u00bbCluster\u00ab von Autoren bestimmen, aber auch Plagiate \ufb01nden.\nWie bestimmt man schnell die \u00c4hnlichkeit von Texten?\n\u2013Ein Ma\u00df der \u00c4hnlichkeit von zwei Texten k\u00f6nnte sein, zu z\u00e4hlen, wie h\u00e4u\ufb01g Worte in\nbeiden Texten vorkommen .\n\u2013Hat man zwei Texte mit je hundert Seiten, so sollen wir schnell f\u00fcr jedes Wort des einen\nTextes heraus\ufb01nden, ob es in dem anderen Text vorkommt.\n3.1.2 Fallbeispiel II: Virusdatenbanken\n3-5 3-5 DieRNA-Virus-Datenbank\nEine /r.sc/n.sc/a.sc-Virus-Datenbank speichert die Genome von Viren :\nAnfragen an eine RNA-Virus-Datenbank\n\u2013Das Blut einer Person oder eines Organismus wird untersucht.\n\u2013Man isoliert /r.sc/n.sc/a.sc-Schnipsel im Blut.\n\u2013Ziel ist es nun herauszu\ufb01nden, ob und wenn von welchem Virus die Schnipsel stammen.\nWie findet man schnell Schnipsel?\n\u2013Die/r.sc/n.sc/a.sc-Virus-Datenbank enth\u00e4lt aktuell 939 Viren.\n\u2013Ein Viren-Genom ist zwischen 3.000 und ca 500.000 Basen lang.\n\u2013Wir sollen also schnell und fehlertolerant einen Schnipsel unter hunderten Megabasen-\npaaren \ufb01nden.\nScreenshot by Till Tantau3 A&D: Indizieren mit B\u00e4umen\n3.2 Tries29\n3.2 Tries\n3-6 3-6 Die besonderen Eigenschaften von Strings.\nZur Verwaltung von Strings eigenen sich Suchb\u00e4ume und Hash-Tabellen nur bedingt:\nProbleme bei Hash-Tabellen zur String-Verwaltung\n\u2013Es dauert lange, einen Hash-Wert eines l\u00e4ngeren Strings auszurechnen.\n\u2013Ein \u00bbfehlertolerantes\u00ab Suchen ist nicht m\u00f6glich.\nProbleme bei bin\u00e4ren Suchb\u00e4umen zur String-Verwaltung\n\u2013Der lexikographische Vergleich \u00e4hnlicher Strings ist langsam.\n\u2013Fehlertolerantes Suchen ist schwierig.\n3.2.1 Die Idee\n3-7 3-7 Die Idee hinter Tries.\nEin Trie (von \u00bbinformation re trieval\u00ab) ist eine Datenstruktur speziell zur Verwaltung vieler\nStrings .\nIDefinition: Trie\nSieSein Alphabet. Ein Trie \u00fcber Sist ein gerichteter Baum mit folgenden Eigenschaften:\n1.Jede Kante ist mit einem Symbol aus Sbeschriftet.\n2.F\u00fcr jeden Knoten gibt es f\u00fcr jedes Symbol maximal eine ausgehende Kante, die so\nbeschriftet ist.\nBeispiel: Ein Trie \u00fcber S=fa;c;g;tg\na c g\na t g c\naa t\n3.2.2 Grundoperationen\n3-8 3-8 Ein Trie kann eine Menge von Strings speichern.\nMan kann Tries benutzen, um eine Menge von Strings zu speichern:\n\u2013Einige Knoten des Tries werden markiert .\n\u2013Jeder Pfad von der Wurzel zu einem markierten Knoten entspricht einem String .\nBeispiel\na c g\na t g c\naa t\nDieser Trie speichert die Menge faa;at;ag;c;cca;ga;gtg.303 A&D: Indizieren mit B\u00e4umen\n3.2 Tries\n3-9 3-9 Die Grundoperationen auf Tries\nSuchen eines Strings in einem Trie\n1function search (String w,TrieT) :boolean\n2 c root(T)\n3 fori 1tolength (w)do\n4 ifc=null then\n5 return false\n6 else\n7 c child of cwith edge label w[i]\n8 return true ifc is marked ,otherwise false\nEinf\u00fcgen eines Strings in einen Trie\n1algorithm insert (String w,TrieT)\n2 c root(T)\n3 fori 1tolength (w)do\n4 ifc has no child with edge label w[i]then\n5 create new child of cwith edge label w[i]\n6 c child of cwith label w[i]\n7 mark c\n1algorithm delete (String w,TrieT)\n2 //Search\n3 c root(T)\n4 fori 1tolength (w)do\n5 ifc=null then\n6 return\n7 else\n8 c child of cwith edge label w[i]\n9 ifc is not marked then\n10 return\n11 else\n12 //Ok,wefound w\n13 unmark c\n14 //Ifcwasaleaf,remove itanditsparents uptothenextunmarked node\n15 while c is a leaf and c is not marked do\n16 p parent of c\n17 delete c\n18 c p\n3-10 3-10 .Zur \u00dcbung\n1.Erstellen Sie einen Trie f\u00fcr folgende Menge von Strings: fbarfoo ;foobar ;foo;bar;barfuss ;\nfoolg.\n2.Wie lange dauert eine Suche nach einem String wder L\u00e4ngejwjin diesem Trie\n2.1h\u00f6chstens und\n2.2mindestens ?\n(Die \u00bbLaufzeit\u00ab sei gemessen in der Anzahl der untersuchten Knoten.)\n3-11 3-11 Die Grundoperationen auf Tries.\nISatz\nEinen String win einen Trie der Gr\u00f6\u00dfe gt\n\u2013einzuf\u00fcgen dauert O(jwj),\n\u2013zu l\u00f6schen dauert O(jwj)und\n\u2013zu suchen dauert O(jwj).3 A&D: Indizieren mit B\u00e4umen\n3.2 Tries31\nISatz\nEin Trie f\u00fcr eine Menge Mvon Strings hat eine maximale Gr\u00f6\u00dfe vonP\nw2Mjwj.\nBemerkungen\n\u2013Die oberen Schranken f\u00fcr die Laufzeiten sind unabh\u00e4ngig von der Gr\u00f6\u00dfe des Tries gt.\n\u2013Auch Hashing kann keine besseren Laufzeiten liefern, da es Q(jwj)dauert, den Hash-\nWert eines Strings wzu berechnen.\n\u2013Man ben\u00f6tigt schonP\nw2MjwjPlatz, um die Elemente der Menge M\u00fcberhaupt im\nSpeicher zu halten.\n3-12 3-12 Tries im Einsatz: Textvergleich\nWir k\u00f6nnen Tries wie folgt nutzen, um herauszu\ufb01nden, wie viele Worte zwei Texte gemein-\nsam haben :\n\u2013Durchlaufe den ersten Text. F\u00fcge dabei jedes gefundene Wort in einen Trie ein.\n\u2013Durchlaufe nun den zweiten Text und \u00fcberpr\u00fcfe f\u00fcr jedes gefundene Wort, ob es im\nersten Text vorkommt.\nO\ufb00enbar ben\u00f6tigt dieses Verfahren Zeit O(n+m), wenn die Texte die L\u00e4ngen nundmhaben.\n3.2.3 Pfad-Kompression: Patricia-B\u00e4ume\n3-13 3-13 Ein Trie f\u00fcr Methodennamen\nNehmen wir an, wir bauen einen Trie f\u00fcr die Methodennamen in einem Java-Programm .\nDann h\u00e4tte dieser folgende Form:\n\u2013Nahe der Wurzel verzweigt er stark, da die Namen ganz unterschiedlich anfangen.\n\u2013Sp\u00e4ter gibt es dann aber immer wieder lange Ketten von Knoten.\nBeispiel: Trie f\u00fcr die Methoden der Java-Klasse java .lang .Compiler\nc\nd\neomm\npand\nileC lasses\nisab le\nnab le\n3-14 3-14 Patricia-B\u00e4ume sind komprimierte Tries.\nIDefinition: Patricia-Baum\nEinPatricia-Baum ist ein Baum, der aus einem Trie entsteht, indem man\n1.alle Pfade im Trie ohne Verzweigungen und ohne Markierungen durch eine einzelne\nKante ersetzt und\n2.diese mit dem Wort beschriftet, das entlang dieses Pfades stand.\n(\u00bbPatricia\u00ab steht f\u00fcr \u00bbPractical Algorithm To Retrieve Information Coded in Alphanume-\nrics\u00ab .)\nBeispiel: Patricia-Baum f\u00fcr die Methoden von java .lang .Compiler\ncom\ndisable\nenablemand\npileClasses323 A&D: Indizieren mit B\u00e4umen\n3.3 Tries\n3.2.4 Implementation\n3-15 3-15 Implementationsdetails I\nSpeicherung von Patricia-B\u00e4umen\nDie in einem Trie gespeicherten Strings stehen oft auch noch an anderer Stelle im Speicher:\nBeispielsweise beim Trie der Worte eines Textes. In diesem Fall kann man in einem Patricia-\nBaum nur die Indizes in den Text speichern statt dem Text selbst .\nBeispiel: Patricia-Baum mit expliziten Kantenbeschriftungen\n1a\n2l\n3i\n4c\n5e\n67f\n8o\n9u\n10n\n11d\n1213a\n14n\n1516a\n17n\n18t\n1920a\n21t\n2223t\n24h\n25e\n2627t\n28r\n29e\n30e\na\nfound\ntlice\nn\ntt\nhe\nree\nBeispiel: Patricia-Baum mit impliziten Kantenbeschriftungen\n1a\n2l\n3i\n4c\n5e\n67f\n8o\n9u\n10n\n11d\n1213a\n14n\n1516a\n17n\n18t\n1920a\n21t\n2223t\n24h\n25e\n2627t\n28r\n29e\n30e\n[1,1]\n[7,11]\n[23,23][2,5]\n[10,10]\n[21,21][18,18]\n[24,25]\n[28,30]\n3-16 3-16 Implementationsdetails II\nSpeicherung der Kinder\nDas Problem der vielen Kinder\nEin Trie hat in der N\u00e4he der Wurzel einen hohen Verzweigungsgrad . Dort kann es f\u00fcr jedes\nSymbol des Alphabets ein Kind geben. Bei gro\u00dfen Alphabeten wie dem Unicode ist unklar,\nwie man die Kinder e\ufb03zient speichert .\n.Zur Diskussion\nGeben Sie f\u00fcr jede der folgenden Arten der Speicherung der Kinder eines Knotens Vorteile\nundNachteile an:\n1.Als verkettete Liste\n2.Als sortierter Array\n3.Als balancierter Suchbaum\n4.Als Hashtabelle3 A&D: Indizieren mit B\u00e4umen\n3.3 Suffix-Tries und -Trees33\n3.3 Suffix-Tries und -Trees\n3-17 3-17 Das Problem: Die Suche in einem Virus-Genom\nWie haben bis jetzt Tries daf\u00fcr genutzt, ganze Worte in einem langen Text schnell zu \ufb01n-\nden. Sucht man aber in einem Virus-Genom nach einem /r.sc/n.sc/a.sc-Schnipsel, so gibt es keine\n\u00bbWortgrenzen\u00ab.\nBeispiel\nHIV-Genom ...atatttgctataaagaaaaaagacrgtact\naaatggagaaaattagtagatttcagagaa...\nSchnipsel acccgattattgga\n3.3.1 Suffix-Tries\n3-18 3-18 Die L\u00f6sung: Suffix-Tries.\nBig Idea\nGegeben sei ein langer Text ohne Wortgrenzen (also ein langes Wort w2S\u0003), in dem wir\nsuchen wollen.\n1.Betrachte die Menge Saller Su\ufb03xe von w.\n2.Bilde den Trie dieser Menge.\nDiesen nennt man dann Su\ufb03x-Trie des Wortes.\nBeispiel: Der Suffix-Trie des Wortes \u00bbanfang\u00ab\nDie Menge Saller Su\ufb03xe von w=anfang ist:\nS=fanfang ;nfang ;fang;ang;ng;g;eg\nDer Trie dieser Menge ist:\na\nf\ng\nnng\nf a n g\na n g\ng\nfa n g\n3-19 3-19 Wie man mit Suffix-Tries Worte findet.\nILemma\nSeiTder Su\ufb03x-Trie von w. Istvein Teilwort von w, so gibt es in Teine Pfad beginnend bei\nder Wurzel, der genau mit vbeschriftet ist.\nBeweis. Kommt vinwvor, so ist vinsbesondere der Anfang (Pr\u00e4\ufb01x) eines Su\ufb03x svonw.\nFolglich beginnt der Pfad in T, der zu sf\u00fchrt, mit v.\nBeispiel: \u00bbfan\u00ab kommt in \u00bbanfang\u00ab vor\nDer rote Pfad zeigt, dass \u00bbfan\u00ab in \u00bbanfang\u00ab vorkommt.\na\nf\ng\nnng\nf a n g\na n g\ng\nfa n g343 A&D: Indizieren mit B\u00e4umen\n3.3 Suffix-Tries und -Trees\n3-20 3-20 .Zur \u00dcbung\nWie gro\u00df ist ein Su\ufb03x-Trie eines Wortes mit zehn Buchstaben\n1.h\u00f6chstens und\n2.mindestens?\n3.3.2 Suffix-Trees\n3-21 3-21 Suffix-Trees sind Patricia-Varianten von Suffix-Tries\nSu\ufb03x-Tries k\u00f6nnen sehr gro\u00df werden , sie sind dann aber auch hochgradig redundant, da sie\nviele lange, gleichartige Ketten enthalten.\nKodiert man Su\ufb03x-Tries als Patricia-B\u00e4ume, so ist dies besonders e\ufb03zient .\nIDefinition\nEinSu\ufb03x-Tree eines Wortes wist der Patricia-Baum der Menge aller Su\ufb03xe von w.\nISatz\nDer Su\ufb03x-Tree eines nichtleeren Wortes what h\u00f6chstens 2jwj\u00001Knoten.\nBeweis. Ein Su\ufb03x-Tree hat h\u00f6chstens jwjBl\u00e4tter, da jedes Blatt einem nichtleeren Su\ufb03x\nentspricht. Jeder Baum mit nBl\u00e4ttern hat h\u00f6chstens n\u00001innere Knoten vom Grad minde-\nstens 2. Jeder Knoten in einem Patricia-Baum hat einen Grad von mindestens 2. Also gibt\nes h\u00f6chstensjwj\u00001innere Knoten und somit h\u00f6chstens 2jwj\u00001Knoten insgesamt.\n3-22 3-22 Der Suffix-Tree von \u00bbanfang\u00ab.\nDas Wort\n1a\n2n\n3f\n4a\n5n\n6g\nExplizite und implizite Darstellungen des Suffix-Tree\nan\nfang\ng\nnfang\ng\nfang\ng[1,2]\n[3,6]\n[6,6]\n[2,2][3,6]\n[6,6]\n[3,6]\n[6,6]\n3-23 3-23 Zur Berechnung von Suffix-Trees.\nO\ufb00enbar kann man einen Su\ufb03x-Tree in quadratischer Zeit berechnen: Man f\u00fcgt einfach alle\nSu\ufb03xe nacheinander ein.\nEs geht jedoch wesentlich besser:\nISatz: Ukkonnen, 1995\nMan kann den Su\ufb03x-Tree eines Wortes win Zeit O(jwj)berechnen.\nBemerkungen: Der Algorithmus ist zwar nicht \u00fcberm\u00e4\u00dfig komplex, aber auch nicht ganz\neinfach. Die Hauptschwierigkeit liegt (mal wieder) darin, zu beweisen, dass er funktioniert.\nHierzu sei auf den Artikel von Ukkonnen verwiesen.3 A&D: Indizieren mit B\u00e4umen\nZusammenfassung dieses Kapitels35\nZusammenfassung dieses Kapitels\n3-24 3-24 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationSuchen in Wort-Mengen\nTeilen-und-Herrschen,\nIndizierung\nTries\nLaufzeitanalyse\nSuche nach vinO(jvj)Suchen in langen Texten\nTeilen-und-Herrschen,\nIndizierung\nSu\ufb03x-Tries\nSu\ufb03x-Trees\nPlatzanalyse\nGr\u00f6\u00dfe O(jwj2)\nGr\u00f6\u00dfe O(jwj)\nITrie und Patricia-Baum\nEinTrieist ein Suchbaum, in dem jeder Knoten f\u00fcr jedes Zeichen eines Alphabets maximal\nein Kind hat. Ein Patricia-Baum ist eine kompakte Darstellung von Tries, in der Pfade zu\neinzelnen Kanten \u00bbkomprimiert\u00ab sind.\nISuffix-Tries und -Trees\n1.EinSu\ufb03x-Trie ist der Trie der Su\ufb03xes eines Wortes.\n2.EinSu\ufb03x-Tree ist der Patricia-Baum des Su\ufb03x-Trie.\nZum Weiterlesen\n[1]Esko Ukkonen, On-line construction of su\ufb03x trees, Algorithmica , 14(3):249\u2013260,\n1995.\nDer in diesem Artikel vorgestellte Linearzeitalgorithmus zur Konstruktion von Su\ufb03x-Trees ist,\nzumindest was die Beschreibung angeht, einfacher als fr\u00fchere Algorithmen und vor allen Dingen\nist es ein Online-Algorithmus: Die Zeichen des Wortes werden von links nach rechts gelesen;\nfr\u00fchere Algorithmen bauten den Su\ufb03x-Tree hingegen \u00bbvon rechts nach links\u00ab auf. Der Artikel\nist sch\u00f6n lesbar und mit vielen Beispielen ausgestattet.\n\u00dcbungen zu diesem Kapitel\nBei den folgenden \u00dcbungen werden wir uns etwas genauer mit dem Zungenbrecher\nIn Ulm und um Ulm und um Ulm herum.\nbesch\u00e4ftigen.\n\u00dcbung 3.1 Konstruktion von Tries, leicht\nKonstruieren Sie einen Trie, der genau die W\u00f6rter des obigen Zungenbrechers enth\u00e4lt. Zwischen Gro\u00df-\nund Kleinbuchstaben soll dabei nicht unterschieden werden.\n\u00dcbung 3.2 Konstruktion von Suffix-Tries und -Trees, leicht\nWir betrachten die zusammengeschobene und vereinfachte Variante\nu l m u n d u m u l m\ndes obigen Satzes. Konstruieren Sie zuerst den Trie aller Su\ufb03xe dieses Wortes und formen diesen\ndann in einen Su\ufb03x-Tree um.\n\u00dcbung 3.3 Konstruktion von Suffix-Tries und Trees, mittel\nKonstruieren Sie f\u00fcr den Text\nf i s c h e r f i s c h e n f i s c h\nzuerst den Trie aller Su\ufb03xe und formen diesen dann in einen Su\ufb03xtree um.364 A&D: Indizieren mit Arrays\n4-1 4-1\nKapitel 4\nA&D: Indizieren mit Arrays\nVon der Kunst, einen kompakten Index zu erstellen\n4-2 4-2Lernziele dieses Kapitels\n1.Die Datenstruktur des Su\ufb03x-Arrays kennen\n2.Den /d.sc/c.sc/three.taboldstyle-Algorithmus verstehenInhalte dieses Kapitels\n4.1 Suffix-Arrays 37\n4.1.1 Su\ufb03x-B\u00e4ume sind gut... . . . . . . . . 37\n4.1.2 ...aber nicht gut genug . . . . . . . . . 37\n4.1.3 Die Idee . . . . . . . . . . . . . . . . . 37\n4.1.4 Vergleich von Su\ufb03x-Arrays und\nSu\ufb03x-B\u00e4umen . . . . . . . . . . . . . . 38\n4.2 Der DC3-Algorithmus 38\n4.2.1 Das Ziel: ein Linearzeitalgorithmus . . . 38\n4.2.2 Der Algorithmus im \u00dcberblick . . . . . 39\n4.2.3 Schritt 1: Sortierung der taktlosen Su\ufb03xe 40\n4.2.4 Schritt 2: Sortierung der taktvollen Su\ufb03xe 41\n4.2.5 Schritt 3: Verschmelzung . . . . . . . . 43\nWorum\nes heute\ngehtWorum\nes heute\ngehtIn diesem Kapitel wird es musikalisch, die Algorithmen tanzen zu Strings im Drei-Viertel-\nTakt. Anlass ist die Implementation des Di\ufb00erence-Cover-Modulo-3-Algorithmus, besser\nbekannt als /d.sc/c.sc/three.taboldstyle-Algorithmus, bei der an \u00bbtaktvollen\u00ab Stellen im String andere Dinge pas-\nsieren als an den eher \u00bbtaktlosen\u00ab.\nZiel des Algorithmus ist die Berechnung eines Su\ufb03x- Arrays, neben Su\ufb03x-Tries und Su\ufb03x-\nTrees die dritte Art, Su\ufb03xe zu speichern. Su\ufb03x-Arrays sind konzeptionell sehr einfach (der\nArray der Startpositionen der Su\ufb03xes eines Strings in lexikographischer Reihenfolge), wes-\nhalb sie auch sehr gut und einfach in der Praxis nutzbar sind.\nGenau wie bei Su\ufb03x-Trees ist das Hauptproblem bei Su\ufb03x-Arrays, sie e\ufb03zient zu berech-\nnen. Eine naive Implementation ben\u00f6tigt genau wie bei Su\ufb03x-Trees quadratische Zeit und\ndies ist bei Eingabegr\u00f6\u00dfen von mehreren Megabytes inakzeptabel. Der tanzende /d.sc/c.sc/three.taboldstyle-Algo-\nrithmus aus diesem Kapitel meistert die Aufgabe hingegen in linearer Zeit. Im Vergleich\nentspricht /d.sc/c.sc/three.taboldstyledaher einem sprintenden Eiskunstl\u00e4ufer, w\u00e4hrend die naiven Algorithmen\nzur Erstellung von Su\ufb03x-Arrays eher ein beh\u00e4biges Elefantenballett au\ufb00\u00fchren.4 A&D: Indizieren mit Arrays\n4.1 Suffix-Arrays37\n4.1 Suffix-Arrays\n4.1.1 Suffix-B\u00e4ume sind gut. . .\n4-4 4-4 Ein Werbeplakat f\u00fcr Suffix-Trees.\nDie zentralen Eigenschaften von Suffix-Trees\n1.Der Su\ufb03x-Tree von wben\u00f6tigt nur O(jwj)Speicher.\n2.Er kann in Zeit O(jwj)konstruiert werden (Ukkonens Algorithmus).\n3.Er erlaubt, in O(jvj)Zeit nach einem Teilwort vvonwzu suchen.\nBeispiel: Der Suffix-Tree von \u00bbanfang\u00ab\n1a\n2n\n3f\n4a\n5n\n6g[1,2]\n[3,6]\n[6,6]\n[2,2][3,6]\n[6,6]\n[3,6]\n[6,6]\n4.1.2 . . . aber nicht gut genug\n4-5 4-5 Suffix-Trees sind eine dolle Sache, aber. . .\nNachteile\n\u2013Die Speicherung der Kinder eines Knotens ist schwierig, insbesondere bei gro\u00dfen Al-\nphabeten wie dem Unicode.\n\u2013F\u00fcr jeden Knoten muss ein Objekt gespeichert werden, das wiederum mindestens zwei,\neher noch mehr Verweise enth\u00e4lt.\n\u2013Algorithmen wie der von Ukkonen ben\u00f6tigen noch weitere Verweise in jedem Knoten.\nInsgesamt ben\u00f6tigt ein Su\ufb03x-Tree pro Byte des zu speichernden Wortes etwa 12 bis 24 Byte\nan Speicher .\nZiel\nEine Datenstruktur, die \u00bbfast genauso gut\u00ab wie ein Su\ufb03x-Tree ist, aber \u00bbeinfacher zu benut-\nzen\u00ab ist.\n4.1.3 Die Idee\n4-6 4-6 Suffix-Arrays sind einfach und kompakt\nDie zwei Ideen hinter Suffix-Arrays\nSeiwein Wort und sei wi=w[i;:::;jwj]der an Stelle ibeginnende Su\ufb03x von w.\n1.Wir betrachten wieder die Menge aller Su\ufb03xe von wundsortieren diese lexikogra-\nphisch . Dies ergibt \u00bbfast\u00ab den Su\ufb03x-Array.\n2.Wir m\u00fcssen die Su\ufb03xe aber nicht explizit speichern, es gen\u00fcgt, ihre Anfangspositionen\ninwzu speichern.\nIDefinition: Suffix-Array\nSeiwein Wort. Der Su\ufb03x-Array vonwist ein Array Svon Indizes, so dass f\u00fcr jede zwei\nIndizes iundjmiti<jgilt\nwA[i]<wA[j]:\nHierbei steht <, wie auch im Folgenden, f\u00fcr die lexikographische Ordnung.\nBeispiel: Suffix-Array von \u00bbanfang\u00ab\nDas Wort \u00bbanfang\u00ab hat\n1.dieSu\ufb03xe \u00bbanfang\u00ab, \u00bbnfang\u00ab, \u00bbfang\u00ab, \u00bbang\u00ab, \u00bbng\u00ab, \u00bbg\u00ab und e,\n2.also sortiert: e, \u00bbanfang\u00ab, \u00bbang\u00ab, \u00bbfang\u00ab, \u00bbg\u00ab, \u00bbnfang\u00ab, \u00bbng\u00ab.\n3.Als Array von Anfangspositionen: [7;1;4;3;6;2;5].384 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus\n4.1.4 Vergleich von Suffix-Arrays und Suffix-B\u00e4umen\n4-7 4-7 Suffix-Arrays versus Suffix-Trees.\nSu\ufb03x-Arrays sind fast so schnell wie Su\ufb03x-Trees:\n\u2013Zur Erinnerung: Der Su\ufb03x-Tree von werlaubt es uns, f\u00fcr jedes Wort vinZeitO(jvj)\nzu testen, ob vinwvorkommt.\n\u2013Mit einem Su\ufb03x-Arrays geht dies auch schnell: Man f\u00fchrt einfach eine bin\u00e4re Suche\nnach vdurch, was O(jvjlogjwj)dauert.\nSu\ufb03x-Arrays brauchen viel weniger Platz:\n\u2013O\ufb00enbar ben\u00f6tigt ein Su\ufb03x-Array nur log2jwjBits pro Zeichen des Arrays, also in der\nPraxis zwischen 1 und 4 Byte.\n\u2013Ein Su\ufb03x-Tree ben\u00f6tigt eher 12 bis 24 Byte pro Zeichen des Wortes.\nSu\ufb03x-Arrays sind einfach:\n\u2013Der geringe Platzverbrauch erlaubt es, gr\u00f6\u00dfere Teile des Arrays im Cache zu halten.\n\u2013Er kann als short []oder int[]gespeichert werden, deren Verarbeitung sich besser\noptimieren l\u00e4sst als die von Knoten-Objekten.\n4-8 4-8 .Zur \u00dcbung\nF\u00fcr Anf\u00e4nger Bestimmen Sie den Su\ufb03x-Array und den Su\ufb03x-Baum von \u00bbannanas\u00ab.\nF\u00fcr Profis Wie kann man mittels des Su\ufb03x-Array von wnach einem Wort vin Zeit O(jvj+\nlogjwj)suchen statt lediglich in Zeit O(jvjlogjwj)?\n4.2 Der DC3-Algorithmus\n4.2.1 Das Ziel: ein Linearzeitalgorithmus\n4-9 4-9 Wie schnell kann man Suffix-Arrays berechnen?\nEin Su\ufb03x-Array ist nichts anderes als die Sortierung der Su\ufb03xe von w, weshalb man ihn\ndurch jeden beliebigen Sortieralgorithmus bestimmen kann. Man muss dabei noch nicht\neinmal die Su\ufb03xe explizit hinschreiben , sondern speichert nur die Anf\u00e4nge.\nBeispiel: Sortierung mit Bubble-Sort\n\u00bbUnsortierter\u00ab Array\n[ ] 1\nanfang2\nnfang3\nfang4\nang5\nng6\ng7\ne\nErster Tausch\n[ ] 1\nanfang3\nfang2\nnfang4\nang5\nng6\ng7\ne\nZweiter Tausch\n[ ] 1\nanfang3\nfang4\nang2\nnfang5\nng6\ng7\ne\nDritter Tausch\n[ ] 1\nanfang4\nang3\nfang2\nnfang6\ng5\nng7\ne\nVierter Tausch\n[ ] 1\nanfang4\nang3\nfang2\nnfang5\nng7\ne5\nng\nEndresultat\n[ ] 7\ne1\nanfang4\nang3\nfang6\ng2\nnfang5\nng4 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus39\nBeobachtung\nNehmen wir an, wir nutzen Merge-Sort als Sortieralgorithmus. Dann werden O(nlogn)Ver-\ngleiche durchgef\u00fchrt. Der lexikalische Vergleich zweier Su\ufb03xe kann nicht l\u00e4nger als O(n)\ndauern.\nHieraus ergibt sich: Man kann den Su\ufb03x-Array eines Wortes win Zeit O(n2logn)berechnen\nmittels Merge-Sort.\nUnser Ziel\nEinLinearzeit-Algorithmus f\u00fcr die Berechnung von Su\ufb03x-Arrays.\n4.2.2 Der Algorithmus im \u00dcberblick\n4-10 4-10 Wie man Suffix-Arrays in linearer Zeit berechnet.\nISatz: K\u00e4rkk\u00e4inen, Sanders, Burkhardt, 2003\nMan kann den Su\ufb03x-Array eines Wortes win Zeit O(jwj)berechnen.\nDer Algorithmus, genannt /d.sc/c.sc/three.taboldstyle, ist ein Teile-und-Herrsche-Algorithmus . Es gab vorher schon\nden Algorithmus von Farach, der grob so arbeitet:\n\u2013Teile die Stellen des Wortes in gerade und ungerade Stellen auf.\n\u2013Berechne rekursiv Su\ufb03x-Arrays hierzu.\n\u2013Setze diese durch einen sehr komplexen Merge-Schritt zusammen.\nNeuam/d.sc/c.sc/three.taboldstyle-Algorithmus ist:\n\u2013Man halbiert nicht die Array-Gr\u00f6\u00dfe, sondern\n\u2013reduziert die Gr\u00f6\u00dfe nur auf zwei Drittel.\n\u2013Dadurch wird alles \u00bbganz einfach\u00ab.\n4-11 4-11 \u00dcberblick zum DC3-Algorithmus\nIDefinition: Takt\nSeiwein Wort. Wir nennen je drei aufeinanderfolgende Stellen in weineTakt. Die jeweils\nerste Stelle eines Takts hei\u00dft taktvoll . Die anderen beiden Stellen hei\u00dfen taktlos .\nF\u00e4ngt ein Su\ufb03x an einer taktvollen Stelle in wan, so nennen wir es taktvoll, sonst taktlos.\nBeispiel\nDas Wort \u00bbanfang\u00ab hat\n\u2013die taktvollen Su\ufb03xe \u00bbanfang\u00ab, \u00bbang\u00ab und eund\n\u2013die taktlosen Su\ufb03xe \u00bbnfang\u00ab, \u00bbfang\u00ab, \u00bbng\u00ab und \u00bbg\u00ab.\nDie drei Schritte des /d.sc/c.sc/three.taboldstyle-Algorithmus.\nErster Schritt: Sortierung der taktlosen Suffixe\nWir sortieren die Menge der taktlosen Su\ufb03xe. Dies ist (fast) die Bestimmung eines Su\ufb03x-\nArrays, weshalb wir dies rekursiv machen k\u00f6nnen.\nBeispiel: Wir sortieren rekursiv \u00bbnfang\u00ab, \u00bbfang\u00ab, \u00bbng\u00ab, \u00bbg\u00ab zu \u00bbfang\u00ab, \u00bbg\u00ab, \u00bbnfang\u00ab, \u00bbng\u00ab.\nZweiter Schritt: Sortierung der taktvollen Suffixe\nWir sortieren dann die Menge der taktvollen Su\ufb03xe. Dies geht ganz einfach, wenn man die\nSortierung der taktlosen hat.\nBeispiel: Wir sortieren rekursiv \u00bbanfang\u00ab, \u00bbang\u00ab, ezue, \u00bbanfang\u00ab, \u00bbang\u00ab.\nDritter Schritt: Verschmelzung\nWir verschmelzen die beiden sortierten Listen wie bei Merge-Sort.404 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus\n4.2.3 Schritt 1: Sortierung der taktlosen Suffixe\n4-12 4-12 Superzeichen . . .\nIn dem Algorithmus werden wir \u00f6fter Tripel von Zeichen (also die Zeichen eines \u00bbTaktes\u00ab)\nalsein Zeichen behandeln.\nIDefinition: Superzeichen\nSeiSein Alphabet. Ein Superzeichen ist ein Zeichen aus dem Alphabet S3.\nBeispiel\nAus den drei Zeichen a, n und f wird das \u00bbSuperzeichen\u00ab [anf], wobei die eckigen Klammern\nklar machen sollen, dass es sich um einZeichen handelt.\nSortierung von Zeichen und Superzeichen\n\u2013Wie setzen voraus, dass unsere Alphabete sortiert sind , es also eine lineare Ordnung\naufSgibt.\n\u2013Diese \u00fcbertr\u00e4gt sich auf Superzeichen , in dem diese \u00bbwie im Telefonbuch\u00ab sortiert wer-\nden: [aac]<[aba]<[abc]<[def].\n4-13 4-13 . . . und Superstrings\nFasst man in einem String je drei Zeichen zu einem Superzeichen zusammen, so erh\u00e4lt man\neinen Superstring.\nIDefinition: Superstring\nSeiw2S3nein Wort. Dann ist der Superstring zuw2(S3)ndas Wort, das entsteht, wenn\nman je drei Zeichen von wzu einem Superzeichen zusammenfasst.\nIst die L\u00e4nge von wnicht durch 3teilbar, so erg\u00e4nzen wir am Ende von wnoch \u00bbNull-\nZeichen\u00ab, bis dies der Fall ist.\nBeispiel\n\u2013Aus dem Wort \u00bbanfang\u00ab wird der Superstring \u00bb[anf][ang]\u00ab der L\u00e4nge 2.\n\u2013Aus dem Wort \u00bbfang\u00ab wird der Superstring \u00bb[fan][g00]\u00ab der L\u00e4nge 2.\n4-14 4-14 Die Alphabete von Superstrings\nEin Problem\nDurch die Rekursion werden vom Algorithmus erst Superstrings , dann Supersuperstrings\nund sp\u00e4ter Supersuper...superstrings gebildet: Der ganze Faust ist irgendwann nur ein Zei-\nchen. Dadurch entstehen gigantische Alphabete und man kann nicht mehr zwei Supersu-\nper...superzeichen in Zeit O(1)vergleichen .\nDie L\u00f6sung\nNormalerweise sind die Alphabete (wie /a.sc/s.sc/c.sc/i.sc/i.sc oder Unicode) konstant. Der /d.sc/c.sc/three.taboldstyle-Algorithmus\narbeitet hingegen lieber mit dem Alphabet f0;:::;jwjg, also mit \u00bbZahlen\u00ab als \u00bbZeichen\u00ab.\nDies ist keine Einschr\u00e4nkung, denn ein String der L\u00e4nge jwjkann nicht mehr als jwjZeichen\nenthalten und wir k\u00f6nnen die vorhandenen Zeichen zun\u00e4chst sortieren.\nBeispiel\nDen String aaxb2S4kann man darstellen als (1;1;3;2)2f0;:::; 4g4, da in dem String die\nZeichen a, b und x vorkommen und diese sortiert sind: a <b<x.\nBeispiel\nDen Superstring [nfa][ng0][fan][g00]kann man darstellen als (3;4;1;2)2f0;:::; 4g4, da gilt\n[fan]<[g00]<[nfa]<[ng0].\nIn den folgenden Beispielen werden trotzdem weiter Buchstaben verwendet, intern rechnet\nder Algorithmus aber mit Zahlen.4 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus41\n4-15 4-15 Erster Schritt des DC3-Algorithmus: Rekursion auf geeignete Superstrings.\nDer rekursive Aufruf bei DC3\n1.Bilde den Superstring R1vonwohne sein erstes Zeichen.\n2.Bilde den Superstring R2vonwohne seine ersten beiden Zeichen.\n3.Berechne (rekursiv) den Su\ufb03x-Array Sdes verketteten Superstrings R1R2.\nBeispiel: Das Wort \u00bbanfang\u00ab\n1.R1= [nfa][ng0].\n2.R2= [fan][g00].\n3.Der Su\ufb03x-Array von [nfa][ng0][fan][g00]lautet:\n\u2013Die Su\ufb03xe sind [nfa][ng0][fan][g00],[ng0][fan][g00],[fan][g00],[g00]unde.\n\u2013Sortiert sind dies e,[fan][g00],[g00],[nfa][ng0][fan][g00]und[ng0][fan][g00].\n\u2013Also ist der Su\ufb03x-Array [5;3;4;1;2].\n4-16 4-16 Was den Superstring R1R2so besonders macht.\nILemma\nDie Su\ufb03xe von R1R2sind in derselben Reihenfolge wie die Su\ufb03xe von wan den taktlosen\nStellen.\nSu\ufb03xe von \u00bbanfang\u00ab:\ne\nanfang\nang\nfang\ng\nnfang\nng\nSu\ufb03xe von [nfa ][ng0][fan][g00]:\ne\n[fan][g00]\n[g00]\n[nfa][ng0][fan][g00]\n[ng0][fan][g00]\n4.2.4 Schritt 2: Sortierung der taktvollen Suffixe\n4-17 4-17 Die R\u00e4nge der Suffixe\nIDefinition: Rang eines Suffix\nDerRang eines Su\ufb03x gibt an, wie viele Su\ufb03xe des Wortes lexikographisch kleiner sind.\nDertaktlose Rang gibt an, wie viele taktlose Su\ufb03xe lexikographisch kleiner sind. F\u00fcr den\ntaktlosen Rang des bei Stelle ibeginnenden Su\ufb03x schreiben wir ri; und setzen rn+1=rn+2=\n0.\nDer Rang des i-ten Su\ufb03x ist damit die Stelle im Su\ufb03x-Array, an der iauftaucht.\nBeispiel: R\u00e4nge der Suffixe von \u00bbanfang\u00ab\nBuchstaben anfang\nStelle 123456\nRang 1.5.3.2.6.4.\nBeispiel: Taktlose R\u00e4nge der taktlosen Suffixe von \u00bbanfang\u00ab\nBuchstaben anfang\nStelle ( i) 123456\nTaktloser Rang ( ri) 3.1. 4.2.424 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus\n4-18 4-18 Taktlose R\u00e4nge von taktlosen Suffixen\nDie taktlosen R\u00e4nge der taktlosen Su\ufb03xe von wergeben sich aus dem Su\ufb03x-Array von R1R2\nwie folgt:\n1.Jedem taktlosen Su\ufb03x entspricht genau eine Stelle in R1R2.\n2.Im Su\ufb03x-Array von R1R2kommt diese Stelle an einer Position ivor.\n3.Dann ist der gesuchte Rang gerade i\u00001.\nBeispiel\nDer Su\ufb03x-Array von \u00bb[nfa][ng0][fan][g00]\u00ab ist [5;3;4;1;2]. Hieraus ergibt sich:\nBuchstaben anfang\nStelle 123456\nTaktloser Rang 3.1. 4.2.\nBeispielsweise gilt:\n\u2013Das taktlose Su\ufb03x \u00bbnfang\u00ab entspricht \u00bb[nfa][ng0][fan][g00]\u00ab, dem ersten Su\ufb03x von\nR1R2, welches an Position 4vorkommt.\n\u2013Das taktlose Su\ufb03x \u00bbfang\u00ab entspricht \u00bb[fan][g00]\u00ab, dem dritten Su\ufb03x von R1R2, wel-\nches an Position 2vorkommt.\n4-19 4-19 Zweiter Schritt des DC3-Algorithmus: Sortierung der taktvollen Suffixe\nDielexikographische Sortierung der taktvollen Su\ufb03xe ermittelt man nun wie folgt:\n1.Bilde f\u00fcr jede taktvolle Position idas Paar (w[i];ri+1).\n2.Sortiere die Paare mittels Radix-Sort in linearer Zeit.\nBeispiel\nBuchstaben a nfa ng\nStelle 1 234 56\nTaktloser Rang 3.1. 4.2.\nPaare (a;3) (a;4)\nSortierung (a;3) (a;4)\nBeispiel\nBuchstaben a nfa eng er\nStelle 1 234 567 89\nTaktloser Rang 4.3. 1.5. 2.6.\nPaare (a;4) (a;1) (g;2)\nSortierung (a;1) (a;4) (g;2)\n4-20 4-20 Zusammenfassung bis hierher\nDer Algorithmus hat bis jetzt Folgendes berechnet:\n1.Eine Sortierung der taktlosen Su\ufb03xe untereinander.\n2.Eine Sortierung der taktvollen Su\ufb03xe untereinander.\n3.Die taktlosen R\u00e4nge der taktlosen Su\ufb03xe.\nZiel ist es nun, daraus die Sortierung aller Su\ufb03xe zu ermitteln.4 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus43\n4.2.5 Schritt 3: Verschmelzung\n4-21 4-21 Das Konzept des Vorrangs\nIDefinition: Vorrang\nGegeben seien eine taktlose Stelle iund eine taktvolle Stelle t. In folgenden F\u00e4lle hat iVor-\nrang:\n1.Stelle i+1ist taktlos und (w[i];ri+1)<(w[t];rt+1).\n2.Stelle i+2ist taktlos und (w[i];w[i+1];ri+2)<(w[t];w[t+1];rt+2)gilt.\nAnderenfalls hat tVorrang.\nBeispiel\nBuchstaben anfaenger\nStelle 123456789\nTaktloser Rang 4.3. 1.5. 2.6.\n\u2013Die taktlose Stelle 3 hat Vorrang vor der taktvollen Stelle 7, da (f;a;1)<(g;e;6).\n\u2013Die taktlose Stelle 2 hat keinen Vorrang vor der taktvollen Stelle 7, da (n;3)>(g;2).\n4-22 4-22 Zentrale Eigenschaft des Vorrangs\nILemma\nEine taktlose Stelle ihat genau dann Vorrang vor einer taktvollen Stelle t, wenn wi<wt.\nBeweis. Wir zeigen zwei Richtungen:\n1.Stelle ihabe Vorrang vor t. Isti+1taktlos und (w[i];ri+1)<(w[t];rt+1)so ist entweder\nw[i]<w[t]und damit auch wi<wtoderw[i] =w[t]undri+1<rt+1und somit ebenfalls\nwi<wt.\nIsti+2taktlos und (w[i];w[i+1];ri+2)<(w[t];w[t+1];rt+2), so argumentiert man\ngenauso.\n2.Es gelte wi<wt. Dann gibt es zwei F\u00e4lle: Ist i+1taktlos, so ist entweder w[i]<w[t]\noder w[i] =w[t]undwi+1<wt+1und somit ri+1<rt+1. Damit hat iVorrang.\nIst hingegen i+2taktlos, so sieht man mit einem \u00e4hnlichen Argument, dass (w[i];w[i+\n1];ri+2)<(w[t];w[t+1];rt+2)und somit iwieder Vorrang hat.\n4-23 4-23 Der DC3-Algorithmus\nSchritt 3: Verschmelzung der Listen\nVerschmelzung der Listen\nVerschmelze die sortierten Listen LundTder taktlosen und der taktvollen Su\ufb03xe wie folgt:\n1M empty list\n2while neither LnorTis empty do\n3 if\ufb01rst element of Lhas precedence (Vorrang )then\n4 remove \ufb01rst element of Land append it toM\n5 else\n6 remove \ufb01rst element of Tand append it toM\n7\n8append remaining elements of LorTtoM444 A&D: Indizieren mit Arrays\n4.2 Der DC3-Algorithmus\n4-24 4-24 Zwei Beispiele f\u00fcr die Verschmelzung in Aktion\nBeispiel\nBuchstaben anfang\nStellen 123456\nTaktloser Rang 3.1. 4.2.\nT: anfang, ang\nL: fang, g, nfang, ng\nM:\nVorrang \u00bbanfang\u00ab an Stelle 1 wegen (a;n;1)<(f;a;4).\nT: ang\nL: fang, g, nfang, ng\nM: anfang\nVorrang \u00bbang\u00ab an Stelle 4 wegen (a;n;2)<(f;a;4).\nT:\nL: fang, g, nfang, ng\nM: anfang, ang\nKopiere den Rest.\nT:\nL:\nM: anfang, ang, fang, g, nfang, ng\nFertig.\nBeispiel\nBuchstaben anfaenger\nStelle 123456789\nTaktloser Rang 4.3. 1.5. 2.6.\nT: aenger, anfaenger, ger\nL: enger, er, faenger, nfaenger, nger, r\nM:\nVorrang \u00bbaenger\u00ab an Stelle 4 wegen (a;1)<(e;5).\nT: anfaenger, ger\nL: enger, er, faenger, nfaenger, nger, r\nM: aenger\nVorrang \u00bbanfaenger\u00ab an Stelle 1 wegen (a;4)<(e;5).\nT: ger\nL: enger, er, faenger, nfaenger, nger, r\nM: aenger, anfaenger\nVorrang \u00bbenger\u00ab an Stelle 5 wegen (e;5)<(g;2).\nT: ger\nL: er, faenger, nfaenger, nger, r\nM: aenger, anfaenger, enger\nVorrang \u00bber\u00ab an Stelle 8 wegen (e;6)<(g;2).\nT: ger\nL: faenger, nfaenger, nger, r\nM: aenger, anfaenger, enger, er\nVorrang \u00bbfaenger\u00ab an Stelle 3 wegen (f;a;1)<(g;e;6).\nT: ger\nL: nfaenger, nger, r\nM: aenger, anfaenger, enger, er, faenger\nVorrang \u00bbger\u00ab an Stelle 7 wegen (g;2)<(n;3).\nT:\nL: nfaenger, nger, r\nM: aenger, anfaenger, enger, er, faenger, ger\nKopiere den Rest.\nT:\nL:\nM: aenger, anfaenger, enger, er, faenger, ger, nfaenger, nger, r4 A&D: Indizieren mit Arrays\nZusammenfassung dieses Kapitels45\nFertig.\n4-25 4-25 Zusammenfassung der Geschwindigkeit des DC3-Algorithmus.\nDie Laufzeit T(n)des Algorithmus bei Worten der L\u00e4nge nerrechnet sich wie folgt:\n1.Die Bestimmung des Strings R1R2im ersten Schritt ben\u00f6tigt Zeit O(n). Die Rekursion\nben\u00f6tigt Zeit T(2\n3n). Die Bestimmung der taktlosen R\u00e4nge ben\u00f6tigt wieder Zeit O(n).\n2.Die Sortierung der taktvollen Su\ufb03xe mittels Radix-Sort ben\u00f6tigt O(n)Zeit.\n3.Die Verschmelzung ben\u00f6tigt O(n)Zeit.\nDies liefert die Rekursionsgleichung\nT(n) =T(2\n3n) +Q(n):\nDas Master-Theorem liefert (wegen Herrschaftsexponent h=log3=2(1) =0und Teilungs-\nexponent t=1), dass T(n) =Q(n)gilt.\nZusammenfassung dieses Kapitels\n4-26 4-26 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationSuchen in langen Texten\nIndizierung\nSu\ufb03x-Arrays\nZeitanalyse\nPlatzanalyse\nSuche nach v\ninO(jvj+logjwj)\nGr\u00f6\u00dfejwjbis4jwjErstellung von Su\ufb03x-Arrays\nTeilen-und-Herrschen\n/d.sc/c.sc/three.taboldstyle-Algorithmus\nMaster-Theorem\nLineare Laufzeit\nISuffix-Array\nDerSu\ufb03x-Array vonwist der Array der Startpositionen der Su\ufb03xe von win lexikographi-\nscher Reihenfolge.\nISatz\nDer Su\ufb03x-Array von wl\u00e4sst sich in Zeit O(jwj)berechnen mittels des /d.sc/c.sc/three.taboldstyle-Algorithmus.\nZum Weiterlesen\n[1]Juha K\u00e4rkk\u00e4inen, Peter Sanders, Stefan Burkhardt, Linear Work Su\ufb03x Array Con-\nstruction, Journal of the ACM , 53(6):918\u2013936, 2006.\nDieser Artikel beweist, dass man nicht immer hochkomplexe Mathematik braucht, um es in das\nJournal of the /a.sc/c.sc/m.sc zu scha\ufb00en \u2013 das \u00bbNature\u00ab oder \u00bbScience\u00ab der Informatik. Der Beweis der\nKorrektheit des in dem Artikel vorgestellten /d.sc/c.sc/three.taboldstyle-Algorithmus passt auf drei (!) Zeilen und eine\nvollst\u00e4ndige (!) Implementation des Algorithmus ist im original C++-Code auf 82 Zeilen ange-\ngeben. Tats\u00e4chlich ist der Artikel erschienen gerade weilder vorgestellte Algorithmus so einfach\nist; insbesondere dann, wenn man ihn mit den komplexen Ideen fr\u00fcherer Verfahren vergleicht.465 A&D: Textkompression\n5-1 5-1\nKapitel 5\nA&D: Textkompression\nFss dch krz nd knpp.\n5-2 5-2Lernziele dieses Kapitels\n1.Zeichenbasierte Kompressionsalgorithmen kennen\n2.Burrows-Wheeler-basierte\nKompressionsalgorithmen kennenInhalte dieses Kapitels\n5.1 Einf\u00fchrung 47\n5.2 Zeichenweise Kompression 48\n5.2.1 Die Idee . . . . . . . . . . . . . . . . . 48\n5.2.2 Hu\ufb00man-Kodierung . . . . . . . . . . . 48\n5.2.3 Move-To-Front-Kodierung . . . . . . . . 49\n5.3 Kompression von Zeichenwiederholungen 50\n5.3.1 Die Idee . . . . . . . . . . . . . . . . . 50\n5.3.2 Run-Length-Kompression . . . . . . . . 50\n5.4 Suffixbasierte Kompression 51\n5.4.1 Die Idee . . . . . . . . . . . . . . . . . 51\n5.4.2 Burrows-Wheeler-Transformation . . . . 51\n5.4.3 Seward-Kompression . . . . . . . . . . 52\n5.5 *W\u00f6rterbuchbasierte Kompression 53\n5.5.1 Die Idee . . . . . . . . . . . . . . . . . 53\n5.5.2 Lempel-Ziv-Welch-Kompression . . . . 53\n\u00dcbungen zu diesem Kapitel 55\nWorum\nes heute\ngehtWorum\nes heute\ngeht\nie Eei, a ee eie eie eua aueie, i i a eu. eiieeie i e i iee iee oioa, ie oae eia euae.\nNicht verstanden? Probieren Sie es mal hiermit:\nD rknntns, dss Txt n gwssn Rdndnz fwsn, st ncht gnz n. Bsplsws st s n vln Schrftsystmn ptnl, d\nVkl nfch wgzlssn.\nDie beiden Textbeispiele lehren uns einiges dar\u00fcber, wie viel Information eigentlich in einem\nText steckt. Wie man sieht, kann man bei vielen Worten einfach die Vokale weglassen und\nversteht noch immer recht gut, was gemeint ist (wie bei \u00bbSchrftsystmn\u00ab). Man k\u00f6nnte auch\nsagen, dass die eigentliche Information \u00bbin den Konsonanten\u00ab steckt. Viele Schriftsysteme\nmachen sich dies zu Nutze und lassen die Vokale in der Regel ganz weg.\nDie Textbeispiele lehren uns auch, dass die normale Art, einen Text zu speichern \u2013 n\u00e4mlich\nals String, wo pro Zeichen ein oder zwei Byte spendiert werden \u2013 nicht besonders e\ufb03zient\nist. Man kann Texte komprimiert speichern, ohne dabei irgendetwas an der Bedeutung zu\n\u00e4ndern.\nDie Welt der Kompressionsalgorithmen ist ausgesprochen gro\u00df. Wenn Sie in Theoretischer\nInformatik gut aufgepasst haben, dann wissen Sie beispielsweise, dass Kolmogorov das Kon-\nzept des ultimativen Kompressionsalgorithmus eingef\u00fchrt hat. Die Kolmogorov-Kompression5 A&D: Textkompression\n5.1 Einf\u00fchrung47\nist beweisbar die beste \u00fcberhaupt m\u00f6gliche \u2013 und leider k\u00f6nnen wir auch beweisen, dass sie\nnicht berechenbar ist (und damit vom praktischen Standpunkt eher weniger geeignet). Es gibt\ndamit ein \u00bbideales Verfahren\u00ab, das wir nie werden erreichen k\u00f6nnen; jedoch kann man sich\nalle paar Jahre einen neuen Algorithmus ausdenken, der ein bisschen n\u00e4her an die Qualit\u00e4t\neiner Kolmogorov-Kompression herankommt.\nSie kennen sicherlich schon das Konzept der Hu\ufb00man-Kodierung, bei der lediglich die Zei-\nchen einzeln m\u00f6glichst kurz aufgeschrieben werden. Jedoch nimmt die Hu\ufb00man-Kodierung\nkeine R\u00fccksicht auf die Reihenfolge der Buchstaben und diese ist sicherlich nicht ganz un-\nwichtig.\nWesentlich besser als die einfache Hu\ufb00man-Kodierung sind w\u00f6rterbuchbasierte Algorith-\nmen wie der von Lempel, Ziv und Welch. Diese bauen w\u00e4hrend sie einen Text lesen eine\nArt \u00bbW\u00f6rterbuch\u00ab auf. Wenn ein Wort dann ein zweites Mal auftaucht, wird es nicht wieder\nkomplett hingeschrieben, sondern nur ein Verweis auf die Stelle im W\u00f6rterbuch vermerkt.\nDies kann, wenn das W\u00f6rterbuch nicht zu gro\u00df ist, sehr platze\ufb03zient sein.\nEine erstaunliche Neuerung in der Welt der Kompressionsalgorithmen war zweifelsohne die\nIdee, eine Burrows-Wheeler-Transformation f\u00fcr die Kompression von Texten zu nutzen. Was\ndas genau ist und wie das genau geht, wird ho\ufb00entlich im Laufe dieses Kapitels klar werden;\nes sei nur jetzt schon erw\u00e4hnt, dass Su\ufb03x-Arrays eine entscheidende Rolle spielen werden.\n5.1 Einf\u00fchrung\n5-4 5-4 Manche Zeichen sind wichtiger als andere.\n.Zur Diskussion\nWelche Zeilen aus einem St\u00fcck verbergen sich hinter den folgenden Zeilen?\nae u, a! iooie,\nuiee u eii,\nU eie au eooie!\nuau ui, i eie e\u00fc.\n.Zur Diskussion\nUnd hinter diesen?\nHb nn, ch! Phlsph,\nJrstr nd Mdcn,\nnd ldr ch Thlg!\nDrchs stdrt, mt h\u00dfm Bmhn.\n5-5 5-5 Die Ziel der Textkompression.\nDie Idee\nZiel einer Textkompression ist es, einen String mitm\u00f6glichst wenig Bits zu speichern.\nDie Formalisierung\nSeiSein Alphabet (wie /a.sc/s.sc/c.sc/i.sc/i.sc oder /u.sc/n.sc/i.sc/c.sc/o.sc/d.sc/e.sc ). Ein Kompressionsverfahren ist ein Paar von\nFunktionen:\n1.DerKompressionsfunktion K:S\u0003!f0;1g\u0003und\n2.derDekompressionsfunktion D:f0;1g\u0003!S\u0003.\nHierbei muss gelten D(K(w)) =wf\u00fcr alle w2S\u0003.\nBeispiele von Programmen, die Kompressionsverfahren implementieren, sind gzip ,bzip2 ,\nrar und viele andere.485 A&D: Textkompression\n5.2 Zeichenweise Kompression\n5-6 5-6 W\u00fcnschenswerte Eigenschaften von Kompressionsverfahren.\n1.Die Bitstrings K(w)sollten m\u00f6glichst kurz sein.\n2.Die Bitstrings K(w)sollten f\u00fcr in der Praxis h\u00e4u\ufb01ge wbesonders kurz sein.\n3.Die Berechnung von Ksollte schnell sein.\n4.Die Berechnung von Dmuss schnell sein.\nBeispiel: Vergleich von Kompressionsverfahren\nOriginaldatei ist faust.txt mit 203.693 Zeichen.\ncompress gzip bzip2\nGr\u00f6\u00dfe komprimierter Faust 88.817 82.922 64.257\nProzent von Originalgr\u00f6\u00dfe 44% 41% 32%\nZeit zum Komprimieren 7ms 25ms 32ms\nZeit zum Dekomprimieren 4ms 4ms 16ms\n5.2 Zeichenweise Kompression\n5.2.1 Die Idee\n5-7 5-7 Zeichenweise Kompression: Die Idee\nEin besonders einfaches Kompressionsverfahren ist die zeichenweise Kompression: F\u00fcr je-\ndes Zeichen des Alphabets w\u00e4hlt man (m\u00f6glichst geschickt) einen Bitstring aus. Dann schreibt\nman f\u00fcr die einzelnen Buchstaben eines Wortes die zugeh\u00f6rigen Bitstrings einfach hinter-\neinander weg. Wenn man die Bitstrings geeignet gew\u00e4hlt hat, kann man aus der Bitfolge\nauch das Ursprungswort eindeutig rekonstruieren.\nBeispiel\nSeiS=fa;b;cgund kodieren wir aals0, weiter bals10und schlie\u00dflich cals11, so ergibt\nsich\nWort Bitstring (komprimiertes Wort)\naa 00\nbb 1010\naabc 001011\n5.2.2 Huffman-Kodierung\n5-8 5-8 Die Methode von Huffman\nBig Idea\nBuchstaben, die in einem Text besonders h\u00e4u\ufb01g vorkommen, sollten durch einen kurzen\nBitstring kodiert werden.\nDie Methode von Huffman\n1.Ermittle f\u00fcr jedes Symbol s2S, wie h\u00e4u\ufb01g es in wvorkommt.\n2.Bilde einen \u00bbWald\u00ab wie folgt: F\u00fcr jedes Symbol s2Sf\u00fcge einen \u00bbBaum\u00ab ein, der nur\naus einem Knoten besteht und dessen Wurzel mit der H\u00e4u\ufb01gkeit beschriftet ist.\n3.Wiederhole nun Folgendes, so lange es noch mehr als einen Baum im Wald gibt:\n3.1Finde zwei B\u00e4ume mit minimalen Beschriftungen an den Wurzeln.\n3.2Fasse diese zu einem Baum zusammen, indem ein neuer Wurzelknoten hinzuge-\nf\u00fcgt wird, der mit der Summe der Beschriftungen beschriftet wird.\n3.3Beschrifte eine der neuen Kanten mit 0und die andere mit 1.\n4.Der Bitstring f\u00fcr ein Zeichen sist nun die Folge der Bits entlang des Wegs von der\nWurzel zu dem Symbol.5 A&D: Textkompression\n5.2 Zeichenweise Kompression49\n5-9 5-9 Beispiel der Berechnung einer Huffman-Kodierung\nSeiS=fa;b;c;d;e;fgundw=a f aadeaadeb .\nWald mit initialen H\u00e4ufigkeiten\n5\na1\nb0\nc2\nd2\ne1\nfMinimal sind cundb\n5\na b c2\nd2\ne1\nf1\n01\nSchritt 3\n5\na b c2\nd2\ne f012\n0\n1Schritt 4\n5\na b c d e f012\n0\n1 4\n01\nSchritt 5\n5\na b c d e f010\n1\n016\n0\n1Schritt 6\na b c d e f010\n1\n010\n111\n0\n1\n1 0000 0001 010 011 001\nK(w) =100111010011110100110000 (plus die Kodierungstabelle)\n5-10 5-10 Wie gut ist die Huffman-Kodierung?\nMan kann zeigen:\nISatz\nUnter allen zeichenweisen Kodierungen eines Wortes, bei denen kein kodierender Bitstring\nPr\u00e4\ufb01x eines anderen ist, liefert die Hu\ufb00man-Kodierung die k\u00fcrzeste Kodierung.\n5.2.3 Move-To-Front-Kodierung\n5-11 5-11 Wie man die H\u00e4ufigkeit von Buchstaben erh\u00f6ht\nDas Problem\n\u2013Betrachten wir das Wort\nababababcdcdcdcd\n\u2013Hier kommen alle Buchstaben gleichh\u00e4u\ufb01g vor , weshalb die Hu\ufb00man-Kodierung nichts\nbringt .\nDas Ziel\n\u2013Wir w\u00fcrden gerne das Wort schreiben als\n12121212\u000312121212\nwobei das Sternchen daf\u00fcr steht \u00bbab hier \u00e4ndern 1und2ihre Bedeutung\u00ab.505 A&D: Textkompression\n5.4 Kompression von Zeichenwiederholungen\n\u2013Jetzt kommen 1und2sehr oft vor, das Sternchen hingegen nur einmal, weshalb sich\ndas Wort gut Hu\ufb00man-kodieren l\u00e4sst.\n5-12 5-12 Die Move-To-Front-Kodierung\nBig Idea\n\u2013Bilde eine Tabelle aller Zeichen vonS.\n\u2013Kodiere ein Zeichen als seine Position in der Tabelle , ...\n\u2013...und bewege es dann an den Anfang der Tabelle.\nBeispiel\nWort Kodierung 123Tabelle\n4\nababcdcd abcd\nababcdcd 1 abcd\nababcdcd 12 bacd\nababcdcd 122 abcd\nababcdcd 1222 bacd\nababc dcd 12223 cbad\nababcd cd122234 dcba\nababcdc d1222342 cdba\nababcdcd 12223422 dcba\n5.3 Kompression von Zeichenwiederholungen\n5.3.1 Die Idee\n5-13 5-13 Die Idee der Kompression von Zeichenwiederholungen\nBig Idea\nDas Wort Toooooooooooooor !l\u00e4sst sich kodieren als To14r!.\nBei einer Kompression von Zeichenwiederholungen wird eine h\u00e4u\ufb01ge Wiederholung eines\neinzelnen Zeichens ersetzt durch die Anzahl der Wiederholungen gefolgt von dem Zeichen.\n5.3.2 Run-Length-Kompression\n5-14 5-14 Varianten von Run-Length-Kompression.\n\u2013Eine Run-Length-Kompression komprimiert Zeichenwiederholungen.\n\u2013Da sich aber viele Zeichen gar nicht wiederholen, muss man auch \u00bbNicht-Wiederholung\u00ab\ne\ufb03zient kodieren k\u00f6nnen.\n\u2013Die Verfahren unterscheiden sich darin, wie dies geschieht.\nVariante 1: Escape-Zeichen\n\u2013Man f\u00fchrt ein spezielles \u00bbEscape\u00ab-Zeichen ein.\n\u2013Nur wenn diese Zeichen kommt, so folgt ein Zeichen und eine Anzahl von Wiederho-\nlungen.\nBeispiel\nAusToooooore !!!!wird T\nESCo6re\nESC!4.\nVariante 2: Sonderzeichen f\u00fcr die Anzahl\n\u2013Man f\u00fchrt zwei neue Symbole ein: Die Sonder-Null ( \u00af0) und die Sonder-Eins ( \u00af1).\n\u2013Folgt einem Zeichen eine Folge von Sonder-Nullen und -Einsen, so gibt diese Folge den\nBin\u00e4rcode der Anzahl der Wiederholung an .\n\u2013Da so eine Folge immer mit einer Sonder-Eins anfangen w\u00fcrde, l\u00e4sst man diese weg.\nBeispiel\nAusToooooor !!!!wird To\u00af1\u00af0re!\u00af0\u00af0, denn \u00af1\u00af0steht f\u00fcr 6=110 2und\u00af0\u00af0steht f\u00fcr 4=100 2.\nDieses Verfahren ist besonders in Verbindung mit einer Hu\ufb00man-Kodierung sinnvoll.5 A&D: Textkompression\n5.4 Suffixbasierte Kompression51\n5.4 Suffixbasierte Kompression\n5.4.1 Die Idee\n5-15 5-15 Suffixbasierte Kompression: Die Idee\nDie Probleme\n\u2013Zeichenbasierte Kodierungen nehmen keine R\u00fccksicht auf die Reihenfolge der unter-\nschiedlichen Zeichen.\nSo ist \u00bbinformatik\u00ab viel wahrscheinlicher als \u00bbamniifktr\u00ab; nach \u00bbanfan\u00ab kommt viel\neher ein \u00bbg\u00ab als ein \u00bbx\u00ab.\n\u2013Bei der Kompression von Wiederholungen muss genau ein Zeichen sich wiederholen.\nSie bringt nichts bei abababababab .\nDie L\u00f6sung\n\u2013Man untersucht das Wort global , indem wir seinen Su\ufb03x-Array berechnen.\n\u2013Mittels des Su\ufb03x-Arrays \u00e4ndern wir die Reihenfolge der Buchstaben des Wortes .\n\u2013Das neue Wort enth\u00e4lt dieselbe Information wie das Ursprungswort und hat dieselbe\nL\u00e4nge, es l\u00e4sst sich aber besser komprimieren .\n5.4.2 Burrows-Wheeler-Transformation\n5-16 5-16 Die Burrows-Wheeler-Transformation wirbelt die Buchstaben eines Wortes\ndurcheinander.\nIDefinition\nSeiw2S\u0003ein Wort. Die Burrows-Wheeler-Transformation vonwist ein um einen Buchsta-\nben l\u00e4ngeres Wort BWT (w), das wie folgt bestimmt werden kann:\n1.Bilde alle Su\ufb03xe von w.\n2.Sortiere diese lexikographisch.\n3.Ersetze jedes Su\ufb03x durch den Buchstaben, der inwdirekt vor diesem Su\ufb03x kommt .\nBeispiel: Die Burrows-Wheeler-Transformation von w=anfang\n1. Die Suffixe\nanfang\nnfang\nfang\nang\nng\ng\ne2. Sortierung\ne\nanfang\nang\nfang\ng\nnfang\nng3. Ersetzung\ng\n^\nf\nn\nn\na\na\nAlso ist BWT (anfang ) =g^fnnaa .\n5-17 5-17 .Zur \u00dcbung\nBerechnen Sie BWT (annanas ).\n5-18 5-18 Erste Beobachtungen: Die Burrows-Wheeler-Transformation l\u00e4sst sich gut\nkomprimieren\n\u2013Ausanfang wird g^fnnaa .\n\u2013Hier kommen nun dieselben Buchstaben mehrfach hintereinander vor, was f\u00fcr eine\nRun-Length-Kompression besonders n\u00fctzlich ist.\n\u2013Dasist kein Zufall , denn wenn ein Teilwort wie \u00bbh\u00e4u\ufb01g\u00ab in einem Text mehrfach vor-\nkommt, so wird vor allen mit \u00bb\u00e4u\ufb01g\u00ab anfangenden Su\ufb03xe (fast) immer ein \u00bbh\u00ab stehen.525 A&D: Textkompression\n5.5 Suffixbasierte Kompression\n5-19 5-19 Zweite Beobachtungen: Die Burrows-Wheeler-Transformation ist umkehrbar.\nHinter der Umkehrung der Burrows-Wheeler-Transformation stecken folgende Ideen:\nBig Idea\n\u2013Man stelle sich die Buchstaben von BWT (w)als Spalte vor.\n\u2013Jede Zeile ist nun der Anfang eines Su\ufb03x.\n\u2013Sortiert man die Zeilen stabil , so sind die Zeilen nun gerade die Anf\u00e4nge der sortierten\nSu\ufb03xe .\n\u2013Daher gibt die Sortierreihenfolge f\u00fcr jedes Su\ufb03x an, welches Su\ufb03x sein Vorg\u00e4nger in\nwist.\n5-20 5-20 Beispiel der Umkehrung der Burrows-Wheeler-Transformation\nDie stabile Sortierung. . .\ng\n^\nf\nn\nn\na\nae\nanfang\nang\nfang\ng\nnfang\nng^\na\na\nf\ng\nn\nnanfang\nnfang\nng\nang\ne\nfang\ng. . . gibt die Vorg\u00e4ngersuffixe an\ng\n^\nf\nn\nn\na\nae\nanfang\nang\nfang\ng\nnfang\nng\n5-21 5-21 Geschwindigkeit der Burrows-Wheeler-Transformation\nISatz\nSowohl die Burrows-Wheeler-Transformation wie auch ihre Umkehrung lassen sich in Zeit\nO(n)berechnen.\nBeweis. Das Wort BWT (w)ergibt sich unmittelbar aus dem Su\ufb03x-Array von w. Dieser\nkann aber mit dem DC3-Algorithmus in Zeit O(n)berechnet werden.\nUm die Umkehrung zu berechnen, muss man die Buchstaben von BWT (w)stabil sortieren,\nwas mittels Radix-Sort in Zeit O(n)m\u00f6glich ist, und danach die ermittelte Permutation\neinmal durchlaufen.\n5.4.3 Seward-Kompression\n5-22 5-22 Die Kompression nach Seward in bzip2 .\nDas Programm bzip2 f\u00fchrt (etwas vereinfacht) folgende Schritte zur Kompression eines\nWortes wdurch:\n1.Berechne die Burrows-Wheeler-Transformation von w.\n2.Kodiere diese neu mittels einer Move-To-Front-Kodierung.\n3.F\u00fchre darauf eine Run-Length-Kompression mittels Sonderzeichen f\u00fcr Anzahlen durch.\n4.F\u00fchre eine Hu\ufb00man-Kodierung durch.\nDie Dekomprimierung invertiert all diese Schritte.5 A&D: Textkompression\n5.5 *W\u00f6rterbuchbasierte Kompression53\n5.5 *W\u00f6rterbuchbasierte Kompression\n5.5.1 Die Idee\n5-23 5-23 W\u00f6rterbuchbasierte Kompression: Die Idee\nBig Idea\nStatt einzelnen Zeichen kodieren wir gr\u00f6\u00dfere Teilworte . Diese Teilworte stehen in einem\nW\u00f6rterbuch . Ein Wort wird dann kodiert als Folge von Positionen der Teilworte in dem W\u00f6r-\nterbuch . Das W\u00f6rterbuch ist nicht fest , sondern wird dynamisch in Abh\u00e4ngigkeit des Textes\ngebaut.\n5.5.2 Lempel-Ziv-Welch-Kompression\n5-24 5-24 Die Methode von Lempel-Ziv-Welch\nDie Methode von Lempel, Ziv und Welch beruht auf zwei Ideen:\nDas W\u00f6rterbuch\nDas W\u00f6rterbuch ist ein dynamisch wachsender Array Avon Worten \u00fcber S(also grob ein\nArray <String >). Anfangs enth\u00e4lt der Array f\u00fcr jedes Symbol genau einen Eintrag.\nDie Kodierungsp\u00e4rchen\nEinKodierungsp\u00e4rchen ist ein Paar (i;s), wobei\n\u2013ider Index eines Wortes A[i]ist und\n\u2013sein Symbol ist.\nDieBedeutung eines solchen P\u00e4rchens ist:\n1.Seine Dekodierung ist A[i]s(also A[i]gefolgt von s).\n2.Das W\u00f6rterbuch wird um den neuen Eintrag A[i]serweitert.\n5-25 5-25 Beispiel eine Lempel-Ziv-Welch-Kompression\nSeiS=fa;b;c;d;e;fgundw=a f aadeaadeb .\nAnfangsw\u00f6rterbuch\ni\nA[i]0\na1\nb2\nc3\nd4\ne5\nf\nWort: afaadeaadeb\nKodierungsp\u00e4rchen: noch keine\na fneu im W\u00f6rterbuch\ni\nA[i]0\na1\nb2\nc3\nd4\ne5\nf6\naf\nWort: afaadeaadeb\nKodierungsp\u00e4rchen: (0;f)\naaneu im W\u00f6rterbuch\ni\nA[i]0\na1\nb2\nc3\nd4\ne5\nf6\naf7\naa\nWort: af aadeaadeb\nKodierungsp\u00e4rchen: (0;f)(0;a)545 A&D: Textkompression\nZusammenfassung dieses Kapitels\ndeneu im W\u00f6rterbuch\ni\nA[i]0\na1\nb2\nc3\nd4\ne5\nf6\naf7\naa8\nde\nWort: afaa deaadeb\nKodierungsp\u00e4rchen: (0;f)(0;a)(3;e)\naadneu im W\u00f6rterbuch\ni\nA[i]0\na1\nb2\nc3\nd4\ne5\nf6\naf7\naa8\nde9\naad\nWort: afaade aadeb\nKodierungsp\u00e4rchen: (0;f)(0;a)(3;e)(7;d)\nebneu im W\u00f6rterbuch\ni\nA[i]0\na1\nb2\nc3\nd4\ne5\nf6\naf7\naa8\nde9\naad10\neb\nWort: afaadeaad eb\nKodierungsp\u00e4rchen: (0;f)(0;a)(3;e)(7;d)(4;b)\nDie Kodierungsp\u00e4rchen werden dann jeweils noch mit m\u00f6glichst wenig Bits kodiert. Bei\n/a.sc/s.sc/c.sc/i.sc/i.sc beispielsweise:dlog2(aktuelle Gr\u00f6\u00dfe von A)e+8Die so kodierten P\u00e4rchen k\u00f6nnen\ndann einfach hintereinanderweg stehen.\nZusammenfassung dieses Kapitels\n5-26 5-26 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationKompression von Texten\nEntropiebasierte Kodierung\nHu\ufb00man-Kodierung\nHu\ufb00man-Baum\nWorst-Case-Analyse\nOptimale Codel\u00e4nge unter\npr\u00e4\ufb01xfreien KodierungenKompression von Texten\nText-Transformation\nSeward-Kodierung\nHeuristische Analyse\nHeuristisch gute Kompressionsrate\nIHuffman-Kodierung\nErsetze jedes Zeichen eines Textes durch die Bits entlang des Pfades im Hu\ufb00man-Baum.\nIMove-To-Front-Kodierung\nErsetze jedes Zeichen eines Textes durch seine Position in einer Tabelle und bewege es an\nden Anfang der Tabelle.\nIKompression von Zeichenwiederholungen (Run-Length-Kompression)\nF\u00fchre neue Zeichen \u00af0und \u00af1ein und schreibe statt den Wiederholungen nur deren Anzahl\nbin\u00e4r mittels \u00af0und\u00af1auf (lasse die f\u00fchrende \u00af1weg).5 A&D: Textkompression\n5.6 \u00dcbungen zu diesem Kapitel55\nIBurrows-Wheeler-Transformation\nBilde den Su\ufb03x-Array eines Wortes wund ersetze jedes Su\ufb03x durch das Zeichen, das in w\nvor diesem Su\ufb03x kommt.\nISeward-Kodierung (bzip2)\nBei Eingabe wtue:\n1.Wende die Burrows-Wheeler-Transformation an.\n2.Wende Move-To-Front an.\n3.Wende eine Run-Length-Kompression an.\n4.Wende die Hu\ufb00man-Methode an.\n(Tats\u00e4chlich f\u00fchrt bzip2 noch eine \u00fcber\ufb02\u00fcssige Run-Length-Kompression vorneweg aus\nund benutzt mehrere Hu\ufb00man-B\u00e4ume gleichzeitig.)\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 5.1 L\u00e4nge einer Run-Length-Kompression, mittel\nDas Wort whabe die Form anbmcpdq, wobei a;b;c;d2Sundn;m;p;q\u00152gelte.\n1.Wie lang ist die Run-Length-Kompression von wbei der Variante mit Sonder-Null und Sonder-\nEins (das Alphabet ist also S=fa;b;c;d;\u00af0;\u00af1g?\n2.Wie viele Bits ben\u00f6tigt die Hu\ufb00man-Kodierung (ohne Kodierungstabelle) dieser Run-Length-\nKompression, wenn die Sonder-Null und die Sonder-Eins \u00e4hnlich oft vorkommen?\n3.Wie viele Bits ben\u00f6tigt die Hu\ufb00man-Kodierung, wenn die Zahlen n,m,pundqalles Zweier-\nPotenzen sind?\n\u00dcbung 5.2 Effekt einer Move-To-Front-Kodierung, einfach\nDas Wort whabe die Form anbmcpaq, wobei a;b;c2Sundn;m;p;q\u00152gelte. Geben Sie die Move-\nTo-Front-Kodierung von wan.\nIn den folgenden \u00dcbungen geht es um die Frage, wie gut verschiedene Kompressionsverfahren Wor-\nte komprimieren, die nur aus Wiederholungen eines kurzen Wortes bestehen. Beispielsweise besteht\nababababab aus f\u00fcnf Wiederholungen von ab. Formal geht es also um Worte wder Form un(das be-\ndeutet \u00bb wbesteht aus nvielen Wiederholungen von u\u00ab), wobei die L\u00e4nge mvonumindestens 2sein\nsoll und alle Zeichen von usollen unterschiedlich sein. Das Wort what also L\u00e4nge mnund wir stellen\nuns vor, dass nviel gr\u00f6\u00dfer als mist.\n\u00dcbung 5.3 Burrows-Wheeler-Transformation sich wiederholender Worte, mittel\nSeiw=unmitjuj=m>1und alle Buchstaben in useien unterschiedlich. Geben Sie die Burrows-\nWheeler-Transformation von wzun\u00e4chst f\u00fcr ein Beispiel und dann allgemein an.\nTipp: F\u00fchren Sie eine Bezeichnung f\u00fcr die Liste der in uvorkommenden Buchstaben in sortierter\nReihenfolge ein.\n\u00dcbung 5.4 Kodierungsl\u00e4nge sich wiederholender Worte, schwer\nSeiw=unmitjuj=m>1und alle Buchstaben in useien unterschiedlich.\n1.Wie viele Bits (ohne Kodierungstabelle) ben\u00f6tigt die Hu\ufb00man-Kodierung von w?\n2.Wie viele Bits (hier gen\u00fcgt die O-Klasse) ben\u00f6tigt die Seward-Kompression von w? Benutzen\nSie dazu die Ergebnisse von \u00dcbungen 5.3, dann von 5.2 und dann von 5.1.\n3.\u00dcberpr\u00fcfen Sie Ihr Ergebnis, indem Sie eine Datei, die aus einer Million mal abbesteht, mittels\nbzip2 komprimieren.56Teil III\nOnline-Probleme\nTeil III\nOnline-Probleme\nUm ein m\u00f6gliches Missverst\u00e4ndnis gleich von vornherein auszur\u00e4umen: Das \u00bbOnline\u00ab in\n\u00bbOnline-Probleme\u00ab hat nicht mit \u00bbonline\u00ab zu tun. Wenn heutzutage von \u00bbonline\u00ab die Rede\nist, dann denken wir unwillk\u00fcrlich an Internet-Zug\u00e4nge, das World-Wide-Web oder soziale\nNetzwerke wie Facebook. \u00dcber die Jahre erweitert sich der Begri\ufb00 immer weiter \u2013 aus dem\nWeb ist erst das Web 2.0 geworden und nun gerade das Web 3.0; das Wort \u00bbFacebook\u00ab kennt\nmeine Rechtschreibkorrektur nicht, in ein paar Jahren ist das sicherlich anders \u2013 jedoch denkt\nkaum jemand bei dem Begri\ufb00 \u00bbonline\u00ab an das, worum es in diesem Teil der Veranstaltung\ngehen soll: Um eine temporale Eigenschaft von Eingabedaten. Insofern ist die Bezeichnung\nsicherlich nicht ganz gl\u00fccklich gew\u00e4hlt (wie so viele Bezeichnungen in der Theoretischen\nInformatik), jedoch muss zu ihrer Verteidigung gesagt werden, \u00bbdass sie zuerst da war\u00ab:\nOnline-Probleme wurden bereits von Theoretikern untersucht als Tim Berners-Lee noch an\nseinem Enquire-Programm herumbastelte und noch niemand \u00bbonline war\u00ab.\nBei Online-Problemen geht es darum, dass am Anfang der Berechnung die Eingaben noch\ngar nicht vorliegen. Vielmehr muss man, ganz wie im richtigen Leben, Entscheidungen tref-\nfen aufgrund der bisher vorliegenden Informationen und dann ho\ufb00en, dass die sp\u00e4ter ein-\ntre\ufb00enden Daten diese Entscheidungen rechtfertigen. Wenn einige Entscheidungen getro\ufb00en\nwurden, kommen zus\u00e4tzliche Daten, so dass man sich wieder entscheiden muss, und so fort.\nEin klassisches Beispiel eines Online-Problems ist das Caching-Problem: Bekannterma\u00dfen\nist im Hauptspeicher eines Rechners nicht genug Platz, um den unglaublichen Bedarf an\nvirtuellem Speicher von allen Prozessen eines Systems zu befriedigen. Die L\u00f6sung ist, dass\neben nur ein kleiner Teil des virtuellen Speichers real im Hauptspeicher liegt. Solang der\nProzessor nur auf dieses Teil zugreift, ist alles prima. Wenn aber dann ein Datum aus dem\nvirtuellen Speicher ben\u00f6tigt wird, das noch nicht im Hauptspeicher liegt, so muss dieses\ngeladen werden und daf\u00fcr ein anderes Datum im Hauptspeicher \u00fcberschrieben werden. Die\nspannende Frage ist nun, welches Datum man \u00fcberschreiben sollte. Idealerweise nat\u00fcrlich\neines, das nie wieder oder zumindest m\u00f6glichst lange nicht mehr gebraucht werden wird.\nGenau dies ist aber eine temporale Eigenschaft der Daten und wir wissen leider nicht, welche\nDaten in der Zukunft gebraucht werden und welche nicht.\nNaturgem\u00e4\u00df wird das Caching-Problem erst dadurch wirklich schwierig, dass eben nicht\nbekannt ist, welche Daten in der Zukunft ben\u00f6tigt werden. W\u00fcsste man dies (man spricht\ndann von der \u00bbO\ufb04ine-Variante\u00ab), so ist die Sache wesentlich einfacher und man k\u00f6nnte auch\nbessere Ergebnisse erzielen. Da Theoretiker, insbesondere Komplexit\u00e4tstheoretiker, gerne\nalles und jedes vermessen, w\u00fcssten sie auch gerne, um wie viel genau die O\ufb04ine-Variante\nleichter ist als die Online-Variante. Es w\u00fcrde nahe liegen, den Faktor, um den ein O\ufb04ine-\nVerfahren besser ist als ein Online-Verfahren vielleicht den \u00bbO\ufb04ine-Faktor\u00ab oder auch den\n\u00bbOnline-Faktor\u00ab zu nennen \u2013 jedoch ist die o\ufb03zielle Bezeichnung \u00bbkompetitive Rate\u00ab. Wie\nbereits erw\u00e4hnt: Theoretiker sind nicht besonders gut darin, Namen zu vergeben.6 Klassifikation: Kompetitive Rate57\n6-1 6-1\nKapitel 6\nKlassifikation: Kompetitive Rate\nWas bringt es, wenn man in die Zukunft schauen kann?\n6-2 6-2Lernziele dieses Kapitels\n1.Varianten von Scheduling- und Caching-Problemen\nkennen\n2.Greedy-Verfahren zu deren L\u00f6sung kennen\n3.Konzept des Online-Algorithmus verstehen\n4.Konzept der kompetitiven Rate verstehen\n5.Kompetitive Raten von Algorithmen bestimmen\nk\u00f6nnenInhalte dieses Kapitels\n6.1 Einf\u00fchrung zu Online-Algorithmen 58\n6.1.1 Das Ski-Leihen-Problem . . . . . . . . . 58\n6.1.2 Klassi\ufb01kation: Kompetitive Rate . . . . 59\n6.2 Caching 60\n6.2.1 O\ufb04ine-Strategien . . . . . . . . . . . . 61\n6.2.2 Online-Strategien . . . . . . . . . . . . 63\n6.2.3 Kompetitive Rate . . . . . . . . . . . . 64\n6.3 Scheduling 64\n6.3.1 O\ufb04ine-Strategien . . . . . . . . . . . . 65\n6.3.2 Online-Strategien . . . . . . . . . . . . 65\n6.3.3 Kompetitive Rate . . . . . . . . . . . . 66\n\u00dcbungen zu diesem Kapitel 68\nWorum\nes heute\ngehtWorum\nes heute\ngehtKann man in die Zukunft sehen? Es gibt zumindest gen\u00fcgend Leute, die behaupten, dass sie\nes k\u00f6nnen (Nostradamus, Wahrsager, B\u00f6rsenh\u00e4ndler). Selbst die seri\u00f6se Wissenschaft hat\nsich dieser Frage bereits angenommen: Seit geraumer Zeit versucht man mittels so genannter\nPsi-Experimente zu kl\u00e4ren, ob wir Menschen die Gabe haben, wenigsten eine bisschen in\ndie Zukunft zu sehen.\nEin besonders interessantes Ergebnis hierzu stammt aus dem Jahr /two.taboldstyle/zero.taboldstyle/one.taboldstyle/zero.taboldstyle von Daryl Bem,\nder einen Artikel mit dem Titel Feeling the Future: Experimental Evidence for Anomalous\nRetroactive In\ufb02uence on Cognition and A\ufb00ect, erschienen im Journal of Personality and So-\ncial Psychology, ver\u00f6\ufb00entlicht hat. Die para-wissenschaftliche Literatur ist voll von solchen\nUntersuchungen; das Besondere an dieser Arbeit ist, dass sie in einer anerkannten wissen-\nschaftlichen Fachzeitschrift erschienen ist und dass sie den \u00fcblichen Standards gen\u00fcgt, die\nan psychologische Experimente gestellt werden.\nHier die Ergebnisse leicht (aber wirklich nur leicht) verk\u00fcrzt zusammengefasst: Bei bin\u00e4ren\nEntscheidungsexperimente wurde ein statistisch signi\ufb01kanter E\ufb00ekt gemessen, nach dem\ninsbesondere M\u00e4nner etwas in die Zukunft sehen k\u00f6nnen, wenn man ihnen als Belohnung\npornographische Bilder in der nahen Zukunft zeigt.\nWie man sich vorstellen kann, hat dieses Ergebnis neben einiger Belustigung auch eine Men-\nge an wissenschaftlichen Einw\u00e4nden hervorgebracht. Die Quintessenz ist, dass die statisti-\nschen Methoden, die in dem Artikel genutzt wurden, um die vermeintliche Vorhersage der\nZukunft zu beweisen, schlecht sind. Jedoch sind dies genau dieselben Methoden, die auch in\nvielen anderen Kontexten genutzt werden, beispielsweise wenn es um das Risiko des Passiv-\nRauchens geht. Der Artikel beweist damit eigentlich nur, dass man solche Methoden viel\nkritischer sehen muss als dies \u00fcblich ist. Womit sich wieder der alte Spruch bewahrheitet:\n\u00bbTraue keiner Statistik, die du nicht selbst gef\u00e4lscht hast.\u00ab\nWas hat das aber alles mit Computern zu tun? In diesem Kapitel geht es um die Frage,586 Klassifikation: Kompetitive Rate\n6.1 Einf\u00fchrung zu Online-Algorithmen\nwas es Computern helfen w\u00fcrde, wenn sie in die Zukunft sehen k\u00f6nnten. Wir untersuchen\ndazu Online-Algorithmen , welche immer wieder Entscheidungen f\u00e4llen m\u00fcssen, die sich in\nder Zukunft als eher gut oder doch als weniger gut herausstellen. Ein klassisches Beispiel\nsind die von modernen Betriebssystemen genutzten Caching-Algorithmen, die sich im Falle\neines vollgelaufenen Hauptspeichers \u00fcberlegen m\u00fcssen, welche Teile des Hauptspeichers sie\nauf die Festplatte auslagern sollten. Idealerweise sollte man nat\u00fcrlich nicht genau die Teile\nauslagern, die man gleich wieder braucht; aber daf\u00fcr m\u00fcsste man eben in die Zukunft sehen\nk\u00f6nnen. Um wie viel besser Cache-Algorithmen w\u00fcrden, wenn sie dies k\u00f6nnten, wird durch\ndiekompetitive Rate gemessen.\nWas lehren uns die Untersuchungen von Bem in Bezug auf die kompetitive Rate von Caching-\nAlgorithmen? Vielleicht k\u00f6nnte man Caching-Algorithmen mit einer besseren kompetitiven\nRate bauen, wenn man ihnen eine \u00bbgeeignete Belohnung\u00ab bietet. An pornographischem Ma-\nterial herrscht im Internet ja bekanntlich kein Mangel. Zu kl\u00e4ren w\u00e4re allerdings, worauf\nCaching-Algorithmen wirklich stehen.\n6.1 Einf\u00fchrung zu Online-Algorithmen\n6.1.1 Das Ski-Leihen-Problem\n6-4 6-4 Fahren Sie gerne Ski?\nCreative Commons Attribution Sharealike LicenceDas Ski-Leihen-Problem\nSie beschlie\u00dfen, diesen Winter mal wieder Ski zu fahren. Die Skier aus dem letzten Jahr\nk\u00f6nnen Sie nicht wieder nutzen, da diese vollkommen unmodisch sind. Es stellt sich die\nFrage, ob Sie f\u00fcr diese Saison neue Skier kaufen oder leihen sollten: Sie zu kaufen kostet\nSie einmalig 200 Euro, sie zu leihen kostet Sie 10 Euro am Tag. Was sollten Sie tun?\n6-5 6-5 Das Ski-Leihen-Problem als Online-Problem.\nIDefinition: Online-Problem\nBei einem Online-Problem erh\u00e4lt ein Algorithmus schrittweise Zugri\ufb00 auf die Elemente\neiner Folge von Ereignissen . Nach jedem Ereignis muss der Algorithmus eine (oder mehrere)\nEntscheidungen tre\ufb00en. Das Problem kann Bedingungen angeben, wann die Entscheidungen\nkorrekt sind (wenn auch nicht unbedingt \u00bbgut\u00ab). Eine Ma\u00dffunktion gibt am Ende an, wie gut\ndie Entscheidungen waren.\nBeispiel: Das Ski-Leihen-Problem als Online-Problem\nEreignisse \u00bbHeute brauche ich Skier\u00ab und \u00bbSaison zu Ende\u00ab.\nEntscheidungen \u00bbF\u00fcr einen Tag leihen\u00ab, \u00bbkaufen\u00ab oder \u00bbnichts tun\u00ab.\nBedingungen 1.Das \u00bbSaison zu Ende\u00ab Ereignis kommt genau einmal am Ende.\n2.Die Entscheidung \u00bbnichts tun\u00ab ist erst nach einer \u00bbkaufen\u00ab Entscheidung\nm\u00f6glich.\nMa\u00df Die Anzahl der \u00bbF\u00fcr einen Tag leihen\u00ab Entscheidungen mal Lplus, falls einmal\ndie Entscheidung \u00bbkaufen\u00ab gef\u00e4llt wurde, K.\n6-6 6-6 M\u00f6gliche Online-Algorithmen f\u00fcr das Ski-Leihen-Problem.\nStrategie A (\u00bbreicher Macker\u00ab)\nKaufe gleich am ersten Tag.\nStrategie B (\u00bbkurz probieren\u00ab)\nLeihe die ersten 5 Tage und kaufe am 6. Tag, wenn bis dahin die Saison nicht zu Ende ist.\nStrategie C (\u00bbl\u00e4nger probieren\u00ab)\nLeihe die ersten 19 Tage und kaufe am 20. Tag, wenn bis dahin die Saison nicht zu Ende ist.\nStrategie D (\u00bbsehr lange probieren\u00ab)\nLeihe die ersten 39 Tage und kaufe am 40. Tag, wenn bis dahin die Saison nicht zu Ende ist.\nStrategie E (\u00bbarmer Studie\u00ab)\nLeihe immer.6 Klassifikation: Kompetitive Rate\n6.2 Einf\u00fchrung zu Online-Algorithmen59\n6.1.2 Klassifikation: Kompetitive Rate\n6-7 6-7 Wie gut sind die Strategien?\nO\ufb00enbar haben die Strategien alle Vor- und Nachteile. Bei der Strategie des reichen Mackers\nkann es sein, dass er 200 Euro ausgibt, obwohl die Saison nur einen Tag hat und 10 Euro\n(f\u00fcr einmal Leihen) ausgereicht h\u00e4tte. Er gibt also zwanzig Mal mehr aus als n\u00f6tig . Bei der\nStrategie des armen Studies kann es sein, dass er beliebig viel ausgibt (sehr lange Saison),\nobwohl 200 Euro ausgereicht h\u00e4tten.\n.Zur \u00dcbung\nBestimmen Sie f\u00fcr jede der Strategien, \u00bbwie schlimm es werden kann\u00ab: Um wie viele Mal\nh\u00f6her k\u00f6nnen die Ausgaben gem\u00e4\u00df dieser Strategien sein als wirklich n\u00f6tig?\nF\u00fcr die Strategie \u00bbreicher Macker\u00ab ist der Faktor also 20, f\u00fcr die Strategie \u00bbarmer Studie\u00ab\nsogar1.\n6-8 6-8 Die kompetitive Rate.\nDas Verh\u00e4ltnis von \u00bbKosten gem\u00e4\u00df der Online-Strategie\u00ab versus \u00bbKosten, wenn man die\nEreignisse schon vorher alle kennen w\u00fcrde\u00ab nennt man kompetitive Rate . Der Name r\u00fchrt\ndaher, dass man die Online-Strategie gegen die beste O\ufb04ine-Strategie \u00bbantreten l\u00e4sst\u00ab.\nIDefinition: Kompetitive Rate\nGegeben seien ein Online-Problem und ein Online-Algorithmus hierf\u00fcr. Die kompetitive\nRate des Algorithmus ist das Maximum \u00fcber alle Eingaben von\nMa\u00df der vom Online-Algorithmus gefundenen L\u00f6sung\nMa\u00df der optimalen, von einem O\ufb04ine-Algorithmus gefundenen L\u00f6sung\noder der Kehrwert (bei Maximierungsproblemen).\nDiekompetitive Rate des Online-Problems ist die kompetitive Rate des besten Online-Algorithmus\nf\u00fcr das Problem.\n6-9 6-9 Die kompetitive Rate des Ski-Leihen-Problems.\nAlle m\u00f6glichen Online-Strategien f\u00fcr das Ski-Leihen-Problem sind von der Form: \u00bbLeihe\ndie ersten xTage und kaufe am Tag x+1.\u00ab Die kompetitive Rate einer solchen Strategie ist\nxL+K\nminf(x+1)L;Kg=maxnxL+K\nxL+L;xL+K\nKo\n;\ndenn der \u00bbWorst-Case\u00ab tritt immer dann ein, wenn gerade am Tag x+1der letzte Tag der\nSaison ist: Hier hat die Online-Strategie xL+Kausgegeben, ideal w\u00e4re es aber gewesen,\nentweder x+1Mal zu leihen oder gleich am Anfang zu kaufen \u2013 je nachdem, was billiger\ngewesen w\u00e4re.\nDie kompetitive Rate nimmt ihr Minimum ein, wenn die Br\u00fcche gleich sind, also f\u00fcr x=\nK=L\u00001. Hier ist die kompetitive Rate gerade 2\u0000L=K.\nISatz\nDie kompetitive Rate des Ski-Leihen-Problems ist 2\u0000L=K.\nMit anderen Worten:\n\u2013Die Strategie C (19 Tage leihen, dann kaufen) hat die beste kompetitive Rate.\n\u2013Allgemein hat die Strategie die beste kompetitive Rate, bei der man solange leiht, bis\nman so viel Geld ausgegeben h\u00e4tte, als h\u00e4tte man gleich gekauft. Dann sollte man aber\nkaufen.\n\u2013Dadurch gibt man h\u00f6chstens doppelt so viel aus wie n\u00f6tig.Mal sehen, ob Sie die\nBedeutung der kompeti-\ntiven Rate von 2 \u2013 L/K\nverstanden haben. Wel-\nche Strategie werden\nSie in Zukunft nutzen?\nJetzt die Strategie\n\u201earmer Student\u201c. Wenn\nich aus der Uni raus bin\ndann \u201ereicher Macker\u201c.\nSeufz.606 Klassifikation: Kompetitive Rate\n6.2 Caching\n6.2 Caching\n6-11 6-11 Caching: Virtueller versus realer Speicher.\nAuthor Till Tantau\n6-12 6-12 Caching l\u00e4sst sich als Online-Problem darstellen\nDas formale Caching-Problem\nGegeben sei ein Cache der Gr\u00f6\u00dfe k.\nEreignisse \u00bbrequest (x)\u00ab f\u00fcr Seitennummern x\nEntscheidungen Entweder \u00bb cached \u00ab oder \u00bb evict (y)\u00ab und/oder \u00bb load(x)\u00ab.\nBedingungen Zu jedem Schritt ienth\u00e4lt der Cache Cieine Menge von Seitennummern. Hier-\nf\u00fcr gilt:\n1.Zu Anfang enth\u00e4lt der Cache keine Elemente.\n2.Falls die Entscheidung cached ist, so muss x2Cigelten und Ci+1=Ci.\n3.Falls die Entscheidung load(x)ist, so ist Ci+1=Ci[fxg.\n4.Falls die Entscheidung evict (y)ist, so ist Ci+1=Cinfyg.\n5.F\u00fcr alle Schritte gilt jCij\u0014k.\nMa\u00df Anzahl der Load-Entscheidungen.6 Klassifikation: Kompetitive Rate\n6.2 Caching61\n6-13 6-13 Ein Beispiel des Ablaufs eines Caching-Algorithmus.\n1\nleer\nleer\nleerEreignis\nrequest (8)2\n8\nleer\nleerEntscheidung\nload(8)3\n8\nleer\nleerEreignis\nrequest (5)4\n8\n5\nleerEntscheidung\nload(5)\n5\n8\n5\nleerEreignis\nrequest (6)6\n8\n5\n6Entscheidung\nload(6)7\n8\n5\n6Ereignis\nrequest (8)8\n8\n5\n6Entscheidung\ncached\n9\n8\n5\n6Ereignis\nrequest (6)10\n8\n5\n6Entscheidung\ncached11\n8\n5\n6Ereignis\nrequest (7)12\nleer\n5\n6Entscheidung\nevict (8)\n13\n7\n5\n6Entscheidung\nload(7)14\n7\n5\n6Ereignis\nrequest (8)15\nleer\n5\n6Entscheidung\nevict (7)16\n8\n5\n6Entscheidung\nload(8)\n6.2.1 Offline-Strategien\n6-14 6-14 Ein Offline-Algorithmus f\u00fcr das Caching: Evict-Furthest-In-Future.\nNehmen wir zun\u00e4chst an, dass wir die Zukunft kennen . Wenn nun der Cache voll ist, welchen\nEintrag sollte man ersetzen? Sicher gilt: Wird ein Cache-Eintrag nie wieder verlangt , so kann\ner ersetzt werden. Sicherlich gilt: Eintr\u00e4ge, die \u00bblange Zeit\u00ab nicht mehr gebraucht werden,\nsollten eher ersetzt werden als solche, die \u00bbbald gebraucht werden\u00ab.\n1algorithm evict -furthest -in-future (request (x),C)\n2 ifx2Cthen\n3 return \u00bbcached \u00ab\n4 else ifjCj<kthen\n5 return \u00bbload(x)\u00ab\n6 else\n7 bestimme ein y2C,so dass der Zeitpunkt des n \u00e4chsten request (y)maximal ist\n8 return \u00bbevict (y);load(x)\u00ab626 Klassifikation: Kompetitive Rate\n6.2 Caching\n6-15 6-15 Besser geht\u2019s nicht.\nISatz\nDer Algorithmus Evict-Furthest-In-Future ist ein optimaler O\ufb04ine-Algorithmus f\u00fcr das Caching-\nProblem.\nKommentare zum Beweis\nBeweis. Man beweist diesen Satz mittels eines Exchange-Arguments . Dies funktioniert wie\nfolgt:\n1.Statt der vom Algorithmus erzeugten Liste von Entscheidungen betrachten wir eine\nbeliebige andere . Intuitiv wird diese Liste \u00bbnicht ganz optimal\u00ab sein.\n2.Wir betrachten dann, was passiert, wenn man diese Liste ver\u00e4ndert , indem man \u00bbeine\nnicht optimale Stelle\u00ab gegen \u00bbeine optimale\u00ab austauscht (daher der Name \u00bbExchange-\nArgument).\n3.Entscheidend ist, dass das Ma\u00df der modi\ufb01zierten Liste gleich oder besser als das Ma\u00df\nder urspr\u00fcnglichen Liste ist.\nWiederholt man das Argument oft genug, so erh\u00e4lt man im Ergebnis die vom Evict-Furthest-\nIn-Future-Algorithmus erzeugt Liste, deren Ma\u00df folglich nicht schlechter ist als das einer\nbeliebigen anderen Liste .\nDas schwierigere und entscheidende Argument ist, dass Evict-Entscheidungen immer so wie\nvom Algorithmus gef\u00e4llt werden sollten. Dies sieht man so:\nNehmen wir an, f\u00fcr ein evict (y)wurde y2Cinicht nach der Regel des Algorithmus gew\u00e4hlt.\nEs gibt also ein y02Ci, so dass der n\u00e4chste request (y)fr\u00fcher kommt als request (y0).1 1Hier ist die Folge \u00bbnicht optimal\u00ab.\nBetrachten wir nun die neue Folge, die entsteht, wenn man die Entscheidungsfolge wie folgt\nmodi\ufb01ziert:2 2Der \u00bbExchange\u00ab-Schritt.\n1.Die Entscheidung evict (y)wird ersetzt durch evict (y0).\n2.Die n\u00e4chste load(y)Entscheidung wird ersetzt durch load(y0).\nDann gilt:\n1.Zwischen den beiden Entscheidungen wird nicht auf y0zugegri\ufb00en.\n2.Nach der (neuen) load(y0)Entscheidung ist der Cache-Inhalt derselbe wie urspr\u00fcnglich.\nWir fassen zusammen: Wiederholt man die obigen Argumente so lange wie m\u00f6glich, so\nerh\u00e4lt man eine Folge, in der alle evict (y)immer dasjenige yausw\u00e4hlen, das am l\u00e4ngsten\nnicht mehr zugegri\ufb00en werden wird, und das Ma\u00df der entstandenen Folge ist nicht gr\u00f6\u00dfer\nals das Ma\u00df der Ursprungsfolge.\nDa man auch leicht sieht, dass Load-Entscheidungen nur unmittelbar nach einem zugeh\u00f6-\nrigen Request gef\u00e4llt werden brauchen und Evict-Entscheidungen nur gef\u00e4llt werden brau-\nchen, wenn der Cache voll ist, folgt die Behauptung.\n6-16 6-16 Das Exchange-Argument an einem Beispiel.\nEine Originalliste\n1\n2\n3\nevict (1)\u2013\n2\n3\nload(4)4\n2\n3\nevict (4)\u2013\n2\n3\nload(1)1\n2\n3\ncached1\n2\n3\ncached1\n2\n3\ncached1\n2\n3\n:::request (4)\nrequest (1)\nrequest (2)\nrequest (1)\nrequest (2)6 Klassifikation: Kompetitive Rate\n6.2 Caching63\nDie erste modifizierte Liste: Exchange-Argument f\u00fcr 1 und 3\n1\n2\n3\nevict (3)1\n2\n\u2013\nload(4)1\n2\n4\nevict (4)1\n2\n\u2013\nload(3)1\n2\n3\ncached1\n2\n3\ncached1\n2\n3\ncached1\n2\n3\n:::request (4)\nrequest (1)\nrequest (2)\nrequest (1)\nrequest (2)\nDie zweite modifizierte Liste: load(3)so sp\u00e4t wie m\u00f6glich.\n1\n2\n3\nevict (3)1\n2\n\u2013\nload(4)1\n2\n4\nevict (4)1\n2\n\u2013\nnop1\n2\n\u2013\ncached1\n2\n\u2013\ncached1\n2\n\u2013\ncached1\n2\n\u2013\n:::request (4)\nrequest (1)\nrequest (2)\nrequest (1)\nrequest (2)\nDie dritte modifizierte Liste: evict (4)\u00fcberfl\u00fcssig\n1\n2\n3\nevict (3)1\n2\n\u2013\nload(4)1\n2\n4\nnop1\n2\n4\nnop1\n2\n4\ncached1\n2\n4\ncached1\n2\n4\ncached1\n2\n4\n:::request (4)\nrequest (1)\nrequest (2)\nrequest (1)\nrequest (2)\n6.2.2 Online-Strategien\n6-17 6-17 Online-Strategien: Wenn man die Zukunft nicht kennt. . .\n\u00bbIn der Wirklichkeit\u00ab ist die Zukunft nat\u00fcrlich nicht bekannt (siehe allerdings die Literatur-\nhinweise). Ein Online-Algorithmus f\u00fcr das Caching-Problem muss folglich eine \u00bbPrognose\u00ab\ndurchf\u00fchren, welche Elemente in der Zukunft selten ben\u00f6tigt werden:\nEvict-Least-Recently-Used-Strategie\nImmer, wenn der Cache voll ist, entscheide evict (y)f\u00fcr dasjenige y, f\u00fcr das am l\u00e4ngsten kein\nRequest-Ereignis vorlag.\n.Zur Diskussion\nDie \u00bbHo\ufb00nung\u00ab bei der /e.sc/l.sc/r.sc/u.sc -Strategie ist, dass schon lange nicht mehr genutzte yauch in\nZukunft nicht bald ben\u00f6tigt werden.\nWie sieht eine Folge von Request-Ereignissen aus, bei der diese Strategie besonders schlecht\narbeitet?646 Klassifikation: Kompetitive Rate\n6.3 Scheduling\n6.2.3 Kompetitive Rate\n6-18 6-18 Die kompetitive Rate von ELRU .\nISatz\nDie Evict-Least-Recently-Used-Strategie ist k-kompetitiv (das hei\u00dft, ihrer kompetitive Rate\nist h\u00f6chstens k) bei einer Cache-Gr\u00f6\u00dfe von k.\nBeweis. Man betrachte eine beliebige Eingabe von Ereignissen und nun kaufeinander fol-\ngende Evict-Entscheidungen des /e.sc/l.sc/r.sc/u.sc -Algorithmus. Dann gilt:\n1.Zwischen der ersten und letzten dieser Evict-Entscheidungen m\u00fcssen Request-Ereignisse\nf\u00fcrkunterschiedliche Element vorgekommen sein.\n2.Folglich muss jede andere Strategie hier auch mindestens eine Evict-Entscheidung f\u00e4l-\nlen.\n6-19 6-19 Es geht nicht besser als mit ELRU .\nISatz\nJeder Online-Algorithmus f\u00fcr das Cache-Probleme hat eine kompetitive Rate von minde-\nstens k. Das Problem selbst hat also die kompetitive Rate k.\nBeweis. Dies zeigt man mit einem Adversary-Argument : Ein Adversary (Gegenspieler) be-\ntrachtet, welche Entscheidungen ein gegebener Online-Algorithmus bei einer Eingabefol-\nge f\u00e4llt. Nach jeder Entscheidung evict (y)pr\u00e4sentiert der Adversary gerade request (y)als\nn\u00e4chstes Ereignis, was den Algorithmus zwingt, in jedem Schritt eine Evict-Entscheidung\nzu f\u00e4llen. Der Evict-Furthest-In-Future-Algorithmus braucht hingegen nur jedes k-te Mal ei-\nne Evict-Entscheidung f\u00e4llen (denn k\u00001aufeinanderfolgende Request-Ereignisse beziehen\nsich immer auf Elemente, die schon im Cache sind).\n6.3 Scheduling\n6-20 6-20 Worum es beim Scheduling geht.\nAuthor Turelio, Creative Commons Attribution Sharealike LicenceBeim Scheduling geht es darum, die zeitliche Abfolge von Jobs festzulegen, so dass Bedin-\ngungen eingehalten werden.\n6-21 6-21 Das einfachste Scheduling-Problem.\nIDefinition: Intervall-Scheduling\nGegeben seien Intervalle Ij= [sj;fj)mitj2f1;:::;ng, wobei sjdieStartzeit undfjdie\nEndzeit bezeichne. Ziel ist es, eine m\u00f6glichst gro\u00dfe Auswahl J\u0012f1;:::;ngzu tre\ufb00en, so\ndass f\u00fcr je zwei j;j02JgiltIj\\Ij0=;.\nBeispiel: Eingabe\nI1\nI2\nI3\nI4\nI5\nI6\nI7\nI8\nI96 Klassifikation: Kompetitive Rate\n6.3 Scheduling65\nBeispiel: Optimale nicht\u00fcberlappende Auswahl\nI1\nI2\nI3\nI4\nI5\nI6\nI7\nI8\nI9\n6.3.1 Offline-Strategien\n6-22 6-22 Das Scheduling-Problem kann mit einem Greedy-Verfahren gel\u00f6st werden.\nZur Erinnerung: Greedy-Verfahren \ufb01nden L\u00f6sungen, indem sie iterativ jeweils Teill\u00f6sun-\ngenschrittweise erweitern und dabei \u00bbgierig\u00ab die Erweiterung lokal optimal durchf\u00fchren.\nGreedy-Verfahren \ufb01nden nicht immer optimale L\u00f6sungen.\nGreedy-Algorithmus f\u00fcr das Intervall-Scheduling\n1algorithm earliest -\ufb01nishing -\ufb01rst(I1;:::;In)\n2 J ;\n3 X f1;:::;ng\n4 while X6=;do\n5 j dasjenige j2Xf\u00fcr das fjminimal unter allen j2Xist\n6 J J[fjg\n7 X XnfkjIk\\Ij6=;g\n8 return J\n6-23 6-23 Greedy f\u00fchrt zum Erfolg.\nISatz\nDer Earliest-Finishing-First-Algorithmus \ufb01ndet immer ein optimales Schedule.\nZum Beweis siehe \u00dcbungsaufgabe 6.1.\n6.3.2 Online-Strategien\n6-24 6-24 Die Online-Fassung des Problems.\nIDefinition: Online-Fassung des Intervall-Scheduling-Problems\nEreignisse Intervalle Ij= [sj;fj)\nEntscheidungen \u00bbschedule\u00ab oder \u00bbdo not schedule\u00ab.\nBedingungen Alle Intervalle, f\u00fcr die \u00bbschedule\u00ab entschieden wurden, d\u00fcrfen sich nicht \u00fcber-\nlappen.\nMa\u00df Anzahl der \u00bbschedule\u00ab-Entscheidungen.\nEin Greedy-Online-Algorithmus\nBei Eingabe Ijentscheide \u00bbschedule\u00ab, falls Ijsich mit keinem schon gew\u00e4hlten Intervall\n\u00fcberlappt, sonst \u00bbdo not schedule\u00ab.666 Klassifikation: Kompetitive Rate\nZusammenfassung dieses Kapitels\n6.3.3 Kompetitive Rate\n6-25 6-25 Das Online-Problem ist \u00bbnicht kompetitiv\u00ab.\nISatz\nDas Online-Problem Interval-Scheduling hat eine unendliche kompetititve Rate, das hei\u00dft,\nkein Online-Algorithmus f\u00fcr dieses Problem hat eine kompetititve Rate von h\u00f6chstens kf\u00fcr\nirgendein k.\nKommentare zum Beweis\nBeweis. Seikfest und ein beliebiger Online-Algorithmus f\u00fcr das Interval-Scheduling-\nProblem gegeben. Wir betrachten folgenden Adversary:1 1Hinweis, dass ein Adversary-Argument\ngenutzt wird\n1.Er produziert die Intervalle [1;2),[2;3),[3;4)und so weiter als Ereignisse, bis der\nAlgorithmus zum ersten Mal \u00bbschedule\u00ab entscheidet.\n2.Danach produziert er Intervalle der Gr\u00f6\u00dfe 1=(k+1), die alle nebeneinander innerhalb\ndieses letzten Intervalls liegen.\nNach obiger Regel produziert der Adversary mindestens k+2Intervalle.\nHier ein Beispiel daf\u00fcr, welche Intervalle der Adversary produzieren k\u00f6nnte und welche der\nAlgorithmus ausw\u00e4hlen w\u00fcrde.\nI1\nI2\nI3\nI4\nI5\nI6\nI7\nOptimal w\u00e4re es hingegen, gerade alle anderen Intervalle auszuw\u00e4hlen:\nI1\nI2\nI3\nI4\nI5\nI6\nI7\nWie man an den Beispielen (und auch leicht allgemein) sieht, w\u00e4hlt der Algorithmus nur ein\nIntervall aus, obwohl man k+1Intervalle ausw\u00e4hlen k\u00f6nnte. Folglich ist die kompetititve\nRate schlechter als k.\nZusammenfassung dieses Kapitels\n6-26 6-26 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationSki-Leihen-Problem\nadhoc\nO\ufb04ine: trivial\nOnline: Strategie C\neinfache Rechnung\nKompetitive Rate 2\u0000L=KOnline-Problem Caching\nadhoc\nO\ufb04ine: /e.sc/f.sc/i.sc/f.sc\nOnline: /e.sc/l.sc/r.sc/u.sc\nExchange\u2013Argumente\nKompetitive Rate k6 Klassifikation: Kompetitive Rate\nZusammenfassung dieses Kapitels67\nProblemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationInterval-Scheduling\nGreedy\nO\ufb04ine: /e.sc/f.sc/f.sc\nO\ufb04ine: trivial\nAdversay-Argument\nKompetitive Rate1\n(nicht kompetitiv)\nIOnline- versus Offline-Algorithmen\nGegeben sei ein Online-Problem. Ein Online-Algorithmus f\u00fcr das Problem erh\u00e4lt die Er-\neignisse sequentiell und muss nach jedem Ereignis eine Entscheidung f\u00e4llen. Ein O\ufb04ine-\nAlgorithmus f\u00fcr das Problem erh\u00e4lt alle Ereignisse auf einmal als Eingabe und muss dann\neine Folge von Entscheidungen berechnen.\nIKompetitive Rate\n1.Diekompetitive Rate eines Online-Algorithmus ist das maximale Verh\u00e4ltnis des Ma\u00dfes\nder von ihm gefundenen L\u00f6sungen zum Ma\u00df der vom besten O\ufb04ine-Algorithmus ge-\nfundenen L\u00f6sung.\n2.Diekompetitive Rate eines Problems ist die minimale kompetitive Rate aller Online-\nAlgorithmen f\u00fcr das Problem.\nIKompetitive Rate des Ski-Leihen-Problems\nDie kompetitive Rate des Ski-Leihen-Problems ist h\u00f6chstens 2.\nDie Strategie hierzu: Leihe, bis man den Preis eines Paars ausgegeben hat, danach kaufe\nsofort.\nIKompetitive Rate des Caching-Problems\nDie kompetitive Rate des Caching-Problems f\u00fcr Caches der Gr\u00f6\u00dfe kistk.\nEin optimaler O\ufb04ine -Algorithmus ist Evict-Furthest-In-Future.\nIKompetitive Rate des Interval-Scheduling-Problems\nDie kompetitive Rate des Interval-Scheduling-Problems ist unendlich.\nEin optimaler O\ufb04ine -Algorithmus ist Earliest-Finishing-First.\nZum Weiterlesen\n[1]Daryl J. Bem, Feeling the Future: Experimental Evidence for Anomalous Retroactive\nIn\ufb02uence on Cognition and A\ufb00ect, Journal of Personality and Social Psychology, 100,\n407\u2013425, 2011.\nDies ist der in der Einleitung angesprochene Artikel, der sich mit der Frage besch\u00e4ftigt, ob Men-\nschen in die Zukunft sehen k\u00f6nnen. Der Artikel hat einen gewissen Aufruhr in der wissenschaft-\nlichen Gemeinschaft ausgel\u00f6st; es emp\ufb01ehlt sich sehr, die verschiedenen Repliken zu lesen.686 Klassifikation: Kompetitive Rate\n\u00dcbungen zu diesem Kapitel\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 6.1 Earliest-Finishing-First ist optimal, mittel\nZeigen Sie, dass Algorithmus 6-22, der Earliest-Finishing-First-Algorithmus, immer eine L\u00f6sung mit\nminimalem Ma\u00df berechnet.\nTipp: Benutzen Sie ein Exchange-Argument. Zeigen Sie, dass es \u00bbnie falsch ist\u00ab, immer den Job aus-\nzuw\u00e4hlen, dessen Ende am fr\u00fchesten ist. Gehen Sie dazu \u00e4hnlich wie bei Satz 6-15 vor: Betrachten\nSie eine beliebige andere Auswahl und betrachten Sie die erste Stelle, wo die andere Auswahl von\nder des Earliest-Finishing-First-Algorithmus unterscheidet. Argumentieren Sie, dass man hier einen\nAustausch vornehmen kann, ohne das Ma\u00df zu verschlechtern.\n\u00dcbung 6.2 k-Server-Problem auf den nat\u00fcrlichen Zahlen, mittel\nBeim k-Server-Problem auf den nat\u00fcrlichen Zahlen ist eine Folge von Anfragen a1, ..., anmitaj2N\ngegeben. Ausgehend von einer ebenfalls gegebenen Startkon\ufb01guration c0= (c0\n1;:::;c0\nk)2Nkist eine\nFolge von nKon\ufb01gurationen ci= (ci\n1;:::;ci\nk)2Nkgesucht, die f\u00fcr jede Anfrage ajeine Kon\ufb01guration\ncimitaj2fci\n1;:::;ci\nkgenth\u00e4lt und die SummePn\ni=1Pk\nj=1jci\u00001\nj\u0000ci\njjminimert.\nDask-Server-Problem verallgemeinert viele Caching- und Scheduling-Probleme: Zum Beispiel k\u00f6nn-\nten Anfragen f\u00fcr Hausnummern in einer Stra\u00dfe stehen, zu denen Briefe ausgestellt werden sollen. Die\nkEintr\u00e4ge der Startkon\ufb01guration entsprechen Positionen von kPostboten, die diese Briefe mit m\u00f6g-\nlichst wenig Aufwand austragen wollen. Gesucht ist eine Folge von Kon\ufb01gurationen (Positionen der\nPostboten in der Stra\u00dfe), die alle Anfragen bedient (alle Briefe k\u00f6nnen ausgetragen werden) und da-\nbei die Summe \u00fcber den Di\ufb00erenzen benachbarter Kon\ufb01gurationen (die Bewegungen der Postboten)\nminimiert.\n1.Entwickeln Sie einen O\ufb04ine-Algorithmus, der das k-Server-Problem optimal l\u00f6st.\n2.Formulieren Sie das k-Server-Problem als Online-Problem. Gehen Sie dabei wie in der Vorlesung\nvor und geben Sie die Ereignisse, Entscheidungen, Bedingungen und das Ma\u00df an.\n3.Als ersten Versuch zur L\u00f6sung des Online-Problems betrachten wir folgende Greedy-Strategie:\nIn Runde ibewege den Server mit geringstem Abstand zu aian die Position ai. Geben sie ein\nBeispiel an, aus dem ersichtlich wird, dass dieser Algorithmus nicht k-kompetitiv f\u00fcr irgendein\nkist.\n4.Als zweiten L\u00f6sungsansatz betrachten wir die folgende Strategie, die unter dem Namen /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc\n/c.sc/o.sc/v.sc/e.sc/r.sc bekannt ist: Zu jeder Runde ide\ufb01nieren wir Ii= [minj2f1;:::;kgci\u00001\nj;maxj2f1;:::;kgci\u00001\nj].\nFalls ai=2Ii, bewege den n\u00e4chstgelegensten Server zu ai. Falls ai2Ii, bewege die beiden n\u00e4chst-\ngelegensten Server (die nicht unbedingt gleich weit von aientfernt sein m\u00fcssen) mit gleicher\nGeschwindigkeit Richtung ai. Wenn einer der Server aierreicht, stoppe beide Server.\nSimulieren Sie den Algorithmus /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc f\u00fcr Ihr Beispiel aus Punkt 3. Wie ist die Rate\nzwischen der optimalen und der von /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc gefundenen L\u00f6sung?7 Analysemethode: Amortisierung69\n7-1 7-1\nKapitel 7\nAnalysemethode: Amortisierung\nAmortisierte Kosten sind reale Kosten plus R\u00fcckstellungen\n7-2 7-2Lernziele dieses Kapitels\n1.Konzept der amortisierten Analyse verstehen\n2.Die Potential-Methode einsetzen k\u00f6nnenInhalte dieses Kapitels\n7.1 Einf\u00fchrung zur Amortisierung 70\n7.1.1 Fallbeispiel I: Stacks . . . . . . . . . . . 70\n7.1.2 Fallbeispiel II: Z\u00e4hler . . . . . . . . . . 70\n7.1.3 Worst-Case-Kosten . . . . . . . . . . . 71\n7.1.4 Amortisierte Kosten I . . . . . . . . . . 72\n7.2 Die Potential-Methode 73\n7.2.1 Die Idee der R\u00fcckstellung . . . . . . . . 73\n7.2.2 Amortisierte Kosten II . . . . . . . . . . 74\n7.3 Fallbeispiel III: Das Listen-Zugriffsproblem 75\n7.3.1 Online-Algorithmen . . . . . . . . . . . 76\n7.3.2 Kompetitive Rate . . . . . . . . . . . . 76\n\u00dcbungen zu diesem Kapitel 78\nWorum\nes heute\ngehtWorum\nes heute\ngehtDie amortisierte Analyse ist ein Aus\ufb02ug der Theoretischen Informatik in die Finanzwelt.\nAllerdings bekommen die Informatiker die Begri\ufb00e wie immer nicht richtig hin: Wo eine\nBankerin von \u00bbR\u00fcckstellungen\u00ab sprechen w\u00fcrde, spricht man in der Theorie von \u00bbPotentia-\nlen\u00ab, wo sie von \u00bbPreisen\u00ab sprechen w\u00fcrden, spricht man in der Theorie von \u00bbamortisierten\nKosten\u00ab. Ignoriert man aber diese Unbeholfenheiten in der Ausdrucksweise, so l\u00e4sst sich\ndoch festhalten, dass die Theorie der amortisierten Kosten eine gute Idee aus der Finanz-\nwelt umsetzt: Wenn man heute schon genau wei\u00df, welche Kosten in der Zukunft entstehen\nwerden, dann macht es Sinn, jetzt schon R\u00fcckstellungen hierf\u00fcr zu bilden.\nWie der Name schon sagt, ist die amortisierte Analyse eine Analysemethode \u2013 sie hat weder\nauf Algorithmen und Datenstrukturen noch auf deren Entwurf einen Ein\ufb02uss. Sie kommt\nimmer dann zum Einsatz, wenn Algorithmen eine Folge von Operationen abarbeiten, deren\nLaufzeit stark unterschiedlich sein kann und bei denen diese Laufzeiten auch noch vonein-\nander abh\u00e4ngen. Typisch ist es beispielsweise, dass sp\u00e4tere Operationen sehr teuer werden\nk\u00f6nnen, aber nur dann, wenn vorher viele billige Operationen durchgef\u00fchrt wurden.\nHierzu ein Beispiel: Sie wohnen in einer Etagenwohnung und wollen alle paar Jahre mal\nrenovieren. Eine solche \u00bbRenovieren-Operation\u00ab ist nur teuer, wenn vorher viele \u00bbEin-Jahr-\nWohnen-Operationen\u00ab durchgef\u00fchrt wurden. Hingegen ist es recht billig zu renovieren,\nwenn dies kurz vorher bereits einmal geschehen ist. Aus diesem Grund sind f\u00fcr eine belie-\nbige Folge von \u00bbRenovieren-Operationen\u00ab und \u00bbEin-Jahr-Wohnen-Operationen\u00ab die durch-\nschnittlichen Kosten einer Operation eher niedrig. Wenn man dies nun formal zeigen m\u00f6chte,\nso verliert man schnell die \u00dcbersicht, wie teuer die einzelnen Operationen nun sind, denn\nhier h\u00e4ngt es ja von deren Reihenfolge ab (zugegeben, bei einem einfachen Beispiel wie\ndiesem beh\u00e4lt man wohl doch noch den \u00dcberblick; wenn es aber viele Operationen gibt, die\nin komplexer Weise voneinander abh\u00e4ngen, so ist man verloren). Die Idee der R\u00fcckstellung\nhilft nun weiter: In jedem Jahr bilden Sie eine kleine R\u00fcckstellung auf einem Konto. Das so\nangesparte Geld nutzen Sie dann, wenn eine Renovierung ansteht, so dass die Renovierung707 Analysemethode: Amortisierung\n7.1 Einf\u00fchrung zur Amortisierung\nselbst Sie idealerweise gar nichts kostet. Wenn man nun schaut, wie viel Kosten pro Jahr\nentstehen, so ist sofort klar, dass diese konstant sind: In normalen Jahren m\u00fcssen Sie nur die\nR\u00fcckstellung bezahlen (ein kleiner, konstanter Betrag) und das Renovieren kostet Sie quasi\n\u00bbnichts\u00ab, denn sie wird aus den R\u00fcckstellungen bezahlt.\n7.1 Einf\u00fchrung zur Amortisierung\n7.1.1 Fallbeispiel I: Stacks\n7-4 7-4 Von Atomkraftwerken. . .\nCreative Commons Attribution Sharealike LicenseDer Regierung von Dunkelland m\u00f6chte die Kernenergie einf\u00fchren. Sie kann:\n1.Ein neues /a.sc/k.sc/w.sc bauen lassen.\n2.Ein bestehendes /a.sc/k.sc/w.sc abrei\u00dfen lassen.\n3.Nach jedem /g.sc/a.sc/u.sc: auf einmal kviele /a.sc/k.sc/w.sc abrei\u00dfen lassen.\nDie Regierung beauftragt die Firma Happy Atoms GmbH & Co. KG .\nFrage f\u00fcr heute\nWie viel wird der Auf- und Abbau der /a.sc/k.sc/w.scs \u00fcber die Jahre kosten?\n7-5 7-5 . . . und Stacks\nCreative Commons Attribution LicenseAuf einem Stack seien die folgenden Operationen erlaubt:\n1.push (x)\n2.pop\n3.multipop (k): Es werden die obersten kElemente des Stacks entfernt\nDie Frage neu formuliert\nWie viel wird eine l\u00e4ngere Folge von solcher Operationen \u00bbkosten\u00ab?\n7.1.2 Fallbeispiel II: Z\u00e4hler\n7-6 7-6 Teure Z\u00e4hler.\nNormalerweise dauert das Erh\u00f6hen einer Zahl nureinen Takt . Heute soll es aber darum\ngehen, wie lange dies dauert, wenn man die Zahl nur bitweise modi\ufb01zieren kann.\nSpeicherung von Zahlen in Arrays.\nZur Speicherung einer Zahl abenutzen wir\n\u2013einen Array A, wobei\n\u2013A[i]gerade das i-te Bit von aist (beginnend beim /l.sc/s.sc/b.scund Z\u00e4hlung bei 0).\nBeispielsweise gilt f\u00fcr a=110 2, dass A[0] =0,A[1] =1undA[2] =1.\nvoid increment (int[]A)\n{\nint i= 0;\nwhile (A[i] == 1) {\nA[i] = 0;\ni++;\n}\nA[i] = 1;\n}7 Analysemethode: Amortisierung\n7.1 Einf\u00fchrung zur Amortisierung71\n7.1.3 Worst-Case-Kosten\n7-7 7-7 Wie teuer ist eine Folge von Stackoperationen?\nGegeben sei eine Folge der L\u00e4nge nvon Stackoperationen wiepush (a),push (b),push (a),\nmultipop (2),push (c), ...\nFragestellung\nWie lange wird es dauern, diese Folge abzuarbeiten? Dabei nehmen wir an:\n1.Ein Push dauert eine Zeiteinheit.\n2.Ein Pop ebenfalls.\n3.Einmultipop (k)dauert minfs;kgZeiteinheiten, wobei sdie aktuelle Stackh\u00f6he ist, denn\nwir ben\u00f6tigen so viele Pop-Operationen.\nEine einfache Worst-Case-Absch\u00e4tzung liefert:\nISatz\nEine Folge von nStackoperationen dauert O(n2)lange.\nBeweis. Jedes multipop (k)kann h\u00f6chstens n\u00001lange dauern. Da es nOperationen gibt,\nfolgt die Laufzeit.\n7-8 7-8 Wie teuer ist eine Folge von Inkrements?\nGegeben sei eine Folge von Inkrement-Aufrufen der L\u00e4nge n.\nFragestellung\nWie lange wird es dauern, diese Folge abzuarbeiten?\nWir sollen also absch\u00e4tzen, wie lange folgendes Programm braucht:\nint[]A=new int [64]; //Reicht ,wenn nein long ist\nfor (int j= 0; j<n;j++) {\nincrement (A);\n}\nEine einfache Worst-Case-Absch\u00e4tzung liefert:\nISatz\nEine Folge von nInkrement-Aufrufen dauert O(nlogn)lange.\nBeweis. Die Schleife in increment kann lognmal durchlaufen werden. Da es nAufrufe\ngibt, folgt die Laufzeit.\n7-9 7-9 Zusammenfassung der Worst-Case-Kosten der Operationen.\nOperation Reale Worst-Case-Kosten\npush (a) 1\npop 1\nmultipop (k)minfs;kg\nincrement log2n727 Analysemethode: Amortisierung\n7.1 Einf\u00fchrung zur Amortisierung\n7.1.4 Amortisierte Kosten I\n7-10 7-10 Die erste Worst-Case-Absch\u00e4tzung ist zu pessimistisch.\nBetrachten wir statt der Kosten von Stackoperationen die Kosten von Dunkelland:\nOperation Reale Worst-Case-Kosten\n/a.sc/k.sc/w.sc bauen 1 Mrd. Euro\n/a.sc/k.sc/w.sc abrei\u00dfen 1 Mrd. Euro\nKleiner Atomausstieg, kabrei\u00dfen kMrd. Euro\nBig Idea\nEin/a.sc/k.sc/w.sc kann man erst abrei\u00dfen, wenn es gebaut wurde!\nISatz\nDie Kosten einer Folge der L\u00e4nge nvon Bau-, Abrei\u00df- und Atomausstieg-Operationen sind\nh\u00f6chstens 2nMrd. Euro.\nErster Beweis. In einer Folge k\u00f6nnen h\u00f6chstens nBau-Operationen vorkommen (Kosten\nnMrd. Euro) und auch h\u00f6chstens so viele abgerissen werden, was insgesamt Kosten von\nh\u00f6chstens 2nverursacht.\n7-11 7-11 Noch mehr Pessimismus.\n00000\n00001\n00010\n00011\n00100\n00101\n00110\n00111\n01000\n01001\n01010\n01011\n01100\n01101\n01110\n01111\n10000\n10001\nBetrachten wir die Kosten von Inkrement-Aufrufen genauer :\n1.Nur bei jeder zweiten Zahl muss die Schleife \u00fcberhaupt durchlaufen werden.\n2.Nur bei jeder vierten Zahl muss die Schleife mehr als einmal durchlaufen werden.\n3.Nur bei jeder achten Zahl muss die Schleife mehr als zweimal durchlaufen werden.\nAllgemein ist nur bei jeder 2i-ten Zahl ein zus\u00e4tzlicher Schleifendurchlauf n\u00f6tig.\n7-12 7-12 Die echten Kosten von mehreren Inkrements.\nISatz\nDie Laufzeit einer Folge von nAufrufen der Inkrement-Funktion liegt in O(n).\nBeweis. Wir summieren die Anzahl der \u00c4nderungen von Bits \u00bbspaltenweise\u00ab. Die Laufzeit\nist dann:\ndlog2neX\ni=0jn\n2ik\n\u0014ndlog2neX\ni=01\n2i<2n:7 Analysemethode: Amortisierung\n7.2 Die Potential-Methode73\n7-13 7-13 Warum die Worst-Case-Absch\u00e4tzung zu pessimistisch war.\nStacks\nEinmultipop (k)kann zwar sehr teuer sein , aber nur dann, wenn es vorher viele billige Ope-\nrationen gab.Im Schnitt ist ein multipop (k)billig: Da eine Liste von Operationen der L\u00e4nge\nnnur2nKosten verursacht, sind die durchschnittlichen Kosten jeder Operation gerade 2.\nZ\u00e4hler\nEin Inkrement kann zwar teuer sein , aber nur dann, wenn es vorher viele billige Operatio-\nnengab.Im Schnitt ist ein Inkrement billig: Da eine Liste von nInkrements nur 2nKosten\nverursacht, sind die durchschnittlichen Kosten eines Inkrements gerade 2.\n7-14 7-14 Idee der amortisierten Kosten.\nOperation Reale Worst-Case-Kosten Amortisierte Kosten\npush (a) 1 2\npop 1 2\nmultipop (k)minfs;kg 2\nincrement log2n 2\nIDefinition: Amortisierte Kosten allgemein\nEine Zuordnung von Kosten zu Operationen nennt man amortisierte Kosten , wenn f\u00fcr jede\nFolge von Operationen gilt:\nreale Kosten\u0014Summe der amortisierten Kosten :\nBemerkungen:\n\u2013Dierealen Kosten einer einzelnen Operationen k\u00f6nnen beliebig h\u00f6her sein als ihre\namortisierten.\n\u2013Auch wenn hier \u00bbdurchschnittliche Kosten\u00ab betrachtet werden, hat dies nichts mit Average-\nCase-Analysen zu tun .\nMerke: Amortisierte Analysen sind Worst-Case-Analysen.\n7.2 Die Potential-Methode\n7.2.1 Die Idee der R\u00fcckstellung\n7-15 7-15 R\u00fcckstellungen f\u00fcr Kosten in der Zukunft\nDie Regierung von Dunkelland \u00e4rgert es, dass es nach einem /g.sc/a.sc/u.scimmer so teuer wird, wenn\nsie viele /a.sc/k.sc/w.scs auf einmal abrei\u00dfen lassen will. Sie beschlie\u00dft deshalb, folgendes Gesetz zu\nerlassen:\nHappy Atoms muss bei der Bank pro derzeit gebauten /a.sc/k.sc/w.scs 1 Mrd. Euro R\u00fcck-\nstellungen bilden.\nWie hoch m\u00fcssen die Preise von Happy Atoms nun sein?\nNehmen wir an, es gibt gerade sAtomkraftwerke in Dunkelland.\nAufbau In der Rechnung von Happy Atoms an die Regierung von Dunkelland \ufb01nden\nsich folgende Posten:\n1.1 Mrd. Euro Baukosten und\n2.1 Mrd. Euro f\u00fcr R\u00fcckstellungen (vorher waren nur sMrd. Euro n\u00f6tig, nun\ns+1Mrd. Euro).\nDer Preis betr\u00e4gt also 2 Mrd. Euro.\nAbriss Hier m\u00f6chte Happy Atoms kein Geld haben, denn:\n1.Der Abriss kostet zwar 1 Mrd. Euro, aber\n2.dies kann gerade aus der Di\ufb00erenz der R\u00fcckstellungen beglichen werden\n(vorher sMrd. Euro, nun s\u00001Mrd. Euro).\nKleiner Atomausstieg Auch hier braucht Happy Atoms kein Geld:\n1.Der Abriss kostet zwar kMrd. Euro, aber\n2.dies kann gerade aus der Di\ufb00erenz der R\u00fcckstellungen beglichen werden\n(vorher sMrd. Euro, nun s\u0000kMrd. Euro).747 Analysemethode: Amortisierung\n7.2 Die Potential-Methode\n7-16 7-16 Wie man mit R\u00fcckstellungen argumentiert.\nISatz\nDie Kosten einer Folge der L\u00e4nge nvon Bau-, Abrei\u00df- und Atomausstieg-Operationen sind\nh\u00f6chstens 2nMrd. Euro.\nZweiter Beweis. Wir verlangen, dass, wenn es s/a.sc/k.sc/w.scs gibt, die R\u00fcckstellungen gerade s\nMrd. Euro betragen. Dann sind Preise, die Happy Atoms pro Operation verlangen muss:\nPreis =reale Kosten +neue R\u00fcckstellungen \u0000alte R\u00fcckstellung :\nDie so entstehenden Preise sind:\nOperation Preis\n/a.sc/k.sc/w.sc bauen 2 Mrd. Euro\n/a.sc/k.sc/w.sc abrei\u00dfen 0 Mrd. Euro\nKleiner Atomausstieg, kabrei\u00dfen 0 Mrd. Euro\nNun gilt:\nGesamtpreis =reale Gesamtkosten +\nEndr\u00fcckstellungen \u0000Anfangsr\u00fcckstellung\nDa die R\u00fcckstellung nie negativ ist und sie am Anfang 0 Euro betrug , folgt:\nGesamtpreis\u0015reale Gesamtkosten :\nDa der Gesamtpreis o\ufb00enbar h\u00f6chstens 2nMrd. Euro betr\u00e4gt, folgt die Behauptung.\n7.2.2 Amortisierte Kosten II\n7-17 7-17 Die Namen f\u00fcr R\u00fcckstellungen und Preise in der Theoretischen Informatik.\nIn der Theoretischen Informatik spricht man\n\u2013vonPotentialen statt von \u00bbR\u00fcckstellungen\u00ab und\n\u2013vonamortisierten Kosten statt von \u00bbPreisen\u00ab.\nIDefinition: Potentialfunktion\nEine Potentialfunktion Fordnet Datenstrukturen nichtnegative reelle Zahlen zu.\nIDefinition: Amortisierte Kosten f\u00fcr Potentialfunktionen\nEine Operation m\u00f6ge cireale Kosten verursachen und die Datenstruktur diindi+1verwan-\ndeln. Dann sind die amortisierten Kosten der Operation\nai=ci+F(di+1)\u0000F(di):\n7-18 7-18 Der Hauptsatz \u00fcber die amortisierten Kosten.\nISatz\nSeiFeine Potentialfunktion und p1,p2, ..., pneine Folge von Operationen. Die i-te Ope-\nration m\u00f6ge Kosten civerursachen und die Datenstruktur di\u00001indiumwandeln. Dann gilt:\nnX\ni=1ci=F(d0)\u0000F(dn) +nX\ni=1ai:\nFalls also F(d0) =0undF(dn)\u00150, so gilt\nnX\ni=1ci\u0014nX\ni=1ai:\nMerke\nIst die Potentialfunktion nie negativ und anfangs Null, so sind die realen Gesamt kosten h\u00f6ch-\nsten die amortisierten Gesamt kosten.7 Analysemethode: Amortisierung\n7.3 Fallbeispiel III: Das Listen-Zugriffsproblem75\n7-19 7-19 Eine amortisierte Analyse.\nISatz\nDie Ausf\u00fchrung einer Folge der L\u00e4nge nvonpush (x),popundmultipop (k)Operationen\ndauert h\u00f6chstens O(n).\nBeweis. Wir de\ufb01nieren eine Potentialfunktion wie folgt:\nF(S) =H\u00f6he des Stacks S:\nO\ufb00enbar ist F(S)initial 0und nie negativ. Dann sind die amortisierten Kosten jeder Ope-\nration gerade\nOperation amortisierte Kosten\npush (x) 2\npop 0\nmultipop (k)0\nDie amortisierten Kosten von nOperationen sind folglich h\u00f6chstens 2nund somit auch die\nrealen Kosten.\n7-20 7-20 Eine zweite amortisierte Analyse.\nISatz\nDie Laufzeit einer Folge von nAufrufen der Inkrement-Funktion liegt in O(n).\nBeweis. Wir de\ufb01nieren eine Potentialfunktion f\u00fcr einen Array Awie folgt: F(A)sei gerade\ndie Anzahl der 1en in A. O\ufb00enbar ist dies anfangs 0und nie negativ.\nDie amortisierten Kosten eines Inkrements sind:\nai=e+1|{z}\nreale Kosten+F(Ai)\u0000F(Ai\u00001);\nwobei edie Anzahl Einsen am Anfang des Arrays ist. Hier gilt immer ai=2, denn die\nPotential\u00e4nderung ist gerade 1\u0000e: Es werden eEinsen in Nullen verwandelt und eine Null\nin eine Eins.\nDie amortisierten Kosten von nOperationen sind folglich h\u00f6chstens 2nund somit auch die\nrealen Kosten.\n7.3 Fallbeispiel III: Das Listen-Zugriffsproblem\n7-21 7-21 Eine Online-Problemstellung.\nAuthor Jonathan Bondhus, Creative Commons Attribution Sharealike LicenseSie haben einen Papierstapel vor sich. Sie werden wiederholt gebeten, verschiedene Seiten\ndarin zu suchen. Sie d\u00fcrfen den Stapel nur von oben sequentiell durchsuchen. Haben Sie die\nSeite gefunden, k\u00f6nnen Sie diese beliebig viel weiter oben wieder einf\u00fcgen.\nDas formale Listen-Zugriffs-Problem\nEreignisse request (xi)\nEntscheidungen moveto (d)(neue Tiefe von xi).\nBedingungen d\u0014di, wobei didie Tiefe von xiim Stapel war\nMa\u00dfPn\ni=1di767 Analysemethode: Amortisierung\n7.3 Fallbeispiel III: Das Listen-Zugriffsproblem\n7.3.1 Online-Algorithmen\n7-22 7-22 Eine gute Online-Strategie\nDie Move-to-Front-Strategie\nEntscheide immer moveto (0).\n.Zur \u00dcbung\nGeben Sie eine Folge von Ereignissen an, f\u00fcr die diese Strategie eine schlechte kompetitive\nRate hat.\n7.3.2 Kompetitive Rate\n7-23 7-23 Amortisierte Analyse der kompetitiven Rate\nISatz\nDie kompetitive Rate der Move-to-Front-Strategie ist h\u00f6chstens 2.\nBeweis. Wir zeigen, dass die amortisierten Kosten der Move-to-Front-Strategie h\u00f6chstens\ndoppelt so gro\u00df sind wie die der optimalen Strategie. Hieraus folgt nach dem Hauptsatz,\ndass dies auch f\u00fcr die realen Kosten der Move-to-Front-Strategie gilt.\n\u2013Seix1, ..., xneine feste Folge angefragter Elemente.\n\u2013SeiLijeweils die Liste nach dem i-ten Schritt gem\u00e4\u00df den Entscheidungen eines opti-\nmalen O\ufb04ine-Algorithmus.\n\u2013SeiL0\nijeweils die Liste nach dem i-ten Schritt gem\u00e4\u00df den Entscheidungen der Move-to-\nFront-Strategie.\n\u2013Seididie Tiefe von xiinLiundd0\nientsprechend f\u00fcr L0\ni.\n\u2013Zwei Elemente der Liste bilden eine Inversion , wenn ihre Reihenfolge in LiundL0\ni\nunterschiedlich ist.\ni1234567Gesamtkosten\nxicbacbac\nLia\nb\nca\nb\nca\nb\nca\nb\nca\nb\nca\nb\nca\nb\nc\ndi21021028\nL0\nia\nb\ncc\na\nbb\nc\naa\nb\ncc\na\nbb\nc\naa\nb\nc\nd0\ni222222214\nInversionen 0220220\nDe\ufb01niere eine Potentialfunktion wie folgt:\nF(L0\ni) =Anzahl Inversionen zu Li:\nO\ufb00enbar ist Finitial Null und nie negativ.\nDie amortisierten Kosten jeder Entscheidung sind per De\ufb01nition ai=d0\ni+F(L0\ni)\u0000F(L0\ni\u00001).\nWir behaupten, dass gilt:\nai\u00142di:\nHaben wir dies gezeigt, so sind wir fertig: Dann sind die amortisierten Gesamtkosten (und\ndamit auch die realen Gesamtkosten) n\u00e4mlich h\u00f6chstens doppelt so wie die optimalen Ko-\nsten.\nZum Beweis von ai\u00142diargumentieren wir so: Man stelle sich vor, xiwird \u00bblangsam in L0\ni\nnach oben geschoben\u00ab, wohingegen es \u00bbin Lierstmal bleibt, wo es ist\u00ab. Dann wird, solange xi\nunterhalb der Tiefe diist, in jedem Schritt genau eine Inversion zu Liaufgel\u00f6st, ab der Tiefe\ndihingegen wieder je eine neue Inversion gescha\ufb00en. Insgesamt verringert sich das Potential\nerst um d0\ni\u0000di, um sich dann um dizu erh\u00f6hen. Wird nun xiauch in Lilangsam angehoben,\nso werden weitere Inversionen aufgel\u00f6st, das Potential verringert sich also weiter. Insgesamt\nergibt sich eine maximale Potentialsteigerung von di\u0000(d0\ni\u0000di) =2di\u0000d0\ni. Also gilt ai\u0014\nd0\ni+2di\u0000d0\ni=2di.7 Analysemethode: Amortisierung\nZusammenfassung dieses Kapitels77\nZusammenfassung dieses Kapitels\n7-24 7-24 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationStack-Verwaltung\nZ\u00e4hler-Verwaltung\nadhoc\nStack\nZ\u00e4hler\nAmortisierte Analyse\nlineare KostenListen-Zugri\ufb00s-Problem\nadhoc\nMove-To-Front\nAmortisierte Analyse /\nPotential-Methode\n2-kompetitiv\nIAmortisierte Kosten\nEine Zuordnung von Kosten zu Operationen nennt man amortisierte Kosten , wenn f\u00fcr jede\nFolge von Operationen gilt:\nreale Kosten\u0014Summe der amortisierten Kosten :\nIPotentialmethode\n\u2013DiePotentialmethode de\ufb01niert amortisierte Kosten implizit durch eine Potentialfunkti-\nonF.\n\u2013Diese ordnet Datenstrukturen dinichtnegative reelle Zahlen zu.\n\u2013Die amortisierten Kosten einer Operation sind dann\nai= ci|{z}\nreale Kosten+F(di+1)\u0000F(di)| {z }\nPotential\u00e4nderung:\nIHauptsatz \u00fcber amortisierte Kosten\nEs gilt\nnX\ni=1ci\n|{z}\nreale Gesamtkosten=F(d0)\u0000F(dn)|{z }\nGesamtpotentialdi\ufb00erenz+nX\ni=1ai\n|{z}\namortisierte Gesamtkosten:\nFalls das Potential anfangs Null war und nie negativ, so gilt\nreale Gesamtkosten \u0014amortisierte Gesamtkosten :\n(F\u00fcr einzelne Operationen gilt dies hingegen gerade oft nicht.)\nZum Weiterlesen\n[1]Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Cli\ufb00ord Stein, Introduc-\ntion to Algorithms, zweite Au\ufb02age, MIT Press, 2001, Kapitel \u00bbAmortized Analysis\u00ab.787 Analysemethode: Amortisierung\n\u00dcbungen zu diesem Kapitel\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 7.1 Kompetitive Rate des DOUBLE COVER Algorithmus f\u00fcr das k-Server-Problem\nanalysieren, schwer\nIn \u00dcbung 6.2 haben Sie das k-Server-Problem auf den nat\u00fcrlichen Zahlen und den /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc-/c.sc/o.sc/v.sc/e.sc/r.sc -Al-\ngorithmus zu dessen L\u00f6sung kennengelernt. Im Folgenden soll mithilfe der Potential-Methode ge-\nzeigt werden, dass der /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc -Algorithmus f\u00fcr das k-Server-Problem k-kompetitiv ist. Hierzu\nverwenden wir folgende Potentialfunktion: Sei /o.sc/p.sc/t.sc/i.sc/m.sc/a.sc/l.sc ein optimaler o\ufb04ine Algorithmus f\u00fcr das k-\nServer-Problem und q0;q1;:::;qneine Folge von Anfragen. Sei ci= (ci\n1;:::;ci\nk)die Kon\ufb01guration\nvon/o.sc/p.sc/t.sc/i.sc/m.sc/a.sc/l.sc unddi= (di\n1;:::;di\nk)die Kon\ufb01guration von /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc in Runde i. Sei mider mi-\nnimale Wert vonPk\nj=1jci\nj\u0000di\nf(j)j\u00fcber alle bijektiven Zuordnungen f:f1;:::;kg!f 1;:::;kg. Sei\nweiterhin gi=P\n(j;`)2f1;:::;kg2jdi\nj\u0000di\n`j. Wir betrachten die Potentialfunktion F(di) =k\u0001mi+gi,\n1.Geben Sie eine kurze Beispielsequenz von Anfragen an und berechnen Sie dazu eine optimale\nL\u00f6sung, die L\u00f6sung von /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc und die Werte f\u00fcr mi,gi, undF(di)in jeder Runde i.\n2.SeiAein beliebiger O\ufb04ine-Algorithmus f\u00fcr das k-Server Problem. Wandeln Sie Ain einen Al-\ngorithmus A0um, bei dem (1) Server nur auf Anfragen stehen bleibt und (2) der maximal einen\nServer zur gleichen Zeit bewegt. Folgern Sie hieraus, dass man sich bei der Betrachtung von op-\ntimalen Algorithmen auf Solche beschr\u00e4nken kann, die dieses Verhalten haben. Passen Sie Ihre\noptimale Berechnung aus 1. entsprechend an.\n3.Zeigen Sie, dass die amortisierten Kosten aiim Schritt ibez\u00fcglich der Potentialfunktion Fma-\nximal k\u0001GimitGi=Pk\nj=1jci\u00001\nj\u0000ci\njjbetr\u00e4gt. De\ufb01nieren Sie hierzu Di=Pk\nj=1jdi\u00001\nj\u0000di\njjund\nstellen Sie die Gleichung f\u00fcr die amortisierten Kosten aibez\u00fcglich Fauf. Betrachten Sie nun die\nBewegungen von /o.sc/p.sc/t.sc/i.sc/m.sc/a.sc/l.sc und/d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc in Runde igetrennt:\n3.1Beweisen Sie, dass sich durch die Bewegung von /o.sc/p.sc/t.sc/i.sc/m.sc/a.sc/l.sc die Di\ufb00erenz mi\u0000mi\u00001um\nmaximal Gierh\u00f6ht.\n3.2Behandeln Sie die F\u00e4lle qi=2Iiundqi2Iieinzeln und zeigen Sie jeweils, dass sich durch\ndie Bewegung von /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc die Di\ufb00erenz mi\u0000mi\u00001um mindestens Direduziert und\ndie Di\ufb00erenz gi\u0000gi\u00001um maximal (k\u00001)\u0001Dierh\u00f6ht.\nFolgern Sie ai\u0014k\u0001Giund wenden Sie den Hauptsatz der amortisierten Analyse an, um hiermit\nzu zeigen, dass /d.sc/o.sc/u.sc/b.sc/l.sc/e.sc /c.sc/o.sc/v.sc/e.sc/r.sc k-kompetitiv.\n\u00dcbung 7.2 Warteschlangen mit Kellern implementieren, mittel\nWarteschlangen sind Datenstrukturen mit den Methoden enqueue (e)unddequeue (). Durch enqueue (e)\nreiht man ein Element am Ende der Warteschlange ein, mit dequeue ()entnimmt man das erste Element\nder Warteschlange.\nBeschreiben Sie die Implementierung einer Warteschlange mithilfe von zwei Kellern und zeigen Sie\nmit der Potentialmethode, dass die amortisierten Kosten eines Methodenaufrufs O(1)betragen.\nZur Erinnerung: Ein Keller ist eine Datenstruktur mit den Operationen push (e)undpop(), mit denen\nman ein Element in den Keller legen beziehungsweise das oberste Element aus dem Keller entnehmen\nkann. Wir betrachten Keller, bei denen die Laufzeit der Operationen in O(1)liegt.\n\u00dcbung 7.3 Bin\u00e4rer Z\u00e4hler mit R\u00fccksetzbefehl, mittel\nAuf Seite 7-6 habe Sie die Methode\nvoid increment (int[]A)\n{\nint i= 0;\nwhile (A[i] == 1) {\nA[i] = 0;\ni++;\n}\nA[i] = 1;\n}\nzur Inkrementierung eines bin\u00e4ren Z\u00e4hlers kennengelernt. Es wurde bereits mittels einer amortisierten\nAnalyse gezeigt, dass sich die Laufzeit von nAufrufen durch eine Funktion in O(n)absch\u00e4tzen l\u00e4sst.\nNeben der Inkrementierung betrachten wir nun als zus\u00e4tzliche Operation die Zur\u00fccksetzung, bei der\nalle Bits der Zahl auf 0 gesetzt werden. Wir nehmen an, dass man eine Zeiteinheit zum Zur\u00fccksetzen\neines Bits investieren muss.\n1.Passen Sie den Pseudocode der Inkrementiermethode so an, dass sich der Algorithmus immer\ndie Position des h\u00f6chstwertigen 1-Bits merkt.\n2.Geben Sie den Pseudocode einer R\u00fccksetzmethode an, die ausgehend vom h\u00f6chstwertigen 1-Bit\nalle Positionen bis zur Position 0 abl\u00e4uft und 1-Bits auf 0 setzt.\n3.Verwenden Sie die Potentialmethode, um zu zeigen, dass die Laufzeit f\u00fcr nInkrementier- und\nR\u00fccksetzoperationen O(n)betr\u00e4gt.7 Analysemethode: Amortisierung\n\u00dcbungen zu diesem Kapitel79\n\u00dcbung 7.4 Warteschlangen mit Mehrfachentnahme, mittel\nErweitern Sie die Implementierung der Warteschlange aus \u00dcbung 7.2 um multi-dequeue (k), eine Ope-\nration, welche die ersten kElemente aus der Warteschlange entnimmt. Wenden Sie die folgende Po-\ntentialmethode an, um zu zeigen, dass die amortisierten Kosten f\u00fcr jeden Aufruf von enqueue (e),\ndequeue ()undmulti-dequeue (k)konstant sind:\nF=2\u0001Anzahl der Element auf dem Enqueue-Stack +\nAnzahl der Elemente auf dem Dequeue-Stack :\n\u00dcbung 7.5 Bin\u00e4rer Z\u00e4hler mit Division, mittel\nDer bin\u00e4re Z\u00e4hler aus der Vorlesung soll nun um eine Methode division-by-two ()erweitert werden,\nwelche die im Array gespeicherte nat\u00fcrliche Zahl durch 2 dividiert. F\u00fcr den Z\u00e4hler bedeutet dies, dass\ndas/l.sc/s.sc/b.scgel\u00f6scht und alle anderen Eintr\u00e4ge um eine Position in dessen Richtung verschoben werden.\nGeben Sie wie in \u00dcbung 7.3 eine Methode f\u00fcr die neue Operation an und zeigen Sie mithilfe der\nfolgenden Potentialfunktion, dass die Laufzeit f\u00fcr nInkrementier- und Divisionsoperationen O(n)be-\ntr\u00e4gt:\nF=Anzahl der 1-Bits +Wert der Zahl :808 A&D: Union-Find\n8-1 8-1\nKapitel 8\nA&D: Union-Find\nVereinigt Euch!\n8-2 8-2Lernziele dieses Kapitels\n1.Den optimalen Algorithmus zur Verwaltung\ndisjunkter Mengen erkl\u00e4ren k\u00f6nnen.\n2.Die amortisierte Analyse des Algorithmus\nverstehen.Inhalte dieses Kapitels\n8.1 Die Verwaltung disjunkter Menge 81\n8.1.1 Die Problemstellung . . . . . . . . . . . 81\n8.1.2 Die Datenstruktur . . . . . . . . . . . . 82\n8.2 Analyse 85\n8.2.1 Begri\ufb00e: Eimer und M\u00fcnzen . . . . . . 85\n8.2.2 Regeln: Ein- und Auszahlung . . . . . . 85\n8.2.3 Absch\u00e4tzung der amortisierten Kosten . 87\n8.2.4 Absch\u00e4tzung des maximalen Levels . . . 88\nWorum\nes heute\ngehtWorum\nes heute\ngehtIn der etwas fortgeschrittenen Algorithmik st\u00f6\u00dft man oft auf Algorithmen und Datenstruk-\nturen, deren Konstruktion kompliziert ist und die aufw\u00e4ndig zu verwalten sind \u2013 wer schon\neinmal einen AVL-Baum oder einen Fibbonacci-Heap implementieren durfte (h\u00f6chstwahr-\nscheinlich wohl aber eher \u00bbmusste\u00ab), wei\u00df, wovon ich spreche. Die Analyse solcher Struk-\nturen ist dann in der Regel auch kein Zuckerschlecken, schlie\u00dflich wollen viele F\u00e4lle und\nProbleme bedacht werden.\nIn seltenen F\u00e4llen liefert die Algorithmik uns aber Datenstrukturen oder Algorithmen, die\nzwar ganz einfach zu erkl\u00e4ren sind, aber ziemlich schwierig zu analysieren sind. Ein Bei-\nspiel solch einer Datenstruktur sind die Union-W\u00e4lder, um die es in diesem Kapitel geht.\nSie zu erkl\u00e4ren ist wirklich einfach: Es handelt es sich um wurzelgerichtete W\u00e4lder, die drei\nOperationen unterst\u00fctzen: (1) Einen neuen einzelnen Knoten zum Wald als eigenen Baum\nhinzuf\u00fcgen, (2) zwei B\u00e4ume vereinigen, indem die Wurzel des kleineren Baumes ein wei-\nteres Kind der Wurzel des anderen wird und (3) von einem Knoten aus die Wurzel eines\nBaumes suchen und dabei alle Knoten auf dem Pfad zur Wurzel zu direkten Kindern der\nWurzel zu machen. Ok, vielleicht doch keine \u00bbganz\u00ab einfache Datenstruktur, aber kompli-\nziert ist sie nun wirklich nicht.\nGanz anders stehen die Dinge bei der Analyse dieser Datenstruktur. \u00c4ltere Lehrb\u00fccher trau-\nen sich den einfachen (!) Teil der Analyse nur in Abschnitten mit einem dicken Sternchen zu\npr\u00e4sentieren, der wohl andeuten soll, dass die Formel-Orgien in den Analysen de\ufb01nitiv nicht\njugendfrei sind. In den letzten Jahren wurden die Argumente in verschiedenen Ver\u00f6\ufb00entli-\nchungen zwar deutlich vereinfacht, wirklich einfach sind sie trotzdem noch nicht geworden.\nInsofern m\u00f6chte ich auch f\u00fcr die in diesem Kapitel vorgestellte Analyse, die verschiedene\nIdeen aus der Literatur kombiniert, gerne eine FSK 16 Einstufung aussprechen \u2013 um einen\nmathematischen Hardcore-Porno, vor dem man emp\ufb01ndliche Studierendenseelen sch\u00fctzen\nm\u00fcsste, handelt es sich aber nicht (wenn Sie an so etwas interessiert sind, schauen Sie doch\nmal in die Orginalliteratur).8 A&D: Union-Find\n8.1 Die Verwaltung disjunkter Menge81\n8.1 Die Verwaltung disjunkter Menge\n8.1.1 Die Problemstellung\n8-4 8-4 Motivation: Der Algorithmus von Kruskal\n1procedure minimum -spanning -tree(vertices V ,edges E ,edge weights w )\n2 sort the edges by w (ifnecessary )\n3S ;\n4 foreach edge e=fu;vginorder of increasing weight do\n5 ifuandvareindi\ufb00erent components of the graph (V;S)then\n6 S S[\b\nfu;vg\t\n7 return minimum spanning tree (V;S)\nDie Geschwindigkeit dieses Algorithmus h\u00e4ngt an zwei Fragen:\n1.Wie schnell lassen sich die Kanten sortieren?\n2.Wie schnell l\u00e4sst sich die Frage beantworten, ob uundvin der gleichen Zusammen-\nhangskomponente liegen?\nWir suchen eine Datenstruktur, um die zweite Frage schnell zu beantworten.\n8-5 8-5 Gesucht: Eine Datenstruktur zur Verwaltung disjunkter Mengen.\nZiele\nWir suchen ein Datenstruktur zur Verwaltung von Familien disjunkter Mengen \u2013 in unserem\nFall bilden jeweils die Knoten einer Zusammenhangskomponente eine Menge. Wir m\u00fcssen\nMengen vereinigen k\u00f6nnen undtesten k\u00f6nnen, ob Element in derselben Menge liegen.\nIdeen\n\u2013Jede Menge wird durch eines seiner Element repr\u00e4sentiert , es hei\u00dft der Repr\u00e4sentant.\n\u2013Es gibt eine Methode \ufb01nd(x), die den Repr\u00e4sentanten der Menge \ufb01ndet, diexenth\u00e4lt.\n\u2013Zwei Elemente xundysind genau dann in derselben Menge, wenn \ufb01nd(x) =\ufb01nd(y).\nWir suchen also eine Datenstruktur, die folgende Operationen unterst\u00fctzt:\ninit(x)Erzeugt eine einelementige Menge mit dem Element x.\nrep(x)Gibt den Repr\u00e4sentanten der Menge zur\u00fcck, die xenth\u00e4lt.\nunite (x;y)Vereinigt die Mengen, die xundyenthalten.\n8-6 8-6 Der Algorithmus von Kruskal mit unserer Datenstruktur\n1procedure minimum -spanning -tree(vertices V ,edges E ,edge weights w )\n2 sort the edges by w (ifnecessary )\n3 foreach v2Vdo\n4 init(v)\n5S ;\n6 foreach edge e=fu;vginorder of increasing weight do\n7 fu rep(u)\n8 fv rep(v)\n9 iffu6=fvthen\n10 unite (fu;fv)\n11 S S[\b\nfu;vg\t\n12 return minimum spanning tree (V;S)828 A&D: Union-Find\n8.1 Die Verwaltung disjunkter Menge\n8.1.2 Die Datenstruktur\n8-7 8-7 Zur Diskussion\nMachen Sie Vorschl\u00e4ge, wie Sie eine Datenstruktur f\u00fcr disjunkte Menge implementieren\nk\u00f6nnten!\n8-8 8-8 Der Union-Wald: Mengen als B\u00e4ume.\nStruktur eines Union-Waldes\n1.Die Elemente jeder Menge bilden einen (\ufb02achen) Baum , die Familie von Mengen damit\neinen Wald .\n2.Der Repr\u00e4sentant eines Baumes ist seine Wurzel .\n3.Wirvereinigen zwei Mengen (= B\u00e4ume), indem wir die Wurzel eines Baumes zu einem\nweiteren Kind der Wurzel des anderen Baums machen.\nBeispiel: Ein Wald f\u00fcr\b\nfag;fb;c;f;g;hg;fd;e;i;j;k;lg;fm;ng\t\na b\nc f g\nhd\ne\ni j k lm n\nBeispiel: Der Wald nach der Vereinigung von fb;c;f;g;hgundfd;e;i;j;k;lg\na d\nb\nc f g\nhe\ni j k lm\nn\n8-9 8-9 Je flacher der Baum, desto besser.\nDie \ufb01nd-Operation ist umso teurer, je tiefer der Baum ist. Deshalb gilt: Je \ufb02acher der Baum,\ndesto besser.\nDrei Ideen:\n1.Jeder Knoten hat ein Attribut \u00bb rank\u00ab, das bei 1 beginnt und das wir nur \u00bbwiderwillig\nerh\u00f6hen\u00ab.\n2.Bilden wir die Vereinigung zweier B\u00e4ume, so h\u00e4ngen wir immer den Baum mit klei-\nnerem Wurzelrang unter den mit gr\u00f6\u00dferem Wurzelrang. (Sind die R\u00e4nge gleich, muss\nerst \u00bbwiderwillig\u00ab einer explizit erh\u00f6ht werden.)\n3.Wenn wir eine Wurzel suchen und sowieso schon alle Knoten auf dem Pfad zur Wur-\nzel \u00bbanfassen\u00ab, k\u00f6nnen wir sie auch gleich auf dem R\u00fcckweg zu Kindern der Wurzel\nmachen.\n8-10 8-10 Vier Methoden, auf denen sich die Operationen \u00bbinit\u00ab, \u00bbrep\u00ab und \u00bbunite\u00ab leicht\naufbauen lassen.\n1procedure makeset (x)\n2 x.parent null\n3 x.rank 1\n4\n5procedure increase (x)\n6 //Precondition :xisaroot\n7 x.rank x.rank +1\n8\n9procedure link(x,y)8 A&D: Union-Find\n8.1 Die Verwaltung disjunkter Menge83\n10 //Precondition :xandyareroots andhave di\ufb00erent ranks\n11 case x .rank <y.rank:x.parent y\n12 case x .rank >y.rank:y.parent x\n13\n14procedure \ufb01nd(x)\n15 ifx.parent =null then\n16 return x\n17 else\n18 x.parent \ufb01nd(x.parent )\n19 return x.parent\n8-11 8-11 Beispiel einer Folge von Operationen.\nStart nach sieben Makesets\n1 1 1 1 1 1 1\nIncrease auf roten Knoten\n1 1 1 1 1 1 1\nResultat\n2 1 1 1 1 1 1\nLinke nun rote Knoten\n2 1 1 1 1 1 1\nResultat\n2\n11 1 1 1 1\nLinke nun wieder rote Knoten\n2\n11 1 1 1 1\nResultat\n2\n1 11 1 1 1\nIncrease auf roten Knoten\n2\n1 11 1 1 1\nResultat\n2\n1 12 1 1 1848 A&D: Union-Find\n8.2 Die Verwaltung disjunkter Menge\nResultat nach weiterem Link\n2\n1 12\n11 1Resultat nach weiterem Increase\n2\n1 13\n11 1\nResultat nach weiterem Link\n3\n2\n1 111 1Finde nun roten Knoten\n3\n2\n1 111 1\nErgebnis\n3\n2\n11 11 1Finde nun roten Knoten\n3\n2\n11 11 1\nErgebnis\n3\n2 1 1 11 1\n8-12 8-12 .Zur \u00dcbung\nBestimmen Sie den Wald, der aufgrund der folgenden Folge von 20 Operationen entsteht:\n1.makeset (a),makeset (b),makeset (c),makeset (d),makeset (e),\n2.increase (a),increase (a),increase (a),increase (a),\n3.increase (b),increase (b),increase (b),\n4.increase (c),increase (c),\n5.link(d;c),\n6.link(e;c),\n7.link(b;c),\n8.link(a;b),\n9.\ufb01nd(c),\n10.\ufb01nd(e).\n8-13 8-13 Ein paar einfache Beobachtungen.\n1.Der Elternknoten eines Knoten hat immer einen echt h\u00f6heren Rang .\n2.Ist ein Knoten erstmal keine Wurzel mehr, \u00e4ndert sich sein Rang nie wieder .\n3.Auf einem Pfad zur Wurzel steigen die R\u00e4nge strikt an.8 A&D: Union-Find\n8.2 Analyse85\n8.2 Analyse\n8-14 8-14 Wie lange dauert eine Folge von Operationen?\nWir suchen eine obere Schranke f\u00fcr die Laufzeit vonnvon Operationen (Makeset, Increase,\nFind, Link). Dazu f\u00fchren wir eine amortisierte Analyse durch mit einer Potentialfunktion.\nEs wird leicht sein zu sehen, dass die amortisierten Kosten O(na(n))sind, wenn wir die\nFunktion a(n)gro\u00df genug w\u00e4hlen. Es wird schwieriger sein zu sehen, dass wir als a(n)die\numgekehrte Ackermann-Funktion w\u00e4hlen k\u00f6nnen .\n8.2.1 Begriffe: Eimer und M\u00fcnzen\n8-15 8-15 Grundideen der Analyse\nWir stellen uns vor, dass neben jedem Knoten a(n)Eimer stehen, nummeriert von 1bis\na(n). (Wir werden a(n)erst sp\u00e4ter bestimmen.) In die Eimer werden wir nach und nach\ngleichm\u00e4\u00dfig M\u00fcnzen einf\u00fcllen (bei Makeset- und Increase-Operationen). Bei einer Find-\nOperation werden wir maximal eine M\u00fcnze pro Knoten entfernen. Tun wir dies, so entneh-\nmen wir die M\u00fcnze dem ersten nichtleeren Eimer. Seine Nummer nennen wir den Level des\nKnotens. Das Potential eines Waldes ist die Summe aller M\u00fcnzen in allen Eimern in allen\nKnoten.\n1\n1. 2. 3.1\n1. 2. 3.2\n1. 2. 3.1\n1. 2. 3.4\n1. 2. 3.1\n1. 2. 3.3\n1. 2. 3.\n2\n1. 2. 3.\n1\n1. 2. 3.\n\u2013Jeder Knoten hat a(n) =3Eimer.\n\u2013DasPotential des roten Knotens ist 4, das des blauen 2, das Gesamtpotential ist 40.\n\u2013DerLevel des roten Knotens ist 2, des blauen 3. Alle Wurzelknoten haben Level 1.\n8.2.2 Regeln: Ein- und Auszahlung\n8-16 8-16 Die genauen Regeln, wie M\u00fcnzen in die Eimer kommen.\nRegel f\u00fcr makeset (x)\nEin Knoten xbeginnt mit einer M\u00fcnze pro Eimer.\nRegel f\u00fcr increase (x)\nF\u00fcge jedem Eimer von xeine M\u00fcnze hinzu.\nRegel f\u00fcr link(x;y)\nTue nichts.\nRegel f\u00fcr \ufb01nd(x)\nSeien x1bisxkdie Knoten auf dem Pfad von xzur Wurzel w, mit x1=xundxk=w. F\u00fcr\ni2f1;:::;k\u00001gentferne eine M\u00fcnze bei xi, wenn es ein j2fi+1;:::;k\u00001ggibt mit\nxi:level =xj:level. (\u00bbPro Level wird beim wurzeln\u00e4chsten Knoten keine M\u00fcnze entfernt.\u00ab)\n8-17 8-17 Der Wald nach increase (red).\n1\n1. 2. 3.1\n1. 2. 3.2\n1. 2. 3.1\n1. 2. 3.4\n1. 2. 3.2\n1. 2. 3.3\n1. 2. 3.\n2\n1. 2. 3.\n1\n1. 2. 3.868 A&D: Union-Find\n8.2 Analyse\n8-18 8-18 Der Wald nach \ufb01nd(blue).\n1\n1. 2. 3.1\n1. 2. 3.2\n1. 2. 3.1\n1. 2. 3.4\n1. 2. 3.2\n1. 2. 3.3\n1. 2. 3.\n2\n1. 2. 3.\n1\n1. 2. 3.\n8-19 8-19 Beispiel 1 des Effekts eines \u00bbFind\u00ab.\n1\n1. 2. 3. 4.2\n1. 2. 3. 4.3\n1. 2. 3. 4.4\n1. 2. 3. 4.5\n1. 2. 3. 4.\n\ufb01nd(blue)\n1\n1. 2. 3. 4.2\n1. 2. 3. 4.3\n1. 2. 3. 4.4\n1. 2. 3. 4.5\n1. 2. 3. 4.\n8-20 8-20 Beispiel 2 des Effekts eines \u00bbFind\u00ab.\n1\n1. 2. 3. 4.2\n1. 2. 3. 4.3\n1. 2. 3. 4.4\n1. 2. 3. 4.5\n1. 2. 3. 4.\n\ufb01nd(blue)\n1\n1. 2. 3. 4.2\n1. 2. 3. 4.3\n1. 2. 3. 4.4\n1. 2. 3. 4.5\n1. 2. 3. 4.\nSkript Skript F\u00fcr alle, die die obige Beschreibung mit Eimern und M\u00fcnzen mal \u00bbmathematisch aufgeschrieben\nsehen m\u00f6chten\u00ab, hier die De\ufb01nitionen noch einmal etwas formaler:\nIDefinition: Potentialfunktion\nF\u00fcr eine Folge o1;:::;onder Makeset-, Increase-, Link- oder Find-Operationen sei Dnder resultierende\nWald gem\u00e4\u00df dem Algorithmus von Folie 8-10. Wir erweitern (konzeptionell) die Attribute der Knoten\n(derzeit \u00bbparent\u00ab und \u00bbrank\u00ab) um ein weiteres Attribut \u00bbcoins\u00ab. Wie sich die Werte dieses Attributs\nver\u00e4ndern, wird sp\u00e4ter de\ufb01niert. Es gilt aber:\nF(Di) =X\nxist Knoten in Dix:coins :\nIDefinition: Level\nF\u00fcr einen Knoten xsei der Level des Knoten de\ufb01niert als\nx:level = (a(n) +1)\u0000dx:coins =x:ranke\nIDefinition: \u00c4nderung des Coins-Attributs\nDurch eine Operation oim\u00f6ge sich der Wald DzuD0\u00e4ndern. Die Coins-Attribute \u00e4ndern sich dann\nin Abh\u00e4ngigkeit davon, wie oilautet, wie folgt:8 A&D: Union-Find\n8.2 Analyse87\nmakeset (x)InD0giltx0:coins =a(n). (In jedem Eimer ist initial eine M\u00fcnze, was bei a(n)Eimern\ngerade a(n)M\u00fcnzen entspricht.)\nincrease (x)Es gilt x0:coins =x:coins +a(n). (Pro Eimer wird eine M\u00fcnze hinzugef\u00fcgt.)\nlink(x;y)Keine \u00c4nderungen der Coins-Attribute.\n\ufb01nd(x)Seien x1bisxkdie Knoten auf dem Pfad von xzur Wurzel r, mit x1=xundxk=r. F\u00fcr\ni2f1;:::;k\u00001gsei\nx0\ni:coins =(\nxi:coins\u00001;wenn es ein jmiti<j<kundxi:level =xj:level gibt,\nxi:coins ; sonst.\n8-21 8-21 .Zur \u00dcbung\nBestimmen Sie das Potential des Waldes, der aufgrund der folgenden Folge von 20 Opera-\ntionen entsteht ( a(n) =3):\n1.makeset (a),makeset (b),makeset (c),makeset (d),makeset (e),\n2.increase (a),increase (a),increase (a),increase (a),\n3.increase (b),increase (b),increase (b),\n4.increase (c),increase (c),\n5.link(d;c),\n6.link(e;c),\n7.link(b;c),\n8.link(a;b),\n9.\ufb01nd(c),\n10.\ufb01nd(e).\n8.2.3 Absch\u00e4tzung der amortisierten Kosten\n8-22 8-22 Die amortisierten Kosten pro Operation sind h\u00f6chstens a(n).\nISatz\nSeia(n)gro\u00df genug gew\u00e4hlt, dass niemals alle Eimer bei irgendeinem Knoten leer werden.\nDann sind die amortisierten Kosten jeder Makeset-, Increase-, Link- und Find-Operation\nh\u00f6chstens O(a(n))und damit die realen Gesamtkosten einer Folge nsolcher Operationen\nh\u00f6chstens O(n\u0001a(n)).\nBeweis. Die amortisierten Kosten einer Operation sind die realen Kosten plus die \u00c4nderung\ndes Potentials. Bei uns ist die Potential\u00e4nderung gerade die Anzahl an hinzugef\u00fcgten M\u00fcn-\nzen. Die realen Kosten von makeset (x)undincrease (x)sind 1und es werden a(n)M\u00fcnzen\nhinzugef\u00fcgt, was O(1+a(n))amortisierte Kosten ergibt. Die realen Kosten von link(x;y)\nsind 1und es werden keine M\u00fcnzen hinzugef\u00fcgt, was O(1)als amortisierte Kosten ergibt.\nDie realen Kosten von \ufb01nd(x)ist die L\u00e4nge des Pfades von xzur Wurzel. Bis auf a(n)vie-\nle Knoten entfernen wir aber f\u00fcr jeden Knoten auf diesem Pfad eine M\u00fcnze; das Potential\nwird also genau um die Pfadl\u00e4nge gesenkt \u2013 bis auf die a(n)Knoten, die dann genau die\namortisierten Kosten darstellen.888 A&D: Union-Find\n8.2 Analyse\n8.2.4 Absch\u00e4tzung des maximalen Levels\n8-23 8-23 Wie schnell leeren sich die Eimer?\nUnsere Analyse ergibt eine \u00bbum so bessere Absch\u00e4tzung\u00ab, je kleiner wir a(n)w\u00e4hlen k\u00f6n-\nnen. Wir m\u00fcssen daher wissen, \u00bbwie schnell die Eimer leer werden\u00ab. F\u00fcr einen Knoten xsei\np=x:parent . Wir zeigen gleich, dass\n1.der erste Eimer erst leer ist, wenn p:rank >2\u0001x:rank;\n2.der zweite Eimer erst leer ist, wenn p:rank >2x:rank=2\"x:rank;\n3.der dritte Eimer erst leer ist, wenn p:rank >2\"\"x:rank,\n4.der vierte Eimer erst leer ist, wenn p:rank >2\"\"\"x:rank,\n5.und so weiter.\nDie \u00bbPfeil-Notation\u00ab ist nach Knuth wie folgt de\ufb01niert:\na\"nb=8\n><\n>:a\u0001b f\u00fcrn=0,\na f\u00fcrb=1und\na\"n\u00001(a\"n(b\u00001))sonst.\n8-24 8-24 Level 1: Der erste Eimer\nILemma\nSeixein Knoten, dessen erster Eimer leer ist. Dann gilt p:rank\u00152\u0001x:rank.\nBeweis. Wenn zum allerersten Mal eine M\u00fcnze entfernt wird, gilt schon x:rank <p:rank. Es\nwird nur dann eine M\u00fcnze entfernt, wenn sp\u00e4ter auf dem Pfad von xzur Wurzel ein weiterer\nKnoten x0kommt, dessen erster Eimer ebenfalls nicht leer ist. Da x0:rank <p0:rank undxals\nneuen Elternknoten die Wurzel wdes Pfades bekommt mit p0:rank\u0014w:rank, folgt, dass der\nneue Elternknoten wvonxnach dem \u00bbUmh\u00e4ngen\u00ab einen um mindestens 1h\u00f6heren Rang\nhat. Damit der erste Eimer von xkomplett leer wird, muss sich also x:rank mal der Rang des\nElternknotens um mindestens 1 erh\u00f6ht haben.\nw\np0\nx0\n1. 2. 3. 4.\np\nx\n1. 2. 3. 4.Rang: +?Rang: +1\ufb01nd(x)w\np0\nx0\n1. 2. 3. 4.\np\nx\n1. 2. 3. 4.Rang: +?+1\n8-25 8-25 Level 2: Der zweite Eimer\nILemma\nSeixein Knoten, dessen zweiter Eimer leer ist. Dann gilt p:rank >2x:rank.\nBeweis. Es wird nur dann eine M\u00fcnze aus dem zweiten Eimer entfernt, wenn sp\u00e4ter auf\ndem Pfad von xzur Wurzel ein weiterer Knoten x0kommt, dessen zweiter Eimer ebenfalls\nnicht leer ist. F\u00fcr diesen gilt nach dem vorigen Lemma 2x0:rank <p0:rank und somit auch\n2x:rank <w:rank. Da wneuer Elternknoten von xwird, verdoppelt sich der Rang des El-\nternknotens jedes Mal, wenn eine M\u00fcnze aus dem zweiten Eimer entfernt wird. Damit der\nzweite Eimer von xkomplett leer wird, muss sich also x:rank mal der Rang des Elternknotens\nverdoppelt haben.8 A&D: Union-Find\nZusammenfassung dieses Kapitels89\nw\np0\nx0\n1. 2. 3. 4.\np\nx\n1. 2. 3. 4.Rang:\u0001?Rang:\u00012\ufb01nd(x)w\np0\nx0\n1. 2. 3. 4.\np\nx\n1. 2. 3. 4.Rang:\u0001?\u00012\n8-26 8-26 Level 3: Der dritte Eimer\nILemma\nSeixein Knoten, dessen dritter Eimer leer ist. Dann gilt p:rank >2\"\"x:rank.\nBeweis. \u00c4hnlich wie in den vorigen zwei Lemmas gilt: Wir entnehmen nur dann eine M\u00fcnze\ndem dritten Eimer, wenn es ein x0mit leerem zweiten Eimer sp\u00e4ter auf dem Pfad gibt. Nach\ndem vorigen Lemma gilt beim Sprund von x0zup0, dass der Rang von x0:rank aufp0:rank >\n2x0:rankspringt \u2013 und damit dann auch der von x:rank aufw:rank. Jede Entnahme einer M\u00fcnze\naus dem dritten Eimer erh\u00f6ht somit den Rang des Elternknoten von rauf2r. Damit der Eimer\nleer wird, m\u00fcssen wir also \u00bb x:rank oft potenzieren\u00ab, was gerade 2\"\"x:rank ergibt.\n8-27 8-27 Die weiteren Eimer und a(n).\nILemma\nSeixein Knoten, dessen i-ter Eimer leer ist. Dann gilt p:rank >2\"\u0001\u0001\u0001\"|{z}\ni\u00001Malx:rank =2\"i\u00001\nx:rank.\nBeweis. Per Induktion \u00fcber i; das Argument ist genau das aus den Lemmata vorher.\nIFolgerung\nW\u00e4hlen wir a(n)als kleinestes imitn\u00142\"i2, so wird nie ein Eimer leer werden.\nDieses a(n)ist gerade die umgekehrte Ackermann-Funktion .\nZusammenfassung dieses Kapitels\n8-28 8-28 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationDisjunkte Mengen verwalten\nDivide-and-Conquer\nLaziness\nUnion-W\u00e4lder\nAmortisierte Analyse mit\nPotentialmethode\nn\u0001a(n)Laufzeit908 A&D: Union-Find\nZusammenfassung dieses Kapitels\nISatz\nEine beliebige Folge von nMakeset-, Increase-, Find- und Link-Operationen kann in Zeit\nO\u0000\nn\u0001a(n)\u0001\nabgearbeitet werden.\nIBeweisidee\n1.Das Potential ist die Summe aller M\u00fcnzen in den a(n)Eimern pro Knoten.\n2.Makeset und Increase platzieren eine M\u00fcnze in jedem Eimer.\n3.F\u00fcr die Kosten eines Find bezahlen wir mit einer M\u00fcnze pro Knoten, au\u00dfer bei den\njeweils ersten Knoten eines Levels, was nur a(n)viele sind.\n4.Deri-te Eimer von xkann erst leer werden, wenn der Elternrang mindestens 2\"i\u00001\nx:rank betr\u00e4gt.\n5.Ist daher a(n)die umgekehrte Ackermann-Funktion, so gehen uns nie die M\u00fcnzen aus.\n(Man kann zeigen, dass jede Datenstruktur f\u00fcr die Verwaltung disjunkter Mengen diese\nKosten hat.)Teil IV\nZufall als Entwurfsmethode91\nTeil IV\nZufall als Entwurfsmethode\nEs gibt im Deutschen sogar ein Sprichwort, dass uns dringend davon abr\u00e4t, dem obigen\nMotto der nachfolgenden Kapitel zu folgen:\n\u00bb\u00dcberlasse nichts dem Zufall.\u00ab\nUnd es stimmt ja: Computer m\u00fcssen sehr akribisch vorgehen, wenn sie Probleme l\u00f6sen.\nSchon ein umgekipptes Bit in den Daten \u2013 geschweige denn im Code \u2013 bringt einen Rech-\nner schnell zum Blue-Screen. Trotz dieser berechtigten Einw\u00e4nde ist \u00bbder Zufall\u00ab ein aus-\ngesprochen n\u00fctzlicher Geselle, wenn es um den Entwurf von Algorithmen geht.\nAls allen Leserinnen und Lesern ho\ufb00entlich wohlvertrautes Beipsiel betrachten wir den\nQuicksort-Algorithmus mit der \u2013 zugegebenerma\u00dfen eher d\u00fcmmlichen \u2013 Wahl des jeweils\nersten Elements des zu sortierenden Arrays als Pivot-Element. Wenn nun die zu sortieren-\nden Daten bereits sortiert sind, dann hat Quicksort eine sehr miese Laufzeit von W(n2). Sind\ndie Eingabedaten hingegen zuf\u00e4llig, so ist die Laufzeit im Erwartungswert nur O(nlogn).\nDer Quicksort ist aber nur das bekannteste Beispiel eines Algorithmus, der mit zuf\u00e4lligen\nDaten sehr gut umgehen kann, mit \u00bbspeziellen\u00ab Daten hingegen eher schlecht.\nVor diesem Hintergrund kann man bei einem Algorithmus wie Quicksort entweder einfach\n\u00bbho\ufb00en\u00ab, dass die Daten halt sch\u00f6n zuf\u00e4llig sind und, wenn sie es nicht sind, eben in den\nsauren Laufzeitapfel bei\u00dfen. Alternativ kann man aber eben auch selber den Zufall ins Spiel\nbringen: Bei Quicksort ist dies ganz einfach, indem man statt dem ersten Element einfach\nein zuf\u00e4lliges Element als Pivot-Element nutzt.\nGenerell geht es in diesem Teil des Skripts um die \u00bbGeisteshaltung\u00ab, den Zufall in das\nDesign von Algorithmen einzubauen. Wie wir sehen werden, bringt dies in vielen F\u00e4llen\nenorme Beschleunigungen in den Algorithmen \u2013 ohne dass die Korrektheit der Ergebnisse\ndarunter leiden m\u00fcsste.929 A&D: Suchen in hashbaren Daten\n9-1 9-1\nKapitel 9\nA&D: Suchen in hashbaren Daten\nKuckucks-Hash-Tabellen sind perfekt, dynamisch und verdammt schnell\n9-2 9-2Lernziele dieses Kapitels\n1.Ideen und Methodik des Hashings au\ufb00rischen\n2.Hash-Tabellen mit ver\u00e4nderlicher Gr\u00f6\u00dfe analysieren\nk\u00f6nnenInhalte dieses Kapitels\n9.1 Grundlagen zu Hash-Tabellen 93\n9.1.1 Die Idee . . . . . . . . . . . . . . . . . 94\n9.1.2 Verkettung . . . . . . . . . . . . . . . . 95\n9.1.3 Lineares Sondieren . . . . . . . . . . . . 95\n9.1.4 Hash-Funktionen . . . . . . . . . . . . . 96\n9.2 Dynamische Gr\u00f6\u00dfenanpassung 96\n9.2.1 Die Verdoppelung-Halbierungs-Strategie 96\n9.2.2 Amortisierte Analyse . . . . . . . . . . 97\n\u00dcbungen zu diesem Kapitel 99\nWorum\nes heute\ngehtWorum\nes heute\ngehtWas ist eigentlich ein \u00bbHash\u00ab? Wie es sich f\u00fcr ein kurzes englisches Wort geh\u00f6rt, hat es\nziemlich viele Bedeutungen, denn generell kann man bei einem guten englischen W\u00f6rter-\nbuch eine fast beliebige Kombination von vier Buchstaben suchen und mit ziemlicher Sicher-\nheit kommen eine gro\u00dfe Anzahl von Bedeutungen heraus \u2013 von denen au\u00dfer den W\u00f6rter-\nbuchschreibern noch niemand gewusst hat. Beispielsweise bezeichnet das doch eher selten\ngebrauchte Substantiv \u00bbbalk\u00ab solche unterschiedliche Dinge wie \u00bban illegal motion made by\na baseball pitcher that may decieve a base runner\u00ab, \u00bba roughly squared timber beam\u00ab, \u00bbany\narea on a pool or billiard table in which play is restricted in some way\u00ab und bekannterma\u00dfen\nnat\u00fcrlich auch \u00bba ridge left unplowed between furrows\u00ab.\nEin \u00bbHash\u00ab ist nun laut Webster-W\u00f6rterbuch \u00bba dish of cooked meat cut into small pieces\nand recooked, usually with potatoes\u00ab. Es gibt Hashes in verschiedenen Varianten, die auch\nliebevoll aufgez\u00e4hlt werden; die o\ufb00enbar kulinarisch belesenen Autoren des W\u00f6rterbuches\nhaben so auch an Vegetarier im Rahmen eines \u00bbhash of raw tomatoes, chilies, and coriander\u00ab\ngedacht. Entsprechend bedeutet dann \u00bbto hash\u00ab, \u00bbmake meat or other food into a hash\u00ab, man\nkann \u00bbhashing\u00ab also grob als \u00bbverhackst\u00fcckseln\u00ab \u00fcbersetzen. (Anderseits bedeutet \u00bbto make\na hash of something\u00ab so viel wie wie \u00bbes verpatzen\u00ab.)\nWie verhackst\u00fcckselt man nun aber Daten? Dies erscheint doch als eher gef\u00e4hrlicher, viel-\nleicht sogar verbotener Vorgang. Tats\u00e4chlich wird beim Hashing ein Objekt genommen\nund daraus durch wildes \u00bbHerumrechnen\u00ab eine einzelne Zahl errechnet \u2013 der so genann-\nteHashwert des Objekts. \u00c4hnlich wie man beim gekochten Hash auch nicht mehr erkennen\nkann, von welchem Rindvieh es stammt, so kann man einem Hashwert auch nicht mehr\nansehen, von welchem Objekt es stammt. Wichtig ist beim Hashing eigentlich nur, dass un-\nterschiedliche Objekte (m\u00f6glichst) unterschiedliche Hashwerte bekommen sollten.\nIn diesem Kapitel m\u00f6chte ich zun\u00e4chst Ihr Wissen \u00fcber Hashtabellen \u00bbetwas au\ufb00rischen\u00ab,\naber auch ein neues Konzept einf\u00fchren: die Idee der dynamischen Gr\u00f6\u00dfenanpassung. Bei\nden meisten Hashtabellen wei\u00df man in dem Moment, wo man sie angelegt, noch nicht, wie\ngro\u00dfe die Tabelle wird. W\u00e4hlt man nun die Gr\u00f6\u00dfe zu klein, so werden viele Hashverfahren\nextrem langsam oder funktionieren gar nicht mehr. W\u00e4hlt man sie zu gro\u00df, so wird unter\nUmst\u00e4nden sehr viel Speicher verschwendet. Die dynamische Gr\u00f6\u00dfenanpassung scha\ufb00t hier9 A&D: Suchen in hashbaren Daten\n9.1 Grundlagen zu Hash-Tabellen93\nAbhilfe: Immer, wenn die Auslastung der Tabelle (der \u00bbLoad-Factor\u00ab) zu sehr von einem\nIdealwert abweicht, wird die Gr\u00f6\u00dfe der Tabelle angepasst. Dies bedeutet aber, dass die Tabel-\nle komplett neu aufgebaut werden muss, was in der Regel ziemlich lange dauert. Hier setzt\nnun aber die uns mittlerweile wohlvertraute amortisierte Analyse ein: Wir werden zeigen,\ndass die amortisierten Kosten der Gr\u00f6\u00dfen\u00e4nderungen konstant sind. Mit anderen Worten:\nWir bekommen die automatische Gr\u00f6\u00dfenanpassung von Hashtabellen geschenkt.\nDas Wort \u00bbHash\u00ab ist \u00fcbrigens im Vergleich zu den Alternativen, die der Thesaurus bereit-\nh\u00e4lt, eher langweilig. Es w\u00fcrde die Informatikliteratur sicherlich etwas au\ufb02ockern, redete\nman statt von Hashwerten von mishmash values , die man statt in eine Hash-Tabelle in eine\ngallimaufry table einf\u00fcgt.\n9.1 Grundlagen zu Hash-Tabellen\n9-4 9-4 Wie schnell geht es wirklich?\nWas Suchb\u00e4ume k\u00f6nnen\nZeitbedarf der Basis-Operationen bei 2-3-Suchb\u00e4umen, wenn nElemente gespeichert sind:\nOperation Zeitbedarf\ninsert (k;v)O(logn)\ndelete (k)O(logn)\n\ufb01nd(k)O(logn)\nDer Platzbedarf ist ebenfalls O(n).\nWas wir gerne h\u00e4tten\nIdealer Zeitbedarf der Basis-Operationen, wenn nElemente gespeichert sind:\nOperation ?\ninsert (k;v)O(1)\ndelete (k)O(1)\n\ufb01nd(k)O(1)\nDer Platzbedarf soll ebenfalls O(n)betragen.\nDas Versprechen der Hash-Tabellen\n\u2013Eine Hash-Tabelle versucht, die drei Basisoperationen in konstanter Zeit erledigt zu\nbekommen.\n\u2013Standard-Verfahren wie lineares Sondieren ben\u00f6tigen jedoch im Worst-Case Zeit O(n).\n\u2013Perfekte Hash-Tabellen unterst\u00fctzen hingegen zumindest das Suchen in Worst-Case Zeit\nO(1).\n9-5 9-5 Die Dinge, nach denen wir heute suchen\n1.Jedes Objekt besteht aus einem Schl\u00fcssel und einem Wert.\n2.DieSchl\u00fcssel sind Zahlen aus dem \u00bbSchl\u00fcsseluniversum\u00ab U=f0;1;2;:::;N\u00001g(oder\nlassen sich als solche kodieren).\nBeispiel\nKonten in einem Graphen\nSchl\u00fcssel Die Knotennummern\nWerte Informationen, die an den Knoten \u00bbh\u00e4ngen\u00ab\n\u00dcbersetzung der Anforderungen nach Java\ninterface HashMap .Entry <Kextends Object ,V>\n{\nK getKey ();\nV getValue ();\nvoid setValue (Vvalue );\n}949 A&D: Suchen in hashbaren Daten\n9.1 Grundlagen zu Hash-Tabellen\n9-6 9-6 Welche Grundoperationen sind erlaubt?\nOperationen, die wir heute unterst\u00fctzen wollen\n1.Einf\u00fcgen von Elementen\n2.L\u00f6schen von Elementen\n3.Suchen nach Elementen anhand von Schl\u00fcsseln\n\u00dcbersetzung der Anforderungen nach Java\ninterface HashMap <Kextends Object ,V>\n{\nvoid insert (Kk,Vv);\nvoid delete (Kk);\nHashMap .Entry <K,V>search (Kk);\n}\n9.1.1 Die Idee\n9-7 9-7 Die einfache Grundidee hinter einer Hash-Tabelle.\nEine Hash-Tabelle Tist ein Array einer bestimmten Gr\u00f6\u00dfe r, wobei rin der Regel viel\nkleiner als Nist. Eine Hash-Funktion hbildet die Schl\u00fcssel \u00bbzuf\u00e4llig\u00ab auf Arraypositionen\nab, also h:U!f0;:::;r\u00001g. Ein zu speichernder Schl\u00fcssel x2Uwird idealerweise an\nPosition h(x)im Array gespeichert.\nBeispiel: Einf\u00fcgen von 14und36mith(x) =xmod r.\nLeere Tabelle\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull\nnull\nnullEinf\u00fcgen von 14\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull\nnull\n14\nEinf\u00fcgen von 36\nT[4]T[3]T[2]T[1]T[0] null\n36\nnull\nnull\n14\nWill man 9an Stelle h(9) =h(14) =4speichern, so muss man etwas Schlaues tun. Die\nHash-Verfahren unterscheiden sich nur dadurch, was das konkret bedeutet.9 A&D: Suchen in hashbaren Daten\n9.1 Grundlagen zu Hash-Tabellen95\n9.1.2 Verkettung\n9-8 9-8 Etwas Schlaues tun I: Verkettung\n\u2013In den Arrayfeldern speichern wir nicht direkt die Schl\u00fcssel , sondern jedes Arrayfeld\nist der Anfang einer verketteten Liste .\n\u2013Alle Objekte, die von der Hashfunktion auf dasselbe Feld abgebildet werden, werden\nin der Liste des Feldes gespeichert.\n\u2013Suchen in dieser Liste ist zwar langsam , aber da Kollisionen selten sind, ist die Liste\nsehrkurz.\nEinf\u00fcgen von 7,9,12.\nLeere Tabelle\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull\nnull\nnullEinf\u00fcgen von 7\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull\nnull7\nEinf\u00fcgen von 9\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull7\n9Einf\u00fcgen von 12\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull7 12\n9\n9.1.3 Lineares Sondieren\n9-9 9-9 Etwas Schlaues tun II: Lineares Sondieren\n\u2013Ist an h(x)kein Platz, so versuchen wir es eben an Stelle h(x) +1.\n\u2013Wenn dort kein Platz ist, dann an Stelle h(x) +2und so weiter.\n\u2013Beim Suchen m\u00fcssen wir dann aber neben h(x)auch alle Folgepositionen durchgehen,\nbis wir eine leere Stelle \ufb01nden.\n\u2013Beim L\u00f6schen muss man eine L\u00f6sch-Markierung an die Stelle setzen.\nEinf\u00fcgen von 7,9,12,34, L\u00f6schen von 7.\nLeere Tabelle\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnull\nnull\nnullEinf\u00fcgen von 7\nT[4]T[3]T[2]T[1]T[0] null\nnull\n7\nnull\nnull\nEinf\u00fcgen von 9\nT[4]T[3]T[2]T[1]T[0] null\nnull\n7\nnull\n9Einf\u00fcgen von 12\nT[4]T[3]T[2]T[1]T[0] null\nnull\n7\n12\n9969 A&D: Suchen in hashbaren Daten\n9.2 Dynamische Gr\u00f6\u00dfenanpassung\nEinf\u00fcgen von 34\nT[4]T[3]T[2]T[1]T[0] 34\nnull\n7\n12\n9L\u00f6schen von 7\nT[4]T[3]T[2]T[1]T[0] 34\nnull\nmark\n12\n9\n9.1.4 Hash-Funktionen\n9-10 9-10 Hash-Funktionen muss man mit Liebe aussuchen.\nMan muss Hash-Funktion hsorgf\u00e4ltig w\u00e4hlen : Da das Universum Uviel gr\u00f6\u00dfer ist als die\nTabelle, muss es notgedrungen sehr viele Kollisionen geben . Liegen diese Kollisionen \u00bbun-\ng\u00fcnstig\u00ab , so kommt es zu Verklumpungen.\nBeispiel\nSeir=256die Tabellengr\u00f6\u00dfe und N=232die Universumsgr\u00f6\u00dfe. Sei h(x) =xmod 256 ,\nalso das \u00bbletzte Byte von x\u00ab. Dann ist die Anzahl der Kollisionen minimal , aber die Hash-\nFunktion trotzdem sehr ungeeignet, wenn das letzte Byte aller zu hashenden Schl\u00fcssel 0\nist.\n9-11 9-11 Universelle Hash-Funktionen haben keine Vorlieben.\nIDefinition: Universelle Hash-Funktionen\nEine Familie Hvon Hash-Funktionen hei\u00dft universell , wenn es f\u00fcr je zwei Schl\u00fcssel xund\nyh\u00f6chstensjHj=rHash-Funktionen h2Hgibt mit h(x) =h(y).\nW\u00e4hlt man eine Hash-Funktion aus einer universellen Familie Hzuf\u00e4llig , so ist f\u00fcr je zwei\nSchl\u00fcssel xundydie Wahrscheinlichkeit einer Kollision nur 1=r. Dies ist dieselbe Wahr-\nscheinlichkeit, als w\u00e4ren h(x)undh(y)komplett zuf\u00e4llig gew\u00e4hlt .\nISatz\nSeip>Neine Primzahl. Dann ist die Familie H=fha;bja;b2f1;:::; p\u00001ggeine uni-\nverselle Hash-Familie mit\nha;b(x) = (( ax+b)mod p)mod r:\nZum (nicht sonderlich schwierigen) Beweis siehe [1].\n9.2 Dynamische Gr\u00f6\u00dfenanpassung\n9.2.1 Die Verdoppelung-Halbierungs-Strategie\n9-12 9-12 Wie gro\u00df sollte eine Hash-Tabelle sein?\n.Zur Diskussion\nSie rufen new SuperDuperHashTable ()auf. Dann f\u00fcgen Sie 10 Elemente ein. Dann\nsuchen Sie 1.000 Mal nach Elementen. Dann f\u00fcgen Sie 10.000 weitere Elemente ein. Dann\nsuchen Sie 1.000 Mal nach Elementen. Dann l\u00f6schen Sie 9.999 Element wieder. Wie gro\u00df\nsollte die Hash-Tabelle sein?\n9-13 9-13 Der Load-Factor einer Tabelle sollte nicht zu gro\u00df und nicht zu klein sein.\nIDefinition: Load-Factor\nDerLoad-Factor einer Hash-Tabelle ist das Verh\u00e4ltnis\na=Anzahl der gespeicherten Elemente\nGr\u00f6\u00dfe der Tabelle (also r):\n\u2013Istasehr klein (zum Beispiel a<0:01), so wird sehr viel Platz verschwendet.\n\u2013Istasehr gro\u00df ( a>0:9), so werden alle Operationen langsam oder unm\u00f6glich.9 A&D: Suchen in hashbaren Daten\n9.2 Dynamische Gr\u00f6\u00dfenanpassung97\n9-14 9-14 Die Idee des Rehashing.\nBig Idea\nImmer, wenn der Load-Factor ein erlaubtes Intervall verl\u00e4sst, so wird die Tabellengr\u00f6\u00dfe\nangepasst, so dass er wieder \u00bbgut\u00ab ist.\nBeispielsweise k\u00f6nnte man verlangen, dass aimmer zwischen amin=0:1undamax=0:4\nliegt. Wird dies verletzt, so wird die Gr\u00f6\u00dfe so angepasst, dass wieder a=aideal=0:25gilt.\nDie Idee ist gut, aber. . .\nDie Gr\u00f6\u00dfe der Hash-Tabelle hat massiven Ein\ufb02uss auf die Positionen der Schl\u00fcssel. Folglich\nm\u00fcssen alle Schl\u00fcssel neu eingef\u00fcgt werden .\n1algorithm checkLoadFactor\n2 r Gr\u00f6\u00dfe von T\n3 n Anzahl Elemente inT\n4a n=r\n5 ifa<aminora>amaxthen\n6 rnew n=aideal\n7 Tnew neue Tabelle mit rnewEintr \u00e4gen\n8 //Rehash :\n9 fori 0tor\u00001do\n10 ifT[i]6=nullandT[i]6=mark then\n11 insert T[i]intoTnew\n12 T Tnew\n9.2.2 Amortisierte Analyse\n9-15 9-15 Was kostet uns die Dynamik?\nISatz\nF\u00fcr eine Hash-Tabelle ohne Gr\u00f6\u00dfenanpassung m\u00f6gen die Basis-Operationen je O(1)lange\ndauern.\nDann kann f\u00fcr die Hash-Tabelle mit Gr\u00f6\u00dfenanpassung eine Folge von mEinf\u00fcge- und\nL\u00f6schen-Operationen h\u00f6chstens O(m2)dauern.\nBeweis. O\ufb00enbar dauert ein Rehash Zeit O(r), maximal also O(m). Da ein Rehash nach\njeder der Operationen passieren kann, k\u00f6nnen mOperationen maximal O(m2)Aufwand be-\ndeuten.\n.Zur Diskussion\nVergleichen Sie diese Laufzeit mit der Laufzeit von mEinf\u00fcge- und L\u00f6schen-Operationen\nin einen 2-3-Baum.\n9-16 9-16 Die amortisierte Laufzeit der Dynamik.\nDie Worst-Case-Absch\u00e4tzung scheint viel zu pessimistisch: Nach einem Rehash m\u00fcssen erst\nviele Einf\u00fcge- oder L\u00f6schen-Operationen kommen , bevor wieder ein Rehash n\u00f6tig ist. Es\nliegt nahe, eine amortisierte Analyse zu versuchen .\nISatz\nF\u00fcr eine Hash-Tabelle ohne Gr\u00f6\u00dfenanpassung m\u00f6gen die Basis-Operationen je Kosten 1\nverursachen.\nDann kann f\u00fcr die Hash-Tabelle mit Gr\u00f6\u00dfenanpassung mit amin<aideal<amaxeine Folge\nvonmEinf\u00fcge- und L\u00f6schen-Operationen h\u00f6chstens Kosten O(m)verursachen.\nBeweis. Wir zeigen die Behauptung nur f\u00fcr amin=1=8,aideal=1=4undamax=1=2; f\u00fcr\nden allgemeinen Fall siehe \u00dcbung 9.1.\nWir de\ufb01nieren eine Potentialfunktion wie folgt: F(Ti)sei gerade der zw\u00f6l\ufb00ache Betrag der\nAbweichung von der idealen Anzahl der Elemente , alsoF(Ti) =12jni\u0000ri=4j. O\ufb00enbar ist\ndies anfangs 0und nie negativ.\nDie amortisierten Kosten des Einf\u00fcgens eines Elements sind:989 A&D: Suchen in hashbaren Daten\nZusammenfassung dieses Kapitels\n1.Wenn kein Rehash n\u00f6tig ist und ni=ri>aideal, so steigt das Potential um 12, also:\nai= 1|{z}\nreale Kosten+F(Ti)\u0000F(Ti\u00001) =13:\n2.Wenn kein Rehash n\u00f6tig ist und ni=ri<aideal, so f\u00e4llt das Potential um 12, also: ai=\n1+F(Ti)\u0000F(Ti\u00001) =\u000011.\n3.Wenn ein Rehash n\u00f6tig ist:\nai=ri+ri\u00001|{z}\nreale Kosten+F(Ti)\u0000F(Ti\u00001) =3\n2ri+0\u000012(ri\u00001=2\u0000ri\u00001=4) =0:\nDie amortisierten Kosten sind also immer durch eine Konstante beschr\u00e4nkt.\nDie amortisierten Kosten des L\u00f6schens sind:\n1.Wenn kein Rehash n\u00f6tig ist, wie beim Einf\u00fcgen gerade \u000011oder 13.\n2.Wenn ein Rehash n\u00f6tig ist:\nai=ri+ri\u00001|{z}\nreale Kosten+F(Ti)\u0000F(Ti\u00001)\n=3ri+0\u000012jri\u00001=8\u0000ri\u00001=4j=0:\nWieder sind die amortisierten Kosten also konstant.\nDie amortisierten Kosten von mOperationen sind folglich h\u00f6chstens 13mund somit auch\ndie realen Kosten.\nZusammenfassung dieses Kapitels\n9-17 9-17 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationSuchen in\nhashbaren Daten\nRandomisierung\nEinfache Hash-Tabellen\nWorst-Case-Analyse\nAverage-Case-Analyse\nO(n)Zugri\ufb00szeit\nO(1)Zugri\ufb00szeitHash-Tabellen mit ver\u00e4nderlicher\nGr\u00f6\u00dfe\nad hoc\nVerdopplungs- /\nHalbierungsstrategie\nAmortisierte Analyse\nLineare Worst-Case-Laufzeit9 A&D: Suchen in hashbaren Daten\n9.3 \u00dcbungen zu diesem Kapitel99\nIHash-Tabellen mit ver\u00e4nderlicher Gr\u00f6\u00dfe\nPasst man die Gr\u00f6\u00dfe von Hash-Tabellen immer dann an, wenn der Load-Factor Schwellwerte\n\u00fcber- oder unterschreitet , so sind bleiben die amortisierte Kosten konstant .\nZum Weiterlesen\n[1]Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cli\ufb00ord Stein, Intro-\nduction to Algorithms, zweite Au\ufb02age, MIT Press, 2001, Kapitel \u00bbHash Tables\u00ab.\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 9.1 Amortisierte Analyse von Hash-Tabellen mit Gr\u00f6\u00dfenanpassung, mittel\nBeweisen Sie Satz 9-16 f\u00fcr beliebige amin,aidealundamax.\nTipp: Passen Sie die Potentialfunktion aus dem Beweis geeignet an.\n\u00dcbung 9.2 Hashing mit dynamischer Gr\u00f6\u00dfenanpassung, mittel\nIm Folgenden soll eine Hashtabelle mit linearer Sondierung, dynamischer Gr\u00f6\u00dfenanpassung und ei-\nner universellen Familie Hp=fha;bja;b2f1;:::; p\u00001ggmitha;b(x) = (( ax+b)mod p)mod r\nvon Hashfunktionen simuliert werden. Das dynamische Verhalten der Tabelle ist durch die Parameter\namin=1=8,aideal=1=4, und amax=1=2beschrieben.\n1.Starten Sie mit einer Tabelle der Gr\u00f6\u00dfe r=4und f\u00fcgen Sie nacheinander Elemente mit den\nSchl\u00fcsseln 3, 5, 1 und 2 ein. Verwenden Sie hierzu die Hash-Funktion h10;162H31. Hierbei\nwird eine Gr\u00f6\u00dfenanpassung notwendig, verwenden Sie nach der Gr\u00f6\u00dfenanpassung die Funktion\nh7;82H31.\n2.L\u00f6schen Sie aus der Tabelle nun die Werte 2 und 5. Geben Sie dabei explizit die Berechnung der\nHashwerte und den gesamten Inhalt der Tabelle nach jeder L\u00f6schoperation an.\n\u00dcbung 9.3 Kuckuck-Hashing simulieren, mittel\nIn dieser Aufgabe werden wir das Kuckuck-Hashing an einem Beispiel simulieren. Hierbei verwenden\nwir die Hashfunktion h11;142H17f\u00fcr die erste der beiden Tabellen und die Hashfunktion h1;102H17\nf\u00fcr die zweite. Eine dynamische Gr\u00f6\u00dfenanpassung soll in dieser Aufgabe erst einmal nicht vorgenom-\nmen werden.\nStarten Sie mit zwei leeren Tabellen der Gr\u00f6\u00dfe r=3f\u00fcr das Kuckuck-Hashing. F\u00fcgen Sie nacheinander\ndie Elemente 3, 5, 2, 7 und 10 ein.10010 A&D: Perfektes Hashing\n10-1 10-1\nKapitel 10\nA&D: Perfektes Hashing\nKuckucks-Hash-Tabellen sind perfekt, dynamisch und verdammt schnell\n10-2 10-2Lernziele dieses Kapitels\n1.Idee des perfekten Hashings und einfache perfekte\nHash-Verfahren kennen\n2.Kuckucks-Hash-Tabellen verstehen und\nimplementieren k\u00f6nnenInhalte dieses Kapitels\n10.1 Perfektes Hashing: Die Anforderung 101\n10.2 Statische perfekte Hash-Tabellen 101\n10.2.1 Geburtstagstabellen . . . . . . . . . . . 101\n10.2.2 Analyse der statischen perfekten\nHash-Tabellen . . . . . . . . . . . . . . 103\n10.3 Kuckucks-Hashing 104\n10.3.1 Die Idee . . . . . . . . . . . . . . . . . 104\n10.3.2 Die Implementation . . . . . . . . . . . 104\n10.3.3 Analyse der dynamischen perfekten\nHash-Tabellen . . . . . . . . . . . . . . 107\n\u00dcbungen zu diesem Kapitel 111\nWorum\nes heute\ngehtWorum\nes heute\ngehtDie bekanntesten Hash-Tabellen sind sichlicher die Hash-Tabellen mit linearer Sondierung:\nIst eine Stelle in der Tabelle schon voll, so sucht man sich eben den n\u00e4chsten leeren Platz.\nDiese Methode ist in der Praxis ziemlich gut (genaugenommen ist sie fast unschlagbar gut),\njedoch gibt es aus theoretischer und (ganz selten) auch aus praktischer Sicht einen gravie-\nrenden Nachteil: Es kann sein, dass es recht lange dauert, ein Element zu \ufb01nden. Das ist\nzwar sehr unwahrscheinlich, aber eben auch nicht unm\u00f6glich.\nAus diesem Grund hat man lange geforscht, eine Variante von Hash-Tabellen zu \ufb01nden, die\ngenauso schnell ist wie Hash-Tabellen mit linearer Sondierung, die aber eine garantierte\nkonstante Suchzeit hat. Mit dieser Problematik haben sich dann Theoretiker extensiv be-\nsch\u00e4ftigt und auch Erfolg gehabt. Jedoch hatte dieser \u00bbErfolg\u00ab einen Sch\u00f6nheitsfehler: die\nentwickelten \u00bbDynamic Amortized Perfect Hash Tables\u00ab haben alle nur denkbaren sch\u00f6-\nnen theoretischen Eigenschaften \u2013 in der Praxis sind Hash-Tabellen mit linearer Sondierung\ntrotzdem viel schneller. Das liegt daran, dass diese Tabellen intern doch reichlich komplex\naufgebaut sind und der Verwaltungsoverhead enorm ist.\nDas Blatt hat sich 2001 gewandelt, als auf dem European Symposium on Algorithms das\nKuckucks-Hashing vorgestellt wurde. Diese Methode hat drei entscheidende Vorteile:\n1.Sie ist in der Praxis fast genauso schnell wie lineares Sondieren.\n2.Sie garantiert, dass die Suche nie mehr als zwei Speicherzugri\ufb00e braucht (und damit\nimmer eine sehr kleine konstante Zeit braucht).\n3.Sie ist sehr einfach zu verstehen und zu implementieren.\nDamit ist Kuckucks-Hashing eine praktische Methode des Hashings, die aber auch die Theo-\nriegemeinde gl\u00fccklich gemacht hat: Wie wir in diesem Kapitel sehen werden, ist die Analyse\ndes Kuckucks-Hashing durchaus interessant und deutlich komplexer als die eigentliche Da-\ntenstruktur (das haben sie dann mit der Union-Find-Datenstruktur gemeinsam).10 A&D: Perfektes Hashing\n10.2 Perfektes Hashing: Die Anforderung101\n10.1 Perfektes Hashing: Die Anforderung\n10-4 10-4 Was sollte die perfekte Hash-Tabelle leisten?\nDasSuchen nach einem Schl\u00fcssel ist in einer Hash-Tabelle sicherlich die h\u00e4u\ufb01gste Operation.\nMan kann zeigen, dass diese im Schnitt dauern:\n\u2013O(1+a)bei Hash-Tabellen mit Verkettung und\n\u2013O(1\n1\u0000a)bei Hash-Tabellen mit linearem Sondieren.\nIn beiden F\u00e4llen ist jedoch die Worst-Case-Zeit O(n), wenn es nElemente gibt.\nIDefinition: Perfekte Hash-Verfahren\nEin Hash-Verfahren hei\u00dft perfekt , wenn die Zeit f\u00fcr eine Suche im Worst-Case O(1)betr\u00e4gt.\n(Einf\u00fcge- und L\u00f6schen-Operationen k\u00f6nnen hingegen l\u00e4nger dauern.)\n10.2 Statische perfekte Hash-Tabellen\n10.2.1 Geburtstagstabellen\n10-5 10-5 Der statische Fall.\nWir wollen zun\u00e4chst einige recht starke Voraussetzungen machen:\n1.Die Menge der nzu hashenden Werte ist fest (statisch) und von vornherein bekannt.\n2.Speicherplatz ist kein Problem.\nBeispiele\n\u2013Schl\u00fcsselworte in einer Programmiersprache.\n\u2013Erlaubte /x.sc/m.sc/l.sc-Tags aufgrund einer Document-Type-Description.\n\u2013Optionen eines Shell-Befehls.\n10-6 10-6 Interludium: Das Geburtstagsparadoxon.\n.Zur Diskussion\nWie hoch ist die Wahrscheinlichkeit, dass zwei der Anwesenden im H\u00f6rsaal am gleichen\nTag des Jahres Geburtstag haben?\n\u2013Bei 23 Anwesenden ist sie h\u00f6her als 50%.\n\u2013Bei 50 Anwesenden ist sie h\u00f6her als 97%.\n\u2013Bei 57 Anwesenden ist sie h\u00f6her als 99%.\n\u2013Bei 100 Anwesenden ist sie h\u00f6her als 99,99997%.\nAllerdings:\n\u2013Bei 10 Anwesenden ist sie unter 12%.\n\u2013Bei 20 Anwesenden ist sie unter 42%.\n10-7 10-7 Interludium: Das Geburtstagsparadoxon auf dem Mars\n.Zur Diskussion\nWie hoch ist die Wahrscheinlichkeit, dass zwei der Anwesenden im H\u00f6rsaal am gleichen Tag\ndesMars-Jahres Geburtstag haben, wenn die Vorlesung auf dem Mars statt\ufb01nden w\u00fcrde ?\nNASA, Public domainDas Mars-Jahr hat 687 Tage.\n\u2013Bei 32 Anwesenden ist sie h\u00f6her als 50%.\nAllerdings:\n\u2013Bei 10 Anwesenden ist sie unter 10%.\n\u2013Bei 20 Anwesenden ist sie unter 26%.10210 A&D: Perfektes Hashing\n10.2 Statische perfekte Hash-Tabellen\n10-8 10-8 Wie man das Geburtstagsparadoxon in ein Hash-Verfahren umwandelt.\nIdee zum perfekten statischen Hashing\nW\u00e4ren die Geburtstage gerade die Hashwerte und w\u00e4ren die Tage des Jahres die m\u00f6gli-\nchen Tabelleneintr\u00e4ge, so erh\u00e4lt man mit \u00fcber 50% Wahrscheinlichkeit ein Hashing ohne\nKollisionen , wenn es nur 22 zu hashende Studenten gibt.\nAllgemein gilt:\nISatz\nIstr=n2, so gibt es bei einer zuf\u00e4lligen Wahl der Hash-Funktion mit Wahrscheinlichkeit\nmindestens 50% keine Kollisionen.\nBeweis. Die Wahrscheinlichkeit f\u00fcr eine Kollision von zwei Werten ist 1=r=1=n2. Es gibt\u0000n\n2\u0001\nPaare. Der Erwartungswert f\u00fcr die Anzahl Xan Kollisionen ist somit E[X] =\u0000n\n2\u00011\nn2=\n1\n2n2\u0000n\nn2<1=2. Nach der Markov-Ungleichung gilt Pr[X\u0015t]\u0014E[X]=tund f\u00fcr t=1ist also\ndie Wahrscheinlichkeit, dass es mindestens eine Kollision gibt, kleiner als 1=2.\nMerke\nSind in einem H\u00f6rsaal nStudierende auf einem Planeten mit Umlaufzeit n2Tagen, so haben\nmit weniger als 50% Wahrscheinlichkeit zwei am selben Tag des Planeten-Jahres Geburts-\ntag.\n10-9 10-9 Geburtstagstabellen: perfektes statisches Hashing.\n1algorithm build_birthday_table (k1;v1;:::;kn;vn)\n2 p pick a prime larger than N\n3 start:\n4 T new Entry [n2]\n5 a random value between 1andp\u00001\n6 b random value between 1andp\u00001\n7 fori 1tondo\n8 ifT[ha;b(ki)]is empty then\n9 T[ha;b(ki)] (ki;vi)\n10 else\n11 goto start\n12 return (T;p;a;b)\n13\n14algorithm search (key)\n15 e T[ha;b(key)]\n16 ife is not null and e .key=keythen\n17 return e\n18 else\n19 return null\n10-10 10-10 Und jetzt noch Platz sparen. . .\nDas perfekte Hashing mit einer Tabelle der Gr\u00f6\u00dfe n2funktioniert schon ganz gut, aber der\nPlatzbedarf ist f\u00fcr gr\u00f6\u00dfere nviel zu gro\u00df.\nDer Trick mit den zweistufigen Tabellen\nUm Platz zu sparen, gehen wir wie folgt vor: Wir benutzen zun\u00e4chst doch eine Tabelle der\nGr\u00f6\u00dfe r=n, wodurch (fast) mit Sicherheit viele Kollisionen zustande kommen. Deshalb\nspeichern wir f\u00fcr jede Stelle alle Elemente, die auf diese Stelle gehasht werden, in einer\n(kleinen) perfekten Geburtstagstabelle . Diese Tabellen haben zwar quadratische Gr\u00f6\u00dfe in\nder Anzahl ihrer Eintr\u00e4ge , jedoch gibt es ja auch nur wenige Eintr\u00e4ge . Die Analyse zeigt\ngleich, dass mit hoher Wahrscheinlichkeit die Summe der Gr\u00f6\u00dfen der Geburtstagstabelle\nlediglich O(n)ist.10 A&D: Perfektes Hashing\n10.2 Statische perfekte Hash-Tabellen103\n10-11 10-11 Der Aufbau der statischen perfekten Hash-Tabelle.\nEine Tabelle\nT[4]T[3]T[2]T[1]T[0] null\nnull\nnullT2\nT4\nDie Haupttabelle links mit Hashfunktion hspeichert Verweise auf viele kleine Geburtstags-\ntabellen Ti. Jede speichert:\n1.Alle Schl\u00fcssel kmith(k) =i. Seien dies niviele.\n2.Die (quadratische) Gr\u00f6\u00dfe ri=n2\ni.\n3.Die zuf\u00e4llig gew\u00e4hlten Zahlen aiundbif\u00fcr eine lokale Hashfunktion hai;bif\u00fcr diese\nTabelle.\nO\ufb00enbar ben\u00f6tigt die Gesamttabelle insgesamt Platz\nr\u00001X\ni=0n2\ni:\n10-12 10-12 Die Algorithmen hinter der statischen perfekten Hash-Tabelle.\n1algorithm build_perfect_table (k1;v1;:::;kn;vn)\n2 T new Table [n]\n3 h random hash function forT(pickh=ha;bforrandom aandb)\n4\n5 L new List [n]\n6 fori 1tondo\n7 append (ki;vi)toL[h(ki)]\n8\n9 fori 1tondo\n10 T[i] build_birthday_table (L[i])\n11\n12algorithm search (key)\n13 return T[h(key)].search (key)\n10.2.2 Analyse der statischen perfekten Hash-Tabellen\n10-13 10-13 Viele Geburtstagstabellen brauchen weniger Platz als nur eine.\nISatz\nDer Erwartungswert f\u00fcr die Gesamtgr\u00f6\u00dfe der statischen perfekten Hash-Tabelle ist\nE\"r\u00001X\ni=0n2\ni#\n<2n:\nBeweis. Dan2\ni=ni+2\u0000ni\n2\u0001\nallgemein gilt, l\u00e4sst sich der Erwartungswert wie folgt umschrei-\nben:\nE\u0014r\u00001X\ni=0n2\ni\u0015\n=E\u0014r\u00001X\ni=0ni\n|{z}\n=n\u0015\n+2E\"r\u00001X\ni=0\u0012ni\n2\u0013#\n:\nDie SummePr\u00001\ni=0\u0000ni\n2\u0001\nist die Anzahl aller Kollisionen. Da wir universelles Hashing benut-\nzen, ist die Wahrscheinlichkeit einer Kollision gerade 1=r=1=n.Folglich ist der Erwar-\ntungswert f\u00fcr die Anzahl an Kollisionen gerade\u0000n\n2\u00011\nn=n\u00001\n2. Setzt man dies oben ein, so\nerh\u00e4lt man die Behauptung.10410 A&D: Perfektes Hashing\n10.3 Kuckucks-Hashing\n10-14 10-14 Zusammenfassung zum statischen perfekten Hashing.\nUmnWerte in einer statischen perfekten Hash-Tabelle zu speichern, geht man wie folgt vor:\n1.W\u00e4hle eine Hash-Funktion hzuf\u00e4llig.\n2.Ermittle f\u00fcr jedes i2f0;:::;r\u00001g, wie viele Elemente nihierauf gehasht w\u00fcrden.\n3.Ist die Summe S=Pr\u00001\ni=0n2\nigr\u00f6\u00dfer als 4n, so w\u00e4hle eine andere Hash-Funktion und\nstarte neu. (Die Wahrscheinlichkeit hierf\u00fcr ist nach der Markov-Ungleichung nur Pr[S\u0015\n4n]\u0014E[S]=4n<1=2.)\n4.Anderenfalls baue mit dem Algorithmus build _perfect _table die perfekte Hash-Tabelle.\n5.Die resultierende Tabelle hat Gesamtgr\u00f6\u00dfe 4nundSuchen ben\u00f6tigt immer und somit\nauch im Worst-Case zwei Speicher-Zugri\ufb00e .\n10.3 Kuckucks-Hashing\n10.3.1 Die Idee\n10-15 10-15 Ideen muss man haben.\nDie Situation bis 2001\nDie Idee der statischen perfekte Hash-Tabelle kann man zu einer dynamischen perfekten\nHash-Tabelle weiterentwickeln. Dabei kommt dann allerdings eine komplexe Datenstruktur\nheraus , die in der Praxis untauglich ist.\nDas Kuckucks-Hashing\nIm Jahr 2001 wurde dann aber ein v\u00f6llig neues Verfahren f\u00fcr das dynamische perfekte\nHashing pr\u00e4sentiert, das v\u00f6llig anders ist als alle bisherigen und extrem simple ist.\n10-16 10-16 Eine geniale Idee.\nDas Kuckucks-Hashing baut auf folgenden Grundideen auf:\n1.Es gibt zwei Hash-Tabellen mitzwei Hash-Funktionen statt nur einer.\n2.Jedes Element ist entweder in der einen oder in der anderen Tabelle an seinem Hash-\nWert. (Hierdurch wird die Tabelle perfekt.)\n3.F\u00fcgt man ein Element ein und sind in beiden Tabellen die Eintr\u00e4ge schon belegt, so f\u00fcgt\nman das Element trotzdem ein und daf\u00fcr \ufb02iegt ein Element raus (Kuckucks-Operation),\ndas dann aber in die andere Tabelle kommt . Ist doch auch kein Platz, so wiederholt sich\ndas Spiel.\nCreative Commons Attributen Sharealike License10.3.2 Die Implementation\n10-17 10-17 Der Aufbau im Detail.\nIDefinition\nEine Kuckucks-Hash-Tabelle besteht aus zwei normalen Hash-Tabellen T1undT2mit fol-\ngenden Eigenschaften:\n1.Jede Tabelle hat mindestens r=2nEintr\u00e4ge (es w\u00fcrde aber schon r= (1+e)nreichen).\n2.Die zugeh\u00f6rigen Hash-Funktionen h1undh2sind zuf\u00e4llig und unabh\u00e4ngig.\n3.Jeder Schl\u00fcssel kist entweder an der Stelle T1[h1(k)]gespeichert oder an T2[h2(k)].\nBeispiel: Kuckucks-Hash-Tabelle\nSeien h1(x) = ( 2xmod 7 )mod 5 undh2(x) = ( 3xmod 7 )mod 5 .\n2 T1[4]T1[3]1 T1[2]4 T1[1]T1[0]\nT2[4]T2[3]3 T2[2]T2[1]T2[0]10 A&D: Perfektes Hashing\n10.3 Kuckucks-Hashing105\n10-18 10-18 Suchen und L\u00f6schen sind wirklich einfach.\n1algorithm search (key)\n2 e T1[h1(key)]\n3 ife is not null and e .key=keythen\n4 return e\n5 else\n6 e T2[h2(key)]\n7 ife is not null and e .key=keythen\n8 return e\n9 else\n10 return null\n11\n12algorithm delete (key)\n13 e T1[h1(key)]\n14 ife is not null and e .key=keythen\n15 T1[h1(key)] null\n16 else\n17 e T2[h2(key)]\n18 ife is not null and e .key=keythen\n19 T2[h2(key)] null\nO\ufb00enbar brauchen beide Operationen nur konstante Zeit (und sind auch praktisch sehr schnell).\n10-19 10-19 Das Einf\u00fcgen und der Kuckuck.\nBeim Einf\u00fcgen kann es passieren, dass ein Element weder in die erste noch in die zwei-\nte Tabelle an den gew\u00fcnschten Platz passt . In diesem Fall scha\ufb00t die Kuckucks-Operation\nPlatz:\nKuckucks-Operation\nSeii2f1;2gundj=3\u0000i\u00bbder Index der anderen Tabelle\u00ab. Nehmen wir an, in Tiist an\nStelle hi(k)ein Element mit Schl\u00fcssel kgespeichert. Die Kuckucks-Operation entfernt nun\ndieses Element aus Tiund f\u00fcgt es an der Stelle hj(k)in der Tabelle Tjein.\nO\ufb00enbar kann es bei der Kuckucks-Operation vorkommen, dass am \u00bbZielort Tj[hj(k)]\u00bb wie-\nder kein Platz ist. Dann f\u00fchren wir wieder die Kuckucks-Operation hierauf aus, bis wir einen\nPlatz \ufb01nden.\nSollte man in eine Endlosschleife geraten, so muss man sich etwas Schlaues ausdenken.\n10-20 10-20 Beispiel eines Einf\u00fcgevorgangs.\nSeien h1(x) = (xmod 100 )div10undh2(x) =xmod 10 .\nAusgangstabelle\n41 T1[4]T1[3]23 T1[2]10 T1[1]T1[0]\nT2[4]T2[3]22 T2[2]T2[1]T2[0]Einf\u00fcgen von 12 mit h1(12) =1undh2(12) =2\n41 T1[4]T1[3]23 T1[2]10 T1[1]T1[0]\nT2[4]T2[3]22 T2[2]T2[1]T2[0]\nKuckuck auf 10: Fliegt raus, 12 rein,. . .\n41 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]T2[3]22 T2[2]T2[1]T2[0]Kuckuck auf 10: . . . und kommt bei h2(10) =0rein\n41 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]T2[3]22 T2[2]T2[1]10 T2[0]10610 A&D: Perfektes Hashing\n10.3 Kuckucks-Hashing\nDirektes Einf\u00fcgen von 123\n41 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]T2[1]10 T2[0]Einf\u00fcgen von 43\n41 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]T2[1]10 T2[0]\nKuckuck auf 41: Fliegt raus, 43 rein,. . .\n43 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]T2[1]10 T2[0]Kuckuck auf 41: . . . und kommt bei h2(41) =1rein\n43 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]41 T2[1]10 T2[0]\nEinf\u00fcgen von 110\n43 T1[4]T1[3]23 T1[2]12 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]41 T2[1]10 T2[0]Kuckuck auf 13: Fliegt raus, 110 rein,. . .\n43 T1[4]T1[3]23 T1[2]110 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]41 T2[1]10 T2[0]\nKuckuck auf 13: . . . und kommt bei h2(12) =2rein\n43 T1[4]T1[3]23 T1[2]110 T1[1]T1[0]\nT2[4]123 T2[3]22 T2[2]41 T2[1]10 T2[0]Kuckuck auf 22: Fliegt raus, 12 rein,. . .\n43 T1[4]T1[3]23 T1[2]110 T1[1]T1[0]\nT2[4]123 T2[3]12 T2[2]41 T2[1]10 T2[0]\nKuckuck auf 22: . . . und kommt bei h1(22) =2rein\n43 T1[4]T1[3]23 T1[2]110 T1[1]T1[0]\nT2[4]123 T2[3]12 T2[2]41 T2[1]10 T2[0]Kuckuck auf 23: Fliegt raus, 22 rein,. . .\n43 T1[4]T1[3]22 T1[2]110 T1[1]T1[0]\nT2[4]123 T2[3]12 T2[2]41 T2[1]10 T2[0]\nKuckuck auf 23: . . . und kommt bei h2(23) =3rein\n43 T1[4]T1[3]22 T1[2]110 T1[1]T1[0]\nT2[4]123 T2[3]12 T2[2]41 T2[1]10 T2[0]Kuckuck auf 123: Fliegt raus, 23 rein,. . .\n43 T1[4]T1[3]22 T1[2]110 T1[1]T1[0]\nT2[4]23 T2[3]12 T2[2]41 T2[1]10 T2[0]\nKuckuck auf 123: . . . und kommt bei h1(123) =2rein\n43 T1[4]T1[3]22 T1[2]110 T1[1]T1[0]\nT2[4]23 T2[3]12 T2[2]41 T2[1]10 T2[0]10 A&D: Perfektes Hashing\n10.3 Kuckucks-Hashing107\n10-21 10-21 Pseudo-Code des Einf\u00fcgevorgangs.\n1algorithm insert (k;v)\n2 insert_me (k;v)\n3 ifT1[h1(k)]is empty then\n4 T1[h1(k)] insert_me\n5 else ifT2[h2(k)]is empty then\n6 T2[h2(k)] insert_me\n7 else\n8 count_down n\n9 i 1\n10 swap (insert_me ,Ti[hi(insert _me:k)])\n11 while insert_me is not null and count_down >0do\n12 i 3\u0000i\n13 swap (insert_me ,Ti[hi(insert _me:k)])\n14 count_down count_down -1\n15 ifinsert_me is not null then\n16 rehash\n17 insert (insert_me )\n10-22 10-22 Zusammenfassung zur Implementation von Kuckucks-Hash-Tabelle\nLaufzeit der Grundoperationen\n1.Einf\u00fcge-Operationen ben\u00f6tigen immer maximal zwei Zugri\ufb00e.\n2.L\u00f6schen-Operationen ben\u00f6tigen ebenfalls maximal zwei Zugri\ufb00e.\n3.Einf\u00fcge-Operationen k\u00f6nnen, sogar mehrfach, zu einem Rehash der gesamten Tabelle\nf\u00fchren.\nEffekt der Gr\u00f6\u00dfenanpassung\nEs kann zu einem \u00dcberlauf oder Unterlauf der Tabelle kommen:\n1.Wir setzen amax=1=16, (wobei allerdings schon1\n2(1+e)reichen w\u00fcrde). Falls also die\nTabellen insgesamt mehr als r=8Elemente speichern sollen, werden sie vergr\u00f6\u00dfert und\nein Rehash durchgef\u00fchrt.\n2.Man kann aminbeliebig setzen. Ein Wert von beispielsweise 1=64bietet sich an.\nWir haben schon gesehen, dass die amortisierten Kosten hiervon konstant sind.\n10.3.3 Analyse der dynamischen perfekten Hash-Tabellen\n10-23 10-23 Wie gut funktioniert das Kuckucks-Hashing?\nISatz\nSei eine Folge von Einf\u00fcge-, Such- und L\u00f6schen-Operationen gegeben. Dann ben\u00f6tigen diese\nOperationen folgende Zeiten:\n1.Alle Such-Operationen ben\u00f6tigen immer maximal Zeit O(1).\n2.Die L\u00f6schen-Operationen ben\u00f6tigen amortisierte Zeit O(1). Wenn amin=0, so ben\u00f6-\ntigen sie sogar immer maximal Zeit O(1).\n3.Die Einf\u00fcge-Operationen ben\u00f6tigen erwartete amortisierte Zeit O(1).\nDie ersten beiden Punkte sollten klar sein. Im Folgenden soll also der dritte Punkte bewiesen\nwerden und wir nehmen vereinfachend an, dass einfach nur nElemente in zwei leere Tabelle\nder Gr\u00f6\u00dfe jeweils r=2neingef\u00fcgt werden .10810 A&D: Perfektes Hashing\n10.3 Kuckucks-Hashing\n10-24 10-24 Erste Vorbereitung: Der \u00bbKuckucksgraph\u00ab\nF\u00fcr die Analyse ist o\ufb00enbar wichtig, wie Elemente entlangt von \u00bbKanten\u00ab zwischen den\nbeiden Tabellen wechseln. Wir f\u00fchren daher Begri\ufb00e ein, die das \u00bbmodellieren\u00ab:\nIDefinition: Kuckucksgraph\nSeien T1undT2zwei zum Teil gef\u00fcllte Kuckuckstabellen. Der zugeh\u00f6rige Kuckucksgraph\nist ein ungerichteter Multigraph (es kann Kanten mehrfach geben) mit 2rKnoten, n\u00e4mlich je\neinen pro \u00bbSlot\u00ab der beiden Tabellen, und f\u00fcr jeden der nnichtleeren Eintr\u00e4ge xin einer der\nbeiden Tabellen einer ungerichteten Kante zwischen dem Knoten, der Slot h1(x)der ersten\nTabelle entspricht, und dem Knoten, der Slot h2(x)der zweiten Tabelle entspricht.\nBeispiel\nKuckuckstabelle\n41 T1[4]T1[3]23 T1[2]10 T1[1]T1[0]\nT2[4]T2[3]22 T2[2]T2[1]110 T2[0]Kuckucksgraph\n10-25 10-25 Zweite Vorbereitung: Der \u00bbKuckuckspfad\u00ab.\nIDefinition: Kuckuckspfad\nEinKuckuckspfad von snach tist eine nichtleere Folge e1;:::;ekvonunterschiedlichen\nKanten im Kuckucksgraph, so dass\n1.die erste Kante senth\u00e4lt und die letzte tund\n2.je zwei aufeinander folgende Kante einen Knoten gemeinsam haben.\nBemerkung: \u00bbUnterschiedliche\u00ab Kanten bedeutet, dass die Kanten im Multigraphen unter-\nschiedlich sein m\u00fcssen, wohl aber zwischen denselben Knoten verlaufen k\u00f6nnen.\nBeispiel\nDie roten Kanten (a;b)bilden einen Kuckuckspfad von s1nach t1, die gr\u00fcnen Kanten (c;d)\neinen von s2nach t2. Die Folgen (a;b;c)und(a;b;a)bilden keine Kuckuckspfade:\ns1=t1\nt2\ns2a\nb\ncd\n10-26 10-26 Die zwei Schritte der Analyse: Die erwartete Laufzeit ist immer O(1).\nWir zeigen, dass in den beiden folgenden F\u00e4llen die erwartete Laufzeit f\u00fcr das Einf\u00fcgen\neines Elementes xnurO(1)betr\u00e4gt:\n1.Das Einf\u00fcgen \u00bbgelingt\u00ab.\n2.Das Einf\u00fcgen \u00bbmisslingt\u00ab und wir m\u00fcssen (mindestens) einmal rehashen.\n10-27 10-27 Analyse des Falls \u00bbdas Einf\u00fcgen gelingt\u00ab.\nFalls das Einf\u00fcgen von xgelingt, so muss Folgendes geschehen sein: Beginnend bei einem\nKnoten im Kuckucksgraph sind Elemente immer entlang von Kanten des Graphen verscho-\nben worden, bis irgendwann Platz f\u00fcr das letzte Element war.\nBeobachtung 1\nAlle Verschiebungen waren entlang von Kanten innerhalb einer Zusammenhangskompo-\nnente des Kuckucksgraphen.10 A&D: Perfektes Hashing\n10.3 Kuckucks-Hashing109\nBeobachtung 2\nEntlang einer Kante kann ein Element h\u00f6chstens zweimal verschoben werden (sonst ger\u00e4t\nman n\u00e4mlich in eine Endlosschleife).\nIFolgerung\nDie erwartete Gr\u00f6\u00dfe der Zusammenhangskomponenten ist eine obere Schranke f\u00fcr die er-\nwartete Laufzeit im Fall \u00bbdas Einf\u00fcgen gelingt\u00ab.\n10-28 10-28 Das kleine Pfadl\u00e4ngenlemma.\nILemma\nSeien sundtKnoten in einem Kuckucksgraphen. Dann ist die Wahrscheinlichkeit, dass es\nim Kuckucksgraphen eine Kante von snach tgibt, h\u00f6chstens 1=(2r).\nBeweis.\nF\u00fcr eine beliebige Kante xgilt: Die Wahrscheinlichkeit f\u00fcr h1(x) =sund f\u00fcr h2(x) =tbetr\u00e4gt\njeweils 1=r. Insgesamt ist die Wahrscheinlichkeit, dass xgerade sundtverbindet, h\u00f6chstens\n1=r2. Da es n=r=2Kanten gibt, verbindet mit Wahrscheinlichkeit h\u00f6chstens n=r2\u00141=(2r)\neine von ihnen gerade sundt.\n10-29 10-29 Das gro\u00dfe Pfadl\u00e4ngenlemma.\nILemma\nSeien sundtKnoten in einem Kuckucksgraphen und l\u00151eine L\u00e4nge. Dann ist die Wahr-\nscheinlichkeit, dass der k\u00fcrzeste Kuckuckspfad von snach tgenau L\u00e4nge lhat, h\u00f6chstens\n1=(2lr).\nBeweis durch Induktion \u00fcber l.F\u00fcrl=1liefert das kleine Pfadl\u00e4ngenlemma die Behaup-\ntung. F\u00fcr Schritt von l\u00001zulgibt es genau dann einen k\u00fcrzesten Pfad Kuckuckspfad von\nsnach t, wenn es einen Knoten xgibt mit:\n1.Es gibt einen k\u00fcrzesten Kuckuckspfad von snach xder L\u00e4nge l\u00001.\n2.Es gibt eine (weitere) Kante zwischen von xnach t.\nNach Induktionsvoraussetzung ist die Wahrscheinlichkeit f\u00fcr den ersten Punkt 1=(2l\u00001r)f\u00fcr\nein festes xund1=(2r)f\u00fcr den zweiten, was zusammen 1=(2lr2)ergibt.\nDa es rm\u00f6gliche xgibt (alle in der tgegen\u00fcberliegenden Seite), folgt die Behauptung.\n10-30 10-30 Die erwartete Gr\u00f6\u00dfe der Zusammenhangskomponenten.\nILemma\nDie erwartete Gr\u00f6\u00dfe einer Zusammenhangskomponente des Kuckucksgraphen ist O(1).\nBeweis. Zwei unterschiedliche Knoten xundyliegen h\u00f6chstens mit Wahrscheinlichkeit 1=r\nin derselben Zusammenhangskomponente: Es muss daf\u00fcr einen Weg irgendeiner L\u00e4nge\nl\u00151geben, was mit WahrscheinlichkeitP1\nl=11=(2lr) =1=rder Fall ist. Hieraus folgt\nsofort, dass die erwartete Anzahl weiterer Elemente, die in derselben Komponente wie ein\ngegebenes xsind, h\u00f6chstens 2r=r=2sind.\nDamit ist gezeigt, dass der Fall \u00bbdas Einf\u00fcgen gelingt\u00ab erwartete Zeit O(1)dauert.\n10-31 10-31 Analyse des Falls \u00bbdas Einf\u00fcgen misslingt\u00ab.\nZun\u00e4chst eine einfache, aber entscheidende Beobachtung:\nBeobachtung\nDer Fall \u00bbdas Einf\u00fcgen misslingt\u00ab kann nur auftreten, wenn es einen Kreis im Kuckucks-\ngraphen gibt, also einen Kuckuckspfad der L\u00e4nge l\u00152von einem Knoten szusder ersten\nTabelle.\nNun gilt:\n\u2013F\u00fcr ein konkretes sist die Wahrscheinlichkeit hiervon aber nach dem Pfadl\u00e4ngenlemma\nh\u00f6chstens 1=(2lr)\u00141=(4r); summiert \u00fcber alle m\u00f6glichen salso h\u00f6chstens r=(4r) =\n1=4.\n\u2013Es gibt also nur mit Wahrscheinlichkeit 1=4\u00fcberhaupt einen Rehash innerhalb der n\nEinf\u00fcge-Operationen .11010 A&D: Perfektes Hashing\nZusammenfassung dieses Kapitels\n\u2013Analog gibt es dann nur mit Wahrscheinlichkeit 1=42zwei Rehashes innerhalb der n\nEinf\u00fcge-Operationen\n\u2013und so weiter f\u00fcr iRehashes mit Wahrscheinlichkeit 1=4i.\nDieerwartete Anzahl Rehashes w\u00e4hrend der nEinf\u00fcge-Operationen ist also h\u00f6chstensP1\ni=1i=4i=\nO(1).\nDa jeder Rehash h\u00f6chstens O(n)lange dauert, sind die erwarteten amortisierten Rehash-\nKosten von nEinf\u00fcge-Operationen O(n)und damit O(1)pro Operation.\nZusammenfassung dieses Kapitels\n10-32 10-32 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationSuchen in\nstatischen hashbaren Daten\nRandomisierung und\nTeile-und-Herrsche\nTabelle von\nGeburtstagstabellen\nWorst-Case-Analyse\nSuche in O(1)Suchen in\ndynamischen hashbaren Daten\nRandomisierung und geniale Idee\nKuckucks-Hashing\nWorst-Case-Analyse\nAverage-Case-Analyse\nSuche in O(1)(Worst-Case)\nAndere Operationen O(1)\n(Average-Case)\nIPerfektes Hashing\nEin Hash-Verfahren ist perfekt , wenn die Such-Operationen im Worst-Case O(1)dauert.\nStatische perfekte Hash-Verfahren sind:\n\u2013Geburtstagstabellen\n\u2013Tabellen von Geburtstagstabellen\nLetztere ben\u00f6tigen auch nur Platz O(n).\nIKuckucks-Hashing\nDasKuckucks-Hashing ist ein dynamisches perfektes Hash-Verfahren. Die Grundideen sind:\n1.Es gibt zwei Hash-Tabellen.\n2.Jedes Element ist an einer seiner Hash-Stellen in einer der Tabellen.\n3.Passt ein neues Element an keinen der Pl\u00e4tze, wird eines der dort be\ufb01ndlichen Elemente\nverdr\u00e4ngt (Kuckucks-Operation), das dann wieder ein Element in der anderen Tabelle\nverdr\u00e4ngt und so fort.\nZum Weiterlesen\n[1]Rasmus Pagh and Flemming F. Rodler, Cuckoo Hashing, Journal of Algorithms,\n51(2):122\u2013144, 2004.\nIn dieser Arbeit wird das Kuckucks-Hashing eingef\u00fchrt und analysiert. Diese Arbeit ist auch\nf\u00fcr Nicht-Theoretiker sehr gut lesbar, da das Verfahren eben nicht nur theoretisch er\u00f6rtert wird,\nsondern auch ausf\u00fchrlichen praktischen Vergleichstests unterzogen wurde. Laut den Autoren\nscheint dies \u00fcberhaupt die erste Arbeit zu sein, in der eine Reihe von Standard-Hashverfahren\nauf einer aktuellen Rechnerarchitektur verglichen wurden. Dabei f\u00f6rdern die Autoren ebenso\nspannende wie \u00fcberraschende Ergebnisse zu Tage: Beispielsweise schneidet das recht triviale\nlineare Sondieren durch die Bank am besten ab, was an der besonders guten Ausnutzung der\nCache-Architektur moderner Prozessoren liegt. Im Vergleich dazu ist beispielsweise Hashing\nmit Verkettung viel schlechter. Kuckucks-Hashing h\u00e4lt sich recht wacker und kommt bis auf\n20% bis 30% an die Geschwindigkeit des linearen Sondierens heran, garantiert daf\u00fcr aber auch\nperfektes Hashing (also nur zwei Zugri\ufb00e beim Suchen im Worst-Case). Dynamische Varian-\nten von Verfahren, die auf Geburtstagstabellen aufbauen, sind mit Faktor 6 aufw\u00e4rts so weit\nabgeschlagen, dass sie gar nicht untersucht wurden.10 A&D: Perfektes Hashing\n10.4 \u00dcbungen zu diesem Kapitel111\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 10.1 Hashing-Verfahren implementieren, mittel bis schwer\nIn dieser Aufgabe soll das Hashing mit linearer Sondierung und Kuckucks-Hashing jeweils mit dyna-\nmischer Gr\u00f6\u00dfenanpassung in Java implementiert werden. Gehen Sie dabei wie folgt vor:\n1.Erstellen Sie zwei Klassen, die jeweils das Interface\npublic interface Hashfunction {\nint hash (int key);\n}\nimplementieren. Eine Klasse SimpleHash soll zur Berechnung der einfachen Hashfunktion\nh(x) =xmod rverwendet werden k\u00f6nnen, die andere Klasse UniversalHash soll Hashfunk-\ntionen der Familie Hprepr\u00e4sentieren.\n2.Implementieren Sie Hashing mit linearer Sondierung zun\u00e4chst ohne dynamische Gr\u00f6\u00dfenanpas-\nsung. Kapseln Sie die Implementierung in eine Klasse, die das Interface\npublic interface HashMap {\nvoid insert (int key);\nvoid delete (int key);\nboolean search (int key);\n}\nimplementiert. Objekte dieser Klassen repr\u00e4sentieren Hashtabellen, die auf linearer Sondierung\nbasieren. Erweitern Sie Ihre Klasse um eine Methode rehash , die beim Einf\u00fcgen und L\u00f6schen\naufgerufen wird, um die Gr\u00f6\u00dfe der Tabelle anzupassen. Sie k\u00f6nnen annehmen, dass amin=1=8,\naideal=1=4undamax=1=2gilt.\n3.Implementieren Sie Kuckuck-Hashing zun\u00e4chst ohne dynamische Gr\u00f6\u00dfenanpassung in einer Klas-\nse, die das Interface HashMap implementiert. Nehmen Sie an, dass e=1gilt. Erweitern Sie das\nKuckuck-Hashing um eine geeignete rehash -Methode. Sie k\u00f6nnen zur Vereinfachung amin=\n1=16,aideal=1=8undamax1=4annehmen.\n4.Testen Sie Ihr Programm f\u00fcr verschiedene Beispieleingaben. F\u00fcgen Sie Ihrer Abgabe zwei dieser\nEingaben bei und erl\u00e4utern Sie ausf\u00fchrlich, wie man das Programm kompilieren, starten und\ntesten kann. Kommentieren Sie das Programm umfassend.11211 Entwurfsmethoden: Zufall\n11-1 11-1\nKapitel 11\nEntwurfsmethoden: Zufall\nGott w\u00fcrfelt nicht, Computer schon\n11-2 11-2Lernziele dieses Kapitels\n1.Die Konzepte \u00bbzuf\u00e4llige Eingabe\u00ab, \u00bbzuf\u00e4llige\nAusgabe\u00ab und \u00bbzuf\u00e4llige Entscheidung\u00ab\nunterscheiden k\u00f6nnen\n2.Einsch\u00e4tzen k\u00f6nnen, wie verl\u00e4sslich\nZufallsalgorithmen sind\n3.Wichtige Klassen von Zufallsalgorithmen erl\u00e4utern\nk\u00f6nnen\n4.Einfache Beispiele von Zufallsalgorithmen\nimplementieren k\u00f6nnenInhalte dieses Kapitels\n11.1 Arten des Zufalls 113\n11.1.1 Zuf\u00e4llige Eingaben . . . . . . . . . . . . 113\n11.1.2 Zuf\u00e4llige Ausgaben . . . . . . . . . . . 114\n11.1.3 Zuf\u00e4llige Entscheidungen . . . . . . . . 115\n11.2 Zufallsalgorithmen: Beispiele 115\n11.2.1 Term\u00e4quivalenz pr\u00fcfen . . . . . . . . . . 115\n11.2.2 Das\nSchwarz-Zippel-DeMillo-Lipton-Lemma 117\n11.2.3 Perfekte Matchings pr\u00fcfen . . . . . . . . 118\n11.3 Zufallsalgorithmen: Arten 120\n11.4 Zuf\u00e4llig und trotzdem zuverl\u00e4ssig? 120\n11.4.1 Wahrscheinlichkeitsverst\u00e4rkung . . . . . 120\n11.4.2 Sehr unwahrscheinlich = nie . . . . . . . 121\n\u00dcbungen zu diesem Kapitel 122\nWorum\nes heute\ngehtWorum\nes heute\ngehtWenn man ehrlich ist, dann steht die Informatik mit dem Zufall ziemlich auf Kriegsfu\u00df: Es\nwird vieltechnischer Aufwand getrieben, dass immer alles sch\u00f6n deterministisch abl\u00e4uft\nund \u2013 in der Tat \u2013 wenn auch nur ein Bit im Code des Kernels eines Rechners mal spontan\n\u00bbumkippt\u00ab, dann st\u00fcrzt der Rechner mit ziemlicher Sicherheit ab. Um so erstaunlicher ist es\ndaher, dass der Zufall speziell in der Algorithmik eine ganz wichtige Rolle spielt. Hierf\u00fcr\ngibt es zwei Gr\u00fcnde:\n1.Zufallsalgorithmen sind oft viel schneller als \u00bbnormale\u00ab Algorithmen...\n2....und dabei genauso zuverl\u00e4ssig.\nDen ersten Punkt sieht man intuitiv noch recht leicht ein: Wenn ich die Antwort raten kann,\ndann geht das doch sicherlich schnell? Tats\u00e4chlich ist die Sachlage etwas komplizierter, da\nesnicht um Nichtdeterminismus geht, wo auf mehr oder weniger magische Weise Maschi-\nnen die richtigen Antworten auf ihren B\u00e4ndern \ufb01nden. Vielmehr m\u00fcssen auch Zufallsalgo-\nrithmen \u00bbrichtig hart arbeiten\u00ab, um die L\u00f6sung zu \ufb01nden \u2013 der Zufalls \u00bbhilft\u00ab ihnen nur\nirgendwie dabei.\nBeim zweiten Punkt wird aber sicherlich auch die wohlwollende Leserin skeptisch werden:\nIst es nicht gerade das Wesen des Zufalls, dass \u00bbimmer etwas schiefgehen kann\u00ab, wenn\ndie Wahrscheinlichkeit auch gering sein mag? Und wie oft haben wir schon geh\u00f6rt, dass\ndieses oder jenes \u00bbso unwahrscheinlich ist, dass es praktisch ausgeschlossen ist\u00ab? (Als bei-\nspielsweise die ersten Atomkraftwerke in den 50er Jahren gebaut wurden, haben Professoren\nverk\u00fcndet, dass eine Kernschmelze statistisch nur alle 27.000 Jahre vorkommen kann; also\nin der Praxis niemals. Nun ja.) Daher ist meiner Meinung nach die wichtigste Erkenntnis,\ndie Sie bitte aus diesem Kapitel mitnehmen, folgende: Die Wahrscheinlichkeiten, dass gut11 Entwurfsmethoden: Zufall\n11.1 Arten des Zufalls113\ngebaute Zufallsalgorithmen \u00bbdummerweise\u00ab einen Fehler machen, ist so gering, dass dies\nin diesem Universum nicht vorkommt. Hier ein paar Dinge, die in jeder Nanosekunde um\nGr\u00f6\u00dfenordnungen wahrscheinlicher sind, als dass ein Zufallsalgorithmus \u00bbPech hat\u00ab:\n\u2013Die Hardware des Rechners versagt spontan (ein Bit kippt um).\n\u2013Das Geb\u00e4ude, in dem der Rechner steht, bricht spontan in sich zusammen.\n\u2013Die Stadt, in dem der Rechner steht, wird durch einen Asteroiden zerst\u00f6rt.\n\u2013Die Erde wird von einer Vogonischen Kriegs\ufb02otte zerst\u00f6rt, um einer Hyperraumumge-\nhungsstra\u00dfe Platz zu machen.\nMan kann es auch so formulieren: Zufallsalgorithmen sind die gr\u00f6\u00dften Gl\u00fcckspilze, die sie\nsich vorstellen k\u00f6nnen. Sie haben einfach nie Pech.\n11.1 Arten des Zufalls\n11-4 11-4 Zufall in der Informatik \u2013 eine gute Idee?\nWarum wir Zufall in der Informatik nicht m\u00f6gen\n\u2013In der Technischen Informatik d\u00fcrfen Bits im Speicher nicht umkippen.\n\u2013\u00bbZuf\u00e4llige\u00ab Ergebnisse sind nicht reproduzierbar und lassen sich schlecht debuggen\n(\u00bbvorhin ging es noch\u00ab).\nWarum wir Zufall in der Informatik trotzdem brauchen\n1.Manche Eingaben sind wahrscheinlicher als andere. Algorithmen sollten das ausnutzen.\n2.Kryptographische Algorithmen brauchen Zufallszahlen, da diese sich nicht erraten las-\nsen.\n3.\u00bbZufallsalgorithmen\u00ab sind schneller als \u00bbnormale\u00ab Algorithmen.\n11.1.1 Zuf\u00e4llige Eingaben\n11-5 11-5 Alle Eingaben sind gleich, aber manche sind gleicher als andere.\nEine Freundin sagt Ihnen: \u00bbSortieren geht in Zeit O(nlogn).\u00ab Damit meint sie, dass es ei-\nnen Algorithmus gibt, der jede Eingabe von nZahlen in Zeit O(nlogn)sortieren kann. Bei\nmanchen Eingaben kann es aber nat\u00fcrlich trotzdem schneller gehen (zum Beispiel, wenn die\nEingabe schon sortiert ist).\nDie Idee\nWenn \u00bbschnelle\u00ab Eingaben \u00bbh\u00e4u\ufb01g vorkommen in der Praxis\u00ab, dann ist die erwartete Lauf-\nzeiteventuell schneller als die Worst-Case-Laufzeit.\nMan betrachtet dazu Verteilungen auf den m\u00f6glichen Eingaben einer Wortl\u00e4nge (zum Bei-\nspiel Gleichverteilungen). Dann betrachtet man den Erwartungswert der Laufzeit f\u00fcr diese\nVerteilung der Eingaben. Man nennt das Ergebnis die Average-Case-Laufzeit des Algorith-\nmus.\n11-6 11-6 Average-Case-Analysen sind mit Vorsicht zu genie\u00dfen.\nLeider ist es schwer, die \u00bbVerteilung der Eingaben in der Praxis\u00ab mathematisch zu beschrei-\nben. Die Gleichverteilung ist es jedenfalls nicht, wie folgendes Beispiel zeigt:\nBeispiel: Ein Algorithmus f\u00fcr das Erreichbarkeitsproblem\nFolgender Algorithmus \ufb01ndet heraus, ab es einen Weg von snach tin einem Graphen G\ngibt, indem er zun\u00e4chst pr\u00fcft, ob es einen Weg der L\u00e4nge 2gibt, und, wenn dies nicht der\nFall ist, eine normale Tiefensuche durchf\u00fchrt.\n1input G= (V;E);s2V;t2V\n2foreach x2Vdo\n3 if(s;x)2E^(x;t)2Ethen\n4 return true\n5return solve_reachability_using_dfs (G,s,t)11411 Entwurfsmethoden: Zufall\n11.1 Arten des Zufalls\nISatz\nDie erwartete Laufzeit des Algorithmus ist konstant.\nBeweis. Da alle Graphen gleich wahrscheinlich sind, gilt f\u00fcr alle Knoten u;v2V, dass\nPr[(u;v)2E] =1=2. Damit gilt f\u00fcr jedes beliebige x, dass Pr[(s;x)=2E_(x;t)=2E] =3=4.\nFolglich wird Zeile 4 nach genau kSchleifendurchl\u00e4ufen erreicht mit Wahrscheinlichkeit\n(3=4)k\u00001(1=4). Wir erwarten deshalb, dass Zeile 4 erreicht wird nach Zeit\nnX\nk=1k\u00003\n4\u0001k\u000011\n4<3:\nWird schlie\u00dflich Zeile 4 nie erreicht, so ben\u00f6tigt Zeile 5 Zeit O(n2). Dies ist aber sehr un-\nwahrscheinlich (n\u00e4mlich (3=4)n) und somit ist der Beitrag zur erwarteten Laufzeit von Zei-\nle 5 nur n2(3=4)n=o(1).\n11-7 11-7 Zusammenfassung zu zuf\u00e4lligen Eingaben.\nMerke\nBei einer Average-Case-Analyse steckt der Zufall in den Eingaben und in der Analyse . Es\nist in der Regel unklar, was die \u00bbrichtige\u00ab Eingabeverteilung ist. Der Algorithmus ist v\u00f6llig\ndeterministisch und nutzt keinen Zufall.\n11.1.2 Zuf\u00e4llige Ausgaben\n11-8 11-8 Zuf\u00e4llige Ausgaben braucht man bei Simulationen und in der Statistik.\nWill man reale Systeme simulieren, so braucht man oft viele Zufallszahlen . Das tri\ufb00t auf\nComputerspiele genauso zu wie auf Stichprobenanalysen .\nDie Idee\nEinPseudozufallszahlengenerator ist eine leicht berechenbare Folge von Zahlen, die \u00bbtypi-\nsche statistische Eigenschaften einer zuf\u00e4lligen Folge hat\u00ab .\nstatic unsigned long int next = 1;\nint rand (void ) {\nnext =next *1103515245 + 12345;\nreturn (unsigned int)(next /65536) % 32768;\n}\nvoid srand (unsigned int seed ) {\nnext =seed ;\n}\n11-9 11-9 Zuf\u00e4llige Ausgaben braucht man in der Kryptographie.\nAutor Sean MacEntee, Creative Commons\nLicense 2.0In der Kryptographie werden sehr oft Strings oder Zahlen ben\u00f6tigt, die man nicht erraten\nkann. Beispielsweise wird jede https-Verbindung mit einem (neuen) Session-Key verschl\u00fcs-\nselt.\nDie Idee\nF\u00fcr die Kryptographie braucht man Algorithmen, deren Ausgaben \u00bbechte\u00ab Zufallszahlen\nsind, die nicht vorhergesagt werden k\u00f6nnen.\nEs ist eine ganz schlechte Idee, Funktionen wie rand() in der Kryptographie zu nutzen\n(warum?). \u00bbEchte\u00ab Zufallszahlen erzeugt man mittels chaotischer physikalischer Vorg\u00e4nge ,\nwie die Messung der Parit\u00e4t der Anzahl der Nanosekunden zwischen zwei Tastaturanschl\u00e4-\ngen.\nBy www.cgpgrey.com, CC BY 2.011-10 11-10 Zusammenfassung zu zuf\u00e4lligen Ausgaben.\nMerke\nIn der Statistik und der Kryptographie braucht man Algorithmen, deren Ausgabe m\u00f6glichst\nzuf\u00e4llig ist . Solche Algorithmen nennt man Zufallszahlengeneratoren . F\u00fcr Anwendungen in\nder Statistik m\u00fcssen diese m\u00f6glichst einfach und reproduzierbar sein, in der Kryptographie\nhingegen gerade nicht.11 Entwurfsmethoden: Zufall\n11.2 Zufallsalgorithmen: Beispiele115\n11.1.3 Zuf\u00e4llige Entscheidungen\n11-11 11-11 Algorithmen k\u00f6nnen \u00bbintern\u00ab zuf\u00e4llig arbeiten.\nAlgorithmen k\u00f6nnen intern den Zufall nutzen. Ein typisches Beispiel sind Hash-Tabellen:\nIntern werden die Elemente entsprechend der Hash-Funktionen \u00bbgeschickt\u00ab verteilt, \u00bbnach\nau\u00dfen\u00ab merkt man davon aber nichts, da die Grundoperationen Suchen, Einf\u00fcgen, L\u00f6schen\ndieimmer gleichen Ergebnisse liefern; lediglich die Geschwindigkeit \u00e4ndert sich. \u00dcberra-\nschenderweise l\u00e4sst sich Zufall aber auch nutzen, wenn lediglich \u00bbJa/Nein\u00ab-Entscheidungen\ngef\u00e4llt werden m\u00fcssen wie bei der Frage \u00bbIst xprim?\u00ab.\nDie Idee\nEinAlgorithmus mit internem Zufall sollf\u00fcr alle Eingaben (also im Worst-Case) immer\ndas richtige Ergebnis bestimmen \u2013 es darf nur \u00bbmal schneller und mal langsamer\u00ab gehen in\nAbh\u00e4ngigkeit von zuf\u00e4lligen internen Entscheidungen.\n11-12 11-12 Vergleich der Arten des Zufalls\nWelche Aspekte sind vom Zufall beein\ufb02usst?\nModell\nEingabe\nAusgabe\nBerechnung\nLaufzeit\nAverage-Case-Analyse ja nein nein nein\nPseudozufallszahlen nein pseudo nein nein\nKryptographische Schl\u00fcssel nein ja nein nein\nZufallsalgorithmen (ZPP) nein nein ja ja\nZufallsalgorithmen (BPP, RP) nein minimal ja nein\n11.2 Zufallsalgorithmen: Beispiele\n11.2.1 Term\u00e4quivalenz pr\u00fcfen\n11-13 11-13 Sind zwei Terme gleich?\nDas Term-\u00c4quivalenz-Problem\nEingabe Zwei arithmetische Terme sundtmit Variablen, kodiert als arithmetische B\u00e4u-\nme mit Zahlen und Variablen an den Bl\u00e4tttern und +- und\u0002-Knoten im Inneren.\nFrage? Beschreiben beide Terme dieselbe Funktion?\n(Beachte: Die Eingabe ist syntaktisch, die Frage ist semantisch.)\nBeispiel\nDie folgenden Terme beschreiben dieselbe Funktion:\n+\n\u0002\nx x\u0002\n5 x\u0002\n+\nx +\n2 3\u0002\nx +\n\u00002 3\n11-14 11-14 .Zur Diskussion\n1.Wie schnell k\u00f6nnen Sie zwei Terme auf \u00c4quivalenz \u00fcberpr\u00fcfen, wenn in ihnen keine\nVariablen vorkommen?\n2.Wie schnell geht dies, wenn genau eine Variable vorkommt?11611 Entwurfsmethoden: Zufall\n11.2 Zufallsalgorithmen: Beispiele\n11-15 11-15 Die Ideen zur L\u00f6sung des Term-\u00c4quivalenz-Problems\nVon der Gleichheit zum Null-Test\nDie arithmetischen B\u00e4ume beschreiben Polynomfunktionen . Nun sind zwei Polynomfunk-\ntionen, wenn ihre Di\ufb00erenz 0ist. Wir m\u00fcssen also \u00bbnur\u00ab heraus\ufb01nden, ob die durch einen\narithmetischen Baum beschriebene Polynomfunktion die Nullfunktion ist .\nBig Idea\nPolynomfunktionen, die nicht konstant 0sind, haben nur wenige Nullstellen!\n1input treeT(x1;:::;xk),where x1;:::;xkare the variables\n2M geeignete Menge von Zahlentupeln //Siehe \u00dcbungen\n3foreach (a1;:::;an)2Mdo\n4v T(a1;:::;ak)//evaluate Tforthese numbers\n5 ifv6=0then\n6 return \u2018\u2018Tis not always 0\u2019\u2019\n7return \u2018\u2018Tis always 0\u2019\u2019\n11-16 11-16 Beispiel der \u00dcberpr\u00fcfung\nWertet folgender Baum Timmer zu 0aus?\n\u0002\n+\nx\u00001\u0002\n\u0002\n\u0002\nx y+\nx\u00003\u0002\n+\nx\u00002+\ny\u00001\n\u2013Setzen wir x=0undy=0, so ist T(0;0) =0.\n\u2013Ebenso f\u00fcr alle anderen xundy=0.\n\u2013Ebenso f\u00fcr alle xundy=1.\n\u2013Ebenso f\u00fcr x2f0;1;2;3gundy=2.\n\u2013Aber f\u00fcrx=4undy=2giltT(4;2) =48!\n11-17 11-17 .Zur \u00dcbung\nWie viele Nullstellen haben folgende Polynom vom Grad d?\np1(x;y;z) = (x\u00001)(x\u00002)\u0001\u0001\u0001(x\u0000d)+\n(y\u00001)(y\u00002)\u0001\u0001\u0001(y\u0000d)+\n(z\u00001)(z\u00002)\u0001\u0001\u0001(z\u0000d);\np2(x;y;z) =xyz:\n11-18 11-18 Nullstellen trifft man nicht mit Dartpfeilen.\nIstp(x1;:::;xn)nicht das Nullpolynom, so haben gerade gesehen, dass pzwar unendlich\nviele Nullstellen haben kann, aber:\n\u2013Sie werfen einen Dartpfeil auf den Raum Rn.\n\u2013Dann ist die Wahrscheinlichkeit, dass der Pfeil eine Nullstelle tri\ufb00t, gleich 0(!), da die\nNullstellen eine (n\u00001)-dimensionale (Hyper)\ufb02\u00e4che bilden.\nEtwas mathematischer ausgedr\u00fcckt:\nPr\n(a1;:::;an)2Rn[p(a1;:::;an) =0] =0:11 Entwurfsmethoden: Zufall\n11.2 Zufallsalgorithmen: Beispiele117\nEine frustrierende Situation\n\u2013Im Raum Rnsind die Nullstellen von pso rar, dass sie \u00bbunm\u00f6glich zuf\u00e4llig zu tre\ufb00en\u00ab\nsind.\n\u2013Wenn wir aber deterministisch das Gitter f1;:::;dgndurchprobieren, k\u00f6nnen gerade\ndort \u00fcberall Nullstellen sein.\n11.2.2 Das Schwarz-Zippel-DeMillo-Lipton-Lemma\n11-19 11-19 Wie wahrscheinlich ist es, eine Nullstelle zu treffen?\nILemma: DeMillo, Lipton, Schwarz, Zippel\nSeip(x1;:::;xn)ein Polynom vom Grad d, das nicht konstant 0ist, und sei S\u0012Reine\nendliche Menge von Werten. Dann ist\nPr\n(a1;:::;an)2Sn[p(a1;:::;an) =0]\u0014d=jSj:\nBeweis. F\u00fcrn=1folgt das Lemma sofort daraus, dass p(x)h\u00f6chstens dNullstellen haben\nkann. F\u00fcr den Induktionsschritt von n\u00001aufnschreiben wir pwie folgt um:\np(x1;:::;xn) =dX\ni=0xi\n1qi(x2;:::;xn):\nDapnicht das Nullpolynom ist, gibt es ein maximales i, so dass qinicht das Nullpolynom\nist. Sei jdieses i. Dann hat qjh\u00f6chstens Grad d\u0000j, da das Monom xj\n1qj(:::)h\u00f6chstens\nGrad dhat.\nNun gilt f\u00fcr beliebige (a2;:::;an)2Sn: Istqj(a1;:::;an)6=0, so ist p(x1;a2;:::;an)ein\nPolynom vom Grad jund somit ist Pra12S[p(x1;a2;:::;an) =0]\u0014j=jSj. Damit ist die Ge-\nsamtwahrscheinlichkeit f\u00fcr p(a1;:::;an) =0die Summe folgender F\u00e4lle:\n1.qj(a1;:::;an) =0, was nach Induktionsvoraussetzung mit Wahrscheinlichkeit (d\u0000j)=jSj\npassiert, und\n2.qj(a1;:::;an)6=0und dann aber p(x1;a2;:::;an) =0, was wie eben gezeigt dann mit\nWahrscheinlichkeit j=jSjpassiert.\nIn der Summe ist die Wahrscheinlichkeit (d\u0000j)=jSj+j=jSj=d=jSj.\n11-20 11-20 Ein Zufallsalgorithmus f\u00fcr den Term-\u00c4quivalenz-Test.\n1input treeT(x1;:::;xk),where x1;:::;xkare the variables\n2\n3foreach i2f1;:::;ngdo\n4ai pick_randomly _f rom (f1;:::; 100dg)\n5\n6ifT(a1;:::;ak)6=0\n7 then return \u2018\u2018Tis not always 0\u2019\u2019\n8 else return \u2018\u2018Tis presumably always 0\u2019\u2019\nISatz\nDer Zufallsalgorithmus hat Laufzeit O(n2). Gibt er \u201c Tis not always 0\u201d aus, so ist dies\nimmer korrekt. Gibt er \u201c Tis presumably always 0\u201d aus, so ist dies mit mindestens Wahr-\nscheinlichkeit 99% korrekt.\nDer Satz folgt unmittelbar aus dem Lemma.11811 Entwurfsmethoden: Zufall\n11.2 Zufallsalgorithmen: Beispiele\n11.2.3 Perfekte Matchings pr\u00fcfen\n11-21 11-21 Eine Anwendung: Perfekte Matchings finden.\nDas Perfekte-Matching-Problem\nEingabe Ein ungerichteter Graph.\nFrage Enth\u00e4lt der Graph ein perfektes Matching? (Eine Teilmenge der Kanten, so dass\njeder Knoten an genau einer Kante beteiligt ist?)\nBeispiel\nFolgender Graph hat ein perfektes Matching (rot dargestellt):\n1\n3\n45 62\n78\nMan kann das Perfekte-Matching-Problem in polynomieller Zeit l\u00f6sen, aber das ist sehr\nkomplex. Wir werden jetzt einen Zufallsalgorithmus bauen, der viel einfacher und schneller\nist.\n11-22 11-22 Die Tutte-Matrix\nIDefinition\nSeiG= (f1;:::;ng;E)ein ungerichteter Graph. Die Tutte-Matrix TGvonGist wie folgt\nde\ufb01niert (die vi;jsind jeweils neue Variablen):\nTG\ni j=8\n><\n>:vi;j f\u00fcr(i;j)2E;i<j;\n\u0000vj;if\u00fcr(i;j)2E;i>j;\n0 sonst :\nBeispiel\na\nbcde\nf\ngh ij\nk1\n3\n45 62\n78\n0\nBBBBBBBBBB@0 i a 0 f 0 0 0\n\u0000i0 0 0 e 0 0 j\n\u0000a 0 0 b 0 0 0 0\n0 0\u0000b 0 c 0k0\n\u0000f\u0000e0\u0000c 0 d 0 0\n0 0 0 0 \u0000d 0g h\n0 0 0\u0000k 0\u0000g0 0\n0\u0000j0 0 0\u0000h0 01\nCCCCCCCCCCA\n11-23 11-23 Die Tutte-Matrix und perfekte Matchings\nISatz\nEin Graph Ghat genau dann ein perfektes Matching, wenn die Determinante von TGnicht\ndas Nullpolynom ist.\nBeweis. Nach der Leibniz-Formel gilt:\ndetTG=X\np(\u00001)sgnpTG\n1;p(1)TG\n2;p(2)\u0001\u0001\u0001TG\nn;p(n):\nErste Beobachtungen:\n1.Jede Permutation pkann man eindeutig darstellen als Menge von zyklischen Vertau-\nschungen .11 Entwurfsmethoden: Zufall\n11.3 Zufallsalgorithmen: Beispiele119\n2.Damit entspricht peiner \u00dcberdeckung aller Knoten mit Hilfe von disjunkten Zyklen .\n3.Geht eine Kante entlang eines Zyklus nicht entlang einer Kante im Graphen, so ist der\nBeitrag der Permutation in der obigen Summe 0.\n4.Es tragen also genau die \u00dcberdeckungen von Gmit Zyklen zu detTGbei.\nBeispielsweise ist der Beitrag folgender Permutation 0:\na\nbcde\nf\ngh ij\nk1\n3\n45 62\n78\nDer Beitrag folgender Permutation ist j2a f dgkb :\na\nbcde\nf\ngh ij\nk1\n3\n45 62\n78\nNehmen wir f\u00fcr die erste Richtung an, Ghat ein perfektes Matching M=fe1;:::;en=2g.\nDann gibt es genau eine Permutation p, die immer genau die beiden Knoten jeder Kante in\nMvertauscht. Diese Permutation tr\u00e4gt das Monom e2\n1e2\n2\u0001\u0001\u0001en=22zum Polynom der Deter-\nminante bei, die somit nicht konstant 0ist.\nF\u00fcr die zweite Richtung habe nun Gkein perfektes Matching. Sei nun eine beliebige Per-\nmutation pgegeben, deren Zyklen \u00bbentlang von Kanten\u00ab verlaufen. Dann muss einer der\nZyklen ungerade L\u00e4nge haben, denn sonst betrachte jede zweite Kanten auf den Zyklen:\nDiese w\u00fcrden ein perfektes Matching bilden. Betrachte nun die Permutation p0, die iden-\ntisch ist zu p, aber genau den ungerade Zyklus andersherum durchl\u00e4uft . Dann tragen pund\np0dasselbe Monom zur Determinante bei, aber mit umgekehrten Vorzeichen , weshalb sie\nsich gegenseitig aufheben.\na\nbcde\nf\ngi\nk1\n345 62\n7a\nbcde\nf\ngi\nk1\n345 62\n7\n11-24 11-24 Ein Zufallsalgorithmus f\u00fcr das Perfekte-Matching-Problem\n1input graph G= (f1;:::;ng;E)\n2T empty n\u0002nmatrix\n3foreach (i;j)2Ewithi<jdo\n4r pick_randomly _f rom (f1;:::; 100ng)\n5T[i;j] r\n6T[j;i] \u0000r\n7d det(T)//kann inZeitO(n3)berechnet werden\n8ifd6=0\n9 then return \u2018\u2018Ghas a prefect matching \u2019\u2019\n10 else return \u2018\u2018Gpresumably has no perfect matching \u2019\u2019\nISatz\nDer Zufallsalgorithmus hat Laufzeit O(n3). Gibt er \u201c Ghas a prefect matching\u201d aus, so ist\ndies immer korrekt. Gibt er \u201c Gpresumably has no perfect matching\u201d aus, so ist dies mit\nmindestens Wahrscheinlichkeit 99% korrekt.\nDer Satz folgt aus dem Lemma und daraus, dass das Determinanten-Polynom Grad nist.12011 Entwurfsmethoden: Zufall\n11.4 Zuf\u00e4llig und trotzdem zuverl\u00e4ssig?\n11.3 Zufallsalgorithmen: Arten\n11-25 11-25 M\u00f6gliche Arten von Fehlern bei Zufallsalgorithmen.\nAm Anfang hie\u00df es, ein Zufallsalgorithmus m\u00fcsse immer das richtige Ergebnis ausgeben.\nDie vorgestellten Algorithmen machen aber mit Wahrscheinlichkeit 1% einen Fehler, wenn\nsie \u00bbNein\u00ab sagen . Allgemein haben Zufallsalgorithmen f\u00fcnf m\u00f6gliche Ausgaben:\n1.\u00bbJa\u00ab und dies stimmt garantiert.\n2.\u00bbJa\u00ab und dies stimmt mit 99% Wahrscheinlichkeit.\n3.\u00bbUnklar\u00ab und dies passiert mit 1% Wahrscheinlichkeit.\n4.\u00bbNein\u00ab und dies stimmt mit 99% Wahrscheinlichkeit.\n5.\u00bbNein\u00ab und dies stimmt garantiert.\nIDefinition: BPP-, RP-, coRP-, ZPP-Algorithmen\nEinBPP-Algorithmus f\u00fcr ein Problem Pist ein Algorithmus, der\n1.in polynomieller Zeit arbeitet,\n2.intern zuf\u00e4llige Entscheidungen f\u00e4llen darf und\n3.immer eine der f\u00fcnf Ausgaben oben macht.\nSpezialf\u00e4lle von BPP-Algorithmen sind:\n\u2013RP-Algorithmen: Hier kommt Fall 2 nicht vor.\n\u2013coRP-Algorithmen: Hier kommt Fall 4 nicht vor.\n\u2013ZPP-Algorithmen: Hier kommen Fall 2 und 4 nicht vor.\n11-26 11-26 BPP ist gut, RP und coRP sind besser, ZPP ist ideal\nMerke\nBPPDie Ausgabe ist mit 99% Wahrscheinlichkeit richtig.\nRPWie BPP, nur dass \u00bbJa\u00ab-Ausgaben zu 100% richtig sind.\ncoRP Wie BPP, nur dass \u00bbNein\u00ab-Ausgaben zu 100% richtig sind.\nZPPDie Ausgabe stimmt immer, aber mit 1% Wahrscheinlichkeit muss man die Be-\nrechnung wiederholen.\n11.4 Zuf\u00e4llig und trotzdem zuverl\u00e4ssig?\n11.4.1 Wahrscheinlichkeitsverst\u00e4rkung\n11-27 11-27 Kann man Zufallsalgorithmen vertrauen?\nAusHealth & Safety Executive Nuclear Directorate Assessment Report, New Reactor Build\nEDF/AREVA EPR Step 2 PSA Assessment aus dem Jahr 2008:\nC[ore] D[amage] F[requency] ext[ernal] hazards: 7\u000210\u00008extreme weather Re-\nsults/y[ea]r\n(Drei Jahre sp\u00e4ter trat ein ziemlich gro\u00dfer \u00bbCore-Damage\u00ab aufgrund von \u00bbextreme weather\u00ab\nin Fukushima ein.)\nUnsere Zufallsalgorithmen haben eine 1% Chance, dass sie Fehler machen. \u00bbGef\u00fchlt\u00ab ist\ndas bei sicherheitskritischen Anwendungen \u00bbviel zu viel\u00ab. \u00bbGef\u00fchlt\u00ab sollte man gar keine\nAlgorithmen mit positiven Fehlerwahrscheinlichkeiten nutzen. Man kann aber die Fehler-\nwahrscheinlichkeit verringern und die resultierenden Algorithmen sind dann \u00bbsicher\u00ab.\n11-28 11-28 Wie macht man Algorithmen sicher?\nGegeben sei ein RP-Algorithmus A, der ein Problem Qentscheidet (also bei Ausgabe \u00bbJa\u00ab\nimmer richtig liegt und bei Ausgabe \u00bbnein\u00ab zu 99% richtig liegt). Betrachte nun folgenden\nAlgorithmus:\n1input x\n2do500times\n3a output ofAonx\n4 ifa=\u00bbyes\u00ab then\n5 return \u00bbyes\u00ab\n6return \u00bbno\u00ab\nISatz\nDer obige Algorithmus ist ebenfalls ein RP-Algorithmus f\u00fcr A, aber mit Fehlerwahrschein-\nlichkeit 10\u00001000.11 Entwurfsmethoden: Zufall\nZusammenfassung dieses Kapitels121\n11.4.2 Sehr unwahrscheinlich = nie\n11-29 11-29 Eher st\u00fcrzt ein Computer ab, als dass ein Ereignis mit Wahrscheinlichkeit 10\u00001000eintritt.\nEin Speicherriegel hat (Messung aus dem Jahr 2009) in einem typischen realen Rechner\nmindestens alle 10 Jahre einen \u00bbunrecoverable error\u00ab. Damit ist die Wahrscheinlichkeit, dass\nein Computer in einer gegebenen Nanosekunde aufgrund eines Speicherfehlers abst\u00fcrzt,\nmindestens\n1\n109\n|{z}\nNanosekunden pro Sekunde\u0001 109\n|{z}\nSekunden pro 10 Jahre\u0001 1012\n|{z}\nM\u00f6gliche aktuelle Speicheradresse\nalso mindestens 10\u000030. Das ist viel, viel mehr als10\u00001000.\nAber, ist \u00bbsehr unwahrscheinlich\u00ab nicht \u00bbaber trotzdem m\u00f6glich\u00ab?\n11-30 11-30 Ein Ereignis mit Wahrscheinlichkeit 10\u00001000tritt in diesem Universum nicht ein.\nBetrachten wir die Wahrscheinlichkeit, dass eine Ereignis eintritt\n\u2013bei irgendeinem Atom im Universum ( 1080)\n\u2013w\u00e4hrend einer der Planck-Zeiten innerhalb der ersten Sekunde nach dem Urknall ( 1045)\n\u2013oder w\u00e4hrend einer Planck-Zeit innerhalb aller Sekunden seit dem Urknall ( 1018)\n\u2013oder w\u00e4hrend einer Milliarde weiterer Zeitalter des Universums ( 109).\nEs gibt also 1080+45+18+9=10152M\u00f6glichkeiten, zu denen das Ereignis eintreten kann.\nWenn dies jedes Mal eine Chance von 10\u00001000hatte, so passiert dies wenigstens einmal mit\nWahrscheinlichkeit\n10\u0000848:\nAlso nie.\nZusammenfassung dieses Kapitels\n11-31 11-31 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationPolynomial-Identity-Testing\nRandomisierung\nZuf\u00e4llig Werte f\u00fcr Variablen\nFehlerwahrscheinlichkeits-Analyse\nRP-AlgorithmusPerfektes-Matching-Problem\nRandomisierung\nDeterminante der Tutte-Matrix\nFehlerwahrscheinlichkeits-Analyse\nRP-Algorithmus\nIZufallsalgorithmus\n\u2013EinZufallsalgorithmus nutzt intern Zufallsentscheidungen.\n\u2013Ob der Algorithmus schnell arbeitet oder das richtige Ergebnis liefert, h\u00e4ngt nicht von\nder Eingabe ab.\n\u2013Man kann Zufallsalgorithmen wiederholt auf eine Eingabe anwenden, um die Fehler-\nwahrscheinlichkeit beliebig klein zu bekommen .\n\u2013Zufallsalgorithmen machen in der Praxis nie (!) Fehler .12211 Entwurfsmethoden: Zufall\n\u00dcbungen zu diesem Kapitel\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 11.1 Nullstelen eines Polynom auf einem Gitter, mittel\nSeip(x1;:::;xn)ein Polynom vom Grad dungleich dem Nullpolynom. Zeigen Sie, dass f\u00fcr wenigstens\nein Tupel (a1;:::;an)2f1;:::;d+1ggiltp(a1;:::;an)6=0.\nTipp: Sie k\u00f6nnen dies entweder direkt durch Induktion \u00fcber nbeweisen oder das Schwarz-Zippel-\nDeMillo-Lipton-Lemma benutzen (nur wie?...).\n\u00dcbung 11.2 Algorithmus f\u00fcr den Term-Nullpolynom-Test, leicht\nIn dem Algorithmus von Folie ??w\u00e4hlen Sie M=f1;:::;d+1gn. Zeigen Sie:\n1.Der Algorithmus liefert nun immer das richtige Ergebnis. ( Tipp: Nutzen Sie \u00dcbung 11.1.)\n2.Die Laufzeit des Algorithmus ist beschr\u00e4nkt durch O(nn).\n\u00dcbung 11.3 BPP-Algorithmen sicher machen, schwer\nSie haben einen BPP-Algorithmus Agegeben, der ein Problem Ql\u00f6st, der also in polynomieller Zeit zu\neiner Eingabe xdie Ausgabe \u00bb x2Q\u00ab oder \u00bb x=2Q\u00ab macht und dies stimmt mit 99% Wahrscheinlichkeit.\nDie Fehlerwahrscheinlichkeit des Algorithmus liegt also unter 2\u00006. Sie suchen nun einen Algorithmus,\ndess Fehlerwahrscheinlichkeit unter 2\u00001000liegt.\nWie k\u00f6nnen Sie Amodi\ufb01zieren, um dies zu erreichen? Geben Sie den Algorithmus und seine Wahr-\nscheinlichkeitsanalyse an.\nTipp: Cherno\ufb00-UngleichungTeil V\nEntwurfsmethoden f\u00fcr schwere Probleme123\nTeil V\nEntwurfsmethoden f\u00fcr schwere\nProbleme\nDie in den vorherigen Kapiteln untersuchten Problemen waren einfach.\nWer sich durch die Details des /d.sc/c.sc/three.taboldstyle-Algorithmus geackert hat oder wer versucht hat, die\namortisierte Analyse des Kuckucks-Hashing nachzuvollziehen, der mag den vorherigen Satz\nf\u00fcr arrogant halten, er stimmt aber trotzdem: Im Sinne der Komplexit\u00e4tstheorie von Proble-\nmen ist das Erstellen eines Su\ufb03x-Arrays ein fast triviales Problem, geht es doch sicherlich in\npolynomieller Zeit zu l\u00f6sen. Aus Sicht der klassischen Komplexit\u00e4tstheorie ist alles \u00bbleicht\u00ab,\nwas in der Klasse Pliegt; die Schwierigkeiten fangen erst oberhalb davon an. Man k\u00f6nnte\nargumentieren, dass die komplexit\u00e4tstheoretische Sicht auf Probleme Theoretiker erfreuen\nmag, sie doch aber eine falsche Sicht zumindest auf das Algorithmendesign ist: Es macht\npraktisch eben doch einen himmelweiten Unterschied, ob man einen Su\ufb03x-Array in Zeit\nO(n)oder in Zeit Q(n2logn)berechnen kann.\nZiel dieses letzten Teils ist zu zeigen, dass die Untersuchung der Komplexit\u00e4t von Proble-\nmen sehr wohl dazu beitragen kann, sie e\ufb03zient zu l\u00f6sen. Bekannterma\u00dfen gibt es viele\nNP-vollst\u00e4ndige Probleme, die sich nach heutigem Kenntnisstand nicht in polynomieller\nZeit l\u00f6sen lassen. Bei solchen Problemen muss man zu anderen Methoden greifen als zu\ndenen, die wir f\u00fcr die \u00bbeinfachen\u00ab Probleme aus den vorherigen Kapiteln kennengelernt\nhaben. Besonders wichtig ist dabei, zun\u00e4chst \u00fcberhaupt erst zu erkennen , dass ein Problem\nschwierig ist \u2013 wie wir sehen werden, steckt der Teufel hier oft im Detail, schon eine leichte\nUmformulierung kann die Komplexit\u00e4t eines Problems drastisch \u00e4ndern.\nWenn nun aber ein Problem als zweifelsfrei schwierig zu l\u00f6sen identi\ufb01ziert wurde, was\ndann? Wir werden uns zwei Verfahren etwas genauer anschauen: Zum einen Approxima-\ntionsalgorithmen, welche in vielen F\u00e4llen sehr gute Ergebnisse liefern, und zum anderen\nFixed-Parameter-Verfahren. Letztere funktionieren zwar nur unter \u00bbg\u00fcnstigen Bedingun-\ngen\u00ab, liefern dann aber spektakul\u00e4re Ergebnisse: Probleme, die sich auf Supercomputern\nnur bis Eingabegr\u00f6\u00dfen von vielleicht n=50l\u00f6sen lie\u00dfen konnten hiermit auf normalen\nComputern f\u00fcr n=500gel\u00f6st werden.12412 Entwurfsmethode: Approximation\n12-1 12-1\nKapitel 12\nEntwurfsmethode: Approximation\nDie 80-20-Regel\n12-2 12-2Lernziele dieses Kapitels\n1.Die Entwurfsmethode \u00bbApproximation\u00ab kennen und\nApproximationsalgorithmen entwerfen k\u00f6nnen\n2.Wichtige Approximationsalgorithmen f\u00fcr mehrere\nschwere Probleme kennenInhalte dieses Kapitels\n12.1 Optimierungsprobleme 125\n12.1.1 Das Konzept . . . . . . . . . . . . . . . 125\n12.1.2 Ma\u00df und G\u00fcte . . . . . . . . . . . . . . 125\n12.2 Approximationsalgorithmen 126\n12.2.1 Das Konzept . . . . . . . . . . . . . . . 126\n12.2.2 Handelsreisender in der Ebene . . . . . . 127\n12.2.3 Bin-Packing . . . . . . . . . . . . . . . 130\n12.2.4 Vertex-Cover . . . . . . . . . . . . . . . 131\n12.3 *Approximationsschemata 132\n12.3.1 Das Konzept . . . . . . . . . . . . . . . 132\n12.3.2 Rucksack-Problem . . . . . . . . . . . . 132\n\u00dcbungen zu diesem Kapitel 135\nWorum\nes heute\ngehtWorum\nes heute\ngehtF\u00fcr viele der schweren Probleme, die wir in den letzten Kapiteln kennen gelernt haben, ist es,\nmit Verlaub gesagt, bescheuert, die Probleme exakt zu l\u00f6sen. Nehmen wir dazu das Beispiel\ndes Handelsreisenden. Wie wir gesehen haben, ist dieses Problem NP-vollst\u00e4ndig, weshalb\nes bei gro\u00dfen Eingaben sehr lange dauert oder schlichtweg unm\u00f6glich ist, optimale Rund-\nreisen zu berechnen. Bei realen Eingaben werden aber beispielsweise die Entfernungen, die\nja Teil der Eingabe sind, mit einem gewissen Fehler behaftet sein; die k\u00fcrzeste Rundreise\nf\u00fcr diese fehlerbehafteten Eingaben muss gar nicht die real k\u00fcrzeste Rundreise sein. Was\nwireigentlich suchen, sind L\u00f6sungen f\u00fcr das Handelsreisenden-Problem, die \u00bbdicht dran\u00ab\nsind am Optimum. Wenn die Eingaben beispielsweise eine Messungenauigkeit von, sagen\nwir, einem Prozent haben, so reicht es, eine L\u00f6sung zu \ufb01nden, die bis zu einem Prozent vom\nOptimum abweichen darf \u2013 genauere L\u00f6sungen sind reine Augenwischerei.\nIst es leichter, solch \u00bbapproximative\u00ab L\u00f6sungen f\u00fcr NP-vollst\u00e4ndige Probleme zu berechnen,\nals diese exakt zu l\u00f6sen? Hier kann man nur die \u00bbJuristen-Antwort\u00ab geben: Es kommt drauf\nan. Bei einigen NP-vollst\u00e4ndigen Problemen ist es vielleichter, diese approximativ zu l\u00f6sen,\nbei anderen ist schon dies schwierig. Die Approximationstheorie ist ein (sehr weit entwickel-\ntes) Teilgebiet der Theorie, das sich genau mit der Frage besch\u00e4ftigt, welche Probleme sich\ngut approximieren lassen und welche nicht.\nIn diesem Kapitel werden wir eine ganze Reihe von Approximationsalgorithmen kennen-\nlernen. Dies soll aber nicht dar\u00fcber hinwegt\u00e4uschen, dass es solche Algorithmen eben nicht\nimmer gibt. Dies ist beispielsweise f\u00fcr das F\u00e4rbeproblem der Fall: Man kennt keinen e\ufb03zi-\nenten Algorithmus, um beliebige Graphen mit einer minimalen Anzahl an Farben zu f\u00e4rben\n(so dass benachbarte Knoten unterschiedliche Knoten haben); man kennt noch nicht einmal\neinen Algorithmus, der einen beliebigen Graphen mit, sagen wir, einer Million mal so vielen\nFarben wie unbedingt n\u00f6tigt f\u00e4rbt. Wie immer in der Theorie sind allerdings solche \u00bbunte-\nren Schranken\u00ab schwierig zu beweisen, weshalb hierf\u00fcr auf weiterf\u00fchrende Veranstaltungen\nverwiesen sei.12 Entwurfsmethode: Approximation\n12.1 Optimierungsprobleme125\n12.1 Optimierungsprobleme\n12.1.1 Das Konzept\n12-4 12-4 Worum geht es bei \u00bbProblemen\u00ab wirklich?\nBei vielen Problemen gibt es nicht \u00bbdie eine richtige Antwort\u00ab. Vielmehr gibt es \u00bbviele\nrichtige L\u00f6sungen\u00ab, auch wenn diese \u00bbunterschiedlich gut\u00ab sind. Unsere bisherige Formali-\nsierung von Problemen als einfache Ja/Nein-Probleme wird diesem Umstand nicht gerecht.\nIDefinition: Optimierungsproblem\nEinOptimierungsproblem f\u00fcr ein Alphabet Sbesteht aus:\n1.Einer L\u00f6sungsrelation S. Dies ist eine Teilmenge von S\u0003\u0002S\u0003. Ist ein Paar (i;s)in dieser\nMenge, so hei\u00dft seine L\u00f6sung zu der Eingabe i.\n2.Einer Ma\u00dffunktion. Dies ist eine Funktion m:S!N, die jeder L\u00f6sung ein Ma\u00df zuord-\nnet.\n3.Einem Typ. Dieser ist entweder \u00bbMaximierung\u00ab oder \u00bbMinimierung\u00ab.\n12-5 12-5 Beispiel: Vertex-Cover als Optimierungsproblem\nEingaben Ungerichtete Graphen G= (V;E).\nL\u00f6sungen Kontenmengen M, so dass f\u00fcr jede Kante von Gmindestens eines ihrer Enden\ninMliegt. (F\u00fcr allefu;vg2Emuss gelten u2Moder v2M.)\nMa\u00df Gr\u00f6\u00dfe von M.\nZielMinimierung.\nBeispiel\n.Zur \u00dcbung\nFormulieren Sie das Handelsreisenden-Problem sinnvoll als formales Optimierungsproblem.\n12.1.2 Ma\u00df und G\u00fcte\n12-6 12-6 Gute versus schlechte L\u00f6sungen.\nIDefinition\nSeiPein Optimierungsproblem. Dann ist\n\u2013dasoptimale Ma\u00df zur Eingabe das minimale (oder maximale) Ma\u00df aller L\u00f6sungen zu\nx,\n\u2013dieG\u00fcte einer konkreten L\u00f6sung szuxdas Verh\u00e4ltnis\nMa\u00df von s\noptimales Ma\u00df einer L\u00f6sung zu x:\nBei Maximierungsproblemen ist die G\u00fcte gerade der Kehrwert, so dass sie immer min-\ndestens 1 ist.\nMerke\nDasMa\u00df einer L\u00f6sung gibt ihre \u00bbabsoluten Kosten\u00ab an. Die G\u00fcte einer L\u00f6sung gibt an, um\nwelchen Faktor sie schlechter ist als eine optimale L\u00f6sung.12612 Entwurfsmethode: Approximation\n12.2 Approximationsalgorithmen\n12-7 12-7 Beispiele f\u00fcr Ma\u00df und G\u00fcte.\nBeispiel\n1.Das Ma\u00df der roten L\u00f6sung ist 4(vier Knoten).\n2.Das Ma\u00df der optimalen L\u00f6sung (drei Knoten) ist 3.\n3.Damit ist die G\u00fcte der roten L\u00f6sung 4=3.\n12.2 Approximationsalgorithmen\n12.2.1 Das Konzept\n12-8 12-8 Die 80-20-Regel.\nEine Lebensweisheit\nDie 80-20-Regel ist eine empirische Beobachtung betre\ufb00end viele reale Projekte:\n\u2013Es werden 80% der Arbeit in 20% der Zeit erledigt.\n\u2013Umgekehrt ben\u00f6tigen 20% der Arbeit 80% der Zeit.\nBeispiel\nWenn Sie eine Bachelor-Arbeit schreiben, werden 80% des Textes \ufb02ott von der Hand gehen.\nSie werden dann aber 80% Ihrer Zeit auf den \u00bbFeinschli\ufb00\u00ab des Textes verwenden.\nBeispiel\nWenn Sie ein Programm schreiben, werden 80% des Codes schnell geschrieben sein. Sie\nwerden dann aber 80% Ihrer Zeit auf den \u00bbFeinschli\ufb00\u00ab des Codes verwenden.\nBig Idea\nLasse den Feinschli\ufb00 weg! (Und spare 80% der Zeit.)\n12-9 12-9 Formale Definition von Approximationalgorithmen.\nIDefinition\nSeiPein Optimierungsproblem und r\u00151. Ein r-Approximationsalgorithmus f\u00fcr Pist ein\n(Polynomialzeit-)Algorithmus, der f\u00fcr jede Eingabe xeine Ausgabe smit folgenden Eigen-\nschaften liefert:\n1.sist eine L\u00f6sung f\u00fcr x, das hei\u00dft, (x;s)2S.\n2.Die G\u00fcte von sist kleiner oder gleich r.\nBeispiel\nEin2-Approximationsalgorithmus f\u00fcr das Handelsreisenden-Problem liefert immer eine\nRundreise, die h\u00f6chstens doppelt so lang wie k\u00fcrzeste Rundreise ist.\nBeispiel\nEin1;01-Approximationsalgorithmus f\u00fcr das Rucksack-Problem liefert immer eine Auswahl\nvon Gegenst\u00e4nden, deren Wert bis auf 1% an den Wert der optimalen Auswahl herankommt.12 Entwurfsmethode: Approximation\n12.2 Approximationsalgorithmen127\n12.2.2 Handelsreisender in der Ebene\n12-10 12-10 Der Handelsreisende in der Ebene\nIDefinition: /m.sc/i.sc/n.sc-/t.sc/s.sc/p.sc\nEingaben Eine Menge MvonSt\u00e4dten und eine Funktion m:M\u0002M!N, die je zwei\nSt\u00e4dten Reisekosten zwischen diesen St\u00e4dten zuordnet.\nL\u00f6sungen Rundreisen durch die St\u00e4dte, so dass jede Stadt genau einmal besucht wird.\nMa\u00df Kosten der Rundreise.\nZielMinimierung.\nIDefinition: Euklidisches Handelsreisendenproblem\nDas Problem /m.sc/i.sc/n.sc-/e.sc/u.sc/c.sc/l.sc/i.sc/d.sc/i.sc/a.sc/n.sc-/t.sc/s.sc/p.sc ist de\ufb01niert wie /m.sc/i.sc/n.sc-/t.sc/s.sc/p.sc , nur muss Meine Menge von\nPunkten in der Ebene sein und mmuss St\u00e4dtepaare auf ihre Distanz in der Ebene abbilden\n(gerundet).\nISatz\nDie Entscheidungsvariante /e.sc/u.sc/c.sc/l.sc/i.sc/d.sc/i.sc/a.sc/n.sc-/t.sc/s.sc/p.sc istNP-vollst\u00e4ndig.\nBeweisideen. Man f\u00fchrt folgende Kette von Reduktionen durch:\n1.Reduziere /h.sc/a.sc/m.sc/i.sc/l.sc/t.sc/o.sc/n.sc auf/p.sc/l.sc/a.sc/n.sc/a.sc/r.sc-/h.sc/a.sc/m.sc/i.sc/l.sc/t.sc/o.sc/n.sc (nur planare Graphen sind als Eingabe zu-\ngelassen), indem kreuzende Kanten mittels geeigneter Gadgets simuliert werden.\n2.Reduziere nun weiter auf /g.sc/r.sc/i.sc/d.sc-/h.sc/a.sc/m.sc/i.sc/l.sc/t.sc/o.sc/n.sc (nur Teilgraphen eines Gitters sind als Einga-\nben zugelassen). Hierzu ersetzt man Knoten durch kleine Quadrate und Kanten durch\nlange, zwei Knoten breite \u00bbTentakel\u00ab.\n3.Reduziere dann weiter auf /e.sc/u.sc/c.sc/l.sc/i.sc/d.sc/i.sc/a.sc/n.sc-/t.sc/s.sc/p.sc .\n12-11 12-11 Das Handelsreisendenproblem in der Ebene ist 2-approximierbar.\nISatz\nEs gibt einen 2-Approximationsalgorithmus f\u00fcr /m.sc/i.sc/n.sc-/e.sc/u.sc/c.sc/l.sc/i.sc/d.sc/i.sc/a.sc/n.sc-/t.sc/s.sc/p.sc .\nBeweis. Der Algorithmus funktioniert f\u00fcr eine Menge Mvon St\u00e4dten so:\n1.Erzeuge einen vollst\u00e4ndigen Graphen, dessen Knoten St\u00e4dte sind und dessen Kanten\nmit den Entfernungen der St\u00e4dte gewichtet sind.\n2.Berechne ein minimales Ger\u00fcst des Graphen.\n3.Erzeuge eine Eulertour aus dem Ger\u00fcst durch Verdopplung aller Kanten .\n4.Verk\u00fcrze die Eulertour, bis jeder Knoten nur einmal besucht wird.\nDass dieser Algorithmus eine 2-Approximation liefert, sieht man so: Die k\u00fcrzeste Rundreise\nhabe die L\u00e4nge x. L\u00f6schen wir daraus eine Kante, so erhalten wir einen Pfad, der noch k\u00fcrzer\nist. Dieser Pfad ist ein Ger\u00fcst. Also ist das Gewicht ydes minimalen Ger\u00fcsts noch kleiner:\ny<x. Die Eulertour hat L\u00e4nge 2y<2x. Die ausgegebene Tour ist k\u00fcrzer als die Eulertour,\nalso k\u00fcrzer als 2x.\n12-12 12-12 Der Algorithmus an einem Beispiel.\nSchritt 0: Die Eingabe.12812 Entwurfsmethode: Approximation\n12.2 Approximationsalgorithmen\n12-13 12-13 Der Algorithmus an einem Beispiel.\nSchritt 1: Der Distanzgraph.\n2,22,2\n13,2\n325\n25,13,6\n2,85,14,1\n3,6\n3,2\n12-14 12-14 Der Algorithmus an einem Beispiel.\nSchritt 2: Das Ger\u00fcst.\n2,22,2\n13,2\n325\n25,13,6\n2,85,14,1\n3,6\n3,2\n12-15 12-15 Der Algorithmus an einem Beispiel.\nSchritte 3 und 4: Geradeziehen der Eulertour.\n1 2 3 4\n5\n12-16 12-16 Das Handelsreisendenproblem in der Ebene ist 1,5-approximierbar.\nISatz: Christofides\nEs gibt einen 1,5-Approximationsalgorithmus f\u00fcr /m.sc/i.sc/n.sc-/e.sc/u.sc/c.sc/l.sc/i.sc/d.sc/i.sc/a.sc/n.sc-/t.sc/s.sc/p.sc .\nBeweis. Der Algorithmus ist \u00e4hnlich wie eben:\n1.Erzeuge den vollst\u00e4ndigen Graphen G.\n2.Berechne ein minimales Ger\u00fcst Rdes Graphen.\n3.Betrachte die Menge Ualler Knoten in Rmit ungeradem Grad (in Bezug auf R).\n4.Berechne ein Matching Mmit minimalem Gewicht auf dem von UinGinduzierten\nTeilgraphen.\n5.Vereinigt man RundM, so hat jeder Knoten geraden Grad und somit gibt es eine Eu-\nlertour.12 Entwurfsmethode: Approximation\n12.2 Approximationsalgorithmen129\n6.Verk\u00fcrze die Eulertour, bis jeder Knoten nur einmal besucht wird.\nDieser Algorithmus liefert eine 1,5-Approximation: Die k\u00fcrzeste Rundreise habe die L\u00e4n-\ngex. Wieder hat das Ger\u00fcst ein Gesamtgewicht von y<x. Weiter hat das Matching ein\nGesamtgewicht von z<x=2, denn jede Rundreise durch den ganzen Graphen induziert eine\n(nur noch k\u00fcrzere) Rundreise durch nur die Knoten von U, indem man Knoten au\u00dferhalb von\nU\u00fcberspringt. Aus dieser Rundreise kann man zwei Matchings gewinnen, indem man nur\ndie Kanten an gerade oder nur die an ungerade Stellen betrachtet. Das minimales Matching\nMist folglich mindestens so gut wie das bessere dieser Matchings und hat deshalb h\u00f6chstens\ndas halbe Gewicht der besten Rundreise durch G. Damit hat die Eulertour L\u00e4nge y+z<3\n2x\nund die ausgegebene Tour ist k\u00fcrzer als die Eulertour, also k\u00fcrzer als3\n2x.\n12-17 12-17 Der Christofides-Algorithmus an einem Beispiel.\nSchritt 0: Die Eingabe.\n12-18 12-18 Der Christofides-Algorithmus an einem Beispiel.\nSchritt 1: Das Ger\u00fcst und die Menge U.\nRR\nRR\nR2U2U\n2U 2U\n12-19 12-19 Der Christofides-Algorithmus an einem Beispiel.\nSchritt 2: Das Matching auf UinG.\nM\nRR\nRR\nRM13012 Entwurfsmethode: Approximation\n12.2 Approximationsalgorithmen\n12-20 12-20 Der Christofides-Algorithmus an einem Beispiel.\nSchritt 3: Die Eulertour auf R[Mund das Geradeziehen.\n1 2\n12.2.3 Bin-Packing\n12-21 12-21 Illustration des Bin-Packing Problems.\nCopyright by S. M\u00fcller, Creative Commons Attribution 2.5.!\nCopyright by Allan Patrick, Creative Commons Attribution-ShareAlike 3.0.\n12-22 12-22 Das Bin-Packing-Problem\nIDefinition: /m.sc/i.sc/n.sc-/b.sc/i.sc/n.sc-/p.sc/a.sc/c.sc/k.sc/i.sc/n.sc/g.sc\nEingabe Eine Liste von Objektgr\u00f6\u00dfen (g1;:::;gn)und eine Eimergr\u00f6\u00dfe b.\nL\u00f6sungen Zuordnung von Objekten zu Eimern, so dass die Summe der Gr\u00f6\u00dfen aller Ob-\njekte, die demselben Eimer zugeordnet sind, maximal bist.\nMa\u00df Anzahl der benutzten Eimer.\nZielMinimierung.\nMan kann mittels einer Reduktion von /t.sc/r.sc/i.sc/p.sc/a.sc/r.sc/t.sc/i.sc/t.sc/e.sc-/m.sc/a.sc/t.sc/c.sc/h.sc/i.sc/n.sc/g.sc zeigen:\nISatz\nDie Entscheidungsvariante /b.sc/i.sc/n.sc-/p.sc/a.sc/c.sc/k.sc/i.sc/n.sc/g.sc istNP-vollst\u00e4ndig.\n12-23 12-23 Ein Greedy-Algorithmus f\u00fcr Bin-Packing.\nISatz\nEs gibt einen 2-Approximationsalgorithmus f\u00fcr /m.sc/i.sc/n.sc-/b.sc/i.sc/n.sc-/p.sc/a.sc/c.sc/k.sc/i.sc/n.sc/g.sc .\nBeweis. Der Algorithmus wiederholt f\u00fcr jeden Gegenstand:\n1.Finde, von links beginnend, den ersten Eimer, in den der Gegenstand noch passt.\n2.Platziere den Gegenstand in diesen Eimer.\nDies funktioniert: Betrachten wir eine L\u00f6sung, die First-Fit produziert hat. Dann passt der\nInhalt von je zwei nebeneinander stehenden Eimern nicht in einen einzigen Eimer (sonst\nh\u00e4tte First-Fit das n\u00e4mlich getan). Also muss auch in einer optimalen L\u00f6sung f\u00fcr je zwei\nvon First-Fit benutzte Eimer mindestens ein Eimer genutzt werden.12 Entwurfsmethode: Approximation\n12.2 Approximationsalgorithmen131\n12.2.4 Vertex-Cover\n12-24 12-24 Vertex-Cover l\u00e4sst sich sehr leicht approximieren.\nISatz\nDie Entscheidungsvariante /v.sc/c.scistNP-vollst\u00e4ndig.\nBeweis. Triviale Reduktion von /i.sc/n.sc/d.sc/e.sc/p.sc/e.sc/n.sc/d.sc/e.sc/n.sc/t.sc-/s.sc/e.sc/t.sc .\nISatz\nEs gibt einen 2-Approximationsalgorithmus f\u00fcr /m.sc/i.sc/n.sc-/v.sc/c.sc .\nBeweis. Verwende folgenden Algorithmus:\n1while there is an uncovered edge do\n2 pick any edgefu;vg\n3 add both uandvtothe vertex cover\n4 delete uandvand all pending edges from the graph\nDa von jeder Kante mindestens ein Endpunkt gew\u00e4hlt werden muss, ist die gefundene L\u00f6-\nsung h\u00f6chstens doppelt so gro\u00df wie die optimale.\nEs ist kein besserer Algorithmus bekannt.\n12-25 12-25 Der triviale Algorithmus in Aktion.\nStart\na b cde\nf1\ncd fe\na b2\nfe\na b cd3\na b cde\nf\n12-26 12-26 Ein \u00bboffensichtlich besserer\u00ab, greedy-basierter Algorithmus.\nNehmen wir einen Knoten nicht nach Mauf, so m\u00fcssen wiralle Nachbarn aufnehmen.\nDaher erscheint es sinnvoll, immer den Knoten mit h\u00f6chstem Grad zu w\u00e4hlen .\nGreedy-Algorithmus f\u00fcr Vertex-Cover\n1while there is an uncovered edge do\n2 pick a vertex vof maximum degree\n3 addvtothe vertex cover\n4 delete vand all pending edges from the graph\nISatz\nDer Greedy-Algorithmus ist kein Approximationsalgorithmus.\nBeweis. In \u00dcbung 12.1 wird gezeigt, dass die Approximationsg\u00fcte bis zu lnnbetragen kann\n(und somit unbeschr\u00e4nkt ist).13212 Entwurfsmethode: Approximation\n12.3 *Approximationsschemata\n12-27 12-27 Der Greedy-Algorithmus in Aktion.\nStart\na b cde\nf1\na b cd fe2\na cd fe\nb3\ncd fe\nb a\n4\nd fe\nb a c\n12.3 *Approximationsschemata\n12.3.1 Das Konzept\n12-28 12-28 Wie viel sollten Sie f\u00fcr eine Klausur lernen?\nBob hat folgende Beobachtung beim Lernen f\u00fcr seine Klausuren gemacht:\n\u2013Lernt er zwei Stunde lang, so besteht er.\n\u2013Lernt er vier Stunden lang, so bekommt er im Schnitt eine 3,0.\n\u2013Lernt er acht Stunden lang, so bekommt er im Schnitt eine 2,0.\n\u2013Lernt er 16 Stunden lang, so bekommt er im Schnitt eine 1,7.\n\u2013Lernt er 32 Stunden lang, so bekommt er im Schnitt eine 1,3.\n\u2013Lernt er 64 Stunden lang, so bekommt er im Schnitt eine 1,2.\n\u2013Lernt er 128 Stunden lang, so bekommt er im Schnitt eine 1,1.\n\u2013Selbst wenn er beliebig viel lernt, so kann er keine 1,0 garantieren.\n.Zur Diskussion\nWie viel sollte Bob f\u00fcr eine Klausur lernen?\n12-29 12-29 Die Idee des Approximationsschemas.\nIDefinition\nSeiPein Optimierungsproblem. Ein Approximationsschema ist eine Familie von Algorith-\nmenAkf\u00fcrk2f1;2;3;:::g, so dass Akgerade ein (1+1=k)-Approximationsalgorithmus\nf\u00fcrPist.\nMan kann also P\u00bbbeliebig gut approximieren\u00ab, jedoch kann dies \u00bbimmer aufw\u00e4ndiger\u00ab\nwerden mit steigendem k.\n12.3.2 Rucksack-Problem\n12-30 12-30 Zur Erinnerung: Das Rucksack-Problem\nCopyright by Marieke Kuljjer, Creative Commons Attribution-ShareAlike!\nCopyright by user Joadl, Creative Commons Attribution-ShareAlike12 Entwurfsmethode: Approximation\n12.3 *Approximationsschemata133\nIProblem: Das Optimierungsproblem /m.sc/a.sc/x.sc-/k.sc/n.sc/a.sc/p.sc/s.sc/a.sc/c.sc/k.sc\nEingabe Eine Folge (w1;:::;wn)vonWerten und eine Folge (g1;:::;gn)vonGewichten\nund ein Maximalgewicht m.\nL\u00f6sungen Teilmengen I\u0012f1;:::;ng, so dassP\ni2Igi\u0014m.\nMa\u00df Der GesamtwertP\ni2Iwider ausgew\u00e4hlten Gegenst\u00e4nde.\nZielMaximierung.\n12-31 12-31 Es gibt ein Approximationsschema f\u00fcr das Rucksack-Problem\nISatz\nEs gibt ein Approximationsschema f\u00fcr das Rucksack-Problem.\nBeweisplan.\n1.Zur Erinnerung: In \u00dcbung 2.2 haben wir einen Algorithmus kennen gelernt, der das\nProblem in Zeit O(nW)l\u00f6st, wobei Wder Gesamtwert aller Gegenst\u00e4nde ist.\n2.F\u00fcr gro\u00dfe Wdauert dies zwar zu lange, aber wenn wir das Problem nur approximativ\nl\u00f6sen wollen, so gen\u00fcgt es, die Werte zu runden .\n3.Statt gro\u00dfer, genauer Werte wie (120001 ;320016 ;420004 ;150067 )rechnet man mit\nden kleinen Werten (12;32;42;15), was dann schnell geht.\n4.Wie stark man rundet, macht man von der gew\u00fcnschten Genauigkeit abh\u00e4ngig.\nSkript:\nBeweis-\ndetailsSkript:\nBeweis-\ndetails Das Approximationsschema\n1input Werte (w1;:::;wn)\n2input Gr\u00f6\u00dfen(g1;:::;gn)\n3input Rucksackgr \u00f6\u00dfem\n4input k\n5m maxfw1;:::;wng\n6fori 1tondo\n7 w0\ni \u0004\nwin(k+1)\nm\u0005\n8return the set Icomputed by the exact algorithm on (w0\n1;:::;w0n;g1;:::;gn;m)\nEin Beispiel f\u00fcr die Rundung\nOriginaleingabe\nWert\nsize 51234\nsize 82200\nsize 11309\nsize 21310\nsize 21299\nsize 52501\nsize 31250\nEingabe mit reduzierter Genauigkeit\nWert\nsize 512\nsize 822\nsize 113\nsize 213\nsize 212\nsize 525\nsize 312\nDie Korrektheit des Tricks sieht man so: Sei Jeine L\u00f6sung f\u00fcr die Originaleingabe und sei x=13412 Entwurfsmethode: Approximation\nZusammenfassung dieses Kapitels\nP\nj2Jwjderen Wert. Sei weiter Idie optimale L\u00f6sung f\u00fcr (w0\n1;:::;w0n;g1;:::;gn;m). Dann gilt:\nX\ni2Iwi=m\nnkX\ni2Iwin(k+1)\nm\u0015m\nnkX\ni2I\u0016\nwin(k+1)\nm\u0017\n\u0015m\nnkX\nj2J\u0016\nwjn(k+1)\nm\u0017\n\u0015m\nnkX\nj2J(wjn(k+1)\nm\u00001)\n\u0015x\u0000nm\nn(k+1)\n\u0015x\u0000x=(k+1) =x(1\u00001=(k+1)) =x=(1+1=k):\nDer Wert der gefundenen L\u00f6sung kommt also bis auf den Faktor 1+1=kan den Wert der optimalen\nL\u00f6sung heran.\nZusammenfassung dieses Kapitels\n12-32 12-32 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationEuklidisches /t.sc/s.sc/p.sc\nApproximation\nChristo\ufb01des-Algorithmus\nG\u00fcten-Absch\u00e4tzung\n1,5-ApproximationBin-Packing\nVertex-Cover\nApproximation\nFirst-Fit\nTrivial\nG\u00fcten-Absch\u00e4tzung\n2-Approximation\nProblemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationRucksack-Problem\nApproximation,\nDynamische Tabellen\nRundungsalgorithmus\nG\u00fcten-Absch\u00e4tzung\nApproximationsschema\nIApproximationsalgorithmus\nEinr-Approximationsalgorithmus f\u00fcr ein Optimierungsproblem Pist ein Polynomialzeital-\ngorithmus, der bei jeder Eingabe xeine L\u00f6sung der G\u00fcte kleiner oder gleich rliefert.\nIApproximationsschema\nEinApproximationsschema f\u00fcr ein Optimierungsproblem Pist eine Folge A1,A2,A3und so\nweiter, so dass Akein(1+1=k)-Approximationsalgorithmen f\u00fcr Pist.\nINP-Vollst\u00e4ndigkeit ist nicht dasselbe wie Approximierbarkeit\nF\u00fcr viele NP-vollst\u00e4ndige Optimierungsprobleme kennt man Approximationsalgorithmen,\nf\u00fcr andere hingegen nicht.12 Entwurfsmethode: Approximation\n12.4 \u00dcbungen zu diesem Kapitel135\n\u00dcbungen zu diesem Kapitel\n\u00dcbung 12.1 Greedy-Heuristik f\u00fcr Vertex-Cover, schwer\nBeweisen Sie, dass die Greedy-Heuristik von Seite 12-26 eine Worst-Case-Approximationsg\u00fcte von\nQ(lnn)aufweist. Gehen Sie dazu wie folgt vor:\n1.Finden Sie f\u00fcr jedes neinen Graphen Gn, so dass die G\u00fcte der Ausgabe des Greedy-Algorithmus\ninW(lnn)liegt.\nTipp: Benutzen Sie einen bipartiten Graphen als Gn. Ein Ufer von Gnsollte die Gr\u00f6\u00dfe nhaben.\nDas andere Ufer sollte aus n\u00001Bl\u00f6cken B2,B3, ..., Bnvon Knoten bestehen, wobei BiGr\u00f6\u00dfe\nbn=ichat und jeder Knoten in Bizuiunterschiedlichen Knoten des ersten Ufers verbunden ist.\n2.Zeigen Sie, dass die Greedy-Heuristik immer eine L\u00f6sung \ufb01ndet, deren G\u00fcte h\u00f6chstens lnnist.\nTipp: Sei dder maximale Grad in einem Graphen G. Dann musss eine minimale Knoten\u00fcber-\ndeckung CvonGmindestensjEj=d\u0014jCjKnoten enthalten. Ist Eidie Menge an Kanten, die\nnach Runde inoch vorhanden sind und dider maximale Knotengrad nach Runde i, so w\u00e4hlt\nGreedy immer einen Knoten vom Grad di. Damit gilt aberjEi+1j\u0014jEij\u0000di\u0014jEij\u0000jEij=jCj=\njEij(1\u00001=jCj). Hieraus folgtjEij\u0014jEj(1\u00001=jCj)i. Um den Beweis abzuschlie\u00dfen, nutzen Sie\n(1\u0000x)n\u0014e\u0000nx.13613 Entwurfsmethode: Fixed-Parameter\n13-1 13-1\nKapitel 13\nEntwurfsmethode: Fixed-Parameter\nWie man Vertex-Cover f\u00fcr Milliarden von Knoten und Kanten l\u00f6st\n13-2 13-2Lernziele dieses Kapitels\n1.Konzept des Fixed-Parameter-Algorithmus\nverstehen\n2.Fixed-Parameter-Algorithmus f\u00fcr das\nVertex-Cover-Problem kennen\n3.Konzept der Kernelisierung verstehen\n4.Kernelisierungs-Algorithmus f\u00fcr das\nVertex-Cover-Problem kennenInhalte dieses Kapitels\n13.1 NP-Vollst\u00e4ndig hei\u00dft nicht \u00bbunl\u00f6sbar\u00ab 138\n13.1.1 Sind \u00bbschwere\u00ab Probleme \u00bbschwer\u00ab? . . 138\n13.1.2 Fallbeispiel: Vertex-Cover . . . . . . . . 139\n13.1.3 Ein ehrgeiziges Ziel . . . . . . . . . . . 139\n13.2 Der Fixed-Parameter-Ansatz 140\n13.2.1 Der Pakt mit dem Teufel . . . . . . . . . 140\n13.2.2 Eine brillante Idee . . . . . . . . . . . . 141\n13.2.3 Verfeinerungen der Idee . . . . . . . . . 142\n13.2.4 Das allgemeine Konzept . . . . . . . . . 144\n13.3 Kernelisierung 144\nWorum\nes heute\ngehtWorum\nes heute\ngehtNeulich bei einer Tasse Cappuccino belauschte ich am Nebentisch ein Informatikerehe-\npaar, er Theoretiker, sie Praktikerin.1Sie diskutierten die Frage, ob das Problem /p.sc/l.sc/a.sc/n.sc/a.sc/r.sc-/three.taboldstyle-\n/c.sc/o.sc/l.sc/o.sc/r.sc/a.sc/b.sc/l.sc/e.sc ein \u00bbschweres\u00ab Problem sei.\nTheoretiker Liebling, ich habe neulich zeigen k\u00f6nnen, dass das planare 3-F\u00e4rbbarkeitsproblem\nein wirklich schwieriges Problem ist.\nPraktikerin (eher auf ihr Himbeert\u00f6rtchen konzentriert) Ach ja?\nTheoretiker (sichtlich erfreut \u00fcber die Nachfrage) Ja, das Problem ist n\u00e4mlich NP-vollst\u00e4ndig!\nPraktikerin (schiebt sich ein St\u00fcck Himbeert\u00f6rtchen in den Mund) Hmmpf?\nTheoretiker (interpretiert dies als Au\ufb00orderung, auszuholen) Also, ich habe zeigen k\u00f6nnen,\ndass es eine Reduktion von /three.taboldstyle-/c.sc/o.sc/l.sc/o.sc/r.sc/a.sc/b.sc/l.sc/e.sc auf dieses Problem gibt. Wie du ja\nwei\u00dft, ist 3-F\u00e4rbbarkeit ein klassisches NP-vollst\u00e4ndiges Problem und folglich\ngen\u00fcgt es, hiervon eine Reduktion anzugeben, um die Vollst\u00e4ndigkeit zu zeigen.\nPraktikerin (trinkt noch einen Schluck Ka\ufb00ee) Und damit ist das Problem schwer?\nTheoretiker (etwas verunsichert ob der Frage) Ja, nat\u00fcrlich, Schatz. Du wei\u00dft doch, NP-\nvollst\u00e4ndige Probleme sind schwer, denn wenn wir eines von ihnen in polyno-\nmieller Zeit l\u00f6sen k\u00f6nnten, dann k\u00f6nnten wir ja alleProblem in NPin polyno-\nmieller Zeit l\u00f6sen, was ja noch niemand gescha\ufb00t. Man k\u00f6nnte, sagen wir, sogar\ndas Faktorisierungsproblem in polynomieller Zeit l\u00f6sen \u2013 und dann w\u00fcrde die\ngesamte IT-Sicherheitsinfrastruktur dieses Planeten in sich zusammenbrechen!\nPraktikerin (schiebt den Rest ihres Himbeert\u00f6rtchens zur Seite und schaut ihn an) Na und?\nTheoretiker (v\u00f6llig entgeistert) Wie \u00bbna und\u00ab?\nPraktikerin (trinkt noch einen Schluck Ka\ufb00ee) Du meinst, man kann das planare 3-F\u00e4rb-\nbarkeitsproblem nicht e\ufb03zient l\u00f6sen, weil dann das Faktorisierungsproblem als\nProblem in NPe\ufb03zient l\u00f6sbar w\u00e4re?\n1Es sei an dieser Stelle angemerkt, dass meine Rechtschreibkorrektur zwar das Wort \u00bbTheoretikerin\u00ab kennt,\nnicht jedoch das Wort \u00bbPraktikerin\u00ab, was gendersensibilisierte Menschen sicherlich aufhorchen l\u00e4sst.13 Entwurfsmethode: Fixed-Parameter\n\u00dcbungen zu diesem Kapitel137\nTheoretiker (indigniert) Das \u00bbmeine\u00ab ich nicht nur, dass kann ich beweisen!\nPraktikerin Lass mich dir mal zeigen, was da wirklich passiert: Was sind denn die Eingaben\nbeim Faktorisierungsproblem?\nTheoretiker Na, Zahlen von vielleicht 2000 Bit L\u00e4nge, die faktorisiert werden sollen.\nPraktikerin Genau. Und wie geht dein Argument nun weiter?\nTheoretiker Das Faktorisierungsproblem ist ein typisches Problem in NP: Eine nichtdetermi-\nnistische Turing-Maschine kann zun\u00e4chst einen Faktor raten, dann \u00fcberpr\u00fcfen,\nob dies tats\u00e4chlich einer ist, und dann akzeptieren, wenn ein bestimmtes Bit des\nFaktors 1ist. Die Laufzeit ist polynomiell und, wenn man sich geschickt anstellt,\nvielleicht sogar linear. Nach dem Satz von Cook k\u00f6nnen wir jedes Problem in\nNPauf/c.sc/i.sc/r.sc/c.sc/u.sc/i.sc/t.sc-/s.sc/a.sc/t.sc reduzieren und von dort aus weiter.\nPraktikerin In der Tat. Wie gro\u00df wird denn der Schaltkreis bei der Reduktion auf /c.sc/i.sc/r.sc/c.sc/u.sc/i.sc/t.sc-/s.sc/a.sc/t.sc ?\nTheoretiker (gr\u00fcbelt kurz) Nun, der Schaltkreis hat quadratische Gr\u00f6\u00dfe in der Laufzeit des\nAlgorithmus.\nPraktikerin Also mindestens 4:000:000bei einer Eingabel\u00e4nge von 2000 Bits?\nTheoretiker \u00d6h, ja. Wohl eher noch etwas mehr, da wir ja noch die Boxen aus der Reduktion\nbrauchen. So eher 100:000:000.\nPraktikerin Und dann?\nTheoretiker Nun, der Schaltkreis wird in eine /three.taboldstyle-/s.sc/a.sc/t.sc Formel umgewandelt grob derselben\nGr\u00f6\u00dfe, also mit etwa 100 Millionen Klauseln.\nPraktikerin Und dann?\nTheoretiker Dann auf /n.sc/a.sc/e.sc-/three.taboldstyle-/s.sc/a.sc/t.sc , wobei jede Klausel durch vier Klauseln ersetzt wird.\nPraktikerin Und dann?\nTheoretiker Dann auf /three.taboldstyle-/c.sc/o.sc/l.sc/o.sc/r.sc/a.sc/b.sc/l.sc/e.sc , wobei jede Klausel durch drei Knoten und jede Variable\ndurch zwei Knoten ersetzt wird.\nPraktikerin Und dann?\nTheoretiker Dann wird noch jede Kreuzung durch ein Gadget ersetzt, wodurch je elf neue\nKnoten entstehen. Dann ist aber Schluss!\nPraktikerin Lass mich das mal zusammenfassen: Wenn ich also wissen m\u00f6chte, wie die Fak-\ntorisierung einer 2000-Bit-Zahl aussieht, kann ich stattdessen versuchen zu ent-\nscheiden, ob ein bestimmter planare Graph 3-f\u00e4rbbar ist. Und wie viele Knoten\nhat der?\nTheoretiker (nuschelt) Nun, so um die 10 Milliarden, denke ich.\nPraktikerin Siehst du, deshalb habe ich vorhin \u00bbNa und?\u00ab gesagt. Solche gigantischen Einga-\nben mit einer auch noch extrem konstruierten Form kommen doch in der Praxis\ngar nicht vor!\nTheoretiker Ha! Da kann ich aber auch antworten: \u00bbNa und?\u00ab Schlie\u00dflich k\u00f6nnten sie vor-\nkommen.\nPraktikerin Mag sein. Aber dein sch\u00f6nes NP-Vollst\u00e4ndigkeitsresultat widerspricht \u00fcberhaupt\nnicht der M\u00f6glichkeit, dass es einen Algorithmus gibt, der bei allen in der Praxis\nvorkommenden Eingaben schnell ist. Wie wir gerade gesehen haben, k\u00f6nnte es\ndoch sein, dass wie f\u00fcr planare Graphen mit, sagen wir, maximal einer Million\nKnoten das 3-F\u00e4rbbarkeitsproblem in einer Stunde l\u00f6sen k\u00f6nnte. So viel zum\nThema \u00bb /p.sc/l.sc/a.sc/n.sc/a.sc/r.sc-/three.taboldstyle-/c.sc/o.sc/l.sc/o.sc/r.sc/a.sc/b.sc/l.sc/e.sc ist ein wirklich schwieriges Problem\u00ab...\nBringst du das Geschirr weg?\nDie Diskussion stimmte mich nachdenklich. Kann man das planare 3-F\u00e4rbbarkeitsproblem\nwirklich f\u00fcr beliebige Graphen mit einer Million Knoten schnell l\u00f6sen?\nTats\u00e4chlich kennt man keinen solchen Algorithmus, jedoch ist viel Wahres an dem Argument\nder Praktikerin: Wenn ein Problem NP-vollst\u00e4ndig ist, hei\u00dft das nur, dass es sehr gro\u00dfe und\nsehr konstruierte Eingaben gibt, die schwer zu l\u00f6sen sind. In aller Regel werden das aber\ngerade nicht Eingaben sein, die man \u00bbin der freien Wildbahn\u00ab antri\ufb00t. Die systematische\nUntersuchung von Eingaben, die \u00bbin der Praxis\u00ab vorkommen, hat gleich eine ganze Rei-\nhe von Theorie-Teilgebieten produziert: Average-Case-Analysen, Smoothed-Analysis und13813 Entwurfsmethode: Fixed-Parameter\n13.1 NP-Vollst\u00e4ndig hei\u00dft nicht \u00bbunl\u00f6sbar\u00ab\nFixed-Parameter-Methoden drehen sich alle um die Frage, wie schnell Probleme gel\u00f6st wer-\nden k\u00f6nnen, wenn die Eingaben \u00bbin der Praxis oft vorkommen\u00ab.\nIn diesem Kapitel wollen wir uns ein besonders erfolgreiches Gebiet, die Fixed-Parameter-\nAlgorithmen, etwas genauer anschauen. Es sei aber gleich vorneweg darauf hingewiesen,\ndass sich Theoretiker hier eine De\ufb01nition von \u00bbkommt in der Praxis h\u00e4u\ufb01g vor\u00ab ausgedacht\nhaben. Dass dabei nicht unbedingt etwas \u00fcberm\u00e4\u00dfig Pragmatisches herauskommt, versteht\nsich fast von selbst. Dass die Theoretiker mit ihrer De\ufb01nition aber nicht ganz daneben lie-\ngen, zeigt der Erfolg dieser Ans\u00e4tze: F\u00fcr bestimmte Probleme, insbesondere f\u00fcr das Vertex-\nCover-Problem, lassen sich damit tats\u00e4chlich Eingaben aus der Praxis mit Hunderten oder\ngar Tausenden von Knoten l\u00f6sen.\n13.1 NP-Vollst\u00e4ndig hei\u00dft nicht \u00bbunl\u00f6sbar\u00ab\n13.1.1 Sind \u00bbschwere\u00ab Probleme \u00bbschwer\u00ab?\n13-4 13-4 Wie \u00bbschwer\u00ab sind \u00bbschwere\u00ab Probleme wirklich?\nSind planare 3-F\u00e4rbbarkeit und Vertex-Cover schwer?\nWelche der folgenden Graphen sind 3-f\u00e4rbbar? Welche haben ein Vertex-Cover der Gr\u00f6\u00dfe 3?\n1.a b\ncd\ne\nfg\n2.a b c d e f g\n3.1\n2\n3 4567 8\na\nIst Subset-Sum schwer?\nK\u00f6nnen Sie einige der folgenden Zahlen unterstreichen, so dass die Summe der unterstri-\nchenen Zahlen genau 1.000.000.000 betr\u00e4gt?\n\u20131\n\u201312\n\u2013123\n\u20131234\n\u201312345\n\u2013123456\n\u20131234567\n\u201312345678\n\u2013123456789\n\u20131234567890\nUnd nochmal: Ist Vertex-Cover schwer?\nBetrachten Sie ein \u00bbRiesenrad\u00ab (ein Zyklus mit einem Knoten in der Mitte, der mit allen\nKnoten verbunden ist) mit einer Million Knoten. Besitzt dieser Graph ein\n\u2013Vertex-Cover der Gr\u00f6\u00dfe 3?\n\u2013Vertex-Cover der Gr\u00f6\u00dfe 4?\n\u2013Vertex-Cover der Gr\u00f6\u00dfe 5?\n\u2013Vertex-Cover der Gr\u00f6\u00dfe 500.000?13 Entwurfsmethode: Fixed-Parameter\n13.2 NP-Vollst\u00e4ndig hei\u00dft nicht \u00bbunl\u00f6sbar\u00ab139\n13-5 13-5 Wieso lassen sich schwere Problem manchmal sehr schnell l\u00f6sen?\nWir haben eben f\u00fcr mehrere NP-vollst\u00e4ndige Probleme (/p.sc/l.sc/a.sc/n.sc/a.sc/r.sc-/three.taboldstyle-/c.sc/o.sc/l.sc/o.sc/r.sc/a.sc/b.sc/l.sc/e.sc ,/v.sc/c.scund/s.sc/u.sc/b.sc/s.sc/e.sc/t.sc/s.sc/u.sc/m.sc )\nkonkrete Eingaben sehr schnell per Hand gel\u00f6st .\nM\u00f6gliche Gr\u00fcnde hierf\u00fcr k\u00f6nnten sein:\n1.Die Eingaben sind zu klein , als dass sich die \u00bbSchwere\u00ab der Probleme einstellt.\n(Aber: Was ist mit dem Riesenrad?)\n2.Die Eingaben sind zu speziell , als dass sich die \u00bbSchwere\u00ab der Probleme einstellt.\nBig Ideas\nStrategien zur L\u00f6sung schwieriger Probleme:\n\u2013Betrachte nur \u00bbdurchschnittliche\u00ab Eingaben. Die f\u00fchrt zur Average-Case-Analyse .\n\u2013Betrachte nur Eingaben, die \u00bbleicht verrauscht\u00ab sind. Die f\u00fchrt zur Smoothed-Analysis .\n\u2013Betrachte nur Eingaben, die \u00bbin der Praxis\u00ab vorkommen. Die f\u00fchrt zu Fixed-Parameter-\nAlgorithmen .\n13.1.2 Fallbeispiel: Vertex-Cover\n13-6 13-6 Ein schweres Problem, das wir schnell l\u00f6sen wollen.\nIDefinition: Vertex-Cover\nDie Sprache /v.sc/c.scenth\u00e4lt alle Paare (G;k), wobei Gein ungerichteter Graph ist mit nKnoten\nundmKanten, keine Zahl ist und es in Geine Teilmenge Uder Knoten gibt mit jUj\u0014k, so\ndass jede Kanten von Gwenigstens einen ihrer Endpunkte in Uhat.\nBeispiel\n\u0010\n;3\u0011\n2/v.sc/c.sc.\nDieses Problem ist aus verschiedenen Gr\u00fcnden interessant:\n\u2013Falls die Kanten des Graphen \u00bbKon\ufb02ikte\u00ab repr\u00e4sentieren, so ist ein Vertex-Cover eine\n(kleine) Knotenmenge, die man entfernen kann, um alle Kon\ufb02ikte aufzul\u00f6sen.\n\u2013Falls die Kanten des Graphen \u00bbE\ufb00ekte\u00ab repr\u00e4sentieren, so ist ein Vertex-Cover eine\n(kleine) Knotenmenge, die an allen E\ufb00ekten beteiligt ist.\n\u2013Das Komplement eines minimalen Vertex-Cover ist eine maximale unabh\u00e4ngige Men-\nge.\n13.1.3 Ein ehrgeiziges Ziel\n13-7 13-7 Einsehr ehrgeiziges Ziel.\nUnser prinzipielles Ziel\nEinschneller, exakter Algorithmus, der zwar exponentielle Zeit braucht, aber nur auf sehr\nkonstruierten Eingaben und auf normalen Eingaben sehr schnell ist.\nEin konkretes Ziel\nEs ist ein Algorithmus gesucht, der f\u00fcr alleGraphen mit n=1000 Knoten, m=10000\nKanten und k=80auf einem normalen Computer in wenigen Minuten ein Vertex-Cover\n\ufb01ndet.14013 Entwurfsmethode: Fixed-Parameter\n13.2 Der Fixed-Parameter-Ansatz\n13.2 Der Fixed-Parameter-Ansatz\n13-8 13-8 Ein erster Versuch, das Vertex-Cover-Problem zu l\u00f6sen\nBrute-Force-Algorithmus\n1input G= (V;E),k\n2foreach C\u0012VwithjCj=kdo\n3 ifCis a vertex cover of Gthen\n4 output \u2018\u2018size-kvertex cover exists \u2019\u2019and stop\n5output \u2018\u2018no size -kvertex cover exists \u2019\u2019\nDie Laufzeit ist O(nk), was f\u00fcr Werte wie n=1000 andk=80phantastisch lange dauert .\nBis zum Jahr 1988 glaubt man, dass dies die beste Methode sei, /v.sc/c.sczu l\u00f6sen..\n13-9 13-9 Ein zweiter Versuch, basierend auf Backtracking.\nBacktracking-Algorithmus\n1procedure vc-backtrack (G,k)\n2 ifk<0then\n3 return false\n4 else ifGhas no edges then\n5 return true\n6 else\n7 foreach v2Vdo\n8 G0 Gwithout the vertex v\n9 ifvc-backtrack (G0,k\u00001)then\n10 return true\n11 return false\nFalls es ein Vertex-Cover gibt, so h\u00e4ngt die Laufzeit nun von den Details der Eingabe ab.\nGehen wir die Knoten v2Vin einer \u00bbgeschickten\u00ab Reihenfolge durch (beispielsweise be-\nginnend mit den Knoten maximalen Grades), so k\u00f6nnen wir eventuell schnell ein Vertex-\nCover \ufb01nden. Jedoch gilt: Falls es kein Vertex-Cover der Gr\u00f6\u00dfe kgibt, so ist die Laufzeit\nimmernoch O(nk).\n13.2.1 Der Pakt mit dem Teufel\n13-10 13-10 Downey und Fellows gehen einen Pakt mit dem Teufel ein.\nAutor Julius NisleWas der Teufel will. . .\nDer Teufel erlaubt uns nur, Exponentialzeit-Algorithmen f\u00fcr/v.sc/c.sczu entwerfen. (Oder P=NP\nzu beweisen.)\n. . . das soll er bekommen\nWir entwerfen einen Algorithmus, der zwar exponentielle Zeit braucht , aber in Bezug auf k\nund nicht in Bezug auf noder m.13 Entwurfsmethode: Fixed-Parameter\n13.2 Der Fixed-Parameter-Ansatz141\n13.2.2 Eine brillante Idee\n13-11 13-11 Eine einfache, aber brillante Idee: Backtracking entlang der Kanten !\nFellows Algorithmus\n1procedure vc-backtrack -edges (G,k)\n2 ifk<0then\n3 return false\n4 else ifGhas no edges then\n5 return true\n6 else\n7 pick any edgefx;yg2E\n8 foreach v2fx;ygdo\n9 G0 Gwithout the vertex v\n10 ifvc-backtrack -edges (G0,k\u00001)then\n11 return true\n12 return false\n13-12 13-12 Der Algorithmus von Fellows f\u00fcr k=4.\nStart\na b cde\nfW\u00e4hle Kante\na b cde\nfW\u00e4hle Knoten 1\nb cde\nf\na\nW\u00e4hle Kante\nb cde\nf\naW\u00e4hle Knoten 2\nb cde\nafW\u00e4hle Kante\nb cde\naf\nW\u00e4hle Knoten 3\ncde\naf\nbW\u00e4hle Kante\ncde\naf\nbW\u00e4hle Knoten 4\nde\naf\nb c\nNicht fertig, also Backtracking\nde\naf\nb cLetzte gew\u00e4hlte Kante\ncde\naf\nbW\u00e4hle anderen Knoten 4\nce\naf\nbd14213 Entwurfsmethode: Fixed-Parameter\n13.2 Der Fixed-Parameter-Ansatz\n13-13 13-13 Die Laufzeit von Fellows Algorithmus.\nISatz\nDer Algorithmus ist korrekt und arbeitet in Zeit O(2km).\nBeweis. Die Korrektheit der Ausgabe sieht man durch Induktion \u00fcber die Anzahl man Kan-\nten in G:\n\u2013HatGkeine Kanten, so ist die Ausgabe korrekt.\n\u2013HatGeine Kantefx;yg2E, dann hat Gein Vertex-Cover der Gr\u00f6\u00dfe kgenau dann,\nwenn Gohne xoder Gohne yein Vertex-Cover der Gr\u00f6\u00dfe k\u00001hat.\nBetre\ufb00end die Laufzeit argumentieren wir wie folgt: Die Laufzeit gen\u00fcgt o\ufb00enbar der For-\nmelT(k) =2T(k\u00001) +O(m)undT(0) =O(m). Man \u00fcberzeigt sich leicht, dass dies die\nL\u00f6sung T(k) =Q(2km)hat.\n13.2.3 Verfeinerungen der Idee\n13-14 13-14 Zwei einfache, aber folgenreiche Beobachtungen.\nBeobachtung 1: Graphen ohne Verzweigungen sind einfach\nNehmen wir an, ein Graph hat keine Knoten von Grad 3oder h\u00f6her . Dann besteht er aus-\nschlie\u00dflich aus Kreisen und Pfaden . F\u00fcr solche Graphen ist es aber leicht , ein optimales\nVertex-Cover zu berechnen:\n\u2013Man ben\u00f6tigt f\u00fcr ein Pfad der L\u00e4nge ngenaubn=2cKnoten, um alle seine Kanten zu\n\u00fcberdecken.\n\u2013Man ben\u00f6tigt f\u00fcr einen Kreis der L\u00e4nge ngenaudn=2eKnoten, um alle seine Kanten\nzu \u00fcberdecken.\nBeobachtung 2: Ein Knoten oder seine Nachbarn\nF\u00fcr jeden Knoten veines Graphen und jedes Vertex-Cover Cdes Graphen gilt:\n\u2013vist ein Element von Coder\n\u2013alle Nachbarn von vsind Elemente von Coder\n\u2013beides ist der Fall.\n13-15 13-15 Backtracking entlang \u00bbhochgradiger Knoten\u00ab .\nHochgradiges Backtracking\n1procedure vc-backtrack -high-degree (G,k)\n2 ifk<0then\n3 return false\n4 else ifGhas no vertex of degree at least 3then\n5 compute the size sof the smallest vertex cover of G\n6 return k\u0015s\n7 else\n8 pick a node x2Vof degree 3or more\n9 N fvjfx;vg2Eg,that is ,the neighbors of x\n10 foreach S2ffxg;Ngdo\n11 ifvc-backtrack -high-degree (G\u0000S,k\u0000jSj)then\n12 return true\n13 return false13 Entwurfsmethode: Fixed-Parameter\n13.2 Der Fixed-Parameter-Ansatz143\n13-16 13-16 Das hochgradige Backtracking f\u00fcr k=4.\nStart\na\nb c d e\nf g h iW\u00e4hle hochgradigen Knoten\nb c d e\nf g h ia\nW\u00e4hle hochgradigen Knoten\nb c d e\nf h ia\ngKein Vertex-Cover der Gr\u00f6\u00dfe 2 \u2013 Backtracking\nb c d e\nf h ia\ng\nNachbarn von g\ne\nf h i ga\nb c dKein Vertex-Cover der Gr\u00f6\u00dfe 0 \u2013 Backtracking\ne\nf h i ga\nb c d\nNachbarn von a\na\nf g h ib c d e\n13-17 13-17 Die Geschwindigkeit des hochgradigen Backtrackings.\nISatz\nDer Algorithmus ist korrekt und arbeitet in Zeit O(1:49535knm).\nBeweis. Die Korrektheit sollte klar sein, betrachten wir also Laufzeit. Die Anzahl N(k)an\nrekursiven Aufrufen ist\nN(k)\u0014N(k\u00001) +N(k\u00003) +1;\nN(2) =1;\nN(1) =0;\nN(0) =0;\nda wir zwei rekursive Aufrufe durchf\u00fchren, einen f\u00fcr k\u00001und einen f\u00fcr k\u0000jSj\u0014k\u00003.\nWir behaupten, dass N(k)\u00145k=4\u00001eine L\u00f6sung hiervon ist. Dies sieht man so: F\u00fcr k2\nf0;1;2gist dies der Fall. F\u00fcr den induktiven Schritt von k\u00001aufkbetrachte\nN(k)\u0014N(k\u00001) +N(k\u00003) +1\n\u00145k=4\u00001=4+5k=4\u00003=4\u00001\n= (5\u00001=4+5\u00003=4)| {z }\n\u00190:968<15k=4\u00001:\nDamit ist die Behauptung gezeigt.14413 Entwurfsmethode: Fixed-Parameter\n13.3 Kernelisierung\n13-18 13-18 Das aktuelle State-of-the-Art-Resultat bez\u00fcglich Vertex-Cover.\nISatz: Chen, Kanj, Xia 2006\nMan kann /v.sc/c.scin Zeit O(1:2738k+nm)l\u00f6sen.\nMan beachte: Es gilt 1:2738k<109f\u00fcrk\u001485und der Term O(nm)istadditiv , nicht mul-\ntiplikativ . Dies bedeutet, dass /v.sc/c.scf\u00fcr jeden Graphen mit n=1000 ,m=10000 undk=80\nauf einem normalen Computer in Sekundenbruchteilen (!) gel\u00f6st werden kann .\n13.2.4 Das allgemeine Konzept\n13-19 13-19 Das allgemeine Konzept des Fixed-Parameter-Ansatzes\n\u2013DerPakt mit dem Teufel hat f\u00fcr Vertex-Cover einen Algorithmus ergeben mit folgender\nLaufzeit:\nck\n1|{z}\nexponentielle Abh\u00e4ngigkeit von k\u0001 (nm)c2\n|{z}\npolynomielle Abh\u00e4ngigkeit von nundm:\n\u2013Ist der Parameter kfest, so erh\u00e4lt man einen Polynomialzeitalgorithmus in Bezug auf\nalle anderen Parameter.\nFixed-Parameter-Algorithmen allgemein\nFixed-Parameter-Algorithmen haben immer eine Laufzeit von\nf(k)\u0001lc;\nwobei\n1.feine beliebige Funktion ist (typischerweise so etwas wie 2k),\n2.kein problemspezi\ufb01scher Parameter, der bei typischen Eingaben konstant ist,\n3.ldie Eingabel\u00e4nge ist und\n4.ceine Konstante ist.\n13.3 Kernelisierung\n13-20 13-20 Die Idee der Vorverarbeitung.\n\u00bbSchwere\u00ab Probleme sind nur bei gro\u00dfen Eingaben \u00bbschwer\u00ab. Folglich k\u00f6nnte man versu-\nchen, gro\u00dfe Eingaben immer wieder durch kleinere zu ersetzen, die genau dann Elemente\nder Sprache sind, wenn die gro\u00dfen Eingaben es waren. Diesen Vorgang nennt man \u00bbKerne-\nlisierung\u00ab. Wenn eine Eingabe nicht weiter verkleinert werden kann, nennt man sie einen\nKern .\nBeispiel: Eine m\u00f6gliche Ersetzungsregel f\u00fcr Vertex-Cover\n\u2013In einer Eingabe (G;k)seix2Vein Knoten vom Grad 1.\n\u2013Dann kann (G;k)ersetzt werden durch (G0;k\u00001), wobei in G0sowohl xwie auch sein\n(einziger) Nachbar entfernt wurde.\nDiese Regel ist korrekt, denn statt xkann man auch immer den Nachbarn von xin ein Vertex-\nCover aufnehmen.\nvorher\nx ywird ersetzt durchnachher13 Entwurfsmethode: Fixed-Parameter\nZusammenfassung dieses Kapitels145\n13-21 13-21 Ein Kernelisierungsalgorithmus f\u00fcr Vertex-Cover.\nISatz\nAuf eine Eingabe (G;k)wende man folgende Regeln an, bis es nicht mehr geht:\n1.Falls Geinen isolierten Knoten hat, entferne ihn.\n2.Falls Geinen Knoten vvom Grad mindestens k+1hat, ersetze die Eingabe durch (G\u0000\nv;k\u00001).\nSei(G0;k0)das Resultat. Dann gilt:\n1.(G;k)2/v.sc/c.scgenau dann, wenn (G0;k0)2/v.sc/c.sc.\n2.Falls G0noch mehr als k(k+1)Knoten hat, so gilt (G0;k0)=2/v.sc/c.sc.\nBeweis. Die erste Behauptung ist leicht einzusehen. F\u00fcr die zweite nehmen wir an, dass\nG0tats\u00e4chlich mehr als k(k+1)Knoten hat: Da die Regeln nicht mehr anwendbar sind,\ngibt es keine Knoten vom Grad k0+1inG0. W\u00e4hlt man also beliebige k0\u0014kKnoten aus\nG, so ergeben diese zusammen mit ihren Nachbarn maximal k(k+1)Knoten. Da es in G0\naber mehr Knoten geben soll und diese nicht isoliert sind (denn die erste Regel ist nicht\nanwendbar), folgt, dass es nicht abgedeckte Kanten in G0gibt.\nIFolgerung\nMan kann /v.sc/c.scin Zeit O(jGj+1;49535kk2)l\u00f6sen.\nMit anderen Worten: Das NP-vollst\u00e4ndige Problem \u00bbVertex-Cover\u00ab kann e\ufb03zient gel\u00f6st wer-\nden f\u00fcr beliebig gro\u00dfe Eingaben, so lange k\u001485gilt.\nZusammenfassung dieses Kapitels\n13-22 13-22 Problemstellung\nEntwurfsmethode\nA & D\nAnalysemethode\nKlassifikationVertex-Cover\nFixed-Parameter-Ansatz\nHochgradiges Backtracking\nRekursionsgleichung\nLaufzeit O(1;49535kmn)Vertex-Cover\nKernelisierung und\nFixed-Parameter-Ansatz\nKernelisierung und hochgradiges\nBacktracking\nRekursionsgleichung\nLaufzeit O(jGj+1;49535kk2)\nIFixed-Parameter-Ansatz\nEinFixed-Parameter-Algorithmus f\u00fcr ein Problem l\u00f6st dieses in Zeit\nf(k)\u0001lc;\nwobei feine beliebige Funktion ist, kein problemspezi\ufb01scher Parameter, ldie Eingabel\u00e4nge\nundceine Konstante.\nIBeispiele\nBeispiele, f\u00fcr die man Fixed-Parameter-Algorithmen kennt:\n\u2013Vertex-Cover\n\u2013Tripartites Matching\n\u2013Planares Dominating-Set\nJedoch gibt es auch Probleme, f\u00fcr die man vermutet, dass es keine Fixed-Parameter-Algo-\nrithmen f\u00fcr sie gibt:\n\u2013Clique\n\u2013Dominating-Set\n\u2013F\u00e4rbbarkeit14613 Entwurfsmethode: Fixed-Parameter\nZusammenfassung dieses Kapitels\nIKernelisierung\nUnter der Kernelisierung einer Eingabe xf\u00fcr ein Problem Pversteht man die wiederholte\nErsetzung von xdurch k\u00fcrzere Eingaben x0, so dass:\n1.x2Pgenau dann, wenn x02Pund\n2.die Gr\u00f6\u00dfe des letzten x0ist durch einen problemspezi\ufb01schen Parameter beschr\u00e4nkt.\nMotto: Kernelisierung schadet nie.\nZum Weiterlesen\n[1]J\u00f6rg Flum und Martin Grohe, Parametrized Comlexity Theory, Springer, 2006.\nEin aktuelles Buch zum Thema f\u00fcr alle, die es etwas genauer wissen wollen.", "language": "PDF", "image": "PDF", "pagetype": "PDF", "links": "PDF"}