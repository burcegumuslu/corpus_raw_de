{"title": null, "author": null, "url": null, "hostname": null, "description": null, "sitename": null, "date": "2023-10-26", "id": null, "license": null, "body": null, "comments": "", "commentsbody": null, "raw_text": null, "text": "2023-10-26T23:03:32Z\nhttps://opus4.kobv.de/opus4-fau/oai\noai:ub.uni-erlangen.de-opus:243\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\nccs\nccs:D.2.2\nccs:H.5.1\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nBeitrag zur Methode der Arbeitsplatz-integrierten Assistenz am Beispiel der Formmesstechnik\nContribution to the method of workplace-integrated assistance by the example of form testing\nBeetz, Sebastian\nAssistenzsystem\nExpertensystem\nIntelligentes Tutorsystem\nFertigungsmesstechnik\nddc:600\nDie Komplexit\u00e4t der Formmesstechnik verlangt ein hohes Ma\u00df an Wissen und Erfah-rung, um zuverl\u00e4ssige und nachvollziehbare Messergebnisse erzielen zu k\u00f6nnen. Zur Sicherstellung hochwertiger Messergebnisse ist es erforderlich, dieses Wissen dem Messtechniker in geeigneter Weise bereitzustellen, so dass er es bei der Bearbeitung von Messaufgaben anwenden kann. Zur Entwicklung eines umfassenden, Arbeitsplatz-integrierten Assistenzsystems, das das erforderliche Wissen am Arbeitsplatz bereith\u00e4lt und dessen Anwendung auf eine konkrete Aufgabe aktiv unterst\u00fctzt, wurde eine Entwicklungsmethodik aufgestellt, die dem interdisziplin\u00e4ren Charakter eines Assistenzsystems gerecht wird. Diese Methodik ber\u00fccksichtigt, dass ein Assistenzsystem sowohl technisches, als auch wissensbasier-tes System, Arbeitsplatz-nahe Lernumgebung und rechnergest\u00fctztes Lernsystem ist. Am Beispiel der Formmesstechnik, speziell f\u00fcr den Einsatz am Universalformtester MarForm Primar MX4, wird die Entwicklung eines Arbeitsplatz-integrierten Assistenz-systems gem\u00e4\u00df der Entwicklungsmethodik beschrieben. Dazu wurden einer umfassen-den Analyse der Randbedingungen folgend die erforderlichen Systemeigenschaften definiert und die Systemstruktur f\u00fcr das Assistenzsystem konzipiert. Anschlie\u00dfend wur-de das notwendige Wissen f\u00fcr die Bearbeitung von Formmessungen ermittelt und die Umsetzung des Assistenzsystems auf Hypertextbasis entworfen. Dieser Entwurf wurde nachfolgend auf Basis des Content-Management-Systems Zope prototypisch realisiert.\nThe complexity of form testing requires a high degree of knowledge and experience in order to gain reliable and traceable measurement results. To ensure high-quality meas-urement results, it is essential to provide knowledge to the metrologist in an appropriate manner so that it can be applied during the performance of measurement tasks. A methodology has been developed for designing a comprehensive, workplace-integrated assistance system that provides necessary knowledge on-site in the work-place and actively supports the application of the assistance system within a concrete context. This methodology satisfies the interdisciplinary nature of an assistance system and takes into account that such a system is both a technical and knowledge-based system as well as a workplace-embedded learning environment and a computer-based training system. Using the Universal Formtester MarForm Primar MX4 as an exemplary measurement machine, the development of a workplace-integrated assistance system in accordance with the methodology is realized. At first, a comprehensive analysis of the boundary conditions has been conducted, the required system properties have been defined and the system structure of the assistance system has been designed. Following this, the required knowledge for the performance of form measurements has been collected and the realisation of the assistance system has been designed on hypertext basis. Finally, the design has prototypically been realised in the content management system Zope.\n2006-02-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/243\nurn:nbn:de:bvb:29-opus-3192\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-3192\nhttps://opus4.kobv.de/opus4-fau/files/243/Dissertation_Sebastian_Beetz_2006-02-01.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:320\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:530\nccs\nccs:I.2.10\nccs:I.3.5\nccs:I.4.1\nccs:I.4.7\nccs:I.4.8\nccs:I.5.4\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_ohneAngabe\nAutomatische Erzeugung vollst\u00e4ndiger dreidimensionaler Fl\u00e4chenmodelle\nAutomatic Generation of Complete Three Dimensional Surface Models\nSch\u00f6n, Nikolaus\nBildverarbeitung\nDreidimensionales Modell\nMerkmalsextraktion\nAlgorithmische Informationstheorie\nEntropie <Informationstheorie>\nInformationst\nddc:530\nDie Aufgabenstellung der vorliegenden Arbeit ist die Entwicklung einer Methodik zur automatischen und vollst\u00e4ndigen Rekonstruktion eines Modells der Oberfl\u00e4che von 3-d-Objekten aus optischen Sensordaten in Echtzeit. Dabei soll auf bereits in der Arbeitsgruppe vorhandenen Verfahren aufgebaut sowie neue Verfahren zum Gesamtprozess beigetragen werden. Die wichtigste Teilaufgabe ist die initiale gegenseitige Ausrichtung (Grobregistrierung) verschiedener 3-d-Einzelansichten. Dieser Prozess wurde automatisiert, indem Fl\u00e4chenpunkte aufgrund von Merkmalen ihrer Umgebung und aufgrund ihrer geometrischen Konstellation einander zugeordnet werden. Das Verfahren basiert auf der Selektion \"`auff\u00e4lliger\"' Punkte auf Freiform-Fl\u00e4chen. Die Auff\u00e4lligkeit von Punkten wird quantifiziert auf der Grundlage von Symmetriebetrachtungen und einer allgemeinen Informationstheorie. Das so erhaltene Auff\u00e4lligkeitsma\u00df entspricht einem Ma\u00df f\u00fcr den pragmatischen Informationsgehalt von Fl\u00e4chenbereichen im Hinblick auf die Unterscheidbarkeit von Punkten von ihrer Umgebung. Das Verfahren erreicht eine Echtzeit-Geschwindigkeit in dem Sinne, dass f\u00fcr die Registrierung nicht mehr Zeit gebraucht wird, als ohnehin in der Praxis zwischen zwei Aufnahmen vergeht, etwa 18-80 Sekunden. Die Vollst\u00e4ndigkeit von 3-d-Fl\u00e4chenbeschreibungen ist bei manchen Anwendungen erst dann gegeben, wenn auch die Farbgebung der Oberfl\u00e4che erfasst wurde. Eine weitere Teilaufgabe ist es daher, die dazu in Form von Farbbildern aufgenommene Information in das Fl\u00e4chenmodell zu integrieren. Es wird ein Verfahren beschrieben, mit dem die beleuchtungsunabh\u00e4ngige Farbe aus diesen Bildern extrahiert und die Farbtexturen mehrerer Einzelansichten miteinander und mit der Geometrie-Information fusioniert wird. Die Verfahren wurden so implementiert, dass sie auf Dreiecksnetzen arbeiten. Dadurch sind sie auf allgemeinen Fl\u00e4chentopologien einsetzbar - nicht nur auf Einzelansichten, sondern auch auf Fl\u00e4chenmodellen, in denen bereits mehrere Einzelansichten integriert sind. \u00dcberpr\u00fcft wurden die entwickelten Methoden durch Messungen an Kunstobjekten und im medizinischen Umfeld an Patientengesichtern. Die hier erarbeiteten Methoden haben eine starke Verbindung auch zur Informatik, die sich zunehmend als Modellbildungs-Wissenschaft versteht. Es werden daher mehrfach Bez\u00fcge zu einer allgemeinen Modelltheorie hergestellt.\nThe intention of the present thesis is to develop a methodology for automatic and complete reconstruction of surface models from optical 3D data in real time. Approaches already available in the research group are applied and new methods are contributed to the overall process. The most important subtask is the initial alignment (coarse registration) of different 3D views. This process has been automatized by assigning surface points to each other, exploiting features of their adjacencies and the geometric constellations of the points. The method is based on the selection of \"`salient\"' points on free-form surfaces. The salience of points is quantified on the foundation of symmetry considerations and on a general information theory. The salience measure obtained in this way corresponds to a measure of the content of pragmatic information of surface regions with respect to the distinguishability of points from their neighborhood. The approach achieves real time speed, i.\\,e.\\ registration does not take more time than the usual delay between two acquisitions, about 18-80 seconds. With some applications, completeness of 3D surface descriptions is not provided until the coloring of the surface is acquired. Therefore, an additional subtask is to integrate information taken in the form of color images into the surface model. An approach is described to extract the illumination independent color from these images and to fuse color textures of multiple views with each other and with geometry information. The methods were implemented as to work with triangle meshes. This makes them applicable on general surface topologies - not just on single views but also on surface models in which already several views are integrated. Measurements on works of art and, in the medical field, on patients' faces were conducted to evaluate the developed methods.\n2006-09-04\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/320\nurn:nbn:de:bvb:29-opus-4313\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-4313\nhttps://opus4.kobv.de/opus4-fau/files/320/NikolausSchoenDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:375\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:H.1\nccs:H.3\nccs:H.4\nccs:H.5\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nAufbau einer Wissenskomponente f\u00fcr das aspektorientierte Prozessmanagement\nConstruction of a Knowledge Management Component for the Aspect-Oriented Process Management\nMayer, Udo\nWissensmanagement\nSoftwarearchitektur\nProzessmanagement\nSemantisches Netz\nWissensrepr\u00e4sentation\nOntologie <Wissensverarbeitung>\nMetamodell\nddc:004\nVor dem Hintergrund stetiger technischer Weiterentwicklung und fortschreitender Vernetzung auf allen Ebenen l\u00e4sst sich in den Industrienationen w\u00e4hrend der vergangenen Jahre ein Wandel hin zu einer neuen Wirtschaftsstruktur beobachten. Sie l\u00e4sst sich anhand dreier Charakteristika beschreiben: Dienstleistungsorientierung, Digitalisierung und Globalisierung. Im Vordergrund stehen spezialisierte und hoch entwickelte Dienstleistungen. Daraus erwachsen zwei Herausforderungen. Auf Grund des hohen Grads an Spezialisierung sind die Dienstleistungen kostenintensiv. Eine ineffiziente Verwaltung der notwendigen Vorg\u00e4nge f\u00fchrt zu Mehrkosten und dadurch zu Wettbewerbsnachteilen. Um diesem Problem zu begegnen, wurden Techniken zur Kontrolle und Verwaltung von Prozessen entwickelt, etwa das Workflow-Management. Des Weiteren m\u00fcssen die Mitarbeiter eines Unternehmens einem gehobenen Wissensanspruch gen\u00fcgen, um ausgefeilte und fortgeschrittene Dienstleistungen anbieten zu k\u00f6nnen. Wird Wissen ineffizient verwaltet, besteht nicht nur das Risiko einer Behinderung von Innovation, sondern es kann in Folge von Personalfluktuationen zu einem Wissensverlust kommen, welcher dazu f\u00fchrt, dass die Dienstleistung nicht mehr angeboten werden kann. Um dem entgegenzuwirken, wurden Methoden zum Wissensmanagement entwickelt. Um sich im globalen Wettbewerb zu behaupten, muss ein Unternehmen sowohl Workflow-Management als auch Wissensmanagement betreiben. Die beiden Konzepte wurden in bisherigen Ans\u00e4tzen oft separat verstanden. Diese Arbeit untersucht, wie sich die beiden Konzepte kombinieren lassen und welche Konsequenzen sich daraus ergeben. Dazu wird die aspektorientierte Prozessmodellierung des Workflow-Managements um einen wissensbezogenen Aspekt erweitert, welcher die Anforderungen der Wissensarbeit ber\u00fccksichtigt. In der Konsequenz wird eine Wissensinfrastruktur gefordert, deren Anforderungen und Charakteristika entwickelt werden. Weiterhin wird untersucht, welche Techniken des Wissensmanagements sich f\u00fcr die Organisation innerhalb der Wissensinfrastruktur eignen. Als Konsequenz werden Metamodelle entwickelt, welche unter der Verwendung von Konzepten des Semantic Web eine formale, strukturierte und nachvollziehbare Organisation des Wissens erlauben. Da sich dieses theoretische Modell nur schwer umsetzen l\u00e4sst, wird ein vereinfachtes Modell f\u00fcr das Wissensmanagement im Hinblick auf die Verwendung bei der Prozessverwaltung entworfen. Schlie\u00dflich wird mit SKM ein Prototyp vorgestellt, welcher das Konzept einer Wissensinfrastruktur f\u00fcr das prozessorientierte Wissensmanagement umsetzt.\nInfluenced by continuous technical enhancements and progressive extension of networking in all ranks a change towards a new economical structure can be recognized in the industrial nations over the past few years. Three main characteristics delineate this new strategy: Digitalization, globalization and service orientation. Key element of this new strategy is the provision of highly specialized and sophisticated services. Two challenges arise from this scenario. Because of the high degree of specialization the provision of services is cost intensive. Inefficient management of the necessary processes therefore results in additional costs which pose competitive disadvantages. To face this challenge process control and management technologies like Workflow Management have been developed. Furthermore, employees have to meet an increased demand for knowledge to retain their capability to offer elaborate and advanced services. Inefficient management of knowledge not only leads to obstruction of innovation, but as a result of employee turnover the risk of knowledge drain arises. This in turn can lead to incapacity of offering the service. Methods for Knowledge Management have been developed to antagonize this issue. To bear up against global competition companies have to exercise Workflow Management as well as Knowledge Management. Up to now the two tasks have been approached separately. This thesis examines how they can be combined and what consequences result from such a holistic approach. To do so, the aspect oriented process modeling paradigm is extended by a knowledge related aspect which factors in the requirements of knowledge work. Consequently, the development of a Knowledge Infrastructure is demanded and its requirements and characteristics are developed. Moreover, existing approaches for Knowledge Management are investigated in how applicative they are for organizing the internals of the Knowledge Infrastructure. As a result of this investigation metamodels are developed. They allow for a formal, structured and comprehensible arrangement of knowledge using concepts of the Semantic Web. The contrived theoretical concept is hard to put in practice, therefore a simplified model for Knowledge Management in the context of usage with Process Management is developed. Finally, the prototype SKM is presented. It resembles an experimental implementation of the concepts developed in this thesis.\n2007-01-08\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/375\nurn:nbn:de:bvb:29-opus-4980\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-4980\nhttps://opus4.kobv.de/opus4-fau/files/375/UdoMayerDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:612\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:J.2\nccs:I.3.5\nccs:I.3.7\nccs:I.6.3\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nInteractive Visualization in Interdisciplinary Applications\nInteraktive Visualisierungstechniken in interdisziplin\u00e4ren Anwendungsbereichen\nMeister, Martin\nVisualisierung\nddc:004\nVisualization is any technique used to create meaningful and intuitive images to communicate information. Real-time 3D visualization techniques are commonly employed in engineering to support the interpretation of large amount of complex data and to gain a deeper understanding of the underlying processes. In this work applications and research is presented in which new insights have been gained in disciplines with little engineering background. Problems and their solutions are presented for several disciplines: in archaeology we inspect as well as interpret unique artefacts non-destructively, making an inside view of ancient pottery visible. Using computed tomography scans we offer methods for the illustration of complex fragmented objects and find new interpretations for the manufacturing process. Also in archaeology, we apply our techniques to the visualization of architectural reconstructions. Several examples are given that show how ancient structures can be reconstructed and then rendered in real-time. In palaeontology oceanology we propose and apply computer graphics methods for the inspection and analysis of an endangered species of deep-sea corals. We describe the morphology of the corals and develop a mathematical model that is used to grow and evaluate artificial virtual reefs in order to evaluate their reef building and biodiversity potential. Finally, in biology/geology we use high-resolution terrain data to help calculate maps of sun radiation. Our results have been used for the creation of charts of potential natural vegetation in a project for the Federal Agency of Nature Conservation.\nInteraktive Visualisierungstechniken werden in erster LInie f\u00fcr das Verst\u00e4ndnis von Daten angewendet, die aus dem Bereich der Ingenieurswissenschaften stammen. In Rahmen dieser Arbeit wird gezeigt, dass dieselben Methoden auch einen wesentlichen Beitrag in Disziplinen leisten k\u00f6nnen, die wenig ingenieurstechnischen Hintergrund haben. Probleme und ihre L\u00f6sungen werden f\u00fcr mehrere Disziplinen pr\u00e4sentiert: in der Arch\u00e4logie werden antike Artefakte mit zerst\u00f6rungsfreien Methoden untersucht und neu interpretiert. Mit dem Einsatz von Computertomographie wird es m\u00f6glich, einen Blick in das Innere geschlossener antiker Keramik zu werfen und den restaurierten, komplex fragmentierten Erhaltungszustand aussagekr\u00e4ftig abzubilden und neue Hinweise \u00fcber den Herstellungsproze\u00df zu erhalten. Ebenso in der Arch\u00e4ologie werden Visualisierungstechniken vorgestellt, um Rekonstruktionen antiker Bauwerke durchzuf\u00fchren und darzustellen. Dabei werden geeignete Softwarel\u00f6sungen aufgezeigt, mit denen diese Strukturen rekonstruiert und in Echtzeit dargestellt werden k\u00f6nnen sowie mehrere Beispiele vorgef\u00fchrt. In der Pal\u00e4ontologie und Ozeanologie werden Vorgehensweisen vorgeschlagen, die zur Analyse einer vom Aussterben bedrohten Tiefseekorallenart n\u00fctzlich sind. Zuerst werden mit Computertomografieaufnahmen Korallen erfasst und ihre Morphologie beschrieben. Danach wird ein mathematisches Modell entwickelt, das benutzt wird, um k\u00fcnstliche virtuelle Riffe zu erschaffen und auszuwerten, um Fragen des Riffbildungspotentials und der Biodiversit\u00e4t zu beantworten. Schlussendlich wird eine Anwendung in den Geowisssenschaften pr\u00e4sentiert, in der hoch aufgel\u00f6sten Karten eines Terrainmodells von Bayern verwendet werden, um genaue Sonneneinstrahlungskarten auszurechnen unter Ber\u00fccksichtigung globaler Effekte. Diese Auswertungen helfen, Karten der potentiellen nat\u00fcrlichen Vegetation zu bestimmen, die in einem Projekt f\u00fcr das Bundesamt f\u00fcr Naturschutz angefertigt werden.\n2008-04-23\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/612\nurn:nbn:de:bvb:29-opus-9049\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-9049\nhttps://opus4.kobv.de/opus4-fau/files/612/Promotion_MartinMeister.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:621\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:C.4\nccs:C.2.2\nccs:D.2.8\nccs:D.4.8\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nMeasurement-Based Modeling of Distributed Systems\nMe\u00dfbasierte Modellierung verteilter Systeme\nHielscher, Kai-Steffen Jens\nMonitoring <Informatik>\nLeistungsmessung\nComputersimulation\nDiskrete Simulation\nCluster Server\nApache <Programm>\nGPS <Satellitengeod\u00e4sie>\nddc:004\nA methodology that integrates performance measurements and simulation of computer systems is demonstrated on a web cluster system. Main contributions include a new offline time synchronization system based on exponential Filtering, efficient timestamping using the cycle counter of modern CPUs for software monitoring, flexible instrumentation of Java application servers using aspect-oriented programming, a new input modeling approach for autocorrelated data and a detailed simulation model that includes both operating system and TCP network aspects.\nAnhand eines clusterbasierten Webservers wird eine Methodik vorgestellt, die Messungen und Simulation der Leistung von Rechnersystemen integriert. Wichtige Beitr\u00e4ge sind hierbei ein neues Verfahren zur Offline-Zeitsynchronisation, das auf exponentiellem Filtern basiert, effiziente Generierung von Zeitstempeln mittels der Zyklenz\u00e4hler moderner Prozessoren f\u00fcr das Software-Monitoring, flexible Instrumentierung von Java-Applikationsservern mittels aspektorientierter Programmierung, ein neues Verfahren zur Eingabemodellierung, das die Repr\u00e4sentation autokorrelierter Daten erlaubt, sowie ein detailliertes Simulationsmodell, das sowohl Betriebssystem- wie auch Netzwerk-Aspekte von TCP enth\u00e4lt.\n2008-05-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/621\nurn:nbn:de:bvb:29-opus-9149\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-9149\nhttps://opus4.kobv.de/opus4-fau/files/621/KaiSteffenHielscherDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:639\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:D.3\nccs:I.6\nccs:C.2.2\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nZur konsistenz-basierten Validation von Protokollen mit zeitlichen Anforderungen\nTowards consistency-based validation of protocols with timing requirements\nKresic, Dario\nKommunikationsprotokoll\nAutomat <Automatentheorie>\nVerifikation\nUnifikationsgrammatik\nddc:004\nModell-basierte Testverfahren werden im Paradigma der Constraint-Programmierung angesehen. Demnach wird eine Spezifikation (in Form eines Zeitautomaten oder einer Komposition von Zeitautomaten) zun\u00e4chst als ein Constraint-Programm kodiert. Durch die symbolische Ausf\u00fchrung des Programms werden Testdaten erzeugt, die als Eingaben f\u00fcr Testf\u00e4lle dienen. Auf dieser Basis kann das Modell dann ggf. korrigiert bzw. weiter verfeinert werden. Um Berechnungen effizient durchf\u00fchren zu k\u00f6nnen, werden sog. Konsistenzverfahren eingesetzt, wodurch potentiell weniger Klauseln ausgew\u00e4hlt, der Suchaufwand minimiert und somit die Effizienz gesteigert werden kann. Die im Zusammenhang mit den Konsistenzverfahren zu definierenden Propagierungsmechanismen wurden im Rahmen einer prototypischen Architektur implementiert; zur praktischen Demonstration wurde ein Szenario aus dem Bereich der interaktiven Multimediakommunikation herangezogen.\nModel-based testing is seen in the paradigm of the constraint programming. Thus, a specification (in form of a timed automaton) is coded as a constraint program. By symbolic execution of such a program test data are generated which can be used as inputs for test cases. On this basis a model can be adjusted and improved. In order to make the computing process more efficient, consistency techniques are introduced which help to choose less clauses, to minimize the search costs and hence to increase the efficiency. In connection with the consistency techniques the propagation mechanisms to be defined are implemented in a prototypical framework; for demonstration purposes a scenario from the area of the interactive multimedia communication has been taken.\n2008-06-12\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/639\nurn:nbn:de:bvb:29-opus-9368\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-9368\nhttps://opus4.kobv.de/opus4-fau/files/639/DissGesamt.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1466\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:C.2\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nAdaptive Beaconing for Delay-Sensitive and Congestion-Aware Traffic Information Systems\nSommer, Christoph\nGerman, Reinhard\nDressler, Falko\nVANET\nddc:004\nWe present a new car-to-X communication protocol, Adaptive Traffic Beacon (ATB), which supports the exchange of delay-sensitive traffic information in a wide range of scenarios by flexibly adapting to the availability of infrastructure elements as well as to the network load. From previous work, we see that centralized solutions and flooding based approaches each show benefits and drawbacks depending on traffic density, penetration, network utilization, and other parameters. This observation is in line with findings about intelligent transportation systems that have been developed for specific settings. In order to overcome this limitation, we designed ATB to be adaptive in two dimensions: First, the beacon interval is adapted dynamically and, secondly, the protocol can dynamically make use of available infrastructure elements. We concentrate on a Traffic Information System (TIS) with a focus on congestion-aware communication. Simulation experiments clearly demonstrate that ATB performs well in a broad range of settings. It maintains a non-congested wireless channel to prevent collisions during TIS data exchange.\n2010-11-18\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1466\nurn:nbn:de:bvb:29-opus-21261\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-21261\nhttps://opus4.kobv.de/opus4-fau/files/1466/atb.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1620\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:C.2.1\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nSpezifikation und Implementation des CAN-Arbitrierungsverfahrens in UPPAAL\nSpecification and Implementation of CAN Arbitration in UPPAAL\nKresic, Dario\nHielscher, Kai-Steffen\nGerman, Reinhard\nModel Checking\nKommunikationssystem\nLeistungsbewertung\nddc:004\nIn dieser Arbeit stellen wir eine durch Zeitautomaten modellierte Spezifikation des Mediumzugriffs im CAN-Protokoll vor sowie ihre Implementierung in UPPAAL. Zeitliche Anforderungen wurden dabei durch entsprechende Uhren-Nebenbedingungen erfasst. Dieses Zeitautomaten-Modell wurde anschlie\u00dfend automatisch verifiziert (Model Checking), wobei mehrere Anforderungen identifiziert wurden, die das CAN Protokoll erf\u00fcllen mu\u00df (wie Deadlock-Freiheit des Modells, \u00dcbertragungsrecht f\u00fcr die h\u00f6chstpriore Nachricht, exklusives \u00dcbertragungsrecht f\u00fcr beliebigen CAN-Adapter nach der gewonnenen Arbitrage etc.). All diese Eigenschaften wurden in einer Variante der temporalen Logik CTL spezifiziert; die automatische Verifikation selbst wurde dabei mit UPPAAL durchgef\u00fchrt, einem Model Checker f\u00fcr zeitautomaten-basierte Modelle.\n2011-02-18\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1620\nurn:nbn:de:bvb:29-opus-23535\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-23535\nhttps://opus4.kobv.de/opus4-fau/files/1620/Kresic_IB_Opus.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:1690\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:K.3.2\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nTreating Memory Management and Filesystems as One Topic\nE\u00dfer, Hans-Georg\nBetriebssystem\nSpeicherverwaltung\nDateisystem\nDidaktik\nddc:004\nSpeicherverwaltung und Dateisysteme im Rahmen einer Betriebssysteme-Vorlesung gemeinsam zu behandeln, statt sie als separate Themen vorzustellen, kann das Verst\u00e4ndnis der Studierenden erh\u00f6hen und auch die Klausurergebnisse verbessern. In einer Befragung \u00e4u\u00dferten sie zudem, dass ihnen diese Methode gefiel. Dieser Artikel beschreibt Ver\u00e4nderungen eines klassischen Betriebssysteme-Kurses und die Resultate, die sich aus dem Vergleich der Klausurergebnisse und der Umfrageauswertung ergaben.\nTeaching memory management aligned with filesystems in an Operating Systems course instead of treating them as separate topics can increase students\u2019 understanding and improve their grades in end-of-term examinations. In a survey they also state that they like this method. This article describes modifications made to a classic course and the results gained through exam and survey evaluations.\n2011-04-11\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/1690\nurn:nbn:de:bvb:29-opus-25079\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-25079\nhttps://opus4.kobv.de/opus4-fau/files/1690/tr_cs_2011_04.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2095\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:H.3.3\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nAuthormagic: A Concept for Author Disambiguation in Large-Scale Digital Libraries\nAuthormagic: Ein Konzept zur Autorenidentifikation in Gro\u00dfen Digitalen Bibliotheken\nWeiler, Henning\nInformation Retrieval\nWissensmanagement\nRelevanz-Feedback\nddc:004\nAuthor name ambiguities distort the quality of information discovery in digital libraries. These ambiguities also contribute to the inaccurate attribution of authorship to individual researchers. The latter is especially delicate in research evaluation. To solve this issue, many algorithmic bulk disambiguation approaches have been proposed in the literature. However, no algorithmic approach can solve author ambiguities with an accuracy of 100%. Some online projects allow users to manually create publication lists, which are then regarded as profiles of the researchers. The tedious work to manually assemble such publication lists and the unavailability of scientific material in these projects limit the success of these projects. The \u201cAuthormagic\u201d concept is developed in this thesis to address the author ambiguity issue with a hybrid approach of combining algorithmic and human intelligence. A customized agglomerative clustering approach first determines publication clusters by comparing available metadata. These clusters ideally represent publication profiles of authors. Users of the digital library can then use an interface to make decisions about the correctness of the algorithmic attributions. Every (operator-approved) decision feeds back into the algorithm to increase the overall matching quality in consecutive runs of the algorithm. The concept also targets the need for sustainable disambiguation solutions that are capable of rapidly updating information in an ever-growing publication landscape. Dedicated online processes incrementally update the cluster information, while an offline process continuously re-clusters information. All processes are constrained by unquestionable and invariable user decisions. The Authormagic concept is shown on the example of INSPIRE, a hand-curated database containing the literature corpus of the entire field of High-Energy Physics (HEP). The metadata in INSPIRE is a great basis for the algorithmic part, while a data-quality-cautious community drives the crowd-sourced intelligence acquisition. The algorithm results are evaluated in comparison to the decisions of users. The evaluation results show that the algorithmic approach is an improvement over non-disambiguated searches. The created author profiles contain more accurate publication and bibliometric statistics than before the disambiguation. Overall can be stated that the concept of combining algorithmic and human intelligence can lead to 100% correct author information, if all researchers participate in the decision-making process. The identified requirements for the Authormagic to be successfully implemented in a digital library are: 1) qualitative and complete metadata and 2) a participating community. The reached data quality in combination with the proposed sustainability strategy makes way for novel author-centric services and meaningful bibliometrics.\nMehrdeutigkeiten von Autorennamen verf\u00e4lschen oftmals die Resultate von Suchanfragen in digitalen Bibliotheken. Zus\u00e4tzlich erschweren diese Mehrdeutigkeiten die korrekte Zuordnung von wissenschaftlichen Dokumenten zu den auf den Dokumenten genannten Erstellern. Letzteres ist besonders problematisch in der Bewertung von Forschung und Wissenschaft. In der Literatur wurden einige Ans\u00e4tze vorgestellt, die dieses Problem algorithmisch l\u00f6sen sollen. Jedoch ist kein rein-algorithmischer Ansatz in der Lage, diese Mehrdeutigkeiten zu 100% zu l\u00f6sen. Einzelne Projekte bieten Internetnutzern eine M\u00f6glichkeit zur Erstellung von Publikationslisten. Eine manuelle Erstellung ist allerdings zum einen sehr zeitaufwendig und zum anderen enthalten die Datenbanken dieser Projekte potentiell nur einen Bruchteil der Publikationen eines Autors. Das \u201eAuthormagic\u201c Konzept wurde in der vorliegenden Arbeit entwickelt, um das Problem der Mehrdeutigkeiten mit einem Hybridansatz zu l\u00f6sen. Dieser Ansatz kombiniert algorithmische und menschliche Intelligenz. Zun\u00e4chst wird ein hierarchischer Algorithmus zur Bildung von Publikationslisten verwendet. Die so gebildeten Listen sind idealerweise die Listen von real-existierenden Autoren. Nutzer der digitalen Bibliothek k\u00f6nnen dann Entscheidungen \u00fcber die Korrektheit von algorithmischen Zuordnungen treffen. Diese Entscheidungen werden von den Kuratoren der digitalen Bibliothek gepr\u00fcft und freigegeben. Jede freigegebene Entscheidung tr\u00e4gt beim n\u00e4chsten algorithmischen Prozess zur Entscheidungsfindung bei. Verschiedene Prozesse, die den Datenbestand aktualisieren und inkrementell neu gruppieren, stellen die Nachhaltigkeit von dem Konzept sicher. Das Authormagic Konzept wurde im Rahmen von INSPIRE entwickelt. INSPIRE ist eine digitale Bibliothek mit einer sehr gut erschlossenen Datenbank mit Metadaten von Dokumenten aus dem Bereich der Hochenergiephysik (HEP). Neben den Metadaten tr\u00e4gt die datenqualit\u00e4tsbewusste HEP Nutzergesellschaft zur erfolgreichen Umsetzung des Konzepts in INSPIRE bei. Die Qualit\u00e4t des algorithmischen Teils konnte anhand von Nutzerentscheidungen in INSPIRE evaluiert werden. Die Resultate der Evaluierung zeigen, dass Ergebnisse von autor-spezifischen Suchanfragen durch die Aufl\u00f6sung von Mehrdeutigkeiten deutlich pr\u00e4ziser wurden. Generell kann festgestellt werden, dass eine vollkommene Datenqualit\u00e4t erreicht werden kann, wenn sich alle Individuen am Prozess der Entscheidungsfindung beteiligen. Die identifizierten Voraussetzungen f\u00fcr eine erfolgreiche Umsetzung des Konzepts in einer digitalen Bibliothek sind: 1) qualitative und vollst\u00e4ndige Metadaten und 2) eine Nutzergemeinschaft, die sich am Entscheidungsfindungsprozess beteiligt. Die Aufl\u00f6sung von Mehrdeutigkeiten bereitet den Weg f\u00fcr neuartige und autoren-zentrische Dienste sowie aussagekr\u00e4ftige bibliometrische Analysen.\n2012-03-05\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2095\nurn:nbn:de:bvb:29-opus-31399\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-31399\nhttps://opus4.kobv.de/opus4-fau/files/2095/original_HenningWeilerDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/2095/HenningWeilerDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2149\n2018-11-12\ndoc-type:workingPaper\nbibliography:false\nddc\nddc:004\nccs\nccs:J.3\nccs:C.2.4\nccs:D.2.11\nccs:D.2.13\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nDEUS: Distributed Electronic Patient File Update System\nNeumann, Christoph P.\nRampp, Florian\nLenz, Richard\nElektronische Patientenakte\nKrankenunterlagen\nPublish-Subscribe-System\nSystemintegration\nDokumentenverwaltungssystem\nddc:004\nInadequate availability of patient information is a major cause for medical errors and affects costs in healthcare. Traditional approaches to information integration in healthcare do not solve the problem. Applying a document-oriented paradigm to systems integration enables inter-institutional information exchange in healthcare. The goal of the proposed architecture is to provide information exchange between strict autonomous healthcare institutions, bridging the gap between primary and secondary care. In a long-term healthcare data distribution scenario, the patient has to maintain sovereignty over any personal health information. Thus, the traditional publish-subscribe architecture is extended by a phase of human mediation within the data flow. DEUS essentially decouples the roles of information author and information publisher into distinct actors, resulting in a triangular data flow. The interaction scenario will be motivated. The significance of human mediation will be discussed. DEUS provides a carefully distinguished actor and role model for mediated pub-sub. The data flow between the participants is factored into distinct phases of information interchange. The artefact model is decomposed into role-dependent constituent parts. Both a domain specific (healthcare) terminology and a generic terminology is provided. From a technical perspective, the system design is presented. The sublayer for network transfer will be highlighted as well as the subsystem for human-machine interaction.\n2012-03-28\nworkingpaper\ndoc-type:workingPaper\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2149\nurn:nbn:de:bvb:29-opus-32089\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-32089\nhttps://opus4.kobv.de/opus4-fau/files/2149/FAU_CS_TR_deus_tex.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2164\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:J.2\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nRobust Single-Shot Structured Light 3D Scanning\nRobuste 3D-Vermessung mit strukturierter Beleuchtung in Einzelbildern\nSchmalz, Christoph\nDreidimensionale Rekonstruktion\n3D-Scanner\nMaschinelles Sehen\nKalibrieren <Messtechnik>\nEndoskopie\nddc:004\nIn this thesis a new robust approach for Single-Shot Structured Light 3D scanning is developed. As the name implies, this measurement principle requires only one image of an object, illuminated with a suitable pattern, to reconstruct the shape and distance of the object. This technique has several advantages. It can be used to record 3D video with a moving sensor or of a moving scene. Since the required hardware is very simple, the sensor can also be easily miniaturized. Single-Shot Structured Light, thus, has the potential to be the basis of a versatile and inexpensive 3D scanner. One focus of the work is the robustness of the method. Existing approaches are mostly limited to simple scenes, that is, smooth surfaces with neutral color and no external light. In contrast, the proposed method can work with almost any close-range scene and produces reliable range images even for very low-quality input images. An important consideration in this respect is the design of the illumination pattern. We show how suitable color stripe patterns for different applications can be created. A major part of the robustness is also due to the graph-based decoding algorithm for the pattern images. This has several reasons. Firstly, any color assessments are based on ensembles of pixels instead of single pixels. Secondly, disruptions in the observed pattern can be sidestepped by finding alternative paths in the graph. Thirdly, the graph makes it possible to apply inference techniques to get better approximations of the projected colors from the observed colors. For a typical camera resolution of 780x580, the whole decoding and reconstruction algorithm runs at 25Hz on current hardware and generates up to 50000 3D points per frame. The accuracy of the recovered range data is another important aspect. We implemented a new calibration method for cameras and projectors, which is based on active targets. The calibration accuracy was evaluated using the reprojection error for single camera calibrations as well as the 3D reconstruction errors for complete scanner calibrations. The accuracy with active targets compares favorably to calibration results with classic targets. In a stereo triangulation test, the root-mean-square error could be reduced to a fifth. The accuracy of the combined Structured Light setup of camera and projector was also tested with simulated and real test scenes. For example, using a barbell-shaped reference object, its known length of 80.0057mm could be determined with a mean absolute error of 42\u00b5m and a standard deviation of 74\u00b5m. The runtime performance, the robustness and the accuracy of the proposed approach are very competitive in comparison with previously published methods. Finally, endoscopic 3D scanning is a showcase application that is hard to replicate without Single-Shot Structured Light. Building on a miniature sensor head designed by Siemens, we developed calibration algorithms and apply the graph-based pattern decoding to generate high-quality 3D cavity reconstructions.\nIn dieser Arbeit wird ein neues robustes Verfahren zur 3D-Vermessung durch Strukturierte Beleuchtung in Einzelbildern entwickelt. Dieses Messprinzip ben\u00f6tigt nur ein einzige Aufnahme eines mit einem geeigneten Muster beleuchteten Objekts, um dessen Form und Abstand zu rekonstruieren. Diese Technik hat mehrere Vorteile. Sie kann benutzt werden, um 3D-Videos einer bewegten Szene oder mit einem bewegten Sensor aufzunehmen. Da sein Aufbau sehr einfach ist, ist der Sensor auch gut zur Miniaturisierung geeignet. Strukturierte Beleuchtung in Einzelbildern hat daher das Potential, als Grundlage f\u00fcr vielseitige und g\u00fcnstige 3D-Abtaster zu dienen. Ein Schwerpunkt der Arbeit ist die Robustheit der Messmethode. Existierende Ans\u00e4tze sind meistens auf einfache Szenen beschr\u00e4nkt, das bedeutet glatte Oberfl\u00e4chen in neutralen Farben und kein Fremdlicht. Im Gegensatz dazu kann die vorgeschlagene Methode mit fast jeder Szene im Nahbereich umgehen und zuverl\u00e4ssige Tiefenkarten auch aus Eingangsbildern mit sehr niedriger Qualit\u00e4t erzeugen. Eine wichtige \u00dcberlegung ist in dieser Hinsicht die Gestaltung des Beleuchtungsmusters. Wir zeigen, wie geeignete Farbstreifenmuster f\u00fcr verschiedene Anwendungen erzeugt werden k\u00f6nnen. Ein Gro\u00dfteil der Robustheit beruht auch auf dem graphenbasierten Dekodierungsalgorithmus f\u00fcr die Aufnahmen des Muster. Das hat mehrere Gr\u00fcnde. Erstens werden alle Farbeinsch\u00e4tzungen anhand von Gruppen von Pixeln anstatt Einzelpixeln vorgenommen. Zweitens k\u00f6nnen St\u00f6rungen im beobachteten Muster umgangen werden, indem alternative Pfade im Graphen gefunden werden. Drittens erlaubt es der Graph, Folgerungstechniken anzuwenden, um bessere N\u00e4herungen f\u00fcr die projizierten Farben aus den beobachteten Farben zu erhalten. Mit einer \u00fcblichen Kameraaufl\u00f6sung von 780580l\u00e4uft der gesamte Algorithmus zur Dekodierung und Rekonstruktion mit 25Hz und erzeugt bis zu 50000 3D-Punkte pro Bild. Die Genauigkeit der gewonnenen 3D-Daten ist ein weiterer wichtiger Aspekt. Wir implementierten eine neue Kalibriermethode f\u00fcr Kameras und Projektoren, die auf aktiven Targets basiert. Die Kalibriergenauigkeit wurde sowohl anhand des R\u00fcckprojektionsfehlers f\u00fcr Einzelkamerakalibrierungen, als auch anhand des 3D-Rekonstruktionsfehlers f\u00fcr vollst\u00e4ndige Systemkalibrierungen ermittelt. Mit aktiven Targets wird eine h\u00f6here Genauigkeit als mit klassischen Targets erreicht. Bei einem Test durch Triangulation mit zwei Kameras konnte der mittlere quadratische Fehler auf ein F\u00fcnftel reduziert werden. Die Genauigkeit des Aufbaus zur Strukturierten Beleuchtung aus Kamera und Projektor wurde ebenfalls ausgewertet. Die bekannte L\u00e4nge eines hantelf\u00f6rmigen Referenzobjekts von 80.0057mm konnte mit einem mittleren Fehler von 42\u00b5m und einer Standardabweichung von 74\u00b5m bestimmt werden. Die Rechenzeit, die Robustheit und die Genauigkeit der vorgeschlagenen Messmethode sind im Vergleich mit bisherigen Ans\u00e4tzen sehr konkurrenzf\u00e4hig. Eine Vorzeigeanwendung ist die endoskopische 3D-Abtastung, die ohne die Technik der Strukturierten Beleuchtung in Einzelbildern schwer umzusetzen ist. Aufbauend auf einem von Siemens entworfenen Miniatur-Sensorkopf entwickelten wir Kalibrierverfahren und wenden die graphenbasierte Musterdekodierung an, um hochqualitative 3D-Modelle von Hohlr\u00e4umen zu erzeugen.\n2012-04-18\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2164\nurn:nbn:de:bvb:29-opus-32173\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-32173\nhttps://opus4.kobv.de/opus4-fau/files/2164/original_ChristophSchmalzDissertation.zip\nhttps://opus4.kobv.de/opus4-fau/files/2164/ChristophSchmalzDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2214\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:I.3.8\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nEinsatzm\u00f6glichkeiten von virtuellen, nichtidealen Prototypen in der Toleranzsynthese und -analyse\nApplications of virtual, nonideal prototypes in tolerance synthesis and analysis\nStoll, Tobias\nVisualisierung\nPrototyp\nddc:620\nJedes hergestellte Produkt weist Abweichungen von der im CAD-System festgelegten Idealgestalt auf. Diese entstehen durch ungenaue Fertigungsverfahren, z.B. aufgrund von Werkzeugverschlei\u00df. Auch bei der Montage entstehen Abweichungen, beispielsweise wenn die Heckklappe eines Automobils manuell ausgerichtet und verschraubt wird. Durch genauere Verfahren k\u00f6nnen die resultierenden Abweichungen reduziert werden, dies erfordert jedoch in den meisten F\u00e4llen aufw\u00e4ndige und damit teurere Fertigungs- und Montageverfahren. Aus diesem Grund ist es n\u00f6tig, einen Kompromiss zwischen der Qualit\u00e4t und den Kosten des Produktes einzugehen. Je nach herzustellendem Produkt, Firmenpolitik, Kundenw\u00fcnschen, zur Verf\u00fcgung stehendem Budget und vorhandenem Maschinenpark wird dieser Kompromiss unterschiedlich ausfallen. Um die maximal zul\u00e4ssigen Abweichungen einzuschr\u00e4nken, werden von Produktentwicklern Toleranzen vergeben. Diese werden w\u00e4hrend der Entwurfs- und Ausarbeitungsphase definiert. Die Berechnung und Beurteilung der Auswirkungen der Toleranzen auf f\u00fcr die Erf\u00fcllung der Produktfunktion wichtige Ma\u00dfe ist komplex. Jedes einzelne Ma\u00df wird im Allgemeinen von mehreren Toleranzen beeinflusst und abh\u00e4ngig vom geometrischen Zusammenhang sind die Auswirkungen auf das Funktionsma\u00df unterschiedlich gro\u00df. Aus diesem Grund werden Toleranzanalyseprogramme verwendet, welche es erm\u00f6glichen, die Auswirkungen der vergebenen Toleranzen auf alle f\u00fcr die Funktionserf\u00fcllung relevanten Ma\u00dfe zu berechnen. Um die aufw\u00e4ndigen Berechnungen zu beschleunigen, werden vereinfachte Modellannahmen verwendet. Diese f\u00fchren dazu, dass die Modellerstellung nur durch Spezialisten m\u00f6glich ist. Dadurch wird der Entwicklungsprozess verz\u00f6gert, da die Produktentwickler auf die Ergebnisse des Toleranzanalysespezialisten warten m\u00fcssen. Bei komplexen Produkten kann die Erstellung eines Toleranzanalysemodells mehrere Tage in Anspruch nehmen. Die Interpretation der Ergebnisse ist schwierig, da keine Visualisierungen von Baugruppen erstellt werden k\u00f6nnen. Den Benutzern werden lediglich Berechnungsergebnisse, wie z.B. Verteilungskurven f\u00fcr jedes vorher festgelegte Funktionsma\u00df, pr\u00e4sentiert. In dieser Arbeit werden zwei M\u00f6glichkeiten vorgestellt, wie virtuelle Prototypen den Produktentwicklungsprozess verbessern k\u00f6nnen. Zum einen kann die optische Qualit\u00e4t von Produkten besser eingesch\u00e4tzt werden, zum anderen kann eine genauere Toleranzanalyse durchgef\u00fchrt werden.\nEvery manufactured product has deviations from the ideal shape defined in the CAD system. They result from imprecise production processes, e.g. because of tool wear. Deviations are also caused by manufacturing, for example if a back door of a car is manually aligned. By using more precise methods the resulting deviations can be reduced, but in most cases complex and costly production and manufacturing processes have to be used. Therefore the product developer has to compromise between quality and costs of a product. Depending on the product, company policy, customer\u2019s wishes, available budget and machinery this compromise will be different. To limit the maximum tolerable deviations tolerances are assigned by the product developer. They are defined early in the product development process. The calculation of the impact of tolerances on critical functions is very complex, since every single measurement is generally affected by many tolerances and is also depending on the geometrical context. To calculate the effects of tolerances on functional relevant measurements of complex products tolerance analysis tools are used. To ease the necessary calculations some simplifications are made in the model. They enable the computation in appropriate time, but much knowledge about tools is needed to get meaningful results. Therefore only specialists can use existing tolerance analysis tools. This has negative effects on the product development process, as the creation of a model can take a few days and the product developer has to wait for the results. Additionally the result can only be presented by calculating distribution curves or contributor reports, but no visualisations of resulting products can be made. Therefore tolerance synthesis is rather difficult. In this thesis two possibilities how virtual prototypes can enhance the tolerance allocation process in the early design phases are shown. On the one hand they can be used to assure the aesthetic quality of a product, on the other hand they can be utilized to perform a more precise tolerance analysis.\n2012-05-30\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/vnd.ms-excel\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2214\nurn:nbn:de:bvb:29-opus-32798\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-32798\nhttps://opus4.kobv.de/opus4-fau/files/2214/original_TobiasStollDissertation.doc\nhttps://opus4.kobv.de/opus4-fau/files/2214/TobiasStollDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2741\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:H.1.1\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nFormalisierung digitaler Spuren und ihre Einbettung in die Forensische Informatik\nFormalisation of Digital Evidence and its embedding in Forensic Computing\nDewald, Andreas\nComputerforensik\nFormalisierung\nddc:004\nDie forensische Informatik, die h\u00e4ufig auch als digitale Forensik oder Computerforensik bezeichnet wird, z\u00e4hlt heute zum Standardrepertoire forensischer Ermittlungsinstrumente und wird durch die regelm\u00e4\u00dfige Pr\u00e4senz in den Medien auch in der \u00d6ffentlichkeit immer st\u00e4rker wahrgenommen. Allerdings sind heutige Vorgehensweisen und Methoden der digitalen Forensik eher pragmatisch gepr\u00e4gt und ein Vergleich zu anderen forensischen Wissenschaften zeigt deutliche Defizite, welche sich in zwei Bereiche zusammenfassen lassen: 1. Eine mangelnde Orientierung an anderen forensischen Wissenschaften. 2. Einen Mangel an grundlegender Theorie. Die vorliegende Arbeit versucht beide Problembereiche zu adressieren und damit zur wissenschaftlichen Fundierung des Gebietes der forensischen Informatik beizutragen. Hierzu betrachtet diese Arbeit die Urspr\u00fcnge und Grundlagen forensischer Wissenschaften und zieht Parallelen zur digitalen Forensik. Es wird der Bezug der etablierten Praxis der digitalen Forensik zur Theorie der klassischen Forensik untersucht. Insbesondere \u00fcbertr\u00e4gt diese Arbeit eines der grundlegenden Prinzipien der Forensik - die Theorie vom Ursprung der Spuren von Inman und Rudin (2000) - von der physischen auf die digitale Welt. Dieser Theorie zufolge, bildet die Assoziation den Kern einer jeden forensischen Untersuchung. Daher f\u00fchrt diese Arbeit den Begriff der forensischen Informatik als denjenigen wissenschaftlichen Kernbereich der digitalen Forensik ein, deren Fokus auf der reinen Feststellung von Assoziationen durch Anwendung wissenschaftlicher Methoden der Informatik liegt. Um der Forderung nach theoretischer Fundierung nachzukommen, formalisiert diese Arbeit den Begriff der Spuren auf Basis eines abstrakten Modells. Diese Formalisierung gibt erstmals die M\u00f6glichkeit, genaue (formale) Aussagen dar\u00fcber zu treffen, was digitale Spuren sind und wie sie entstehen. Au\u00dferdem wird formal untersucht, welcher Gestalt die in der forensischen Informatik gel\u00f6sten Rekonstruktionsprobleme sind und welche Einflussfaktoren auf die L\u00f6sbarkeit von dieser Probleme - und damit der Feststellung von Assoziationen in der digitalen Welt - existieren. Diese Ergebnisse werden anhand praktischer Beispiele nachvollzogen und erl\u00e4utert.\nToday, digital forensics (also known as computer forensics) already is a common instrument in forensic investigations. It is due to its massive presence in the media that digital forensics more and more comes to the public mind, too. However, today's practices and methods are of a more pragmatic kind and a comparison with other forensic sciences shows clear deficits. Those deficits can be divided into two areas: 1. A lack of orientation towards other forensic sciences. 2. A lack of fundamental theory. This thesis tries to address both domains and thus to contribute to the scientific foundation of the area of digital forensics. For that purpose, this work examines the origin and fundamentals of forensic science in general and identifies parallels in digital forensics. The relationship between the established practices in digital forensics and the theory of classical forensic sciences is determined. Especially, one of the most fundamental theories in forensics - the theory of the origin of evidence by Inman and Rudin (2000) - is transferred from the physical world to the digital world. Following this theory, the establishment of associations form the core of every forensic investigation. That is why this thesis introduces the notion of forensic computing (ger.: forensische Informatik) as exactly that core area of digital forensics that has a focus on the pure establishment of associations by applying scientific methods of computer science. To comply with the demand for a theoretic foundation, this work formalizes the term of evidence on the basis of an abstract model. For the first time, this formalisation puts us in a position to make exact (formal) statements about what digital evidence is and about its origin. Further, the types of reconstruction problems that are answered by forensic computing is examined formally, as well as factors that are relevant to the solvability of those problems. The results of this formal considerations are then demonstrated and explained as a proof of concept in practical case studies.\n2012-12-07\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2741\nurn:nbn:de:bvb:29-opus-39439\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-39439\nhttps://opus4.kobv.de/opus4-fau/files/2741/original_QUELLELATEX.ZIP\nhttps://opus4.kobv.de/opus4-fau/files/2741/AndreasDewaldDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:2916\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:D.2\nccs:H.5\nccs:H.1.0\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med_ohneAngabe\nKonzeption und Aufbau eines Single-Source Tumordokumentationsablaufs am Comprehensive Cancer Center Erlangen-N\u00fcrnberg\nDesign and implementation of a single-source tumor documentation process at the Comprehensive Cancer Center Erlangen-N\u00fcrnberg\nRies, Markus\nTumorregister\nKrebsregister\nddc:610\nHintergrund und Ziele: B\u00f6sartige Neubildungen stellen mit 24,4 % die zweith\u00e4ufigste Todesursache in Deutschland dar. Da die Patientenversorgung durch eine lange Behandlungsdauer, sowie einen hohen Anteil interdisziplin\u00e4rer und transsektoraler Behandlungsprozesse gepr\u00e4gt wird, ist eine gut strukturierte Tumordokumentation essentiell. Neben der Patientenversorgung werden onkologische Daten auch f\u00fcr Qualit\u00e4tssicherungsverfahren, zur Krebsregistrierung und f\u00fcr Forschungsprojekte erhoben. Diese Szenarien sind bez\u00fcglich Dokumentationsinhalten und Dokumentationsverfahren nicht aufeinander abgestimmt, sondern vielmehr durch Redundanzen und Mehrfachdokumentation gekennzeichnet. Die vorliegende Arbeit entwickelt ein Konzept, in dem onkologische Da-ten einmalig, digital und strukturiert w\u00e4hrend des klinischen Prozesses erfasst werden. Anschlie\u00dfend werden diese Daten, dem Single-Source Ansatz folgend, in den Bereichen medizinisches Prozessmanagement, Qualit\u00e4tssicherung, Krebsregistrierung und Forschung weiterverwendet. Methoden: Die Entwicklung des Konzepts folgt einem dreistufigen Verfahren. Zuerst werden in einer Anforderungsanalyse, einzubeziehende Interessensgruppen und Entwicklungen der onkologischen Versorgungsstruktur durch eine Literaturreche und Interviews mit internen sowie externen onkologischen Experten ermittelt. Danach wird auf dieser Basis ein L\u00f6sungsmodell entwickelt. Hierzu werden, unter Ber\u00fccksichtigung datenschutzrechtlicher Aspekte, existierende onkologische Datens\u00e4tze bez\u00fcglich ihrer Eignung f\u00fcr eine klinisch integrierte Single-Source Tumordokumentation analysiert. Zudem werden bestehende klinische IT-Systeme und Kommunikationsstandards zu einer IT-Architektur zusammengestellt. Abschlie\u00dfend erfolgt die Validierung dieses Modells innerhalb von 22 Projekten am Comprehensive Cancer Center Erlangen-N\u00fcrnberg (CCC EN). Die Umsetzbarkeit des Modells wird anhand konkreter Fallbeispiele belegt. Ergebnisse und Beobachtungen: Die Entwicklung einer Single-Source Tumordokumentation ist ein umfangreiches Projekt, welches die Zusammenarbeit von \u00e4rztlichem und pflegerischem Personal, sowie den Mitarbeitern des Krebsregisters, der IT-Abteilung und des Qualit\u00e4tsmanagements erfordert. Innerhalb der onkologischen Versorgung in Deutschland ist es notwendig, die Qualit\u00e4t der medizinischen Versorgung mit Daten und Fakten zu belegen, interdisziplin\u00e4re sowie transsektorale Zusammenarbeit zu intensivieren und translationale Forschungsprojekte voranzutreiben. Um diese Anforderungen abzudecken und die Weiterverwendung der Daten der Patientenversorgung f\u00fcr die Bereiche medizinisches Prozessmanagement, Qualit\u00e4tssicherung, Krebsregistrierung und Forschung zu erm\u00f6glichen, muss die Tumordokumentation digital und innerhalb des klinischen Behandlungsprozesses erfolgen. Hierf\u00fcr m\u00fcssen bestehende Datens\u00e4tze der Registerdokumentation in klinische Dokumentationspakete aufgesplittet und bestehenden IT-Systemen zugeordnet werden. Durch die Entwicklung einer entsprechenden IT-Architektur k\u00f6nnen diese Daten an-schlie\u00dfend ausgetauscht und damit weiterverwendet werden. Die Fallbeispiele Prostatakarzinom, Psychoonkologie und Melanom belegen die grundlegende Umsetzbarkeit dieses Modells. Praktische Schlussfolgerungen: Die Single-Source Tumordokumentation konnte am CCC EN in den klinischen Prozess integriert werden. Zudem konnte durch Weiterverwendung der Daten der Patientenversorgung Mehrfachdokumentation vermindert werden. Dabei m\u00fcssen medizinische Mitarbeiter durch neue Funktionalit\u00e4ten entlastet und damit zur elektronischen Erfassung von onkologischen Daten motiviert werden. Ein weisungsbefugtes Gremium zur hausinternen Standardisierung onkologischer Abl\u00e4ufe muss zudem organisatorische Abl\u00e4ufe, Dokumentationsstandards und die Ausgestaltung einer einheitlichen IT-Struktur festlegen. Die exemplarische Umsetzung einer Single-Source Tumordokumentation am CCC EN war zeit- und ressourcenintensiv. Durch folgende T\u00e4tigkeiten k\u00f6nnte dieser Aufwand signifikant reduziert werden: \u2022Auf nationaler Ebene m\u00fcssen bestehende Qualit\u00e4tssicherungsverfahren, Zertifizie-rungsprogramme und onkologische Datens\u00e4tze besser aufeinander abgestimmt werden. \u2022Nationale Gremien m\u00fcssen bestehende IT-Standards (HL7, CDA) nutzen, um Syntax und Semantik f\u00fcr den Austausch onkologischer Informationen eindeutig festzulegen. \u2022Durch vermehrte Erfahrungsberichte anderer Kliniken sollten verschiedene Strategien innerhalb der Tumordokumentation herausgearbeitet, verglichen und Best Practices abgeleitet werden.\nBackground and Goals: With 24,4 % cancer is the second leading cause of death in Germany. Because patient care is characterized by long treatment periods as well as a large share of interdisciplinary and trans-sectoral treatment processes, well-structured tumor documentation is important. Oncology data is not only used within patient care, but also for quality assurance purposes, cancer registration and research projects. With regards to documentation content and documentation procedures these scenarios are not harmonized. Instead, the scenarios are characterized by redundancies and multiple documentation. This work develops a concept for electronic gathering of oncology data during clinical treatment process. Following the single-source concept, this data is documented once at its origin and reused afterwards for medical process management, quality insurance, cancer registration and research projects. Methods: The concept is developed in a three-step procedure. In a first step, stakeholders and tendencies in oncology care are identified based on a literature research as well as interviews with internal and external oncology experts. Then, a solution model is developed. With regards to data privacy aspects, it is analyzed if available dataset for cancer registries are suitable for clinical integrated single-source tumor documentation. Furthermore, existing clinical IT applications and documentation standards are combined to an IT-architecture. This solution model is validated in 22 projects at the Comprehensive Cancer Center Erlangen-N\u00fcrnberg (CCC EN). Moreover, the feasibility of the model is proved in several case studies. Results: Development of single-source tumor documentation is a protracted project, requiring collaboration of doctors and nurses as well as employees of the cancer register, the IT department and the quality management department. Within cancer care, it is important to prove quality of care by providing facts and figures, promote interdisciplinary as well as trans-sectoral collaboration and expand translational research projects. To meet these requirements and enable data reuse for medical process management, quality assurance, cancer registration and research, oncology data must be electronically documented during patient care. To enable integration in clinical processes, the available datasets for cancer registries have been splitted into documentation packages and matched to existing IT applications. Because these systems are linked by a suitable IT architecture, documented data can be exchanged and reused afterwards. The feasibility of this concept is demonstrated by the three case studies prostate carcinoma, psycho-oncology and melanoma. Practical conclusions: At the CCC EN single-source tumor documentation has been integrated in the clinical care process. Furthermore, multiple documentation has been reduced by reusing data. It is important to motivate medical employees for electronic documentation of oncology data by providing new features supporting them during patient care. Moreover, a directive committee for hospital internal standardization must define organizational processes, documentation standards and a uniform IT architecture. The exemplary implementation of single-source tumor documentation at the CCC EN has been time and resource intensive. By following these suggestions, the effort could be significantly reduced: \u2022Harmonizing quality assurance programs, certifications and oncology data sets on a national level. \u2022National committees must use existing IT standards (HL7, CDA) to define syntax and semantics of oncology data exchange. \u2022By providing more field reports, different strategies within tumor documentation could be carved out and compared. Finally, best practices could be identified.\n2013-02-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/2916\nurn:nbn:de:bvb:29-opus-41535\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-41535\nhttps://opus4.kobv.de/opus4-fau/files/2916/original_MarkusRiesDissertation.docx\nhttps://opus4.kobv.de/opus4-fau/files/2916/MarkusRiesDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3317\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:D.2.4\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nVariability Bugs in System Software\nVariabilit\u00e4tsfehler in Systemsoftware\nSincero, Julio\nVariabilit\u00e4t\nVariabilit\u00e4tsfehler\nLinux\nSoftwareproduktlinien\nddc:004\nOne of the key aspects of software product line engineering (SPLE) is the handling of variation points that can be combined to form specific products. Variability man- agement is the discipline responsible for the specification, combination and use of variation points. Many researches have contributed to this area by providing formal methods to analyze variability by means of reasoning operations built on top of SAT solvers. The development of system software also faces many challenges regarding vari- ability management. To cope with variability issues, practitioners have employed a diversity of independent tools to tailor system software in order to avoid the over- head of unneeded functionality. In the Linux kernel, which can also be regarded as a software product line, variability management is handled in a heterogeneous man- ner. A set of independent tools is employed on different levels of abstraction, which might lead to inconsistencies in the description of variability. These inconsistencies are called variability bugs. With the novel concepts of source code variability models, model slicing, and multi-model reasoning operations, variability bugs that are possibly hidden by heterogenous variability management approaches can be attacked. The Linux code base serves as an ideal testbed for these novel techniques that conjoin the reasoning operations from SPLE with the challenges imposed by the structure and the sheer size of real-world large-scale code bases. For an efficient, scalable and automatic detection of variability bugs, the tool undertaker implements the concepts of source code variability models, model slicing, multi-model reasoning operations and a SAT solver backend. Applying the undertaker tool to the Linux kernel code base revealed an impressive number of variability bugs, to which fixes were provided to the kernel developers. Their positive feedback confirms the need of tool support for the automatic detection of variability bugs.\nEiner der Schwerpunkte bei der Entwicklung von Softwareproduktlin- ien stellt die Behandlung der Variationspunkte dar, aus denen durch Kombination Produktvarianten gebildet werden k\u00f6nnen. Das Variabilit\u00e4tsmanagement ist ve- rantwortlich f\u00fcr die Spezifikation, Kombination und Verwendung von Variation- spunkten in der Produktlinienentwicklung. Viele Wissenschaftler haben Beitr\u00e4ge zu diesem Gebiet in Form von formalen Methoden zur automatisierten Analyse von Variabilit\u00e4t auf der Basis von Erf\u00fcllbarkeitsl\u00f6sern (SAT solver) geleistet. Die Entwicklung von Systemsoftware steht ebenfalls vor gro\u00dfen Herausforderungen in Bezug auf Variabilit\u00e4tsmanagement. Um die Variabilit\u00e4t in den Griff zu bekommen, wird von Praktikern eine Vielzahl von unabh\u00e4ngigen Werkzeugen verwendet, um Systemsoftware ma\u00dfgeschneidert unter Vermeidung unn\u00fctzer Funktionalit\u00e4t zu er- stellen. Im Linux-Betriebssystemkern, der ebenfalls als Softwareproduktlinie aufge- fasst werden kann, wird Variabilit\u00e4tsmanagement in heterogener Weise durchge- f\u00fchrt: Eine Reihe voneinander unabh\u00e4ngiger Werkzeuge wird auf unterschiedlichen Abstraktionsstufen eingesetzt, was leicht zu Inkonsistenzen bez\u00fcglich der Beschrei- bung von Variabilit\u00e4t f\u00fchren kann. Diese Inkonsistenzen werden Variabilit\u00e4tsfehler (variability bugs) genannt. Mit den neuen Konzepten Source Code Variability Mod- els, Model Slicing und Multi-Model Reasoning Operations k\u00f6nnen Variabilit\u00e4tsfehler behandelt werden, die vom heterogenen Variabilit\u00e4tsmanagement nicht entdeckt werden. Die Linux Codebasis dient als eine ideale Testumgebung f\u00fcr diese neuen Techniken, mittels der die formalen Methoden aus der Softwareproduktlinien-Ent- wicklung auf ihre Anwendbarkeit in umfangreichen, realen Softwareprojekten un- tersucht werden. F\u00fcr eine effiziente, skalierbare und automatische Erkennung von Variabilit\u00e4tsfehlern, implementiert das Werkzeug Undertaker die Konzepte Source Code Variability Models, Model Slicing und Multi-Model Reasoning, sowie einen SAT-solver. Durch Anwendung des Werkzeugs auf den Quellcode des Linux Be- triebssystemkerns wurde eine eindrucksvolle Anzahl von Variabilit\u00e4tsfehlern ent- deckt. Die positive Reaktion der Betriebssystemkern-Entwickler auf die eingesandten Fehlerbeschreibungen best\u00e4tigt die Zweckm\u00e4\u00dfigkeit der werkzeuggest\u00fctzten, au- tomatischen Erkennung von Variabilit\u00e4tsfehlern.\n2013-04-23\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3317\nurn:nbn:de:bvb:29-opus-45966\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-45966\nhttps://opus4.kobv.de/opus4-fau/files/3317/original_diss.zip\nhttps://opus4.kobv.de/opus4-fau/files/3317/diss.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3455\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:I.4.6\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med_ohneAngabe\nBildgebende Charakterisierung und Quantifizierung der Angiogenese bei Arthritis mittels \u03bcCT im Mausmodell\nCharacterization and quantification of angiogenesis in rheumatoid arthritis in a mouse model using Micro-CT\nGayetskyy, Svitlana\nAngiogenese\nddc:610\nAngiogenese ist ein wesentlicher pathophysiologischer Prozess bei chronischen Entz\u00a8undungsreaktionen, insbesondere bei Arthritis, der das Fortschreiten und den Verlauf der Krankheit beeinflusst. Bei der Suche nach m\u00a8oglichen Arthritistherapien sind die therapeutischen Ans\u00a8atze gegen Angiogenese von gro\u00dfem Interesse. Um die \u00a8Anderungen der Knochendurchblutung bei M\u00a8ausen mit rheumatoider Arthritis (RA) zu quantifizieren, wurde ein mehrstufiges Segmentierungsverfahren entwickelt. Dabei wurden die Blutgef\u00a8a\u00dfe im Bereich des entz\u00a8undetes Kniegelenks segmentiert und anschlie\u00dfend quantitative 3D histomorphometrische Parameter berechnet. Zwei Gruppen von M\u00a8ause (RA und WT) wurden in-vitro untersucht und die Ergebnisse miteinander verglichen.\nAngiogenesis is a key pathophysiological process in chronic inflammatory reactions, especially in arthritis, the progression and course of the disease influences. For quantification of changes of bone blood flow in mice with rheumatoid arthritis (RA)a multi-level segmentation method was developed. The blood vessels in the area of inflamed knee joint were segmented and then quantitative 3D histomorphometric parameters were calculated. Two groups of mice (WT and RA) were investigated in vitro and the results compared to each other.\n2013-07-04\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/zip\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3455\nurn:nbn:de:bvb:29-opus-47900\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-47900\nhttps://opus4.kobv.de/opus4-fau/files/3455/original_SvitlanaGayetskyyDissertation.docx\nhttps://opus4.kobv.de/opus4-fau/files/3455/SvitlanaGayetskyyDissertation.pdf\ndeu\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3488\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:D.4.6\nccs:K.6.5\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nUsability vs. Security: The Everlasting Trade-Off in the Context of Apple iOS Mobile Hotspots\nKurtz, Andreas\nFreiling, Felix\nMetz, Daniel\nSicherheit\nBenutzerfreundlichkeit\nDrahtloses lokales Netz\nSmartphone\nPasswort\nddc:004\nPasswords have to be secure and usable at the same time, a trade-off that is long known. There are many approaches to avoid this trade-off, e.g., to advice users on generating strong passwords and to reject user passwords that are weak. The same usability/security trade-off arises in scenarios where passwords are generated by machines but exchanged by humans, as is the case in pre-shared key (PSK) authentication. We investigate this trade-off by analyzing the PSK authentication method used by Apple iOS to set up a secure WPA2 connection when using an iPhone as a Wi-Fi mobile hotspot. We show that Apple iOS generates weak default passwords which makes the mobile hotspot feature of Apple iOS susceptible to brute force attacks on the WPA2 handshake. More precisely, we observed that the generation of default passwords is based on a word list, of which only 1.842 entries are taken into consideration. In addition, the process of selecting words from that word list is not random at all, resulting in a skewed frequency distribution and the possibility to compromise a hotspot connection in less than 50 seconds. Spot tests show that other mobile platforms are also affected by similar problems. We conclude that more care should be taken to create secure passwords even in PSK scenarios.\n2013-07-11\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3488\nurn:nbn:de:bvb:29-opus-47556\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus-47556\nhttps://opus4.kobv.de/opus4-fau/files/3488/report.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3613\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:D.2.4\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nMastering Variability Challenges in Linux and Related Highly-Configurable System Software\nBeherrschung von Variabilit\u00e4tsproblemen in Linux und verwandter hoch-konfigurierbarer Systemsoftware\nTartler, Reinhard\nBetriebssystem\nVariabilit\u00e4t\nLINUX\nSoftware Engineering\nddc:004\nThe compile-time configuration mechanisms of modern system software allow the adaptation to a broad range of supported hardware architectures and application domains. Linux is hereby a both prominent and good example: In version 3.2, Linux provides more than 12.000 user-configurable configuration options, growing rapidly. This high amount of configurability imposes big challenges for developers. First, the declared variability in the configuration tooling, and what is actually implemented in the code, have to be kept in sync. If performed manually, this is a tedious and error-prone task. Second, alternatives implemented in the code make the use of tools for static analysis challenging. Finally, the overwhelming amount of configuration options make finding the best configuration for a given use-case hard for system integrators and developers. In this thesis, I analyze the variability mechanisms in Linux and related system software, in which I reveal many inconsistencies between the variability declaration and implementation. Many of these inconsistencies are hereby provably actual programming errors. It turns out that the extracted variability model is useful for additional applications. The formalized model helps developers with employing existing tools for static analysis more effectively. This allows the systematic revelation of bugs that are hidden under seldom tested configurations. Moreover, my approach enables the construction of a minimal Linux configuration with the extracted variability model and a run-time analysis of the system. This enables system administrators to compile and operate a Linux kernel with significantly reduced attack-surface, which makes the system more secure. In the end, my approach allows the holistic mastering of compile-time variability across the language barriers of the employed tools Kconfig, make and CPP.\nDie von moderner Systemsoftware angebotenen Konfigurationsmechanismen erlauben die Anpassung an eine breite Auswahl von unterst\u00fctzten Hardwarearchitekturen und Anwendungsdom\u00e4nen. Linux ist hierbei ein sowohl prominentes als auch gutes Beispiel: In Version 3.2 bietet Linux mehr als 12.000 vom Benutzer steuerbare Konfigurationsschalter, mit stark steigender Tendenz. Dieses hohe Ma\u00df an Konfigurierbarkeit stellt Entwickler vor gro\u00dfe Herausforderungen. Zum einen muss die in den Konfigurationswerkzeugen deklarierte, mit der im Programmtext umgesetzten Variabilit\u00e4t in Einklang gehalten werden. Bei h\u00e4ndischer Durchf\u00fchrung stellt dies einen m\u00fchsamen und fehleranf\u00e4lligen Arbeitsschritt dar. Zum anderen erschweren die im Programmtext programmierten Alternativen den Einsatz von statischen Analysewerkzeugen. Schlie\u00dflich macht die \u00fcberw\u00e4ltigende Anzahl an Konfigurationsoptionen es Systemintegratoren und Entwicklern schwer, die f\u00fcr einen gegebenen Anwendungsfall beste Belegung der Konfigurationsschalter zu finden. In dieser Arbeit analysiere ich die Variabilit\u00e4tsmechanismen in Linux und verwandter Systemsoftware, bei der ich im ersten Schritt viele Inkonsistenzen zwischen der Deklaration und Umsetzung von Variabilit\u00e4t aufdecke. Viele dieser Inkonsistenzen sind dabei nachweislich tats\u00e4chliche Programmierfehler. Es stellt sich dabei heraus, dass das extrahierte Variabilit\u00e4tsmodell auch f\u00fcr weitere Anwendungen n\u00fctzlich ist. So hilft das formalisierte Modell Entwicklern bestehende statische Analysewerkzeuge effektiver einsetzen zu k\u00f6nnen. Dies erlaubt die systematische Aufdeckung von Programmfehlern, die in selten gew\u00e4hlten Konfigurationen verborgen sind. Dar\u00fcber hinaus erm\u00f6glicht mein Ansatz die Konstruktion einer minimalen Konfiguration mit dem extrahierten Variabilit\u00e4tsmodell und einer Laufzeitanalyse des Systems. Dies erm\u00f6glicht es Systemadministratoren einen Linux-Kern mit einer deutlich verkleinerten Angriffsfl\u00e4che zu \u00fcbersetzen und zu betreiben, was die Sicherheit des Gesamtsystems deutlich erh\u00f6ht. Letztendlich erlaubt mein Ansatz die ganzheitliche Beherrschung der Variabilit\u00e4t in Linux zur \u00dcbersetzungszeit \u00fcber die Sprachgrenzen der eingesetzten Werkzeuge Kconfig, make und CPP, hinweg.\n2013-09-09\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3613\nurn:nbn:de:bvb:29-opus4-36136\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-36136\nhttps://opus4.kobv.de/opus4-fau/files/3613/ReinhardTartlerDissertation.pdf\neng\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3645\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nccs\nccs:H.4.3\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nPrivacy in Smart Grids\nPrivatheit in intelligenten Stromnetzen\nJawurek, Marek\nprivatheit\nddc:000\nElectricity grids are evolving to \"smart grids\". Smart grids employ communication of supply and demand between participants of the grid to\nachieve better efficiency, availability, resilience, etc. than traditional grids.\nConsumer households are connected to the smart grid using smart metering and demand response. Smart metering\ncommunicates the household's electricity consumption at high resolution to the smart grid. Demand response enables service providers to affect the\nhousehold's eletricity demand by the means of different incentives.\nThe communication of a household's electricity consumption leaks information about the household and its inhabitants. Thus, it poses the\ninhabitants' privacy at risk. Existing smart grid deployments address this conflict between utility and privacy\nunsatisfactorily. They either accept the privacy risk or forfeit utility.\nThis dissertation provides an alternative solution to mitigate this conflict. The solution retains utility of the smart\ngrid without compromising consumer privacy. In particular, this dissertation first identifies that the smart grid poses\na privacy risk to consumers. This privacy risk originates from the collection and central storage of consumption\ntraces in naive implementations. Once consumption data is centrally stored there are few techniques one can employ to protect it from various\nattacks. The only viable, i.e. general and utility-preserving, technique is pseudonymization of stored data in\ncombination with proper access control. However, this dissertation shows experimentally on real smart metering data that\npseudonymization of consumption data is not effective. This result drives the main idea of the remainder: Consumption data\nmust not be collected, i.e. leave the household, in the first place.\nThis dissertation provides three privacy-preserving protocols that support essential smart grid computations\n(aggregation, billing, compliance verification) on\nconsumption data, respectively. For each computation the respective protocol only transports the minimal amount of required\ninformation out of the household. Furthermore, to the service provider that interacts with the household, they guarantee\ncomputation results that are as trustworthy as those from non-private protocols.\nStromnetze ver\u00e4ndern sich zu sogenannten intelligenten Stromnetzen. Diese erm\u00f6glichen es Netzteilnehmern ihr\nAngebot bzw. ihre Nachfrage zu kommunizieren, um gegen\u00fcber herk\u00f6mmlichen Stromnetzen beispielsweise h\u00f6here Effektivit\u00e4t, Verf\u00fcgbarkeit oder Widerstandsf\u00e4higkeit zu erreichen.\nDie beiden Technologien \"Smart metering\" und \"Demand response\" binden private Haushalte an intelligente Stromnetze\nan. \"Smart metering\" kommuniziert den zeitlich hochaufgel\u00f6sten Stromverbrauch eines Haushaltes an das intelligente Stromnetz.\n\"Demand response\" erlaubt es Dienstleistern den Stromverbrauch eines Haushaltes mit Hilfe verschiedener Ma\u00dfnahmen zu\nbeeinflussen.\nDie Kommunikation des Stromverbrauchs eines Haushaltes l\u00e4sst jedoch R\u00fcckschl\u00fcsse \u00fcber den Haushalt bzw. die\nBewohner zu und gef\u00e4hrdet somit die Privatheit der Bewohner. Bereits existierende intelligente Stromnetze gehen mit\ndiesem Konflikt zwischen Nutzen des intelligenten Stromnetzes (gegen\u00fcber der herk\u00f6mmlichen Stromnetze)\nund der Privatheitsgef\u00e4hrdung der Haushalte auf eine unbefriedigende Weise um.\nSie akzeptieren entweder das Risiko einer Privatheitsgef\u00e4hrdung oder eine Minderung des Nutzen.\nDiese Dissertation bietet eine alternative L\u00f6sung f\u00fcr diesen Konflikt. Sie bewahrt sowohl Nutzen des intelligenten Stromnetzes\nals auch die Privatheit der Haushalte. Konkret identifiziert diese Dissertation zuerst, dass eine naive Umsetzung eines intelligente Stromnetzes eine\nGef\u00e4hrdung der Privatheit darstellt. Diese Gef\u00e4hrdung beruht darauf, dass in einer naiven Umsetzung Stromverbrauchsdaten\ngesammelt und gespeichert werden. Sobald diese Daten zentral gespeichert sind, stehen kaum Techniken zur Verf\u00fcgung um sie\nvor verschiedenen Angriffen effektiv zu sch\u00fctzen. Die einzige, aufgrund ihrer Universalit\u00e4t und nutzenbewahrenden\nEigenschaft, anwendbare Technik ist die Pseudonymisierung der Daten in Verbindung mit entsprechenden Zugriffskontrollen. Diese Arbeit zeigt\njedoch experimentell mit Hilfe echter Stromverbrauchsdaten die Ineffektivit\u00e4t dieser Technik: Selbst pseudonymisierte Stromverbrauchsdaten k\u00f6nnen ihren Haushalten wieder zugeordnet werden.\nDiese Feststellung f\u00fchrt zu der Maxime, die den restlichen Teilen der Disseration unterliegt: Stromverbrauchsdaten d\u00fcrfen nicht gesammelt werden, bzw. d\u00fcrfen\nHaushalte gar nicht erst verlassen.\nDaher entwickelt diese Dissertation desweiteren drei Protokolle, die die grundlegenden Berechnungen des\nintelligenten Stromnetztes (Aggregation, Abrechnung und \u00dcberpr\u00fcfung der Einhaltung von Verbrauchsvorgaben) auf\nStromverbrauchsdaten in einer privatheitsbewahrenden Weise durchf\u00fchren. Diese Protokolle \u00fcbertragen jeweils nur die\nminimal ben\u00f6tigten Informationen aus dem Haushalt, die f\u00fcr die jeweilige Berechnung ben\u00f6tigt werden.\nGleichzeitig garantieren sie den Dienstleistern, die mit dem Haushalt interagieren, diesselbe Vertrauensw\u00fcrdigkeit der Berechnungsergebnisse\nwie die Protokolle einer naiven Umsetzung.\n2013-10-09\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3645\nurn:nbn:de:bvb:29-opus4-36455\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-36455\nhttps://opus4.kobv.de/opus4-fau/files/3645/Dissertation_MarekJawurek_fuer_finalen_Druck_1.0.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3714\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nddc:300\nccs\nccs:I.2.1\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nApproximation minimaler Steinerb\u00e4ume in Stra\u00dfennetzwerken zur Bestimmung von Treffb\u00e4umen f\u00fcr Gruppennavigationssysteme\nApproximation of Steiner Minimal Trees in Street Networks for the Calculation of Meeting Trees for Group Navigation Systems\nZenker, Bj\u00f6rn\nNavigationssystem\nSteiner-Baum\nOptimierung\nGruppe\nFu\u00dfg\u00e4nger\nddc:000\nddc:300\nAktuelle Fu\u00dfg\u00e4ngernavigationssysteme unterst\u00fctzen nur einzelne Benutzer, obwohl Menschen oft in Gruppen unterwegs sind. Um Navigationsassistenz auch f\u00fcr Menschen, die sich tre\u21b5en, anzubieten, entwirft diese Arbeit eine Formalisierung des r\u00e4umlichen Treffvorgangs und f\u00fchrt diese auf das Steinerbaumproblem in euklidischen Netzwerken zur\u00fcck. Bekannte exakte L\u00f6sungsverfahren f\u00fcr dieses NP-vollst\u00e4ndige Problem weisen f\u00fcr den praktischen Einsatz zu hohe Laufzeiten auf. Deshalb werden in der vorliegenden Arbeit schnellere approximative Algorithmen entwickelt und hinsichtlich Laufzeit und Approximationsfaktor evaluiert. Es werden unter anderem eine Approximation in der euklidischen Ebene, Verfahren der Lokalen Suche wie Stochastisches Bergsteigen und Iterative Lokale Suche zur Verbesserung von Approximationen wie beispielsweise Minimalen Spannb\u00e4umen, und eine um Heuristiken erweiterte Version des Loss Contracting Algorithmus miteinander verglichen. Dabei werden Approximationsfaktoren erreicht, die je nach Algorithmus und Netzwerk weniger als ein Prozent von der Optimall\u00f6sung abweichen.\nCurrent pedestrian navigation systems assist single users only, despite the fact that people often go out together. For establishing navigational assistance for people who meet, a formalisation of the spatial aspects of meetings is conducted. This formalisation, the meeting tree problem, can be reduced to the minimum Steiner tree problem in euclidean networks. Exact algorithms for the solution of this NP-complete problem exhibit a runtime which is too high for practical problem instances. Thus, faster approximation algorithms are developed in this thesis and evaluated with respect to runtime and approximation ratio. Amongst others, the following algorithms are compared to each other: an approximation in the euclidean plane, methods of stochastic local search (including stochastic hillclimbing and iterated local search) for optimizing e.g. approximations based on minimum spanning trees, a version of the Loss Contracting Algorithm, which has been extended by problem speci\ufb01c heuristics. Results show that approximation ratios which deviate by less than one percent from the optimal solution can be reached.\n2013-09-22\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3714\nurn:nbn:de:bvb:29-opus4-37145\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-37145\nhttps://opus4.kobv.de/opus4-fau/files/3714/BjoernZenkerDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3763\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:540\npacs\npacs:31.70.Dk\nmsc\nmsc:81-02\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Chemie\nTheoretical Study of Electronic Properties of Carbon Allotropes\nTheoretische Studien der elektronischen Eigenschaften von Kohlenstoff-Allotropen\nDral, Pavlo\nConfiguration Interaction\nAb-initio-Rechnung\nQuantenchemie\nTheoretische organische Chemie\nDichtefunktionalformalismus\nSemiempirische Methode\nPolycyclische Aromaten\nExziton\nFullerene\nElektronentransfer\nReaktionsmechanismus\nPorphyrine\nddc:540\nThis doctoral thesis describes theoretical investigations of the different physicochemical and above all electronic properties of numerous already discovered and yet to be synthesized modern carbon allotropes, their model compounds and derivatives.\nIn the last century it was ascertained that carbon is not only the most important chemical element for the existence of living beings, but is also becoming increasingly more important for electronics and especially in recent decades for molecular nanoelectronics. Its unique ability to form an unlimited number of chemical compounds results in seemingly infinitely many allotropes that have very different properties. Carbon allotropes that are known till now can be classified first of all by the hybridization of orbitals of carbon atoms: sp-carbon can at least theoretically form linear acetylenic carbon, sp2-carbon \u2013 numerous allotropes with graphenic surfaces such as graphite, graphene, carbon nanotubes and fullerenes, sp3-carbon \u2013 diamond. Their properties can be tuned further via chemical functionalization. Smaller model compounds of sp-carbon allotropes such as polyynes and cumulenes, sp2-carbon allotropes as polycyclic aromatic hydrocarbons, sp3-carbon allotropes as diamondoids are also of large interest, because they can be investigated theoretically and experimentally not only easier, but have also themselves remarkable properties. Moreover, the novel allotropes consisting of the combinations of sp-, sp2- and sp3-hybridized carbons as sp-sp2-graphdiyne, sp-sp3-yne-diamond, sp2-sp3-hexagonite and sp-sp2-sp3-carbon built of fullerene balls connected through carbon chains are thinkable and extended segments of some of them were already synthesized.\nCarbon allotropes, their model compounds and derivatives find more and more often application for the nanoelectronics and electronics as elements of transistors, sensors and memory storage devices, for energy conversion as building blocks of solar cells and for energy storage. Therefore, these substances have been investigated very intensively experimentally and theoretically in the last years. The importance of the studies of the carbon allotropes in research and development was rewarded by the Nobel Prizes in Chemistry in 1996 and in Physics in 2012. The former Nobel Prize was awarded to Robert F. Curl, Harold Kroto and Richard E. Smalley for the discovery of fullerenes and the latter one was given to Andre Geim and Konstantin Novoselov \u201efor the fundamental experiments with two-dimensional material graphene\u201d.\nIn the present work diverse electronic properties of carbon allotropes and related systems that are important for nanoelectronics, energy conversion and storage were studied with different ab initio, semiempirical and density functional theory (DFT) quantum chemical methods. Semiempirical configuration interaction (CI) and DFT-based methods were used for describing excited states of the molecular nanosystems based on the above compounds.\nDetailed ab initio and DFT studies of the excited states of the relatively large nanosystems with many more than a hundred atoms is too computationally expensive with the current development of computer techniques and semiempirical CI methods are therefore sometimes the only choice for such systems. Thus, new semiempirical Unrestricted (HF) Natural Orbitals (UNO) \u2013 CI methods were developed in this work, to solve the challenging task to select the correct active orbitals for semiempirical CI. Moreover, UNO\u2013CIS methods have generally better accuracy than conventional CI methods and comparable or better accuracy than DFT. UNO\u2013CI methods were implemented into semiempirical MO-program VAMP.\nThe optical band gaps of the polyyne series related to the sp-carbon allotrope linear acetylenic carbon were studied with semiempirical UNO\u2013CI and CI methods in the present work. It was shown that the theoretical values of the properties studied are in very good agreement with experimentally available values and observations.\nAfterwards, different model compounds of the sp2-carbon allotropes were considered. Optical band gaps of many polycyclic aromatic hydrocarbons (PAHs) were calculated with semiempirical UNO\u2013CI and CI methods and compared with experimental data and time-dependent (TD) DFT calculations. Next, inclusion energies of heteroatoms and some groups into the interior of PAHs were calculated with the DFT methods. The influence of such doping on such electronic properties as spin state, diradical character, electron affinities (EAs), ionization potentials (IPs), different types of band gaps, exciton binding energy and aromaticity was examined at the semiempirical and DFT levels. What\u2019s more, exceptional properties of the unusual radical ion pair NH4(+)@C60(\u2022-) were theoretically studied, its possible synthesis suggested and the corresponding reaction steps including intermediate endofullerenes potentially interesting for spintronics were calculated. Photoinduced electron transfer (PIET) in systems involving model systems of sp2-carbon allotropes including fullerene C60 and doped PAHs important for energy conversion applications was studied by DFT and semiempirical CI and UNO\u2013CI methods.\nFurthermore, electron transfer processes between electron donating diamondoids including adamantane and oxadiamondoids that are substructures of undoped and oxygen-doped sp3-carbon allotrope diamond, respectively, and electron accepting nitronium-containing compounds were studied. This study explained the experimentally observed reactivity of diamondoids and the distribution of the products of the corresponding reactions.\nFinally, sp2 carbon allotropes with graphenic surfaces are suggested as the plausible candidates for the hydrogen storage that is important for the environmentally friendly energy storage technology. First, careful calibration of the second order M\u00f8ller\u2013Plesset perturbation theory (MP2), DFT and semiempirical methods was performed to find the most accurate methods able to reproduce experimentally observed change of fullerene electron affinity under hydrogenation. Second, the DFT and semiempirical methods confirmed that experimentally observed 1,9-dihydro[60]fullerene is the most stable isomer among 23 possible regioisomers of C60H2 and the study of the influence of electron doping on the hydrogenation of fullerenes with the same methods explained the decomposition of C60H2 under electron reduction. Third, the importance of choosing a DFT functional that describes binding extra electrons correctly in highly negatively charged fullerene derivatives for predicting relative stabilities of the latter species was demonstrated.\nThe close cooperation with experimental studies within the present doctoral thesis proved the effectiveness and even synergic effect of theoretical studies on research and development of the modern applications for nanoelectronics, energy conversion and storage based on carbon allotropes and related systems.\nIn summary, the theoretical methods used and developed can explain the experimentally observed properties very well and have been applied for the prediction of the properties of the unknown compounds.\nIn der vorliegenden Doktorarbeit wird die theoretische Untersuchung der verschiedenen physikalisch-chemischen und vor allem elektronischen Eigenschaften von zahlreichen bereits entdeckten und noch zu synthetisierenden neuartigen Kohlenstoff-Allotropen, deren Modelverbindungen und Derivate dargestellt.\nIm letzten Jahrhundert wurde festgestellt, dass Kohlenstoff nicht nur das wichtigste chemische Element f\u00fcr die Existenz von Lebewesen ist, sondern auch zunehmend wichtiger f\u00fcr Elektronik und besonders in letzten Jahrzehnten f\u00fcr molekulare Nanoelektronik wird. Seine einzigartige F\u00e4higkeit, unbegrenzte Mengen chemischer Verbindungen zu bilden, f\u00fchrt auch dazu, dass es auch scheinbar unendlich viel Allotropen mit sehr unterschiedlichen Eigenschaften hat. Die bis jetzt bekannten Kohlenstoff-Allotropen k\u00f6nnen vor allem nach Hybridisierung der Orbitalen ihrer Kohlenstoffatome klassifiziert werden: sp-Kohlenstoff kann zumindest theoretisch linearen azetylenischen Kohlenstoff bilden, sp2-Kohlenstoff \u2013zahlreiche Allotropen mit graphenischen Oberfl\u00e4chen wie Graphit, Graphen, Kohlenstoffnanor\u00f6hre und Fullerene, sp3-Kohlenstoff \u2013 Diamant. Ihre Eigenschaften k\u00f6nnen weiter durch chemische Funktionalisierung gesteuert werden. Kleinere Modelverbindungen von sp-Kohlenstoff-Allotropen wie Polyine und Kumulene, sp2-Kohlenstoff wie polyzyklische aromatische Kohlenwasserstoffe, sp3-Kohlenstoff wie Diamantoide sind auch von gro\u00dfem Interesse, weil sie nicht nur oft einfacher theoretisch und experimental untersucht werden k\u00f6nnen, sondern auch selbst bemerkenswerte Eigenschaften haben. Au\u00dferdem sind die neuartige Kohlenstoff-Allotropen, die aus der Kombination von sp-, sp2- und sp3-hybridisierten Kohlenstoffen zusammengesetzt sind, wie sp-sp2-Graphdiin, sp-sp3-in-Diamant, sp2-sp3-Hexagonit und sp-sp2-sp3-Kohlenstoffe, die aus mit Kohlenstoffketten verbundenen Fullerenkugeln bestehen, denkbar und erweiterte Ausschnitte von einigen davon wurden bereits synthetisiert.\nKohlenstoff-Allotropen, ihre Modelverbindungen und Derivaten finden immer h\u00e4ufiger Anwendung f\u00fcr Nanoelektronik und Elektronik, z. B. bei Bestandteilen von Transistoren, Sensoren und Speicherger\u00e4ten, f\u00fcr Energiewandlung, wie es bei Bestandteilen von Solarzellen zu finden ist und f\u00fcr Energiespeicherung. Dementsprechend werden diese Substanzen in den letzten Jahren sehr intensiv experimental und theoretisch untersucht. Die Bedeutung der Studien von Kohlenstoff-Allotropen in Forschung und Entwicklung wurde mit den Nobelpreisen f\u00fcr Chemie im Jahre 1996 und f\u00fcr Physik im Jahre 2010 ausgezeichnet. Der erste Nobelpreis wurde Robert F. Curl, Harold Kroto und Richard E. Smalley f\u00fcr die Entdeckung der Fullerene verliehen und der zweite wurde an Andre Geim und Konstantin Novoselov \u201ef\u00fcr grundlegende Experimente mit dem zweidimensionalen Material Graphen\u201c vergeben.\nIn dieser Arbeit werden Kohlenstoff-Allotropen und deren verwandten Verbindungen auf ihre wichtigen Eigenschaften f\u00fcr die Nanoelektronik bzw. Energiewandlung und -speicherung mit verschiedenen quantenchemischen Methoden wie ab initio und semiempirische sowie Dichtefunctionaltheorie (DFT) Verfahren untersucht. Semiempirische Konfigurations-wechselwirkungsmethoden (Configuration Interaction, CI) und DFT-Methoden werden verwendet, um die angeregten Zust\u00e4nde von molekularen Nanosystemen, die auf die oben genannten Verbindungen basiert sind, zu beschreiben.\nDetaillierte ab initio- und DFT-Studien der angeregten Zust\u00e4nde von relativ gro\u00dfen molekularen Nanosystemen mit weit \u00fcber hundert Atomen sind mit der heutigen Entwicklung der Computertechnik zu rechenintensiv und deshalb sind semiempirische CI-Methoden (Configuration Interaction, CI) manchmal die einzige Wahl f\u00fcr solche Systeme. Demzufolge wurden neue semiempirische Unrestricted (HF) Natural Orbitals (UNO) \u2013 CI-Methoden entwickelt, die die anspruchsvolle Aufgabe der Auswahl der richtigen aktiven Orbitale f\u00fcr CI l\u00f6sen. Dar\u00fcber hinaus liefern UNO\u2013CI-Methoden in der Regel h\u00f6here Genauigkeit als die konventionellen CI-Methoden und vergleichbare oder h\u00f6here Genauigkeit als DFT. UNO\u2013CI-Methoden wurden in das semiempirische MO-Programm VAMP implementiert.\nDanach wurden in der vorliegenden Arbeit die optischen Bandl\u00fccken von der homologen Reihe der Polyine, die mit linearem azetylenischem Kohlenstoff (sp-Kohlenstoff-Allotrop) verwandt sind, mit semiempirischen UNO\u2013CI- und CI-Methoden untersucht. Die theoretischen Werte der studierten Eigenschaften stimmen sich sehr gut mit experimentell verf\u00fcgbaren Werten und Beobachtungen \u00fcberein.\nAnschlie\u00dfend wurden verschiedene Modelverbindungen der sp2-Kohlenstoff-Allotropen betrachtet. So wurden die optischen Bandl\u00fccken von vielen polyzyklischen aromatischen Kohlenwasserstoffen (polycyclic aromatic hydrocarbons, PAHs) mit DFT-, semiempirischen UNO\u2013CI- und CI-Methoden berechnet und sowohl mit experimentalen Werten als auch mit DFT-Berechnungen verglichen. Dann wurden die Energien der Versetzung von Heteroatomen und einigen Gruppen ins Innere von PAHs mit DFT-Methoden berechnet. Die Auswirkung einer solchen Dotierung auf die elektronischen Eigenschaften, wie die der Spin-Zust\u00e4nde, der diradikalischen Charaktere, der Elektronenaffinit\u00e4ten (EA), der Ionisierungspotentialen (IP), der verschiedenen Arten von Bandl\u00fccken, der Excitonbindungsenergien und der Aromatizit\u00e4t der PAHs, wurde mit semiempirischen und DFT-Methoden erforscht. Dazu wurden die besonderen Eigenschaften des ungew\u00f6hnlichen radikalischen Ionenpaar NH4(+)@C60(\u2022-) theoretisch untersucht, seine m\u00f6gliche Synthese vorgeschlagen und entsprechende Reaktionsschritte, die die potentiell f\u00fcr Spintronik interessanten offen-schaligen Endofullerene wie Intermediate einschlie\u00dfen, berechnet. Der f\u00fcr die Energiewandlungsanwendungen wichtige photoinduzierte Elektronentransfer (PIET) in den aus Modelsystemen von sp2-Kohlenstoff-Allotropen bestehenden Systemen (Fulleren C60 und dotierte PAHs einschlie\u00dfend) wurde mit DFT und semiempirischen CI- und UNO\u2013CI-Methoden untersucht.\nAu\u00dferdem wurden die Elektronentransferprozesse zwischen Elektronen gebenden Diamantoiden einschlie\u00dflich Adamantan und Oxadiamondoiden, welche die Substrukturen von undotiertem bzw. sauerstoffdotiertem sp3-Kohlenstoff-Allotrop Diamant darstellen, und dem Elektronen akzeptierenden nitroniumhaltigen Verbindungen erforscht. Dabei wurde die experimentell beobachtete Reaktivit\u00e4t der Diamantoiden sowie die Verteilung der Produkte entsprechender Reaktionen erkl\u00e4rt.\nSchlie\u00dflich werden Kohlenstoff-Allotropen mit graphenishen Oberfl\u00e4chen als viel versprechende Kandidaten f\u00fcr die Wasserstoffspeicherung, die wichtig f\u00fcr die umweltfreundliche Energiespeichertechnik ist, vorgeschlagen. Erstens wurden die M\u00f8ller\u2013Plesset St\u00f6rungstheorie zweiter Ordnung (MP2), DFT- und semiempirischen Methoden sorgf\u00e4ltig kalibriert, um die genauesten Methoden zu finden, die die experimentell beobachtete \u00c4nderung der Elektronenaffinit\u00e4t von Fulleren unter Hydrierung reproduzieren k\u00f6nnen. Zweitens best\u00e4tigte die Studie der Auswirkung der Elektronendotierung auf die Hydrierung von Fullerenen mit DFT und semiempirischen Methoden, dass das experimentell beobachtete 1,9-Dihydro[60]fulleren das stabilste Isomer unter 23 m\u00f6glichen Regioisomeren von C60H2 ist, und erkl\u00e4rte die Zersetzung von C60H2 unter Elektronenreduktion. Drittens wurde die Wichtigkeit der Wahl von DFT-Funktional f\u00fcr die korrekte Beschreibung der Bindung von Extraelektronen in hoch negativ geladenen Fullerenederivaten dargelegt, um die relativen Stabilit\u00e4ten letztgenannten Verbindungen zuverl\u00e4ssig vorauszusagen.\nDie enge Zusammenarbeit mit experimentellen Untersuchungen im Rahmen vorliegender Doktorarbeit zeigte die Effektivit\u00e4t und sogar synergetische Effekte der theoretischen Studien f\u00fcr die Forschung und Entwicklung von auf Kohlenstoff-Allotropen und deren verwandten Verbindungen basierten neuartigen Anwendungen f\u00fcr Nanoelektronik, Energiewandlung und -speicherung.\nZusammenfassend l\u00e4sst sich sagen, dass die verwendeten und entwickelten theoretischen Methoden die experimentell beobachteten Eigenschaften sehr gut erkl\u00e4ren k\u00f6nnen sowie f\u00fcr die Vorhersage der Eigenschaften von unbekannten Verbindungen verwendet werden k\u00f6nnen.\n2013-05-10\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3763\nurn:nbn:de:bvb:29-opus4-37630\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-37630\nhttps://opus4.kobv.de/opus4-fau/files/3763/PavloDralDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3855\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:I.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nA Cost Constrained Boosting Algorithm for Fast Object Detection\nMilitzer, Arne\nTietjen, Christian\nHornegger, Joachim\nBoosting\nddc:004\nBoosting methods are among the most widely used machine learning techniques in practice for various reasons. In many scenarios, however, their use is prevented\nby runtime constraints. In this paper we propose a novel technique for reducing the computational complexity of hierarchical classifiers based on AdaBoost, such as the probabilistic boosting tree, which are often used for object detection. We modify AdaBoost training so that the hypothesis generation is no longer based solely on the weak learner\u2019s training error but also on a measure of hypothesis complexity. This is achieved by incorporating a cost function into the optimization process, effectively constraining feature selection, which leads to a reduced overall classifier complexity and thus shorter evaluation times. The validity of the approach is shown in an experimental valuation on real-world data. In a cross validation experiment with a system for automatic segmentation of liver tumors in CT images, the evaluation cost for classifying previously unseen samples could be reduced by up to 76% using the methods described here without losing classification accuracy.\n2013-11-04\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3855\nurn:nbn:de:bvb:29-opus4-38559\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-38559\nhttps://opus4.kobv.de/opus4-fau/files/3855/ConstrainedBoosting.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3856\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nLearning a Prior Model for Automatic Liver Lesion Segmentation in Follow-up CT Images\nMilitzer, Arne\nTietjen, Christian\nHornegger, Joachim\nComputertomographie\nLebertumor\nSegmentierung\nMaschinelles Lernen\nAutomatische Klassifikation\nddc:004\nLiver tumors that are not surgically removed need to be closely monitored. A common procedure for their assessment involves acquiring CT images every few\nmonths and rating disease status based on the largest diameters of a subset of the lesions. The most prominent benefits of automatic lesion segmentation methods in this context are minimization of time consuming interaction\nand the possibility of volumetric measurements. While existing methods could be applied to each image individually, we propose to incorporate information gained\nfrom previous images of the same patient to enhance the segmentation. We learn a Probabilistic Boosting Tree that has an internal representation of tumor growth from a set of training images. Provided a baseline lesion segmentation,\nit can generate a patient specific lesion prior to guide the segmentation in a follow-up image. In this paper, we describe and compare different methods for building the growth model and integrating it into a segmentation system. The validity of the approach is shown in an experimental evaluation on a database of 14 patients. On the 17 pairs of baseline and follow-up images in this database, segmentation performance was measured once without and once with the proposed prior. When comparing the points of 90% sensitivity from each experiment, introducing the prior improved the precision of the segmentation from 82.7% to 91.9%. This corresponds to a reduction of the number of false positive voxels per true positive voxel by 57.8%.\n2013-11-04\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3856\nurn:nbn:de:bvb:29-opus4-38562\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-38562\nhttps://opus4.kobv.de/opus4-fau/files/3856/PriorLearning.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3873\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:C.3\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nEvaluierung gitterbasierter Pfadplanungs-Algorithmen f\u00fcr die Hardwarebeschleunigung mit FPGAs\nEvaluation of path planning algorithms based on cell maps for a hardware acceleration with FPGAs\nSchmidt, Michael\nBahnplanung\nField Programmable Gate Array\nddc:620\nDie Anforderungen an mobile Robotersysteme nehmen stetig zu. Immer mehr Aufgaben sollen nach M\u00f6glichkeit autonom gel\u00f6st werden. Die daf\u00fcr notwendigen Algorithmen werden komplexer, mit einer steigenden Anzahl zu verarbeitende Daten und Operationen. Dabei sind h\u00e4ufig auch hohe Zeitanforderungen an das Verarbeitungssystem gefordert, beispielsweise um Reaktionen bei der Navigation des Roboters innerhalb weniger Millisekunden realisieren zu k\u00f6nnen. Eine M\u00f6glichkeit, den steigenden Anforderungen zu begegnen, ist der Einsatz leistungsf\u00e4higer Verarbeitungseinheiten. FPGAs sind f\u00fcr den Einsatz in Robotik pr\u00e4destiniert, kommen jedoch relativ selten zum Einsatz. Die Gr\u00fcnde sind dabei oft die Kosten und Entwicklungszeiten, die beim Einsatz von FPGAs anfallen. Nichtsdestotrotz bieten FPGAs eine Vielzahl von Vorteilen. Es k\u00f6nnen Hardware-Architekturen speziell f\u00fcr eine Anwendung angepasst werden, mit denen eine schnelle Verarbeitung, durch Parallelisierung und Pipelining, mit geringen Latenzen m\u00f6glich ist. Zudem k\u00f6nnen FPGA-basierte Systeme energieeffizient und auch kompakt realisiert werden.\nDiese Dissertation soll einen Beitrag dazu leisten, die Vorteile f\u00fcr den Einsatz von FPGAs in Robotersystemen aufzeigen. Dazu wird die Pfadplanung, als eine besonders daten- und rechenintensive Anwendung, f\u00fcr den zeitkritischen Einsatz in Robotersystemen untersucht. In dieser Arbeit stehen gitterbasierte Ans\u00e4tze f\u00fcr den zweidimensionalen Fall im Vordergrund. Gitterbasierte Ans\u00e4tze haben den Vorteil, dass f\u00fcr eine konkrete Anwendung sehr einfach, durch \u00c4nderung der Gitteraufl\u00f6sung, ein Kompromiss zwischen Berechnungsdauer und Genauigkeit gefunden werden kann. Weiterhin sind bei vielen gitterbasierten Algorithmen die Laufzeiten ma\u00dfgeblich abh\u00e4ngig von der Aufl\u00f6sung und deswegen f\u00fcr den zeitkritischen Einsatz besonders interessant. Unter einer zeitkritischen Anwendung wird dabei verstanden, dass die Berechnung eines Pfades f\u00fcr den Roboter innerhalb weniger Millisekunden m\u00f6glich ist. Es werden in dieser Arbeit die Analyseergebnisse verschiedener gitterbasierter Algorithmen pr\u00e4sentiert. Dabei hat sich ergeben, dass gitterbasierte Verfahren oft auf Stencil-Codes beruhen, d.h. iterativ mit lokalen\nOperationen realisiert werden. Darauf aufbauend wurden VHDL-Templates f\u00fcr die FPGA-interne und -externe Speicherung der Gitterdaten entwickelt, die in dieser Arbeit vorgestellt werden. Diese erm\u00f6glichen durch eine spezielle, adaptierbare Parallelisierung und Pipelining-Mechanismen, eine volle Aussch\u00f6pfung der zur Verf\u00fcgung stehenden FPGA-Ressourcen, um eine Minimierung der Berechnungszeiten f\u00fcr einen Pfadplanungs-Algorithmus zu erreichen. Auf Basis dieser Templates werden die Implementierungen der Potentialfeld-Methode mit Best-First-Suche, einer Methode basierend auf harmonischen Funktionen, einem Autowellen-Verfahren, einem speziellen Wellenfront-Algorithmus und einem, in dieser Dissertation entwickelten, Ansatzes, realisiert mit Hilfe von Marching Pixels, vorgestellt. Eine Vergleich der Implementierungen, mit GPU- und Multicore-Architekturen, zeigt, dass, je nach den Eigenschaften des Algorithmus, eine andere Architektur f\u00fcr eine schnelle Berechnung von Vorteil sein kann. Neben diesem Vergleich wurde eine Gegen\u00fcberstellung der FPGA-Implementierungen untereinander umgesetzt. Dazu wurde eine spezielle Metrik entwickelt, die, neben der Berechnungszeit, auch den Ressourcenverbrauch und die Pfadg\u00fcte f\u00fcr die Bestimmung eines Qualit\u00e4tsma\u00dfes mit einbezieht. Diese Metrik erm\u00f6glicht eine detailliertere Analyse und Gegen\u00fcberstellung von verschiedenen FPGA-Implementierungen gitterbasiserter Pfadplanungs-Algorithmen. Es hat sich ergeben, dass die Potentialfeld-Methode im besten Fall am schnellsten\nist und am effizientesten auf Standardprozessoren realisiert werden kann. Die FPGA-Realisierung ist dagegen zwar langsamer, kommt daf\u00fcr aber mit sehr wenig Ressourcen aus. Da bei der Potentialfeld-Methode jedoch h\u00e4ufig lokale Minima auftreten, kann die Berechnungszeit sehr stark ansteigen und ist daher f\u00fcr zeitkritische Anwendungen nicht geeignet. Ein spezieller Wellenfront-Algorithmus, zusammen mit dem entwickelten Marching Pixel Verfahren, sind am schnellsten auf FPGAs realisierbar. F\u00fcr Gitter mit einer Aufl\u00f6sung von 1024x1024 ist, mit einem mittleren Virtex-5 FPGA, f\u00fcr den Wellenfront-Algorithmus eine Verarbeitung von bis zu 33 Karten pro Sekunde m\u00f6glich und mit dem Marching Pixel Verfahren bis zu 14 Karten pro Sekunde. Diese Verfahren eignen sich besonders f\u00fcr den Einsatz in zeitkritischen Systemen. Aufgrund der effizienten Realisierung, durch eine m\u00f6gliche interne Speicherung der Daten, zusammen mit einem hohen Parallelisierungsgrad f\u00fcr die Verarbeitung, erreicht weder die Multicore-Implementierung, noch die GPU-Implementierung, so gute Ergebnisse. Der Autowellen-Ansatz kann am effizientesten auf einer GPU implementiert werden. Jedoch sind die Verfahren auf Basis von Autowellen und auch auf Basis von harmonischen Funktionen, am komplexesten. Aufgrund der hohen Berechnungszeiten sollten diese Verfahren in der Praxis nicht eingesetzt werden.\nThe requirements for mobile robot systems are increasing consistently. More and more tasks should be realizable with an autonomous behavior. The required algorithms for these tasks are getting more complex, with an increasing number of operations and data to be processed. At the same time, the processing system in a robot has to fulfill\nalso hard timing constraints. During navigation in a dynamic environment for example, a robot has to react within a few milliseconds to avoid collisions. To meet these constraints, powerful processing architectures are required for future applications. FPGAs are well suited for an application in the field of mobile robotics. Nevertheless, they\nare used only rarely. Often, as reason the higher costs and development times are quoted. But the usage of FPGAs in robot applications has a lot of advantages. Processing architectures can be adapted for special tasks optimally, which allows a fast processing through parallelization and pipelining with also low latencies. Furthermore, FPGAs\nare advantageous in terms of power consumption and allow the realization of very compact mobile robot systems.\nThis thesis makes a contribution towards the advantages of applying FPGAs in robotics. The path planning task, which is computational and also data intensive, will be evaluated for usage in time-critical applications. The focus in this thesis are grid-based approaches which are using a two-dimensional cell map. The advantage of cell maps is that by an adaption of the grid resolution, a compromise between processing speed and precision for the path planning can be found easily for an application. Furthermore, the processing time in grid-based approaches depends mainly on the resolution. Therefore, they are particularly interesting for usage in time-critical applications. In this context, time-critical means that a path has to be found within a few milliseconds. The results of the analysis of several grid-based path planning algorithms are presented in this thesis. They show that a lot of these cell-map based algorithms are realized with so called stencil codes. Stencil codes operate on grids iteratively, with the usage of only local operations, to solve a global task on the grid, e.g. to find a global path. Based on this stencil code principle, VHDL templates were developed for FPGAs for an internal and external storage of the map data. They are also presented in this thesis. The developed templates allow an adaptable parallelization and pipelining, in order to use as much resources as possible on an FPGA for a minimization of the processing time. Based on these templates, the implementation of the Potential Field Method with Best-First Search,\nof an algorithm based on Harmonic Functions, of an Autowave approach, of a special Wavefront method and of an algorithm based on Marching Pixels, which was developed in this thesis, are presented. The comparison between these implementations and GPU and multicore realizations are discussed. They show that the prefered architecture for a fast processing depends on the specific characteristics of an algorithm. Beside this comparison to other architectures, also the FPGA implementations itself were compared among each other. A special metric was developed\nwhich uses not only the processing time but also the resource consumption and the quality for a path of an algorithm, in order to quantify the quality for the FPGA implementation of the algorithm. It is shown that\nthe potential field method is the most efficient on standard processors in the best case. The FPGA implementation is slower but it requires only a few resources. Nevertheless, the frequently arising local minima in this method can lead to a strong increase of the processing time. Therefore, the potential field method is not suitable for an application in time-critical applications. The special wavefront approach and the developed algorithm based on marching pixels are the fastest and most efficient algorithms on FPGAs of all considered approaches. For a grid resolution with 1024x1024, it is possible to process 33 maps per second with the wavefront algorithm and 14 maps per second with the marching pixel algorithm, on a midsize Virtex-5 FPGA. These methods are particularly suitable for robot systems with time-critical constraints. Neither the multicore implementations nor the GPU implementations are as fast as the FPGA implementations of these algorithms. The reason is that the map data can be stored\nin the internal memory of the FPGA which allows a high degree of parallelization. The autowave approach can be realized on GPUs most efficiently. But the autowave approach and also the methods based on harmonic functions are the most complex algorithms of all compared methods. Because of the high processing times, which are required for these approaches, they should not be used in practice.\n2013-11-07\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3873\nurn:nbn:de:bvb:29-opus4-38734\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-38734\nhttps://opus4.kobv.de/opus4-fau/files/3873/MichaelSchmidtDissertation.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3929\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:B.0\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Elektrotechnik\nEntwurf und Realisierung einer mehrkanaligen ZF-Ethernet-Umsetzerarchitektur zur schnellen Funkaufkl\u00e4rung\nLangmann, Frank\nDigitaler Empf\u00e4nger\nGigabit Ethernet\nddc:620\nIn den letzten Jahren hat die Nutzung funkbasierter Kommunikationsmedien immer weiter zugenommen.\nSolche Medien k\u00f6nnen auch milit\u00e4risch oder f\u00fcr kriminelle Zwecke benutzt werden.\nDeshalb wird f\u00fcr Beh\u00f6rden und milit\u00e4rische Einheiten die F\u00e4higkeit zunehmend wichtiger und anspruchsvoller, Funkfrequenzen zu \u00fcberwachen, verbunden mit der M\u00f6glichkeit, Funkemitter zu lokalisieren.\nUm eine breite geographische Abdeckung mit Sicherheitssystemen zur Funk\u00fcberwachung zu erm\u00f6glichen, m\u00fcssen bei steigenden Anforderungen an Bandbreite und Dynamik die Kosten f\u00fcr ein System gesenkt werden.\nEin wesentlicher Kostenfaktor ist dabei die Notwendigkeit mehrerer Empfangsz\u00fcge zur Lokalisierung von Funkemittern mit Hilfe von Gruppenantennen.\nIn dieser Arbeit wird deshalb eine neuartige Systemarchitektur entwickelt, die mehrkanaligen Funkempfang von 30 MHz bis 3 GHz mit einer hohen instantanen Bandbreite von 50 MHz erm\u00f6glicht.\nDie F\u00e4higkeit, das analoge Eingangsspektrum schnell zu scannen, erm\u00f6glicht die \u00dcberwachung gro\u00dfer Bandbreiten mit hoher Detektionswahrscheinlichkeit von Funkemittern.\nDas Empf\u00e4ngerkonzept ist dabei optimiert f\u00fcr mehrkanaligen Empfang, und dabei in der Anzahl der Empfangsz\u00fcge skalierbar.\nDie Empfangsz\u00fcge k\u00f6nnen aber auch unabh\u00e4ngig voneinander verwendet werden.\nGleichzeitig liegt der Fokus auf der ressourceneffizienten Implementierung, um den Empf\u00e4nger kosteng\u00fcnstig umsetzen zu k\u00f6nnen.\nSo wird zum Beispiel nur ein FPGA zur Implementierung des Empf\u00e4ngers eingesetzt.\nAu\u00dferdem werden f\u00fcr den Empf\u00e4nger ausschlie\u00dflich Standardkomponenten (COTS) zur g\u00fcnstigen und einfachen Herstellung verwendet.\nIm Vordergrund steht dabei nicht, ein anderen am Markt bereits kommerziell erh\u00e4ltichen Systemen in den Empfangsparametern \u00fcberlegenes System zu entwerfen.\nVielmehr wird ein neuartiges, bez\u00fcglich des Ressourcenbedarfs und damit der Kosten optimiertes Empfangssystem mit akzeptablen Empfangsparametern entwickelt.\nDie Arbeit umfasst vor allem die Systemarchitektur, mit Schwerpunkt auf der Digitalisierung analoger Zwischenfrequenzsignale, der digitalen Datenerfassung und -\u00fcbertragung mit den dazu n\u00f6tigen Systembaugruppen und die Empf\u00e4ngersteuerung.\nF\u00fcr die n\u00f6tigen Systemkomponenten werden die Anforderungen hergeleitet, die Implementierung beschrieben und die Funktionalit\u00e4t durch Messungen nachgewiesen.\nDas Gesamtkonzept wurde in einem Demonstrator in Hardware, Firmware und Software umgesetzt und damit die Funktionsf\u00e4higkeit des Prinzips gezeigt.\n2013-11-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3929\nurn:nbn:de:bvb:29-opus4-39290\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-39290\nhttps://opus4.kobv.de/opus4-fau/files/3929/Dissertation_Langmann-2013-11-24.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:3952\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:A.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nWertigkeit der Ultraschallparameter und der B-Bildanalyse des fetalen Darms beim Management der fetalen Gastroschisis\nValue of ultrasound parameters and the B-picture analysis of the fetal intestine in the management of fetal gastroschisis\nRei\u00df, Anne-Kathrin\nBauchspalte\nUltraschall\nUltrasound\nGastroschisis\nddc:610\nHintergrund und Ziele:\nBei der Gastroschisis hat die Schwere des Krankheitsbildes einen gro\u00dfen Einfluss auf das Outcome und die Morbidit\u00e4tsrate von betroffenen Neugeborenen.\nDie Evaluierung fetaler sonographischer Darmparameter, wie Darmdurchmesser, Darmwanddicke, Darmlumendurchmesser und Echogenit\u00e4t des Darmlumeninhalts, ist ein wichtiger Aspekt in der Diagnostik und Verlaufsbeurteilung. W\u00e4hrend die Bedeutsamkeit der Darmschlingenerweiterung in zahlreichen Studien erforscht wurde, ist die Relevanz der Echogenit\u00e4t des fetalen Darmlumeninhalts nur unzureichend untersucht. Die normierte Graustufenanalyse eignete sich in anderen Studien zur Charakterisierung von verschiedenen fetalen, neonatalen oder adulten Geweben. Ziel der vorliegenden Studie war es, festzustellen, ob die normierte Graustufenanalyse bei fetaler Gastroschisis Anwendung finden kann und m\u00f6glicherweise die Vorhersage des fetalen Outcomes verbessern kann.\nMethoden (Patienten, Material und Untersuchungsmethoden):\nIm Rahmen unserer retrospektiven Studie wurden in den Universit\u00e4tskliniken Erlangen, Bonn und K\u00f6ln 94 F\u00e4lle mit fetaler Gastroschisis aus dem Zeitraum Januar 2000 bis April 2011 untersucht. Es wurden die pr\u00e4natalen Ultraschallbilder (3. Trimenon) der ungeborenen Kinder, die Patientenakten der M\u00fctter und die Entlassungsbriefe der Neugeborenen ausgewertet. Neben der Erfassung der Basisdaten der M\u00fctter und den Befunden aus Schwangerschaft und Geburt, wurde das postnatale Outcome der Kinder erhoben. Anhand der Ultraschallbilder wurden der maximale Durchmesser des Darmlumens, die maximale Darmwanddicke und der maximale Darmdurchmesser der extraabdominalen Darmschlingen gemessen und mithilfe der Graustufenanalyse wurden normierte Graustufenhistogramme des Inhalts der fetalen Darmlumina erstellt. Pr\u00e4natale Darmschlingenerweiterungen und die normierte Graustufen-Histogramm-Breite (NGHB) des fetalen Darms wurden mit dem postnatalen Outcome verglichen.\nErgebnisse und Beobachtungen:\nDie fetale Gastroschisis hat eine zunehmende Inzidenz und tritt h\u00e4ufig bei jungen M\u00fcttern auf. In drei F\u00e4llen kam es zu einem intrauterinen Fruchttod, es gab eine Totgeburt, zwei Schwangerschaftsabbr\u00fcche und einen selektiven Fetozid.\nDie Entbindung erfolgte im Mittel in der 36. SSW und etwa ein Drittel (35 %) der Neugeborenen war \u201esmall for gestational age\u201c. Auff\u00e4llig war ein erh\u00f6hter Zigarettenkonsum der Schwangeren. Fast ein Drittel der Frauen (29 %) rauchte w\u00e4hrend der Schwangerschaft. Etwa die H\u00e4lfte (51,5 %) der Kinder hatte postpartale Komplikationen, wobei 25% mehr als eine Komplikation hatten. Pr\u00e4natale Darmschlingenerweiterungen (besonders des Darmlumens) korrelierten mit einem erh\u00f6hten Risiko postnataler Komplikationen. Die normierte Graustufenanalyse zeigte h\u00f6chst signifikante Ergebnisse. In der Gruppe der Neugeborenen mit enteralen Komplikationen (62,2) war die NGHB fast doppelt so breit, wie die NGHB der Kinder ohne Komplikationen (34,2). Der Inhalt des Darmlumens ist dementsprechend pr\u00e4partal bei Kindern mit gutem Outcome signifikant homogener (p < 0,001) als bei Kindern mit postpartalen enteralen Komplikationen. Ein Grenzwert von \u2265 43 im 3. Trimenon hatte mit einer Sensitivit\u00e4t von 92 % (22/24) und einer Spezifit\u00e4t von 79 % (23/29) die h\u00f6chste Vorhersagekraft f\u00fcr postnatale enterale Komplikationen.\nPraktische Schlussfolgerungen:\nAufgrund der aktuellen medizinischen Standards, wie der pr\u00e4natalen Ultraschalldiagnostik und ausgereifter therapeutischer M\u00f6glichkeiten in der Neonatologie, ist die fetale Gastroschisis heutzutage in den meisten F\u00e4llen gut therapierbar. Eine regelm\u00e4\u00dfige \u00dcberwachung der Feten und genaue Evaluierung jeglicher neuer Befunde ist \u00e4u\u00dferst wichtig, denn die Auspr\u00e4gung des Krankheitsbildes hat einen gro\u00dfen Einfluss auf das Outcome der Kinder.\nDie NGHB bewies sich als bester Vorhersagewert f\u00fcr ein schlechteres kindliches Outcome mit enteralen Komplikationen und somit auch deutlich verl\u00e4ngertem Klinikaufenthalt.\nBackground and intention:\nOutcome and morbidity rate of newborns with gastroschisis are influenced by the characteristics and development of the disease. Evaluation of prenatal sonographic ultrasound markers like the diameter of the bowel, intestinal wall and intestinal lumen, as well as the echogenicity of the fetal bowel, plays a major role in diagnosis and estimation of the diseases course. While significance of intra- and extra-abdominal bowel dilatation has been investigated in several studies, echogenicity of the fetal bowel represents a feature poorly evaluated to date. Analysis of standardized gray-level-histograms qualified in a few trials in characterization of a variety of fetal, neonatal and adult tissue.\nThe aim of this study was to determine a possible usage of standardized gray-level-analysis in evaluation of fetal gastroschisis and prediction of postnatal outcome.\nMethods (patients, materials and examination methods):\nWithin a period from January 2002 to April 2011 94 pregnancies with fetal gastroschisis have been recorded in a retrospective study at the university hospitals Erlangen, Bonn and K\u00f6ln. The data was taken from the pregnant women\u2019s files, the newborn\u2019s clinical discharge reports and the ultrasound image\u2019s (third trimester) findings. Collected data included the pregnant women\u2019s basic information, course of pregnancy and birth and the children\u2019s postnatal outcome. Due to the ultrasound images extra-abdominal bowel dilatation, thickened intestinal wall and thickened intestinal lumen were measured. Standardized gray-level-histograms were created from representative luminal areas. Prenatal bowel dilatation and standardized grey-level-histogram-width were compared to the children\u2019s postnatal outcome.\nResults and observations:\nGastroschisis shows an increasing incidence, especially among young mothers. There were three intrauterine deaths, one stillbirth, two abortions and one selective termination. The mean gestational age at delivery was 36 weeks and nearly one third was \u201csmall for gestational age\u201d. Cigarette consumption was conspicuously high among the pregnant women. Almost one third (29 %) reported smoking during pregnancy. Half (51,5 %) of the newborns suffered from postnatal complications, whereas 25% had more than one complication. Prenatal bowel dilatation, especially of intestinal lumen, correlated with an increasing risk of postnatal intestinal complications. Particularly the analysis of standardized gray-level-histograms provided significant results. Gray-level-histogram-width was nearly double as wide among the group of newborns with postnatal complication (62,2) as among the group of newborns without complications (34,2). Accordingly the intestinal lumen is significantly more homogenous (p<0,001) within the group of newborns with a good postnatal outcome. A standardized gray-level-histogram-width of \u226543 (third trimester) has the highest predictive power for postnatal intestinal complications, with a sensitivity of 92 % (22/24) and a specificity of 79 % (23/29).\nPractical conclusions:\nGastroschisis is nowadays very treatable due to current medical standards, for instance prenatal sonography and advanced therapeutic methods in neonatology.\nA periodical surveillance of the fetuses is extremely important. Steady evaluation of new findings is advised, as the characteristics and development of the disease have a great influence on the fetal outcome. Standardized gray-level-histogram-width has the best predictive value for a poor fetal outcome with intestinal complications and as a consequence a prolonged hospital stay.\n2013-01-12\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/3952\nurn:nbn:de:bvb:29-opus4-39529\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-39529\nhttps://opus4.kobv.de/opus4-fau/files/3952/Anne-KathrinReiss_Dissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4031\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:I.5.0\npacs\npacs:87.85.Ng\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Physik\ninstitutes:Tech_Informatik\nNovel algorithms and rating methods for high-performance ECG classification\nRockstroh, Christian\nMachine Learning\nElektrokardiogramm\nddc:004\nThe electrocardiogram (ECG) is the most central information provider for the diagnosis of cardiac diseases.\nToday, the computer-aided analysis of short-term ECG-recordings in supine rest, the OR or the ICU\nis a well-established procedure. One major problem, though, is to track the presence and the amount of\narrhythmias over days, weeks and month. As cardiologists can not spend their time on analyzing millions of heart beats for a single patient, an automated computer algorithm is necessary. Those ECG-classifiers assign heart beats to their respective type of arrhythmia or heart rhythm type.\nAnalyzing the current state-of-the-art is obligatory, but quite cumbersome. Performance measures can\nbe misleading. Data is sometimes collected to the advantage of certain classifiers. Therefore, this thesis calculates the same performance criterion for all publications and assesses the likelihood for the reported performance to be achieved in real-life situations. For the purpose of the latter, this thesis introduces the \ufffd -Score which rates each classifier towards eight different aspects by qualitative and quantitative criteria. The final review of 72 ECG-classifiers presents new insights towards the current frontier of research in the field.\nThis thesis transcends this border by exploiting over 1.4 million features incorporating features derived\nfrom common methods like wavelet transformation, time-domain analysis, autoregressive models and\nfrom novel ones like the 2D-segmentation of the bi-spectrum, static delineation and heavy use of partition functions and relational features. In contrast to current practice, this thesis extracts its features from three different segments using several normalization strategies instead of a single one, fixed with respect to size and position.\nHowever, those 1.4 million features have to be reduced to a set of 20 to 60 highly relevant ones. The\nmost potent strategies for this amount of data reside in the text-categorization domain. Unluckily, those\ntechniques are neither geared to process continuous real valued data nor to account for multi-class classification tasks. That is why this thesis extends those strategies to cope with cardinal data by creating new measures of effect size and novel strategies of combining them with regard to a multi-class scenario.\nThe final classifier employs support vector machines. During the course of the thesis, over 200,000 wellconfigured SVMs had to be calculated for certain experiments, necessitating 400 configurations per SVM to be tested. This number could be reduced to 7 by revealing a yet undiscovered connection between the optimal kernel-band-width for time-histograms and the optimal configuration of non-linear SVMs, saving much computation time.\nSeit Jahrzehnten dient das Elektrokardiogramm, kurz EKG, der Diagnose von Herzkrankheiten. Sowohl\nim Operationssaal als auch auf der Intensivstation helfen schon heute computergest\u00fctzte Analysen des\nEKGs die Therapie der Patienten zu verbessern. Bisher werden jedoch vor allem die Herzrate und nur\nauf kurzen Zeitskalen die Morphologie der einzelnen Herzschl\u00e4ge untersucht. Um die Morphologie der\nHerzschl\u00e4ge auch \u00fcber Tage, Wochen oder gar Monate zuverl\u00e4ssig bewerten zu k\u00f6nnen, sind hochleistungsf\u00e4hige EKG-Klassifikatoren notwendig, welche jedem Herzschlag automatisiert einen bestimmten Typ zuordnen.\nDer aktuelle Stand der Forschung bez\u00fcglich EKG-Klassifikatoren und die damit verbundene \u201cFrontier\nof research\u201d lassen sich nur schwer bestimmen. Zum einen wird kein einheitliches G\u00fctema\u00df verwendet.\nZum anderen variiert die Datenbasis auf der diese G\u00fcte erzielt wurde stark. Darum wurde in dieser Arbeit erstmal ein einheitliches G\u00fctema\u00df f\u00fcr 72 Klassifikatoren angewendet und die Datenbasis mit Hilfe des neu-entwickelten \ufffd -Score\u2019s bewertet. Dieser ist ein Ma\u00df f\u00fcr die Wahrscheinlichkeit, dass die angegebene G\u00fcte auch unter Realbedingungen erreicht werden kann.\nDer EKG-Klassifikator der in dieser Arbeit vorgestellt werden soll, geht \u00fcber den aktuellen Stand der\nForschung deutlich hinaus. Hierzu werden zun\u00e4chst 1,4 Millionen EKG-Merkmale extrahiert und anschlie\u00dfend mit neuen Verfahren zur Merkmalsselektion auf 20-60 Merkmale reduziert. Bei der Extraktion kommen sowohl bekannte Methoden wie dieWavelet-Transformation und Zeitreihenanalysen, sowie neuartigen Verfahren zur Auswertung des zweidimensional-segmentierten Bispektrums, der statischen Beschreibung des EKGs und der Verwendung von Partitionsfunktionen und relationalen Merkmalen zum Einsatz. Weiterhin werden erstmals Merkmale aus 3 verschiedenen Segmenten mit unterschiedlichen Normalisierungen extrahiert. Typischerweise wird nur ein festes EKG-Segment f\u00fcr jeden Herzschlag verwendet.\nDie geeignetsten Verfahren zur Merkmalsselektion f\u00fcr diese Menge an Daten stammen aus dem Bereich der Textkategorisierung (z.B. Spam-Filter). Ungl\u00fccklicherweise sind diese Techniken nicht f\u00fcr kardinale Daten ausgelegt und funktionieren vor allem bei bin\u00e4ren Problemen (Spam bzw. kein-Spam). Daher erweitert diese Arbeit die Textkategorisierungstrategien auf reelle Daten und schl\u00e4gt Methoden vor, um diese in meinem multiklassen Szenario (normaler Herzschlag, Arrhythmie 1-12) einsetzen zu k\u00f6nnen.\nDie endg\u00fcltige Klassifikation geschieht mittels Support Vector Machines. Im Verlauf der Arbeit mussten\nf\u00fcr die verschiedensten Experimente insgesamt \u00fcber 200.000 optimal konfigurierte SVMs erstellt werden.\nJede einzelne SVM m\u00fcsste normalerweise mit ca. 400 Konfigurationen getestet werden. Diese Zahl\nkonnte auf 7 und die Gesamtrechenzeit deutlich reduziert werden. Dies war m\u00f6glich durch dass Ausnutzen eine bislang unbekannten Zusammenhangs zwischen der optimalen Kernel-Bandbreite bei Zeithistogrammen und der optmimalen Konfiguration der nicht-linearen SVM.\n2013-12-08\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4031\nurn:nbn:de:bvb:29-opus4-40312\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-40312\nhttps://opus4.kobv.de/opus4-fau/files/4031/ChristianRockstrohDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4039\n2020-09-21\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:510\nccs\nccs:G.\npacs\npacs:78.20.-e\nmsc\nmsc:35L65\nmsc:35Q70\nmsc:35R09\nmsc:74S10\nmsc:93C20\nopen_access\nopen_access:open_access\ncollections\ncollections:FAUUP\ninstitutes\ninstitutes:Nat_Mathematik\nOptimization of Particle Synthesis - New Mathematical Concepts for a Controlled Production of Functional Nanoparticles\nOptimierung in der Partikelsynthese - Neue mathematische Konzepte f\u00fcr eine gezielte Herstellung funktionaler Nanopartikel\nGr\u00f6schel, Michael\nOptimale Kontrolle\nNichtlineare Kontrolltheorie\nNichtlineare Optimierung\nNumerische Mathematik\nMathematische Modellierung\nNanopartikel\nProzessoptimierung\nParameteridentifikation\nTeilchentechnologie\nSteife nichtlineare Differentialgleichung\nBilineares System\nAdjungierte Differentialgleichung\nFinite-Volumen-Methode\nVerteilungsfunktion\nGlattheit Mathematik\nddc:510\nEmbedded in an interdisciplinary research project, the present work investigates the modeling, simulation, and optimization of specific processes for the production of functional nanoparticles. The examined material systems represent a selection of important core building blocks for future nanotechnologies. Depending on the application, a well-defined particle size distribution of the final product is required. The involved mechanisms (e.g. reaction, growth, ripening, and agglomeration) are described in the modeling by a hyperbolic partial integro-differential equation coupled to one or more ordinary differential equations. The successful validation against experimental data combined with the techniques of optimal control theory thereby allow for a targeted manipulation of the underlying processes. The developed methods establish in many cases for the first time a systematic approach for the production of tailor-made nanoparticles.\n2013-12-09\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4039\nurn:nbn:de:bvb:29-opus4-40395\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-40395\n978-3-944057-12-5\nhttps://opus4.kobv.de/opus4-fau/files/4039/DissertationMichaelGroeschel.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4071\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nccs\nccs:C.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nSelf-organization and Optimization of Priority-based Communication Buses\nZiermann, Tobias\norganic computing\nCAN-Bus\nddc:000\nWith the increasing complexity in distributed embedded systems, the communication between the nodes becomes an essential part. A popular approach is to connect components by using priority-based communication buses. Due to cost and physical constraints, however, the amount of communication is limited.\nThis thesis pushes the limitations of priority-based communication buses by two methods: Access control and data rate increase. The access control targets two weaknesses of priority-based communication buses. First, it does not allow\na fair distribution of bandwidth for equally important message streams. Second, the response time for messages with low priority increases with high bus utilization. Based on a game-theoretic model, a reinforcement learning algorithm\nis proposed that uses simple local rules to establish fair bandwidth sharing.\nFurthermore, a dynamic o\ufffd set adaptation algorithm for scheduling messages is introduced that may reduce the average response times.\nThis in turn allows to increase the utilization while maintaining the same response times.\nBoth algorithms are executed fully distributed during run-time, require little computational e\ufffd ort and no additional communication. An increase of data rate on protocol level is proposed by extending the Controller Area Network (CAN). By sending additional data in those time slots where CAN-conform nodes do not listen, devices not dedicated to these boosted data rates can still be used. Prototype implementations of all introduced methods demonstrate the feasibility and gains in \ufffd exibility, response times and data rate on a real physical setup.\n2013-12-17\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4071\nurn:nbn:de:bvb:29-opus4-40710\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-40710\nhttps://opus4.kobv.de/opus4-fau/files/4071/TobiasZiermannDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4117\n2018-11-12\ndoc-type:masterThesis\nbibliography:false\nddc\nddc:006\nccs\nccs:J.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nPerformance Evaluation of the Intel Many Integrated Core Architecture for 3D Image Reconstruction in Computed Tomography\nHofmann, Johannes\nHochleistungsrechnen\nddc:006\nThe computational effort of 3D image reconstruction in Computed Tomography (CT) has required special purpose hardware for a long time. Systems such as custom-built FPGA- systems and GPUs are still widely-used today, in particular in interventional settings, where radiologists require a hard time constraint for reconstruction. However, recently is has been shown that today even commodity CPUs are capable of performing the reconstruction within the imposed time-constraint.\nIn this thesis, we examine the Intel Many Integrated Cores (MIC) architecture for its suit- ability to run the Feldkamp-Davis-Kress (FDK) algorithm\u2014the most commonly used algo- rithm to perform the 3D image reconstruction in cone-beam computed tomography. In com- parison to traditional CPUs the MIC accelerator card, which focuses on numerical applica- tions, is expected to deliver higher performance using the same programming models such as C, C++, and Fortran.\nA thorough analysis of the MIC architecture is performed to determine potential hardware bottlenecks and to distinguish its design from a current state of the art two-socket Intel Sandy Bridge EP CPU system.\nWe study the challenges of efficiently parallelizing the FDK kernel on the Intel MIC and find that careful OpenMP scheduling and thread placement is required due to lack of a shared last level cache. Efficient data sharing on the Intel MIC can only occur between hardware threads of a core via its local L1 and L2 cache segments.\nApart from parallelization, SIMD vectorization is critical for good performance on the In- tel MIC, whose vector registers are twice the size of vector registers found in contemporary CPUs. To classify the difficulty of harnessing the full potential of vectorization on the MIC platform we explore various approaches to vectorize the kernel: Auto-vectorization using the Intel C Compiler and the Intel SPMD Compiler, as well as manual vectorization using C with intrinsics and manual assembly coding.\nWe used the fastest available CPU implementation from Treibig et al., developed for the RabbitCT benchmarking framework, as starting point for our optimizations. By making im- provements to the original implementation, we speed up execution by 25% on the CPU. In line with the estimate of our performance model, measurements on the Intel MIC deliver a speedup of 1.5 in comparison to the reference CPU system. Our analysis reveals the major bottleneck for our application to be shortcomings in hardware: The majority of data re- quired for the reconstruction is scattered in memory; gathering this data into vector registers for processing is still done sequentially on the Intel MIC. While computations in the kernel benefit from vectorization, the sequential loading limits the maximum achievable speedup in accordance with Amdahl\u2019s law.\n2014-01-16\nmasterthesis\ndoc-type:masterThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4117\nurn:nbn:de:bvb:29-opus4-41174\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-41174\nhttps://opus4.kobv.de/opus4-fau/files/4117/thesis.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4195\n2018-11-12\ndoc-type:workingPaper\nbibliography:false\nddc\nddc:000\nddc:070\nccs\nccs:H.4\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Phil_Medienwiss\nContent-Management-Systeme in Fachverlagen: Ergebnisse einer empirischen Erhebung\nHagenhoff, Svenja\nHagenhoff, Svenja\nCross Media Publishing\nContent Management\nMedienwirtschaft\nFachverlag\nddc:000\nddc:070\nThe development and differentiation of devices and technologies (tablets, smart-phones) which are suitable for reception of contents has led to cross-media distribution of contents by the publishing houses. Once created, content is made available parallel in different media channels electronically but also in printed form, an idea which is known as cross media publishing. In order to be able to produce media for reading in different variants, high-performance IT-supported processes are needed that allow an automated generation of different variants. Content Management Systems (CMS) fulfill this purpose. In research, the topic is already covered since the early 2000s, when XML technology emerged. This raises the question of whether cross-media publishing strategies are already being practiced in the publishing practice, and if so, if they are supported by suitable software systems. Discussions with representatives of the industry gave rise to the assumption that cross-media publishing is still not widespread. Where already content through different channels is distributed, system solutions are not yet a standard. Information on the actual use of these systems in publishing houses is missing. This information gap is to be reduced with this empirical study. The focus of the investigation is on German-language special interest publishers.\n2014-02-06\nworkingpaper\ndoc-type:workingPaper\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4195\nurn:nbn:de:bvb:29-opus4-41954\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-41954\nhttps://opus4.kobv.de/opus4-fau/files/4195/EBM%202014_01%20Crossmediales%20Publizieren.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4259\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\npacs\nmsc\nmsc:01-02\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nBernhard Nathanael Gottlob Schreger (1766-1825). Leben und Werk\nSchmidt, Cornelia Marlen\nPerson als Schlagwort[SWP]\nddc:610\nGegenstand dieser Arbeit ist das Leben und Werk des Chirurgen Bernhard Nathanael Gottlob Schreger (1766-1825). Nachdem Schreger in Leipzig sein Medizinstudium absolviert hatte, erreichte ihn 1793 ein Ruf nach Altdorf, wo er vier Jahre eine au\u00dferordentliche Professur f\u00fcr Anatomie, Chirurgie und Geburtshilfe innehatte. Anschlie\u00dfend nahm er einen Ruf als ordentlicher Professor f\u00fcr Medizin und Chirurgie in Erlangen an. Hier verbrachte er die letzten 28 Jahre bis zu seinem Lebensende.\nW\u00e4hrend Schregers Schaffenszeit wurden ihm einige Ehrungen zuteil. Im Jahre 1804 fand seine Ernennung zum preu\u00dfischen Hofrat statt; zahlreiche wissenschaftliche Gesellschaften, darunter die \u201eKaiserlich Leopoldinische Gesellschaft der Naturforscher\u201c, w\u00e4hlten ihn zum Mitglied. Dar\u00fcber hinaus z\u00e4hlte er 1808 zu den Mitbegr\u00fcndern der \u201ePhysikalisch-medizinischen Soziet\u00e4t\u201c in Erlangen.\nSchreger war Zeuge einer Umbruchzeit um die Wende des 18. zum 19. Jahrhundert. Ver\u00e4nderungen auf kultureller, staatlicher und sozialer Ebene zogen Neuerungen in einem Land nach sich, das sich vom Heiligen R\u00f6mischen Reich deutscher Nation zum Deutschen Bund mit einer eigenen Verfassungsgrundlage entwickelt hatte. Die Reichsstadt N\u00fcrnberg und das dazu geh\u00f6rige Altdorf sowie die brandenburgisch-preu\u00dfische Kleinstadt Erlangen kamen in diesem Zeitraum unter die neue bayerische Krone. Unter den Gedanken der Aufkl\u00e4rung setzte die Industrialisierung ein und die Welt der Naturwissenschaften wurde neu geordnet. Zudem zeichnete sich eine verst\u00e4rkte Hinwendung zur Wissenschaft und zu einer Forschung ab, die durch Empirismus und Besinnung auf den eigenen Verstand praktiziert werden sollte. Naturgesetze sollten experimentell jederzeit reproduzierbar sein. Sowohl dieser neue Wissenschaftsbegriff als auch ein ver\u00e4ndertes Gesundheitsbewusstsein revolutionierten die Medizin. Die Bildungsreformen in Preu\u00dfen von Wilhelm von Humboldt (1767-1835) und seine Denkschrift \u00fcber die Organisation des Medizinalwesens von 1809 trieben diese Prozesse voran. So ist es nicht verwunderlich, dass Schregers Zeitgenossen diese Periode als einen Zeitraum der rasanten Entwicklung erlebten.\nSchreger hinterlie\u00df zahlreiche Kasuistiken, die einen Blick in seine T\u00e4tigkeit als Chirurg zulassen. Anhand dieser detailgetreuen Fallbeschreibungen ist es m\u00f6glich, die Arbeit eines Chirurgen um die Jahrhundertwende vom 18. zum 19. Jahrhundert nachzuvollziehen. Schreger entwickelte nicht nur neue Operationsmethoden, sondern auch neuartige Instrumente, mit denen er Operationsabl\u00e4ufe verbessern wollte. Unter den damaligen Umst\u00e4nden bestritten Schreger und seine Kollegen s\u00e4mtliche Eingriffe ohne Asepsis, Antisepsis oder An\u00e4sthesie. Viele Operationen bargen deswegen ein hohes Risiko, da immer die Gefahr von Blutverlust, Infektionen und Traumata bestand.\nIn dieser Arbeit werden neben Schregers therapeutischen T\u00e4tigkeiten neue Aspekte untersucht, die ihn als facettenreichen Gelehrten auszeichnen. Sein Schaffen als Wissenschaftler und Universit\u00e4tslehrer tragen zu dieser Vielseitigkeit bei. Ein Chirurg, der neben seiner therapeutischen T\u00e4tigkeit wissenschaftlich publizierte und als akademischer Lehrer an der Universit\u00e4t arbeitete, erscheint aus heutiger Sicht nicht au\u00dfergew\u00f6hnlich. Schreger befand sich jedoch in einer Umbruchzeit, in der die Chirurgie erst im Begriff war, sich zur Wissenschaft zu entwickeln; ein akademischer Chirurg war keinesfalls die Regel. Korrelierend zu diesem Prozess hatte Schreger seine beachtliche Karriere in dieser Zeit des allgemeinen Wandels aufbauen k\u00f6nnen.\nSchreger lebte in einer Zeit, in der die Medizin von unterschiedlichen wissenschaftlichen Str\u00f6mungen beeinflusst wurde. Die Lehren des schottischen Arztes John Brown (1735-1788), des Vitalisten William Cullen (1710-1790) und des italienischen Physiologen und Naturwissenschaftlers Luigi Galvani (1737-1798) wurden von Schreger aufmerksam studiert. Die Humoralpathologie entwickelte sich zur Solidarpathologie und die Anatomie r\u00fcckte bei den Chirurgen in den Blickpunkt des Interesses. Schreger unternahm seit Beginn seiner Schaffenszeit anatomisch-pathologische Nachbeurteilungen mit Hilfe von Sektion und Autopsie. Als \u201eBegr\u00fcnder der chirurgischen Anatomie\u201c, wie Heinrich Rohlfs (1827-1898) ihn nannte, legte er zudem Wert auf Kenntnisse in der Zergliederungskunst als wichtige Voraussetzung f\u00fcr die sichere Aus\u00fcbung der Chirurgie. Robert Campbell best\u00e4tigte Ende des 19. Jahrhunderts, dass der junge Chirurg ein \u201eakkurater Anatom\u201c sein m\u00fcsse und nicht spekulativ sondern praktisch die Anatomie zu betreiben habe: \u201eAndernfalls muss er sich als blo\u00dfer St\u00fcmper erweisen.\u201c\nIn der ersten H\u00e4lfte des 19. Jahrhunderts etablierte sich die Form der Forschungsuniversit\u00e4t, an der \u00fcberliefertes Wissen nicht nur weitergegeben, sondern auch durch neue Erkenntnisse erweitert wurde. Die Universit\u00e4ten in Leipzig, Altdorf und Erlangen stellten f\u00fcr Schreger eine Plattform dar, auf der er wissenschaftliche Kenntnisse sowohl erlangen, als auch lehren konnte. Das Erlanger Clinicum chirurgicum als universit\u00e4re Institution erm\u00f6glichte die Fortf\u00fchrung seiner Lehrt\u00e4tigkeit. Schregers Gr\u00fcndung dieser ersten chirurgischen Klinik in Erlangen mit Hilfe von Friedrich Wendt (1738-1818) im Jahr 1815 geh\u00f6rt zweifellos zu seinen nachhaltigsten Taten. Auf diese Weise schuf er eine chirurgische Ausbildungsst\u00e4tte f\u00fcr den praxisnahen Unterricht am Krankenbett. Das Clinicum chirurgicum wurde die erste klinische Institution der Fakult\u00e4t. Au\u00dferdem er\u00f6ffnete sich f\u00fcr Schreger durch das Clinicum chirurgicum die M\u00f6glichkeit, wertvolle wissenschaftliche Erfahrungen zu sammeln, die er publizieren konnte.\nIn dieser Arbeit wird die M\u00f6glichkeit ergriffen, das umfangreiche Schaffen eines Universit\u00e4tschirurgen darzustellen, der sich im Spannungsfeld zwischen der alten Chirurgie und der Chirurgie einer neuen Epoche befand. Als akademischer Chirurg und klinischer Universit\u00e4tslehrer war er nicht mehr Teil der alten Chirurgie, w\u00e4hrend er den Durchbruch der neuen Chirurgie mit An\u00e4sthesie und Asepsis gleichwohl nicht mehr erlebte. Gleichzeitig befand er sich in einer Umbruchzeit, die sich \u00fcber die wissenschaftliche Ebene hinaus auf s\u00e4mtliche gesellschaftlichen und wirtschaftlichen Bereiche ausbreitete. Vor diesem Hintergrund soll Schregers T\u00e4tigkeit als Lehrer, Wissenschaftler und schlie\u00dflich als chirurgischer Therapeut beleuchtet werden. Die Betrachtung Schregers aus diesen verschiedenen Blickwinkeln erm\u00f6glicht letztendlich einen vielschichtigen Eindruck von Leben und Werk eines akademischen Chirurgen um 1800.\nSubject of this thesis is the life and opus of the surgeon Bernhard Nathanael Gottlob Schreger (1766-1825). Having completed his studies at the University in Leipzig, Schreger got a reputation to Altdorf in 1793, where he held for four years a non-tenured professorship in anatomy, surgery and midwifery. After this, he took up a position as a tenured professor for medicine and surgery in Erlangen. There, he spent the rest of his life.\nAlong the way, he has received a numerous honours. In the year 1804 took place his appointment to the prussian court counsellor; a large number of scientific societies, including the \u201cKaiserlich Leopoldinische Gesellschaft der Naturforscher\u201c elected him as a member. Furthermore, he was one of the founders of the \u201ePhysikalisch-medizinischen Soziet\u00e4t\u201c in Erlangen.\nSchreger witnessed a time of radical change at the turn of the 19th century. Changes at the cultural, social and political level resulted innovations in a country, that had developed from the Holy Roman Empire of the German Nation to the German Federation with their own constitutional foundation. At this time, the bavarian reign ruled the imperial city Nuremberg and the associated Altdorf as well as the brandenburgisch-prussian provincial town Erlangen. With the idea of the Enlightenment the industrialisation began and the world of the natural sciences has been reordered. In addition, a stronger interest in a science and research, that should be practised by empiricism and reflection on the own mind, became apparent. Natural laws should have been reproducible at any time. Both, this new concept of science and a changing health-consciousness revolutionised the medicine. The education reforms in Prussia of Wilhelm von Humboldt (1767-1835) and his memorandum on the organisation of the medical care (1809) pushed these processes. It is therefore not surprising that Schregers contemporaries experienced this time as a period of rapid development.\nSchreger has left numerous casuistries, that allow a view in his activity as a surgeon. On the basis of these case descriptions it is able to relate to the work of a surgeon at the turn of the 19th century. Schreger developed not only new operational techniques, but also new instruments to improve surgical procederes. Under the circumstances of the time all operations have been contested bey Schreger and his colleagues without asepsis, antisepsis and anaesthesia. Many operations involved a high risk because of the always existent danger of blood loss, infection and trauma.\nAdditionally to Schregers therapeutic skills, new issues, that show his function as a manifold scholar, are discussed in this thesis. His work as a scientist and professor contribute to this versatility. From today's point of view, a surgeon, who published scientific papers and worked as a professor beside his therapeutic activities, seems not unusually. However, Schreger was in a time of transition, when surgery was about to develop to a science; an academic surgeon was not the norm, at all. Correlated to this process, Schreger was able to develop a remarkable academic career.\nSchreger lived in a time, when different scientific approaches affected the medicine. The teachings of the Scottish physician John Brown (1735-1788), of the vitalist William Cullen (1710-1790) and of the Italian Physiologists and natural scientist Luigi Galvani (1737-1798) have been carefully evaluated by Schreger. The humoral pathology developed to the solidism and for the surgeons, the anatomy moved into the centre of attention. Since the beginning of his work, Schreger undertook anatomo-pathological revisions by means of autopsies. As the \u201cfounder of surgical anatomy\u201d, as Heinrich Rohlfs (1827-1898) called him, he placed value in knowledges in anatomy as an essential precondition for a succesful surgery. Robert Campbell confirmed at the end of the 19th century, that the young surgeon must be an accurate anatomist, otherwise he would be a real bumbler.\nIn the first half of the 19th century, the form of research university was established. There, traditional knowledge was not only handed down but also expanded with further knowledge. For Schreger, the universities of Leipzig, Altdorf and Erlangen were a platform, on which he could both, gaining scientific knowledge and doing his lessons as a professor. The Erlanger \u201cClinicum chirurgicum\u201d as an academic institution made the continuation of his teaching activities possible. Without a doubt, Schregers foundation of this first surgical clinic in Erlangen by means of Friedrich Wendt (1738-1818) in the year 1815 was one of his most sustainable achievements. That way, he created a surgical educational institution for bedside-teaching. The Clinicum chirurgicum became the first clinical institution of the faculty. Furthermore, for Schreger the Clinicum chirurgicum opend up the possibility to gather scientific experiences, he could publish. This thesis shows the extensive work of a university surgeon, who was beeing pulled between the old surgery and the surgery of a new age. As an academical surgeon and clinical professor he wasn\u2019t part of the old surgery anymore, nevertheless, he did not live to see the breakthrough of the new surgery with anesthesia and asepsis. At once, he was in a period of transition, that influenced beyond the scientific level the social and economic field. Against this backdrop, Schregers activity as a teacher, scientist and, finally, surgical therapist should be illuminated. Analysing Schreger from different points of view enables a complex impression of the life and work of an academic surgeon around 1800.\n2014-02-14\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4259\nurn:nbn:de:bvb:29-opus4-42599\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-42599\nhttps://opus4.kobv.de/opus4-fau/files/4259/Ver%C3%B6ffDrArbeitSchmidt.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4290\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\npacs\npacs:87.55.ne\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nH\u00e4ufigkeit und Ausma\u00df von Wurzelspitzenresorptionen nach festsitzender Therapie mit Multibracketapparatur - eine retrospektive Studie\nPrevalence and degree of root resorption after treatment with fixed orthodontic appliance - a retrospective study\nR\u00f6hr, Julian\nWurzelspitzenresorption\nKieferorthop\u00e4die\nfestsitzende Therapie\nMultibracketapparatur\nH\u00e4ufigkeit\nAusma\u00df\nddc:610\nEinleitung:\nDer steigende Bedarf an kieferorthop\u00e4dischen Behandlungen, sei es aus \u00e4sthetischen, prophylaktischen oder funktionellen Gr\u00fcnden, erfordert ein stetiges Bem\u00fchen um Verbesserung und Weiterentwicklung kieferorthop\u00e4discher Ger\u00e4te. Der Wunsch nach einer m\u00f6glichst schnellen und effizienten Therapie erfordert Kenntnisse der verwendeten Apparatur, deren Wirkungen und Gefahren. Vor diesem Hintergrund will die vorliegende Arbeit die Qualit\u00e4t der Behandlungsergebnisse sowie H\u00e4ufigkeit und Ausma\u00df von Wurzelresorptionen nach Behandlung mit festsitzender Apparatur pr\u00fcfen.\nMaterial und Methode:\nF\u00fcr die Studie wurden 104 Patienten herangezogen, die sich im Zeitraum von 2004 bis 2009 an der ZMK-Klinik der Universit\u00e4tsklinik Erlangen einer kieferorthop\u00e4dischen Behandlung mit festsitzenden Apparaturen unterzogen. An Befundinformationen standen Patientenakte, Kiefermodelle und R\u00f6ntgenbilder zur Verf\u00fcgung. Diese wurden in einem Erhebungsbogen aufgenommen und statistisch ausgewertet.\nErgebnisse:\nDer durchschnittliche Resorptionswert aller untersuchten Front- und Eckz\u00e4hne des Oberkiefers betrug 1,04 Millimeter bei einer mittleren Behandlungszeit von etwa zwei Jahren (23,6 Monate). Die Zahl der Patienten mit dentalem Tiefbiss nahm w\u00e4hrend der Behandlung ab. Au\u00dferdem wurde f\u00fcr alle Patienten mit dental offenem Biss eine Korrektur erreicht. Die Mittelwerte des Overjets und Overbites konnten in den Normbereich (2-3 mm) \u00fcberf\u00fchrt werden.\nEine l\u00e4ngere Behandlungsdauer ging mit einer erh\u00f6hten Resorption einher. \u00c4hnlich beg\u00fcnstigte der Einsatz von Gummiz\u00fcgen das Fortschreiten von Resorptionen. Bei Betrachtung der Okklusionsverh\u00e4ltnisse zeigte sich, dass Wurzelspitzen von Klasse ll \u2013 (1,14 mm) und Klasse lll \u2013 Patienten (1,06 mm) signifikant ausgepr\u00e4gter betroffen waren als jene von Neutralokklusionen (0,77 mm).\nVorhandene Zahnwurzelanomalien f\u00fchrten hingegen zu keiner signifikant st\u00e4rkeren Wurzelresorption.\nSchlussfolgerung:\nEntz\u00fcndlich bedingte Wurzelresorptionen sind als iatrogene Folgeerscheinung einer kieferorthop\u00e4dischen Behandlung m\u00f6glichst gering zu halten. Es gilt den Patienten vor Therapiebeginn auf alle Risiken aufmerksam zu machen. Korrigierende Zahnbewegungen an Patienten mit ung\u00fcnstigerer Okklusion und ausgepr\u00e4gten Fehlbissen sind unter gr\u00f6\u00dfter Wachsamkeit durchzuf\u00fchren.\nPatienten, die im Laufe der Behandlung erste klinisch signifikante Resorptionen entwickeln, sind engmaschig zu \u00fcberwachen. Je nach individueller Situation m\u00fcssen Behandlungsziele neu \u00fcberdacht werden.\nIntroduction:\nThe increasing need for orthodontic treatments due to esthetic, prophylactic or functional reasons requires a continuous effort to improve and refine orthodontic equipment. Because of the desire for a very fast and efficient treatment profound knowledge of the applied orthodontic devices together with their effect and risk is needed. Against this background the aim of this research is to examine the quality of treatment outcomes and furthermore to check prevalence and degree of root resorption after treatment with fixed orthodontic appliance.\nMaterial and methods:\n104 patients orthodontically treated with fixed appliances between 2004 and 2009 at the University Dental Hospital Erlangen were included in this study. Patient records, jaw models and radiographs were available. The data was recorded and evaluated statistically.\nResults:\nThe mean treatment time was about two years (23.6 month). The number of patients with deep bite decreased during treatment. Furthermore for all patients with open bite an adjustment was achieved. The means of overjet and overbite could be modulated into normal range (2-3 mm).\nA longer treatment time went along with increasing resorptions. Alike the use of elastics pushed on the progression of resorptions. Regarding the occlusion of angle class II patients (1.14 mm) and angle class III patients (1.06 mm) the apical roots showed stronger resorption of root surfaces.\nHowever abnormal root shape did not lead to significantly stronger root resorption.\nConclusion:\nInflammatory root resorptions as an iatrogenic effect should be minimised. It is essential to advise the patient in advance of all possible risks of orthodontic treatment. Special attention needs to be payed to patients with distinctive malocclusion when using corrective tooth movement in orthodontics.\nPatients who show initiating clinical significant resorptions during treatment should be monitored carefully. Depending on the individual situation the treatment aims should be reconsidered.\n2014-02-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4290\nurn:nbn:de:bvb:29-opus4-42901\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-42901\nhttps://opus4.kobv.de/opus4-fau/files/4290/Dissertation%2CEndversion.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nd/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4297\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nddc:660\nccs\nccs:B.\npacs\npacs:00.00.00\nmsc\nmsc:28-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Chemieing\nIn-situ Diagnostik von Zeolithbildungsprozessen auf Basis der Messung von Ultraschalld\u00e4mpfung und -geschwindigkeit\nIn-situ diagnostics of zeolite formation processes based on the measurement of Ultrasonic attenuation and velocity\nBaser, Hasan H\u00fcseyin\nUltraschall\nIn-situ\nKinetik\nZeolith A\nddc:620\nddc:660\nIn dieser Arbeit wurden die Synthesen von Zeolithen mittels Messung von US-D\u00e4mpfung und -geschwindigkeit untersucht. Die Zielstellung der Arbeit war zu \u00fcberpr\u00fcfen, ob Ultra-schall als eine diagnostische Methode zur Prozesskontrolle der Zeolithsynthese eingesetzt werden kann und aus den in-situ Ultraschallergebnissen kinetische und reaktionsmechanisti-\nsche Informationen erhalten werden k\u00f6nnen.\nAusgew\u00e4hlt wurden die hydrothermalen Synthesen von Zeolith A nach dem Sol-Gel-Prozess und die kolloidalen Synthesen von Zeolith A und Silikalith-1 L\u00f6sungen sowie die solvothermale Synthese von CuBTC.\nDie in-situ Ultraschalldaten korrelierten in den Sol-Gel-synthesen mit der Entwicklung der Kristallinit\u00e4t und in den kolloidalen Synthesen mit der Zunahme der Produktmenge.\nDie US-Geschwindigkeit reagierte mehr auf die chemischen und physikalischen \u00c4nderungen (z.B. Ionenkonzentrationen, die Dichte und die Temperatur) in der fl\u00fcssigen Phase. Die US-\nD\u00e4mpfung korrelierte besser die \u00c4nderungen in der festen Phase.\nSomit bietet die Anwendung des Ultraschalls als eine in-situ diagnostische Methode die M\u00f6glichkeit, die \u00c4nderungen des Synthesezustandes einer Zeolithkristallisation sowohl in der\nfl\u00fcssigen Phase als auch in der festen Phase w\u00e4hrend des gesamten Kristallisationsvorganges beobachten zu k\u00f6nnen.\nIn Kristallisationsprozessen von Zeolithen bildet sich oftmals zun\u00e4chst eine metastabile Phase, die sich schlie\u00dflich in eine unerw\u00fcnschte stabilere Phase umwandelt. Aus diesem\nGrund w\u00e4re es w\u00fcnschenswert, den Kristallisationsprozess der Synthese auf eine kosteng\u00fcnstige, robuste und einfacher Weise zu verfolgen. Bei allen Synthesen in dieser Arbeit\nkonnten der Anfang und das Ende der Kristallisation sowie die Dauer der Inkubations- bzw. Kristallisationsdauer exakt in-situ detektiert werden. Somit eignet sich die Methode zur Prozesskontrolle nicht nur in Laborsynthesen sondern auch in den industriellen Gro\u00dfanlagen.\nDurch die erhaltene hohe Anzahl an Datenpunkten konnten die kinetischen Daten wie die Aktivierungsenergie und Reaktionsgeschwindigkeit bzw. Ordnung auf einfacher und sicherer Weise mit geringem Fehler erhalten werden. Durch die in-situ Analyse wurde die Synthese nicht gest\u00f6rt und die Daten konnten trotz der gro\u00dfen Menge schnell ausgewertet werden,\nwas man f\u00fcr eine ex-situ Analyse gebrauchen h\u00e4tte. Somit konnte gezeigt werden, dass die Ultraschalldiagnostik auch als ein Tool zur Erhaltung von kinetischen Daten eingesetzt werden kann.\nObwohl die hier angewendete Ultraschalltechnik keine struktursensitive Analysemethode ist, konnte man aus den Ergebnissen wichtige Information auch \u00fcber den Verlauf der Kristallisationsentwicklung erhalten werden.\nDurch den Einsatz der Ultraschallmessung in dieser Arbeit konnten wichtige Informationen zu den Kristallisationsvorg\u00e4ngen von Zeolithen und MOF\u00b4s bekommen und ein besseres Verst\u00e4ndnis der Arbeitsweise des in-situ Ultraschallmonitorings erhalten werden.\nIn this work the synthesis of the zeolite by means of measurement of ultrasonic attenuation and velocity were examined. The aim of this study was to determine whether ultrasound can be used as a diagnostic method for process control of zeolite synthesis and if information regarding the reaction kinetics and mechanism can be obtained from the in - situ ultrasonic results.\nThe selected synthesis techniques were the hydrothermal synthesis of zeolite A, according to the sol gel and the colloidal synthesis route, the colloidal synthesis of silicalite-1 and the sol-\nvothermal synthesis of CuBTC. In situ ultrasonic data correlated in the sol-gel synthesis with the development of crystallinity and in the colloidal synthesis with an increase in the amount\nof product. The ultrasonic velocity was responding more to the chemical and physical changes (e.g. ion concentration, density and temperature) in the liquid phase. The ultrasonic atten-\nuation was found to correlate better with the changes in the solid phase. The applicability of ultrasound was thereby demonstrated as a diagnostic in-situ method to monitor the changes in the synthesis of zeolite crystallization in both the liquid phase and the solid phase during the whole crystallization process.\nIn crystallization processes of zeolites a metastable phase is often initially formed, which eventually transforms into an undesired stable phase. For this reason, it would be desirable\nto follow the crystallization of the synthesis process in a cost effective, robust and simple way. In all syntheses in this work, the beginning and the end of the crystallization and the\nduration of incubation and crystallization time could be accurately detected in situ. Thus, the method was found to be suitable for process control, not only in laboratory synthesis but also\nin the large industrial suitable.\nDue to the resulting high number of data points, kinetic data such as activation energy and reaction rate or order could be obtained in a simple and safe manner with minimal error. Due to the in - situ analysis, the synthesis was not disturbed and the data could be analyzed quickly in spite of the large data amount required for an ex - situ analysis. Thus, it could be shown that diagnostic ultrasound may also be used as a tool for maintenance of kinetic data.\nAlthough the ultrasound technology applied here is not a structure-sensitive method of analysis, important information could be obtained from the results concerning the course of crys-\ntallization development.\nThrough the use of ultrasound measurement in this work important information regarding the crystallization processes of zeolites and MOFs could be achieved and a better understanding\nof the operation of the in - situ ultrasonic monitoring are obtained.\n2014-02-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4297\nurn:nbn:de:bvb:29-opus4-42979\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-42979\nhttps://opus4.kobv.de/opus4-fau/files/4297/Dissertation_Hasan%20Baser-Final.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4340\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:B.0\npacs\npacs:00.00.00\nmsc\nmsc:94-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nEinfl\u00fcsse des Filterentwurfs auf die Empfangsqualit\u00e4t breitbandiger Funk\u00fcberwachungsempf\u00e4nger\nDehm-Andone, Gunther\nFilter <Elektrotechnik>\nFrequenzfilter\nFilterschaltung\nddc:620\nDiese Arbeit beschreibt Ans\u00e4tze zur Reduktion des Formfaktors, der Kosten und des\nImplementierungsaufwandes eines Empf\u00e4ngers zur Anwendung in COMINT1-Systemen.\nDabei werden eine gezielte Anforderungsanalyse und die dazu passenden L\u00f6sungskonzepte\neinzelner Baugruppen, speziell der Filterkomponenten, vorgestellt. Die markt\u00fcblichen\nFunktionalit\u00e4ten f\u00fcr das als Abh\u00e4nge- und Scanning-Empf\u00e4nger entworfene System wird\ndurch eine ausgewogene Balance sowohl zwischen Analog- und Digitalteil als auch bei der\nDimensionierung der Analogkomponenten bereitgestellt. Als wichtigste Parameter stellen\nsich dabei der sich \u00fcber zwei Dekaden erstreckende Eingangsfrequenzbereich und die\nerforderliche Echtzeitbandbreite und Signalqualit\u00e4t dar. Die gewonnene Systemtopologie\nund deren Umsetzung erm\u00f6glichen letztlich die Erschlie\u00dfung neuer Anforderungsaspekte\nin der Funk\u00fcberwachung. Eine kosteneffiziente Hardware solcher COMINT-Empf\u00e4nger,\nwie es der vorliegende KAIMAN2-Empf\u00e4nger ist, er\u00f6ffnet die Einsatzm\u00f6glichkeit in weitr\u00e4umigen\nund fl\u00e4chendeckenden Szenarien mit einer Vielzahl an Funk\u00fcberwachungsstationen.\nUm die performate Funktion des Empf\u00e4ngers zu erreichen, ist eine Filterung in verschiedenen\nStufen des Empf\u00e4ngers unumg\u00e4nglich. Diese beeinflusst jedoch die erw\u00e4hnte\nSignalqualit\u00e4t. Daher wird hier erstmalig eine umfangreiche Analyse des Einflusses\ndes Filterentwurfs und der Filterimperfektionen auf Systemparameter von COMINTEmpf\u00e4ngern\ndargestellt. Speziell auf den Signalqualit\u00e4tsindikator EVM3 wird hier Augenmerk\ngerichtet. Die Implementierung mittels optimaler Technologie und Topologie\nund die Validierung der daraus abgeleiteten Filterl\u00f6sungen wird im Anschluss pr\u00e4sentiert\nund zeigt, dass die gefundenen Umsetzungen dem Vergleich zu kommerziell erh\u00e4ltlichen\nFiltern standhalten und ihnen in einigen Punkten \u00fcberlegen sind. Auch der Vergleich\nkommerzieller COMINT-Empf\u00e4nger mit dem hier entwickelten KAIMAN-Empf\u00e4nger im\nGesamtsystem zeigt, dass eine vergleichbare Performanz bei kompakterem Formfaktor\nerzielt werden konnte.\nThis work describes approaches to reduce the form factor, costs and implementation effort\nof a receiver for application in COMINT4 systems. Thus, a purposeful requirement analysis\nand its suitable solution concepts for single modules, especially filter components, is\npresented. The customary functionality of the system, that is designed as monitoring and\nscanning receiver, is provided by a balance of the analog and digital part as well as among\nthe analog components. The most important parameters are the input frequency range\nof two decades and the required instantaneous bandwidth and signal quality. In the end,\nthe gained system topology and its realization permit the opening of new applications\nin the radio surveillance domain. A cost efficient hardware of COMINT receivers, such\nas the KAIMAN5 receiver, establishes the application possibility in large and inclusive\nscenarios with a multitude of surveillance stages.\nTo reach this functionality, filtering in several stages of the receiver is inevitable. However,\nthis influences the signal quality. Hence, for the first time a comprehensive analysis\nof the influence of filter design and filter imperfections on system parameters of COMINT\nreceivers is presented. Focus is layed on the signal quality indicator EVM6. The\nimplementation by means of optimal technology and topology and the validation of the\nderived filter solution is presented and shows that the offered realization sustains comparison\nwith commercially available filters and outclasses them in some points. Also the\ncomparison of the whole KAIMAN system with commercial COMINT systems shows\nthat comparable performance could be achieved with a more compact form factor.\n2014-03-13\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4340\nurn:nbn:de:bvb:29-opus4-43407\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-43407\nhttps://opus4.kobv.de/opus4-fau/files/4340/Dehm-AndoneDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4750\n2018-01-26\ndoc-type:report\nbibliography:false\nddc\nddc:000\nccs\nccs:J.m\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nFrom Computer Forensics to Forensic Computing: Investigators Investigate, Scientists Associate\nDewald, Andreas\nFreiling, Felix C.\nDigital Forensics\nIT-Forensics\nComputer Forensics\nddc:000\nThis paper draws a comparison of fundamental theories in traditional forensic science and the state of the art in current computer forensics, thereby identifying a certain disproportion between the perception of central aspects in common theory and the digital forensics reality. We propose a separation of what is currently demanded of practitioners in digital forensics into a rigorous scientific part on the one hand, and a more general methodology of searching and seizing digital evidence and conducting digital investigations on the other. We thereby mark out the route for computer forensics to turn into a true forensic science. To illustrate the feasibility of the proposed path, we supply a couple of practical examples, as well as a list of exemplary questions that should be answered by digital forensic scientists.\n2014-05-28\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4750\nurn:nbn:de:bvb:29-opus4-47508\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-47508\nhttps://opus4.kobv.de/opus4-fau/files/4750/computer_forensics_is_not_forensic_science.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4755\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:D.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nDiOS: Dynamic Privacy Analysis of iOS Applications\nKurtz, Andreas\nWeinlein, Andreas\nSettgast, Christoph\nFreiling, Felix\nApps\niOS\nPrivatsph\u00e4re\nDynamische Analyse\nddc:004\nWe present DiOS, a practical system to perform automated\ndynamic privacy analysis of iOS apps. DiOS provides a highly\nscalable and fully automated solution to schedule apps from the\nofficial Apple App Store for privacy analysis to iOS\ndevices. While apps are automatically executed, user interaction is\nsimulated using random and smart execution strategies, and sensitive API calls\nas well as network connections are tracked. We evaluated the system\non 1,136 of the most popular free apps from the iOS App Store and found out that almost 20% of all investigated apps are tracking users' locations on every app start, one third of all accesses to users' address books are attributed to apps from the social network category and almost half of all apps are tracking users' app usage behavior by incorporating tracking and advertising libraries.\n2014-06-03\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4755\nurn:nbn:de:bvb:29-opus4-47559\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-47559\nhttps://opus4.kobv.de/opus4-fau/files/4755/report.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4767\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:570\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nAPOBEC3G and Emergence of Pathogenic SIV\nAPOBEC3G und die Emergenz eines pathogenen SIVs\nKrupp, Annabel\nRetroviren\nddc:570\nIn nature, new diseases arise when existing viruses adapt to infect new host species. For example, pandemic human immunodeficiency virus type 1 (HIV-1) and AIDS are the result of such a cross-species transmission of a lentivirus from its reservoir (chimpanzees) to humans. Lentiviruses are abundant in African primates and pose a constant danger of additional transmissions to humans, due to the expansion of settlements and growing contact of primates and humans. Therefore the investigation of the origins, natural history and the influence of host genes on the transmission of these lentiviruses will help us to understand, and perhaps even predict future zoonoses.\nCellular restriction factors, which render cells intrinsically resistant to viruses, potentially impose genetic barriers to cross-species transmission and emergence of viral pathogens in nature. One such factor is APOBEC3G (A3G). To overcome A3G-mediated restriction, many lentiviruses encode Vif, a protein that targets A3G for degradation by the proteasome. As is typical for many restriction factor genes, primate A3G displays strong signatures of positive selection. This is interpreted as evidence that the primate A3G locus reflects a long-term evolutionary \u201carms-race\u201d between lentiviruses and their primate hosts. The emergence of SIVmac and outbreaks of simian AIDS in outbred colonies of macaques in the 1970s provides an unusual opportunity to examine the impact of specific, host-encoded restriction factors in the context of an emerging pathogen.\nThis study provides direct evidence that A3G functioned as a barrier to cross-species transmission, both suppressing viral replication and selecting for evolution of viral resistance mutations during emergence of SIVmac. Specifically, this study found that rhesus macaques have multiple, functionally distinct A3G alleles, and that emergence of SIVmac and simian AIDS required adaptation of the virus to evade APOBEC3G-mediated restriction. The evidence provided includes the first comparative analysis of APOBEC3G polymorphism and function in both a reservoir and recipient host species (sooty mangabeys and rhesus macaques, respectively), and identification of adaptations unique to Vif proteins of the SIVmac lineage that specifically antagonize A3G alleles found in rhesus macaque. Demonstrating that interspecies variation in a known restriction factor selected for viral counter-adaptations in the context of a documented case of cross-species transmission, lends strong support to the evolutionary \u201carms-race\u201d hypothesis. Importantly, this study confirms that A3G divergence can be a critical determinant of interspecies transmission and emergence of primate lentiviruses, including viruses with the potential to infect and spread in human populations.\nDie Entstehung neuer Krankheiten kann durch die Anpassung vorhandener Viren an neue Spezies geschehen. Das pandemische menschliche Immunodefizienzvirus Typ 1 (HIV-1) ist das Ergebnis einer solchen arten\u00fcbergreifenden \u00dcbertragung. Ein von Schimpansen stammendes Virus passte sich an seinen neuen Wirt, den Menschen, an und wurde damit zum Verursacher von AIDS. Bis heute scheint weder eine Heilung noch eine Impfung in greifbarer N\u04d3he. HIV verwandte Lentiviren sind reichlich in afrikanischen Primaten verbreitet und stellen durch die Expansion von menschlichen Siedlungen und dem damit vermehrten Kontakt zwischen Mensch und Tier eine st\u04d3ndige Gefahr der erneuten \u00dcbertragung auf Menschen dar. Daher ist die Erforschung der Urspr\u00fcnge, Geschichte und der Rolle von Wirtsgenen in der \u00dcbertragung von Lentiviren wichtig, denn es erweitert unser Verst\u04d3ndnis und kann uns vielleicht sogar erm\u04e7glichen zuk\u00fcnftige Zoonosen voraus zu sagen.\nZellul\u04d3re Proteine, welche die Virusreplikation hemmen k\u04e7nnen, werden als antivirale Restriktionsfaktoren bezeichnet und stellen m\u04e7glicherweise Barrieren zur arten\u00fcbergreifenden \u00dcbertragung von Viren dar und verhindern damit die Entstehung neuer viraler Krankheitserreger. Ein bekannter Restriktionfaktor ist APOBEC3G (A3G). Um den antiviralen Effekt von A3G zu hemmen, exprimieren Lentiviren das Vif-Protein. Durch die Interaktion von Vif mit A3G wird dieses f\u00fcr den Abbau durch das Proteasom markiert. Wie viele andere Restriktionsfaktorgene, zeigt der A3G-Lokus in Primaten eine starke Signatur positiver Selektion. Dies kann als Beweis f\u00fcr eine kompetetive Anpassung des A3G Genlokus an Lentiviren interpretiert werden. Die Entstehung von SIVmac und Ausbr\u00fcche von AIDS in unterschiedlichen Kolonien von Makaken in den 1970er Jahren bietet heute eine einmalige Gelegenheit die Auswirkungen von spezifischen Restriktionsfaktoren im Rahmen eines entstehenden Krankheitserreges zu untersuchen.\nDiese Studie liefert einen direkten Beweis daf\u00fcr, dass A3G tats\u04d3chlich als eine Barriere f\u00fcr eine arten\u00fcbergreifende \u00dcbertragung agierte, indem es die virale Replikation unterdr\u00fcckte und fuer die Entwicklung viraler Resistenzmutationen w\u04d3hrend der Entstehung von SIVmac selektionierte. Ein wichtiges Ergebnis dieser Studie ist, dass Rhesusaffen mehrere, funktionell unterschiedliche A3G Allele besitzen. Daher erforderte die Entstehung von SIVmac und AIDS in Affen eine Anpassung von Seiten des Virus\u2019 um der Restriktion von A3G zu entgehen. Diese Arbeit beinhaltet die erste Vergleichsanalyse von A3G Polymorphismen und deren Funktion sowohl im Reservoirwirt Russmangabe also auch im Empf\u04d3ngerwirt Rhesusaffe. Zudem wurden spezifische Anpassungen der Vif Proteine aus der SIVmac Linie identifiziert, welche in der Lage sind, spezifisch A3G Allele von Rhesusaffen zu antagonisieren. Die hier pr\u04d3sentierten Ergebnisse zeigen, dass genetische Variation in einem bekannten Restriktionsfaktor verantwortlich ist f\u00fcr die Gegenadaptation des Virus im Zuge einer dokumentierten arten\u00fcbergreifenden \u00dcbertragung, und st\u00fctzen somit die Hypothese des genetischen Wettr\u00fcstens.\n2014-10-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4767\nurn:nbn:de:bvb:29-opus4-47670\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-47670\nhttps://opus4.kobv.de/opus4-fau/files/4767/Annabel%20Krupp%20Dissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4777\n2018-01-26\ndoc-type:workingPaper\nbibliography:false\nddc\nddc:000\nccs\nccs:C.2.4\nccs:D.3.0\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nMATINEE: A Quality-of-Service-aware Event Semantics Modeling Language\nWahl, Andreas M.\nFischer, Thomas\nLenz, Richard\nDom\u00e4nenspezifische Programmiersprache\nPublish-Subscribe-System\nddc:000\nDistributed event-based systems have risen in significance over the last few years across many different application domains.\nStill, the configuration of available communication middleware solutions remains an elaborate task driven by technical terms and manual performance optimization.\nWe present the M2etis Quality-of-service-aware Semantics Modeling Language (MATINEE), a domain-specific language for modeling the QoS requirements and semantic properties of event-based systems in the terminology of the application domain.\nNo knowledge of the technical terms of the underlying middleware solution is required by the application developers to conduct the configuration.\nEmbedded in a novel configuration methodology provided by the M2etis project, MATINEE specifications are interpreted to automatically derive optimal middleware configurations without the need for manual optimization.\nThe purpose of MATINEE is to provide a flexible language that can easily be adapted to different application domains.\nWe focus on the language specification of MATINEE, briefly sketch our configuration methodology and present related language approaches.\n2014-06-17\nworkingpaper\ndoc-type:workingPaper\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4777\nurn:nbn:de:bvb:29-opus4-47777\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-47777\nhttps://opus4.kobv.de/opus4-fau/files/4777/matinee.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4782\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:540\nccs\nccs:A.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Chemie\nOne Dimensional Coordination Polymers Based On Bridging N,N Donor Ligands\nEindimensional Koordinationspolymere basierend auf N,N Donorliganden\nFritsch, Nico\nPolymerkomplexe\nRastertunnelmikroskopie\nddc:540\nTo get a better understanding of the coordination behaviour of the ligand trans-1,2-bis(N-methylimidazol-2-yl)ethylene (trans-bie) (2), the first part of this thesis (chapter 3.1) describes the synthesis and characterisation of a range of dinuclear metal(II) acetato paddlewheel polymers with different metal-metal bond orders linked by trans-bie (2). The reaction of trans-bie (2) with the metal(II) acetates of rhodium, ruthenium, molybdenum and chromium led to the formation of the corresponding 1D coordination polymer with the molecular formula [M2(OAc)4(trans-bie)]n (M = Rh (4), Ru (5), Mo(6), Cr(7)) . These complexes have been characterised by elemental analysis, infrared spectroscopy and UV/Vis absorption spectroscopy. It was possible to characterize the polymers [Rh2(OAc)4(trans-bie)]n (4), [Ru2(OAc)4(trans-bie)]n (5) and [Mo2(OAc)4(trans-bie)]n (6) by single crystal X-ray structure determination. A comparison of these polymer structures with the copper(II) acetate polymer [Cu2(OAc)4(trans-bie)]n shows a correlation between the strength of the metal-metal and the metal-nitrogen bond of the coordinating ligand. The stronger the metal-metal bond in the paddlewheel unit the weaker the bond between the metal and the coordinating nitrogen donor. In cooperation with C. WICK from the CLARK group theoretical calculations were performed. CASSCF/CASPT2 and DFT calculations of a single periodic unit of [Mo2(Ac)4(trans-bie)2]n showed a weakening of the possible quadruple bond, which is mainly attributed to weakly overlapping \u03b4 orbitals and a resulting effective bond order (EBO) of 3.3. The calculated metal-metal bond orders in [Rh2(OAc)4(trans-bie)2] and [Ru2(OAc)4(trans-bie)2] indicate single and double bonds with EBO of 0.8 and 1.7, respectively. Furthermore, a complex [Cr2(OAc)4(trans-bie)2] (8) was crystallized, consisting of the chromium paddlewheel unit, which is coordinated by two trans-bie ligands. By reacting zinc(II) acetate with trans-bie (2) a polymer with a trinuclear metal unit was obtained. Here the three zinc centres are bridged by six acetato ligands to form a polymer with the formula [Zn3(OAc)6(trans-bie)]n (9).\nBased on these results in cooperation with T. WAIDMANN of the BURZLAFF group a new N,N donor based imidazole ligand was synthesized (chapter 3.2). Two methylimidazoles were linked by a bisacetylene unit to obtain the new linear ligand bis(N-methylimidazol-2-yl)butadiyne (bmib) (3). In cooperation with the GULDI group the fluorescence properties of bmib (3) were investigated by time dependent absorption spectroscopy. By the reaction of zinc(II) acetate with bmib (3) a 1D coordination polymer was obtained in which the ligand coordinates to a zinc paddlewheel unit on the one side and to a trinuclear zinc unit on the other side similar to zinc polymer 9 mentioned above. The molecular formula of the polymer is [Zn5(OAc)10(bmib)2]n (10).\nIn the final part of this thesis (chapter 3.3) a range of polymers containing a so called sawhorse unit were synthesized. The reaction of [Ru3(CO)12] with acetic acid led to the formation of a polymeric precursor [Ru2(OAc)2(CO)4]n consisting of a dinuclear ruthenium(I) unit. The metal centres are bridged by two acetato groups and each ruthenium is coordinated by two carbonyl ligands. In the next step the reaction of the N,N donor ligands pyrazine (pyz), 1,4-diazabicyclo[2.2.2]octane (DABCO), 4,4\u00b4-bipyridine (4,4\u00b4-bipy), trans-1,2-bis(N-methylimidazol-2-yl)ethylene (trans-bie) (2) and 1,2-di(4-pyridyl)ethylene (bpe) corresponding 1D coordination polymers with the general molecular formula [Ru2(OAc)2(CO)4(L)]n (11-15) were formed. These complexes have been characterised by elemental analysis, infrared spectroscopy and UV/Vis absorption spectroscopy. It was also possible to characterize the organometallic coordination polymers 11 and 12 by single crystal X-ray structure determination. Within a polymer chain the acetato bridged dinuclear units have an alternating \u201cup-down\u201d arrangement in which the acetato bridges on one dimer unit are adjacent to the carbonyl groups on the units on either side. In cooperation with the CLARK group the different bonds by means of DFT and CASSCF calculations as described for paddlewheel polymers were analysed. The metal to metal Mayer Bond Orders (MBO) of 0.66 according to DFT and the effective bond order (EBO) of 0.85 indicate a weakening of a possible single bond (\u03c3 orbitals) due to ligand coordination. For further investigations we imaged the coordination polymer [Ru2(OAc)2(CO)4(trans-bie)]n (11) with STM. It was observed from the topography measurements that the compound adsorbs at defect line of the substrate in a linear arrangement. So far a distinct answer on the formation of a polymer or a dinuclear complex during adsorption on surface cannot be given.\nUm ein besseres Verst\u00e4ndnis \u00fcber das Koordinationsverhalten des Liganden trans-1,2-bis(N-methylimidazol-2-yl)ethylene (trans-bie) (2) zu erlangen, befasst sich der erste Teil dieser Arbeit (Chapter 3.1) mit der Synthese und Charakterisierung einer Reihe von dinuklearen Metall(II)acetat Polymeren mit unterschiedlichen Metall-Metall Bindungsordnungen. Diese so genannten Paddlewheel-Einheiten (Schaufelrad-Einheit) werden durch den trans-bie Liganden verkn\u00fcpft. Die Reaktion von Rhodium(II)-, Ruthenium(II)-, Molybd\u00e4n(II)- bzw. Chrom(II)acetat mit trans-bie (2) f\u00fchrte zur Ausbildung der entsprechenden 1D-Koordinationspolymere mit der allgemeinen Formel [M2(OAc)4(trans-bie)]n (M = Rh (4), Ru (5), Mo (6), Cr (7)). Diese Verbindungen wurden mittels Elementaranalyse, Infrarotspektroskopie und UV/Vis Absorptionsspektroskopie charakterisiert. Des weiteren war es m\u00f6glich die Verbindungen [Rh2(OAc)4(trans-bie)]n (4), [Ru2(OAc)4(trans-bie)]n (5) und [Mo2(OAc)4(trans-bie)]n (6) mittels der Einkristall R\u00f6ntgenstrukturanalyse zu charakterisieren. Ein Vergleich mit der bereits bekannten Polymerstruktur des Kupfer(II)acetats [Cu2(OAc)4(trans-bie)]n zeigte eine Beziehung zwischen der Bindungsst\u00e4rke der Metall-Metall-Einheit und der Metall-Stickstoff-Bindung des koordinierten trans-bie Ligandens. Je st\u00e4rker die Metall-Metall-Bindung in der Paddlewheel-Einheit desto schw\u00e4cher ist die Bindung zwischen dem Metall und dem koordinierten Stickstoffatom des Ligandens. Theoretische Berechnungen zu den gezeigten Polymeren wurden in einer Kooperation mit C. WICK aus der Arbeitsgruppe CLARK durchgef\u00fchrt. CASSCF/CASPTS und DFT Berechnungen f\u00fcr eine einzelne periodische Einheit des Polymers [Mo2(OAc)4(trans-bie)]n (6) zeigten eine Schw\u00e4chung der m\u00f6glichen Vierfachbindung, welche ma\u00dfgeblich auf geringer \u00fcberlappende \u03b4-Orbitale zur\u00fcckzuf\u00fchren ist, was eine effektive Bindungsordnung (EBO) von 3.3 zur Folge hat. Die berechneten Metall-Metall-Bindungsordnungen in den Polymeren [Rh2(OAc)4(trans-bie)]n (4) und [Ru2(OAc)4(trans-bie)]n (5) weisen auf eine Einfach- bzw. Doppelbindungen mit EBO Werten von 0.8 bzw. 1.7 hin. Des weiteren konnte ein Chrom(II)acetat Komplex mit der Formel [Cr2(OAc)4(trans-bie)2] (8) kristallisiert werden, der aus einer Chrom Paddlewheel-Einheit besteht, die von zwei trans-bie Liganden koordiniert wird. Durch die Reaktion von Zink(II)acetat mit trans-bie (2) bildete sich ein Polymer mit einer trinuklearen Metalleinheit. In diesem Fall werden drei Zinkatome durch sechs Acetatgruppen verbr\u00fcckt die ein Polymer mit der Formel [Zn3(OAc)6(trans-bie)]n (9) bilden.\nAufbauend auf diese Ergebnisse wurde in Kooperation mit T.WAIDMANN aus der BURZLAFF-Gruppe ein neuer N,N-Donor basierter Ligand hergestellt (Chapter 3.2). Dabei werden zwei Methylimidazole durch eine Bisacetyleneinheit verbr\u00fcckt um den linearen bis(N-methylimidazol-2-yl)butadiyne (bmib) (3) Liganden zu erhalten. In einer Kooperation mit der Arbeitsgruppe GULDI wurden die Fluoreszenzeigenschaften mittels zeitaufgel\u00f6ster Absorptionsspektroskopie untersucht. Durch die Umsetzung von Zink(II)acetat mit bmib (3) konnte ein 1D-Koordinationspolymer mit der Formel [Zn5(OAc)10(bmib)2]n (10) synthetisiert werden. Der Ligand koordiniert auf der einen Seite an eine Zink-Paddlewheel-Einheit auf der anderen Seite an eine trinucleare Zinkeinheit, \u00e4hnlich dem oben beschriebenen Polymer 9.\nIm letzten Teil dieser Arbeit (Chapter 3.3) wurden Polymere mit einer so genannten Sawhorse-Einheit (S\u00e4gebock-Einheit) hergestellt. Die Reaktion von [Ru3(CO)12] mit Essigs\u00e4ure f\u00fchrt zur Ausbildung einer polymeren Vorstufe [Ru2(OAc)2(CO)4]n, die aus einer zweikernigen Ruthenium(I)-Einheit besteht. Die Metallzentren sind hier durch zwei Acetato-Liganden verbunden, wobei jedes Ruthenium zus\u00e4tzlich durch zwei Carbonylliganden koordiniert ist. Im n\u00e4chsten Schritt wurde durch die Reaktion mit den N,N-Donorliganden Pyrazin (pyz), 1,4-Diazabicyclo[2.2.2]octan (DABCO), 4,4\u00b4-Bipyridin (4,4\u00b4-bipy), trans-1,2-bis(N-methylimidazol-2-yl)ethylene (trans-bie) (2) und 1,2-di(4-pyridyl)ethylene (bpe) das entsprechende 1D Koordinationspolymer mit der Formel [Ru2(OAc)2(CO)4(L)]n (11-15) hergestellt. Diese Verbindungen wurden mittels Elementaranalyse, Infrarotspektroskopie und UV/Vis-Absorptionsspektroskopie charakterisiert. Es war des weiteren m\u00f6glich die metallorganischen Koordinationspolymere 11 und 12 durch Einkristall R\u00f6ntgenstrukturanalyse zu charakterisieren. Innerhalb eines Polymeres alternieren die dinuklearen Einheiten in einer \u201cup-down\u201d Anordnung. Dabei sind die Acetatbr\u00fccken einer Einheit zu den Carbonylgruppen der n\u00e4chsten Einheit benachbart. In einer Kooperation mit der Arbeitsgruppe CLARK wurden die unterschiedlichen Bindungsordnungen durch DFT und CASSCF, \u00e4hnlich wie f\u00fcr die bereits beschriebene Paddlewheelpolymere, berechnet. Die Metall-Metall Mayer Bond Orders (MBO) von 0.66 entsprechend f\u00fcr DFT und die Effektiven Bindungsordnung (EBO) von 0.86 zeigen eine Schw\u00e4chung der m\u00f6glichen Einfachbindung (\u03c3-Orbitale) durch die Ligandenkoordination. F\u00fcr weitere Untersuchen wurde das Polymer [Ru2(OAc)2(CO)4(trans-bie)]n (11) mittels Rastertunnelmikroskopie hin untersucht. Es konnte aufgrund der Oberfl\u00e4chenaufnahmen gezeigt werden, dass die Verbindung sich an einer Defektlinie der Oberfl\u00e4che als lineare Str\u00e4nge anlagert. Eine genaue Antwort \u00fcber die Anordnung der Verbindung als Polymer oder als dinuklearer Komplex auf der Oberfl\u00e4che, kann aufgrund der bisher erhaltenen Aufnahmen nicht gegeben werden.\n2014-06-18\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4782\nurn:nbn:de:bvb:29-opus4-47828\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-47828\nhttps://opus4.kobv.de/opus4-fau/files/4782/FritschNicoDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4827\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:900\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:01-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nJ\u00fcdische Zahn\u00e4rzte in N\u00fcrnberg und F\u00fcrth im Nationalsozialismus. Leben und Schicksal.\nSalzner, Marina\nNationalsozialismus\nddc:900\na) Hintergrund und Ziele\nDie vorliegende Arbeit beschreibt Leben und Schicksal der j\u00fcdischen Zahn\u00e4rzte in N\u00fcrnberg und F\u00fcrth zur Zeit des Nationalsozialismus. Au\u00dferdem zeigt sie die Lebensumst\u00e4nde j\u00fcdischer Studenten und Zahn\u00e4rzte zu dieser Zeit in Hinblick auf die Entwicklung der j\u00fcdischen Gemeinden in N\u00fcrnberg und F\u00fcrth, das Leben an der Universit\u00e4t und die Entwicklungen in der Zahn\u00e4rzteschaft.\nb) Methoden\nAuf Basis archivalischer Quellen aus sechs Archiven und aktueller Forschungsliteratur wurde die Gesamtzahl der j\u00fcdischen Zahn\u00e4rzte und Dentisten in N\u00fcrnberg und F\u00fcrth ermittelt. Anschlie\u00dfend wurde f\u00fcr jede Person eine Biographie verfasst.\nc) Ergebnisse und Beobachtungen\nVon den 29 ermittelten Zahn\u00e4rzten und Zahn\u00e4rztinnen \u00fcberlebten 16 den Holocaust, neun fielen ihm zum Opfer und vier verstarben bereits in den Anfangszeiten des ,Dritten Reichs\u201b bzw. ihr genaues Schicksal ist nicht bekannt.\nd) Praktische Schlussfolgerungen\nDiese Arbeit soll einen Beitrag zu den Forschungen \u00fcber den Nationalsozialismus leisten. Au\u00dferdem ist sie ein Gedenkbuch f\u00fcr die vorgestellten Personen und somit ein Teil der Erinnerungskultur.\n2014-06-26\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4827\nurn:nbn:de:bvb:29-opus4-48272\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-48272\nhttps://opus4.kobv.de/opus4-fau/files/4827/DissertationSalznerMarina.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4857\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nQuality-of-Service-Aware Configuration of Distributed Publish-Subscribe Systems - A Massive Multiuser Virtual Environment Perspective\nDienstg\u00fctebezogene Konfiguration von verteilten Publish-Subscribe Systemen - Eine Perspektive f\u00fcr virtuelle Welten\nFischer, Thomas\nPublish-Subscribe-System\nVerteiltes System\nRechnernetz\nDienstg\u00fcte\nddc:000\nWith the raise of internet-scale applications like massively multiuser virtual environments, some unique challenges were introduced regarding scalability and the maintainability of complex systems. Distributed event-based systems in general and especially publish-subscribe systems offer a scalable and loosely coupled paradigm to address these challenges. However, the large variety of existing technical solutions, with their different optimization targets on the one hand, and the variety of semantics and quality-of-service requirements of different applications on the other hand, introduces a gap that is not easily filled by the application developer.\nThis thesis explores a methodology for the configuration of publish-subscribe systems that closes this gap by providing an automated workflow that translates the requirements and semantics a developer formulates in his domain-specific terminology to a suitable configuration of a publish-subscribe middleware. Hereby, this work covers the design of such a configurable middleware with the focus on design-time configuration, as this configuration method promises to introduce the least overhead. Moreover, a flexible and extensible model for the domain-specific configuration of distributed event-based systems is suggested, accompanied by the corresponding workflow for the provisioning of a customized publish-subscribe middleware.\nCombined, these three parts provide a holistic developer-friendly methodology for quality-of-service-aware configuration of publish-subscribe systems.\nThe thesis concludes with an evaluation of the methodology in form of a discussion of its capabilities and a quantitative analysis of its performance and quality.\nMit der Verbreitung von hochskalierbaren Anwendungen wie Massively Multiplayer Online Games (MMOG), entstanden einige einzigartige Herausforderungen bez\u00fcglich der Skalierbarkeit und Wartbarkeit von komplexen Systemen. Publish-Subscribe Systeme bieten ein skalierbares und lose gekoppeltes Entwurfsparadigma, um diese Herausforderungen anzugehen. Doch die Vielzahl von technischen Ans\u00e4tzen, mit ihren unterschiedlichen Optimierungszielen auf der einen Seite, und der Vielfalt der Semantik und Dienstg\u00fcteanforderungen verschiedener Anwendungen auf der anderen Seite, f\u00fchrt zu einer Kluft, die nicht einfach durch den typischen Anwendungsentwickler \u00fcberbr\u00fcckt werden kann.\nDiese Arbeit untersucht eine Methodologie f\u00fcr die Konfiguration von Publish-Subscribe Systemen, welche diese Kluft mit Hilfe eines automatisierten Workflows schlie\u00dft. Hierbei \u00fcbersetzt er die Anforderungen und Semantik einer Anwendung, die ein Entwickler in seiner dom\u00e4nenspezifischen Terminologie formuliert, in die optimierte Konfiguration einer Publish-Subscribe-Middleware. Dazu beschreibt diese Arbeit ein Framework f\u00fcr konfigurierbare Middleware mit dem Fokus auf Konfigurierbarkeit zur Entwurfszeit. Au\u00dferdem wird ein flexibles und erweiterbares Modell f\u00fcr die dom\u00e4nenspezifische Konfiguration verteilter ereignisbasierter Systeme vorgeschlagen, begleitet von dem entsprechenden Workflow f\u00fcr die Bereitstellung einer ma\u00dfgeschneiderten Publish-Subscribe Middleware. Kombiniert ergeben diese drei Teile eine ganzheitliche, entwicklerfreundliche Methodologie f\u00fcr die dienstg\u00fctebezogene Konfiguration von verteilten Publish-Subscribe Systemen. Die Arbeit schlie\u00dft mit einer Bewertung der Methodologie in Form einer Diskussion \u00fcber die erreichten M\u00f6glichkeiten und einer quantitativen Analyse seiner Leistung und Qualit\u00e4t.\n2014-07-06\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4857\nurn:nbn:de:bvb:29-opus4-48570\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-48570\nhttps://opus4.kobv.de/opus4-fau/files/4857/dissertation-fischer-release-candidate-bib-2014-07-06.pdf\neng\nhttps://creativecommons.org/licenses/by-nc/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4875\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:D.4\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nSloth: The Virtue and Vice of Latency Hiding in Hardware-Centric Operating Systems\nSloth: Tugend und Laster der Latenzverbergung in hardwarezentrischen Betriebssystemen\nHofer, Wanja\nBetriebssystem\nEingebettetes System\nMikrocontroller\nScheduling\nEchtzeitsystem\nEreignisgesteuertes System\nZeitgesteuertes System\nAUTOSAR\nddc:004\nSoftware for embedded systems needs to be tailored to the application requirements to provide for the lowest cost overhead possible; this is especially important for embedded operating systems, which do not provide a business value of their own. State-of-the-art embedded operating systems are tailored to the requirements of the application above, but they abstract from the hardware platform below, missing out on advantageous hardware peculiarities to optimize the non-functional properties of the system. Additionally, they provide the application programmer with a multitude of control flow types, which leads to several severe restrictions and problems for the application development and in the real-time execution of the system---for instance, high-priority tasks can be interrupted by low-priority interrupt service routines at any time.\nThe Sloth operating system design for event-triggered and time-triggered embedded real-time systems as presented in this thesis is unique in that it employs a hardware-centric approach for its scheduling, dispatching, and timing services and makes use of hardware particularities---with the purpose of optimizing the non-functional properties of the operating system and the application. In its implementation, Sloth assigns each task an interrupt source with an appropriately configured priority and maps software activations of that task to setting the request bit of the interrupt source. This way, Sloth has the hardware interrupt subsystem make the scheduling decision with the corresponding dispatch of the interrupt handler, which directly executes the application task function. Time-triggered dispatch tables are encapsulated in hardware timer cell arrays that Sloth pre-configures at initialization time with the corresponding timing parameters, making the hardware timer subsystem execute the dispatcher rounds autonomously at run time.\nThe evaluation of the Sloth operating system implementation shows that its light-weight design mitigates or even eliminates the safety-related problems and restrictions in the real-time execution of the application by providing a unified control flow abstraction in a unified priority space, eliminating the artificial distinction between tasks and ISRs. Additionally, its exhibited non-functional properties show unprecedented levels of efficiency by hiding latencies in the hardware subsystems: A feature-complete Sloth operating system can be tailored to less than 200 lines of source code; it compiles to less than 500 bytes of code memory and as few as 8 bytes of data memory; and it can schedule and dispatch tasks in as few as 12 clock cycles with speed-up factors of up to 106 compared to commercial operating systems. Since Sloth prototypically implements the automotive OSEK OS, OSEKtime OS, and AUTOSAR OS standards and runs on commodity off-the-shelf hardware, it is applicable to a wide range of embedded real-time systems.\nSoftware f\u00fcr eingebettete Systeme wird an die Anwendungsanforderungen angepasst, um die Kosten so niedrig wie m\u00f6glich zu halten; dies betrifft insbesondere eingebettete Betriebssysteme, welche nicht zum Wert des Endprodukts beitragen. Aktuelle eingebettete Betriebssysteme passen sich an die dar\u00fcberliegende Anwendung an, aber abstrahieren von der darunterliegenden Hardwareplattform, was vorteilhafte Hardwarebesonderheiten zur Optimierung nichtfunktionaler Eigenschaften des Systems ungenutzt l\u00e4sst. Au\u00dferdem bieten sie dem Anwendungsprogrammierer eine Vielzahl an Kontrollflusstypen an, was zu diversen schweren Einschr\u00e4nkungen und Problemen bei der Anwendungsentwicklung und in der Echtzeitausf\u00fchrung des Systems f\u00fchrt -- zum Beispiel, dass hochpriore Tasks jederzeit durch niedrigpriore Interrupt-Handler unterbrochen werden k\u00f6nnen.\nDer Entwurf des Sloth-Betriebssystems f\u00fcr ereignisgesteuerte und zeitgesteuerte eingebettete Echtzeitsysteme, welcher in dieser Arbeit vorgestellt wird, ist von besonderer Art, da er einen hardwarezentrischen Ansatz f\u00fcr die Einplanungs-, Einlastungs- und f\u00fcr zeitbasierte Dienste einsetzt und spezifische Hardwareeigenschaften ausnutzt -- mit dem Ziel, die nichtfunktionalen Eigenschaften des Betriebssystems und der Anwendung zu optimieren. In seiner Implementierung weist Sloth jedem Task eine Interruptquelle mit einer entsprechend konfigurierten Priorit\u00e4t zu und bildet Softwareaktivierungen dieses Tasks auf das Setzen des Request-Bits der Interruptquelle ab. Auf diese Weise l\u00e4sst Sloth das Hardware-Interrupt-Subsystem die Einplanungsentscheidung treffen und den entsprechenden Interrupt-Handler einlasten, welcher direkt die Taskfunktion der Anwendung ausf\u00fchrt. Zeitgesteuerte Einlastungstabellen werden in Arrays von Hardware-Timer-Zellen eingebettet, welche von Sloth bei der Initialisierung mit den entsprechenden Zeitparametern vorkonfiguriert werden, um das Hardware-Timer-Subsystem die Einlastungsrunden zur Laufzeit autonom ausf\u00fchren zu lassen.\nDie Evaluation der Implementierung des Sloth-Betriebssystems zeigt, dass der leichtgewichtige Entwurf die sicherheitsrelevanten Probleme und Einschr\u00e4nkungen bei der Echtzeitausf\u00fchrung der Anwendung reduziert oder sogar eliminiert, indem er eine einheitliche Kontrollflussabstraktion in einem vereinigten Priorit\u00e4tenraum anbietet, was die k\u00fcnstliche Unterscheidung zwischen Tasks und Interrupt-Handlern beseitigt. Au\u00dferdem weisen die nichtfunktionalen Eigenschaften ein neues Niveau an Effizienz auf, indem Sloth Latenzen in Hardware-Subsystemen versteckt: Ein vollst\u00e4ndiges Sloth-Betriebssystem kann auf weniger als 200 Zeilen Code angepasst werden, auf weniger als 500 Bytes im Codespeicher und 8 Bytes im Datenspeicher kompiliert werden, und er kann Tasks innerhalb von 12 Taktzyklen einplanen und einlasten -- mit Speed-Up-Faktoren von bis zu 106 im Vergleich zu kommerziellen Betriebssystemen. Da Sloth prototypisch die Standards OSEK OS, OSEKtime OS und AUTOSAR OS implementiert, welche von der Automobilindustrie entwickelt wurden, und da das Betriebssystem auf Standard-Hardware l\u00e4uft, ist der Ansatz anwendbar auf eine breite Menge an eingebetteten Echtzeitsystemen.\n2014-08-07\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4875\nurn:nbn:de:bvb:29-opus4-48754\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-48754\nhttps://opus4.kobv.de/opus4-fau/files/4875/WanjaHoferDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4880\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nResource-efficient Fault and Intrusion Tolerance\nRessourceneffiziente Fehler- und Einbruchstoleranz\nDistler, Tobias\nFehlertoleranz\nByzantinische Einigung\nddc:004\nMore and more network-based services are considered essential by their operators: either because their unavailability might directly lead to economic losses, as with e-commerce applications or online auction services, for example, or because their well-functioning is crucial for the well-functioning of other services, which is, for example, the case for distributed file systems or coordination services. Byzantine fault-tolerant replication allows systems to be built that are able to ensure the availability and reliability of network-based services, even if a subset of replicas fail arbitrarily. As a consequence, such systems not only tolerate fault scenarios in which replicas crash, but also cases in which replicas have been taken over by an adversary as the result of a successful intrusion.\nDespite the fact that several major outages of network-based services in the past have been caused by non-crash failures, industry is still reluctant to broadly exploit the available research results on Byzantine fault tolerance. One of the main reasons for the decision to retain crash-tolerant systems is the high resource demand associated with Byzantine fault-tolerant systems: Besides the need to execute more costly protocols, the more complex fault model also requires Byzantine fault-tolerant systems to comprise more replicas than their crash-tolerant counterparts.\nIn this thesis, we propose and evaluate different protocols and techniques to increase the resource efficiency of Byzantine fault-tolerant systems. The key insights that serve as a basis for all of these approaches are that during normal-case operation it is sufficient for a system to detect (or at least suspect) faults, while during fault handling a system must be able to actually tolerate faults, and that the former usually requires less resources than the latter. Utilizing these insights, we investigate different ways to improve resource efficiency by implementing a clear separation between normal-case operation and fault handling based on two modes of operation: During normal-case operation, a system reduces its resource usage to a level at which it is only able to ensure progress as long as all replicas behave according to specification. In contrast, in case of suspected or detected faults, the system switches to an operation mode in which it may use additional resources in order to tolerate faults.\nAn important outcome of this thesis is that passive replication can be an effective building block for the implementation of a resource-efficient operation mode for normal-case operation in Byzantine fault-tolerant systems. Furthermore, experimental results show that improving the resource efficiency of a system can also lead to an increase in performance.\nNetzwerkbasierte Dienste werden von ihren Betreibern zunehmend als unentbehrlich angesehen: entweder weil ihr Ausfall direkt zu \u00f6konomischen Verlusten f\u00fchren k\u00f6nnte, wie etwa bei elektronischen Handelssystemen oder internetgest\u00fctzten Auktionsdiensten, oder weil die Verf\u00fcgbarkeit anderer Dienste von ihnen abh\u00e4ngt, wie es beispielsweise bei Netzwerkdateisystemen oder Koordinierungsdiensten der Fall sein kann. Das Prinzip der byzantinisch fehlertoleranten Replikation erlaubt es Systemen die Verf\u00fcgbarkeit und Zuverl\u00e4ssigkeit von netzwerkbasierten Diensten sogar dann zu gew\u00e4hrleisten, wenn ein Teil der Replikate willk\u00fcrliches Fehlverhalten zeigt. Solche Systeme k\u00f6nnen daher nicht nur Replikatausf\u00e4lle tolerieren, sondern auch Szenarien, in denen Replikate als Folge von Einbr\u00fcchen von einem Angreifer \u00fcbernommen wurden.\nTrotz der Tatsache, dass willk\u00fcrliches Fehlverhalten von Systemkomponenten in der Vergangenheit zu mehreren schwerwiegenden Ausf\u00e4llen von netzwerkbasierten Diensten gef\u00fchrt hat, werden existierende Forschungsergebnisse aus dem Bereich der byzantinischen Fehlertoleranz weiterhin kaum f\u00fcr den Produktiveinsatz genutzt. Einer der Hauptgr\u00fcnde daf\u00fcr sich wie bisher auf ausfalltolerante Systeme zu beschr\u00e4nken ist der mit byzantinisch fehlertoleranten Systemen verbundene hohe Ressourcenbedarf: Neben der Notwendigkeit des Einsatzes aufwendigerer Protokolle macht es das komplexere Fehlermodell au\u00dferdem erforderlich, dass byzantinisch fehlertolerante Systeme mehr Replikate als vergleichbare ausfalltolerante Systeme bereitstellen.\nDiese Dissertation schl\u00e4gt mehrere Protokolle und Techniken zur Steigerung der Ressourceneffizienz von byzantinisch fehlertoleranten Systemen vor und evaluiert sie. Allen hier pr\u00e4sentierten Ans\u00e4tzen liegen dabei die zentralen Erkenntnisse zugrunde, dass es f\u00fcr den Normalbetrieb ausreicht Fehler erkennen (oder sie zumindest vermuten) zu k\u00f6nnen, wogegen sie im Zuge einer Fehlerbehandlung tats\u00e4chlich toleriert werden m\u00fcssen, und dass ersteres weniger Ressourcen ben\u00f6tigt als letzteres. Aufbauend darauf, werden verschiedene Vorgehensweisen untersucht, wie sich die Ressourceneffizienz eines Systems durch eine klare Trennung des Normalbetriebs von der Fehlerbehandlung und die damit verbundene Einf\u00fchrung zweier Betriebsmodi steigern l\u00e4sst: Im Normalfall befindet sich das System in einem Modus, in dem es seinen Ressourcenverbrauch so weit senkt, dass Fortschritt nur noch gew\u00e4hrleistet ist, solange sich alle Replikate korrekt verhalten. Im Unterschied dazu stehen im Fehlerbehandlungsmodus zus\u00e4tzliche Ressourcen zur Verf\u00fcgung, um Fehler tolerieren zu k\u00f6nnen; in ihn wird umgeschaltet, sobald das Auftreten von Fehlern entweder vermutet oder erkannt wird.\nEin zentrales Resultat dieser Arbeit ist die Erkenntnis, dass passive Replikation ein effektives Mittel zur Implementierung eines ressourceneffizienten Normalbetriebsmodus darstellt. Dar\u00fcber hinaus belegen Evaluationsergebnisse, dass eine verbesserte Ressourceneffizienz auch zu einer gesteigerten Leistungsf\u00e4higkeit eines Systems f\u00fchren kann.\n2014-07-09\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4880\nurn:nbn:de:bvb:29-opus4-48803\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-48803\nhttps://opus4.kobv.de/opus4-fau/files/4880/TobiasDistlerDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:4983\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:I.\npacs\npacs:06.20.-f\nmsc\nmsc:93-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nBayes-Filter zur Genauigkeitsverbesserung und Unsicherheitsermittlung von dynamischen Koordinatenmessungen\nBayesian filter for improved accuracy and uncertainty estimation of dynamic coordinate measurements\nGarcia, Elmar\nMesstechnik, Koordinatenmesstechnik, Systemtheorie\nddc:620\nIn der vorliegenden Arbeit werden neue Methoden und Verfahren der bayesschen Sch\u00e4tzung f\u00fcr die Koordinatenmesstechnik entwickelt, umgesetzt und validiert. Messungen sind aufgrund allgegenw\u00e4rtig wirkender bekannter und unbekannter Einfl\u00fcsse (St\u00f6rungen oder Rauschprozesse) stets abweichungsbehaftet und die tats\u00e4chlichen Koordinaten nicht exakt bestimmbar. Der Messvorgang liefert Informationen \u00fcber die Position in Form von Beobachtungen, die allerdings eine abweichungsbehaftte Abbildung der tats\u00e4chlichen Position repr\u00e4sentieren. Da bei dieser Transformation Informationen verloren gehen, ist eine exakte, eindeutige Berechnung der tats\u00e4chlichen Position aus den vom Messprozess generierten Beobachtungen nicht m\u00f6glich. Sie kann jedoch mathematisch gesch\u00e4tzt werden, da der funktionale Zusammenhang zwischen den tats\u00e4chlichen Objektkoordinaten und den Beobachtungen erhalten bleibt. Das Ziel dieser Arbeit ist die Implementierung und Bereitstellung von Algorithmen zur dynamischen, messdatenbasierten und sensor\u00fcbergreifenden Genauigkeitsverbesserung und Messunsicherheitsermittlung von Koordinatenmessungen. Die L\u00f6sung erfordert vom Anwender weder das Aufstellen einer Modellgleichung des Messprozesses, noch a priori Verteilungsannahmen, erlaubt aber deren Spezifikation bei Bedarf. Durch diese probabilistische Messdatenverarbeitung und -auswertung k\u00f6nnen Abweichungen in den Messdaten reduziert und gleichzeitig Unsicherheitsaussagen abgeleitet werden.\nIn this work, new methods and algorithms of Bayesian estimation are developed, implemented and validated for coordinate metrology. Measurements are due to ubiquitous known and unknown influences (disturbances or noise sources) always inaccurate and thus the actual coordinates can not be determined exactly. The measurement process provides information about the position in form of observations, however this is an imperfect mapping of the true position. Since information is lost in this transformation, the true object coordinates can not be computed completely from the obtained observations. But it is possible to estimate the measurand, because the functional relationship between the true object coordinates and the observations remains.\nThe objective of this work is the implementation and provision of algorithms for dynamic, measurement data\u2013based and sensor\u2013independent accuracy improvement and measurement uncertainty evaluation of coordinate measurements. The solution requires neither an explicit modelling of the measurement process nor a priori distribution assumption, but allows the specification if necessary. This probabilistic measurement data processing and evaluation allows to reduce the deviations in measured data and to deduce measurement uncertainty evaluations.\n2014-07-24\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/4983\nurn:nbn:de:bvb:29-opus4-49833\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-49833\nhttps://opus4.kobv.de/opus4-fau/files/4983/diss_pdf-a.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5024\n2018-01-26\ndoc-type:report\nbibliography:false\nddc\nddc:004\nccs\nccs:D.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nA Framework for Interactive Physical Simulations on Remote HPC Clusters\nKuckuk, Sebastian\nK\u00f6stler, Harald\nVisualisierung\nInteraktivit\u00e4t\nComputersimulation\nNumerische Str\u00f6mungssimulation\nddc:004\nIn this work, we introduce the framework for visualization and interactivity for physics engines in real-time, for short VIPER. It is able to execute various physical simulations, visualize the simulation results in real-time and offer computational steering. Especially interesting in this context are simulations running on remotely accessible HPC clusters. As an example, we present a particulate flow simulation consisting of a coupled rigid body and CFD simulation, the chosen visualization strategy and steering possibilities. Additionally, performance evaluations and a performance prediction model concerning the update rate for remote simulations in the context of the VIPER framework are given.\n2014-07-30\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5024\nurn:nbn:de:bvb:29-opus4-50246\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50246\nhttps://opus4.kobv.de/opus4-fau/files/5024/Kuckuk_2013_VIPER.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5056\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nddc:620\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nGanzheitliche Fehlertoleranz in eingebetteten Softwaresystemen\nUlbrich, Peter\nFehlertoleranz\nVoting\nSoftwaresystem\nRedundanz\nTechnische Sicherheit\nFunktionssicherheit\nEingebettetes System\nArithmetische Codierung\nCodierung\nKosmische Strahlung\nSt\u00f6rsicherheit\nEchtzeitsystem\nReaktives System\nDigitale Regelung\nRegelungssystem\nddc:000\nddc:620\nDurch die technische Entwicklung nimmt die Leistungsf\u00e4higkeit und Parallelit\u00e4t der Hardware stetig zu, ihre Zuverl\u00e4ssigkeit nimmt hingegen tendenziell ab. Im Fokus dieser Arbeit stehen die damit verbundenen, steigenden Fehlerraten f\u00fcr transiente Hardwarefehler. Diese treten zuf\u00e4llig auf und bewirken kurzzeitige Fehlfunktionen, welche sich ausschlie\u00dflich im Feld behandeln lassen. Der Aufbau zuverl\u00e4ssiger Systeme aus unzuverl\u00e4ssigen Komponenten erfordert grunds\u00e4tzlich das Einbringen von Redundanz, zum Beispiel in Form von Replikation. Der naheliegende Einsatz von hardwarebasierter Redundanz scheidet jedoch h\u00e4ufig aus Kosten-, Gewichts- oder sonstiger Ressourcenbeschr\u00e4nkungen aus. An dieser Stelle bietet sich softwarebasierte Redundanz an, da diese selektiv f\u00fcr die Absicherung der tats\u00e4chlich sicherheitskritischen Teile der Anwendungen eingesetzt werden kann. Bestehende Ans\u00e4tze erweisen sich bisher jedoch als unzuverl\u00e4ssig beziehungsweise l\u00fcckenhaft hinsichtlich der erzielbaren Fehlererkennungsleistung. Das Problem hierbei sind kritische Fehlerstellen (engl. single point of failure) und L\u00fccken in der Redundanz. Ein Beispiel sind die f\u00fcr die Replikation notwendigen Mehrheitsentscheider, welche sich ihrerseits nicht weiter replizieren lassen. Obwohl diese Stellen \u00fcblicherweise klein und kurz im Sinne der Ausf\u00fchrung sind, ist ihre Art und Anzahl in hohem Ma\u00dfe anwendungsabh\u00e4ngig. Eine gezielte Beeinflussung der nicht-funktionalen Eigenschaft Zuverl\u00e4ssigkeit ist somit nur schwer durch softwarebasierte Fehlertoleranz zu erreichen. Vereinfacht ausgedr\u00fcckt stellt sich in diesem Fall die schon von Juvenal aufgeworfene Frage: Wer aber wird die W\u00e4chter selbst bewachen? (Juvenal, Satire 6, 347 f.)\nZur L\u00f6sung dieser Frage entwickelt diese Arbeit den kombinierten Redundanzansatz CORED (engl. Combined Redundancy). Dieser ganzheitliche softwarebasierte Fehlertoleranzansatz geht von der redundanten Ausf\u00fchrung der Anwendung im Sinne von Prozessinkarnationen aus und beseitigt die verbliebenen Zuverl\u00e4ssigkeitsengp\u00e4sse durch die gezielte Einbringung von Informationsredundanz in Form von arithmetischer Codierung. CORED eliminiert als W\u00e4chter der W\u00e4chter die kritischen Fehlerstellen vollst\u00e4ndig und bietet somit das R\u00fcstzeug f\u00fcr die systematische Beeinflussung der Zuverl\u00e4ssigkeit auf Ebene des Betriebssystems. Eine vollst\u00e4ndige experimentelle Analyse der eingesetzten arithmetischen Codierung vom Entwurf bis auf die Befehlssatzebene weist dabei die vollst\u00e4ndige und zuverl\u00e4ssige Fehlererkennung und eine signifikante Verbesserung der Erkennungsleistung gegen\u00fcber vergleichbaren Ans\u00e4tzen nach. Die effektive Fehlererkennung reicht dabei von den Eing\u00e4ngen bis zu den Ausg\u00e4ngen des Systems und l\u00e4sst sich im Bedarfsfall auch \u00fcber dessen Grenzen hinaus erweitern.\nRegelungswendungen besitzen aufgrund ihrer Steuerungsfunktion und der engen Kopplung an die Umwelt typischerweise ein ausgewiesenes Schutzbed\u00fcrfnis. Die praktische Anwendbarkeit des CORED-Ansatzes wird daher am Beispiel der Fluglageregelung des unbemannten Luftfahrzeugs I4Copter veranschaulicht. Der Einsatz von CORED erfolgt dabei von dessen Sensorik bis zur Aktorik und ist funktional f\u00fcr den Anwendungsentwickler weitgehend transparent umsetzbar. Mit der Replikation geht jedoch auch eine unvermeidbare Beeinflussung der zeitlichen Eigenschaften der Anwendung einher. Diese hat potenziell negative Auswirkungen auf die Regelungseigenschaften und erschwert die Umsetzung der Fehlertoleranz. Mit CORED@CONTROL untersucht diese Arbeit weitergehende Ans\u00e4tze f\u00fcr die anwendungsgewahre Erweiterung der CORED-Grundbausteine um eine Schnittstelle zwischen Replikation und Regelung. Diese unterst\u00fctzt die Abbildung regelungstechnischer Aufgaben auf replizierbare Prozessinkarnationen des Echtzeitbetriebssystems und erm\u00f6glicht den Austausch der temporalen Eigenschaften mit der Regelungsanwendung.\nAs technology scales, hardware designs for embedded systems offer more performance and parallelism for the price of being less reliable. Therefore, soft-error mitigation is one of the major challenges for safety-critical applications and systems. Soft-errors occur randomly and induce temporary malfunctions of the hardware, which have to be handled actively during execution.\nThe construction of reliable systems from unreliable components generally requires the introduction of redundancy, for example by means of structural replication. Besides adding costly hardware redundancy, virtually sacrificing the technology gain, software-based fault-tolerance offers a selective and resource-efficient alternative. Although being a proven technique in general, existing approaches still suffer from an incomplete coverage or significant residual error rates. Single points of failure and gaps in the intended redundancy domain cause the trouble. A majority voter is a case in point, which in itself cannot be redundant but is mandatory for redundancy schemes. Although these components are usually considered as relatively small and short in terms of execution time, their shape and quantity is highly application specific. Therefore, systematically improving the actual reliability by dint of software-based fault-tolerance tends to be very difficult. In simple terms, software-based redundancy suffers from the causality dilemma of monitoring itself, keeping with Juvenal's critical demand: But who can watch the watchmen? (Satire 6, p. 347 sq.)\nTo solve this problem, this thesis develops the Combined Redundancy (CORED) approach. This holistic software-based fault-tolerance scheme is based on redundant execution of the safety-critical application's processes. Moreover, it prevents redundancy shortages by selectively applying arithmetic coding techniques to cover any gaps in the replicated execution. Thereby, CORED eliminates all remaining single points of failures and acts as the watchman of the watchmen; it thus offers the redundancy tools for a systematic improvement of the non-functional property reliability at the operating system level. An exhaustive experimental evaluation of CORED's building blocks and the employed arithmetic coding - from the design to the machine-dependent implementation - satisfactorily shows the absence of errors and a significant improvement of the overall fault-detection performance in comparison to similar approaches. CORED considers the input data acquisition and the output data distribution and thereby features a full input-to-output protection of safety-critical applications, which can be even extended over system boundaries.\nControl applications usually possess designated reliability requirements because of their safety-related control function and close coupling with the physical environment. Therefore, the general feasibility of CORED is demonstrated by the example of the mission-critical flight control of the I4Copter unmanned aerial vehicle. The approach can be applied transparently to the application developer in terms of functional aspects. However, the redundant execution of the application is inevitably interfering with the temporal properties assumed by the controllers design. This potentially has a negative impact on the control performance and thereby complicates the implementation of CORED-based fault-tolerance in safety-critical control applications. With CORED@CONTROL, this thesis additionally explores an appropriate application-aware interface for CORED in order to support and simplify the mapping of control activities to replicable processes of the underlying real-time operating system. This interface also enables the feedback of the changing temporal properties to the control application.\n2014-08-11\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5056\nurn:nbn:de:bvb:29-opus4-50561\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50561\nhttps://opus4.kobv.de/opus4-fau/files/5056/DissertationPeterUlbrich.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5068\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nddc:510\nddc:530\nddc:620\nccs\nccs:I.6.3\npacs\npacs:41.20.Jb\npacs:42.25.Fx\npacs:42.30.Kq\nmsc\nmsc:68T05\nmsc:78-04\nmsc:90-08\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Uebergreifend_ohneAngabe\ninstitutes:Tech_Informatik\nArtificial Evolution for the Optimization of Lithographic Process Conditions\nK\u00fcnstliche Evolution f\u00fcr die Optimierung von Lithographischen Prozessbedingungen\nF\u00fchner, Tim\nOptimierung\nPhotolithographie\nMehrkriterielle Optimierung\nEvolution\u00e4rer Algorithmus\nMemetischer Algorithmus\nIntegrierte Schaltung\nddc:004\nddc:510\nddc:530\nddc:620\nMiniaturization is a driving force both for the performance and for cost reductions of semiconductor devices. It is therefore carried on at an enormous pace. Gordon Moore proposed and later refined an estimation stating that the minimization of costs would lead to a doubling of the density of integrated circuits every two years. And in fact, this time scale---known as Moore's law---is still aspired at by the major players in the industry. Photolithography, one of the key process steps, has to keep up with this pace. In the past, the introduction of new technologies, including a smaller wavelength of the illumination system or higher numerical apertures (NA) of the projector, has led to a relatively straightforward downscaling approach. Today, optical lithography is confined to argon fluoride excimer lasers with a wavelength of 193 nanometers and an NA of 1.35. The introduction of next generation lithography approaches such as extreme ultraviolet lithography have been delayed and will not be applicable until several years from now. Further scaling hence leads to dramatically decreases process margins since patterns with dimensions of only a fraction of the wavelength have to be lithographically created.\nIn this work, computational methods are devised that are suited to drastically improve process conditions and hence to push resolution beyond former limitations. The lithographic process can be broadly grouped into the stepper components: the illumination system, the photomask, the projection system and the wafer stack. As shown in this dissertation, each element exhibits a number of parameters that can be subjected to optimization. To actually enhance resolution, however, a rigorous simulation and computation regime has to be established. The individual ingredients are discussed in detail in this thesis.\nAccordingly, the models required to describe the lithography process are introduced and discussed. It is shown that the numerical and algorithmic implementation can be regarded as a compromise between exactness and computation time. Both are critical to obtain predictive, yet feasible approaches. Another complication is the multi-scale and multi-physics nature of the first principle process models. Although it is sometimes possible to derive individual optimization-tailored, reduced models, such an approach is often prohibitive for a concise co-optimization of multiple aspects.\nIn this work, we thus examine an approach that allows for a direct integration of first principle models. We investigate the use of evolutionary algorithms (EAs) for that purpose. These types of algorithms can be characterized as flexible optimization approaches that mimic evolutionary mechanisms such as selection, recombination and mutation. Many variants of related techniques exist, of which a number are considered in this dissertation. One chapter of this thesis is dedicated to the discussion of different representations and genetic operators, including motivations of the choices made for the following studies.\nThe lithographic process is characterized not only by a large number of parameters but can also be evaluated by a wide range of criteria, some of which may be conflicting or incommensurable---such as figures of merits like performance and manufacturability. We therefore apply a multi-objective genetic algorithm (GA) that is specifically tailored to identifying ideal compromise solutions. The characteristics of multi-objective optimization, especially when performed with evolutionary algorithms, are discussed in this thesis.\nThere is no such thing as a universal optimizer. EAs, for example, can be considered highly flexible, but they fail to intensively exploit local information. In an attempt to get the best of both worlds, we combine evolutionary with local search routines. We thoroughly discuss our approach to these hybrid techniques and present a number of benchmark tests that demonstrate their successful applications.\nThe majority of optimization problems in lithography are characterized by computationally expensive fitness evaluations. The reduction of the number of candidate solutions is therefore critical to maintain a feasible optimization procedure. To this end, we devised a function approximation approach based on an artificial neural network. Specifically, the GA population constitutes the training pattern for the network. The resulted output is the approximated fitness function. While the global search using the GA is still conducted on the exact search space, the local search is carried out on this approximation, leading to a much reduced runtime. The efficiency and feasibility of this approach is demonstrated by a number of benchmark tests.\nThe algorithms, frameworks and programs developed in the scope of this work are deployed as software modules that are available through the computational lithography environment Dr.LiTHO of the Fraunhofer IISB. The general software structure is briefly discussed. In order to achieve feasible optimization runtimes, rigorous distribution and parallelization techniques need to be employed. For this dissertation, a number of different approaches are devised and discussed in this thesis.\nA variety of application examples demonstrate the benefits of the devised methods. In a first set of examples, source/mask optimization problems are formulated and solved. In contrast to related work, which is mainly aimed at developing models that are specifically tailored to the underlying optimization routines, the direct approach proposed here is capable of directly employing models that are typically used in lithography simulation. A multitude of results using different problem representations is presented. Additional model options including mask topography effects are demonstrated. It is shown that the approach is not restricted to simplistic aerial image-based evaluations but is able to take the process windows and thin-film effects into account. Moreover, an extension to future resolution enhancement techniques, for example, constructively using projector aberrations, is also demonstrated.\nIn another example series, three-dimensional mask optimizations are performed. There, the topography including the materials of the photomask absorber are subjected to optimization. Drastically improved configurations compared to both standard optical and EUV absorbers under various illumination conditions are obtained. In order to cover all aspects of the lithography process, the last section of this thesis is devoted to the optimization of the wafer stack. As an example, the anti-reflective coating applied at the bottom of the resist to reduce standing waves in the resist profile is optimized. Different configurations including single and bi-layer coating systems are examined and optimized for, especially for double patterning applications. Significant improvements in comparison to standard stacks are shown and discussed.\nThe thesis finally concludes with a discussion on the different optimization strategies and the optimization and simulation infrastructure developed for this work. Advantages and challenges of the methodology are highlighted and future directions and potentials are demonstrated.\nMiniaturisierung ist sowohl f\u00fcr die Leistungssteigerung als auch f\u00fcr die Kostensenkung von Halbleiterbauelementen von gro\u00dfer Bedeutung und wird daher mit einer enorm hohen Geschwindigkeit betrieben. Gordon Moore leitete daraus eine Sch\u00e4tzung ab, die besagt, dass die Hersteller gezwungen seien, etwa alle zwei Jahre die Dichte der integrierten Schaltungen zu verdoppeln. Und tats\u00e4chlich verfolgen die Hauptakteure der Industrie dieses Ziel -- bekannt als Moore's Law -- noch heute. Photolithographie, einer der wichtigsten Prozessschritte, hat sich diesem Ziel unterzuordnen.\nIn der Vergangenheit stellte die Einf\u00fchrung neuer Technologiestufen, einschlie\u00dflich kleinerer Wellenl\u00e4ngen der Beleuchtungssysteme oder h\u00f6here numerische Aperturen (NA) der Projektionssysteme, einen relativ einfachen Ansatz dar, Schaltungsstrukturen zu verkleinern. Heute allerdings muss sich die optische Lithographie auf den Einsatz von Argon-basierten Excimer-Laser mit einer Wellenl\u00e4nge von 193 Nanometer und einer NA von 1,35 beschr\u00e4nken. Die Einf\u00fchrung neuer Lithographie-Generationen, beispielsweiser unter Ausnutzung extrem ultravioletten (EUV) Lichtes, verz\u00f6gert sich, so dass mit ihr erst in mehreren Jahren zu rechnen ist. Eine weitere Verkleinerung der Bauelemente f\u00fchrt so zu einer deutlichen Versch\u00e4rfung der Anforderungen an den lithographischen Prozess, da Strukturen mit Abmessungen eines Bruchteiles der zur Verf\u00fcgung stehenden Wellenl\u00e4nge abgebildet werden m\u00fcssen.\nIn dieser Arbeit werden deshalb numerische Methoden entwickelt, die geeignet sind, Prozessbedingungen signifikant zu verbessern und damit Aufl\u00f6sungen jenseits vorheriger Beschr\u00e4nkungen zu erzielen. Der Lithographieprozess kann in folgende Komponenten unterteilt werden: das Beleuchtungssystem, die Photomaske, das Projektionssystem und das Schichtsystem auf der Halbleiterscheibe. Wie in dieser Dissertation gezeigt wird, weist jede dieser Komponenten eine gro\u00dfe Anzahl optimierbarer Parameter auf. Um tats\u00e4chlich eine Verbesserung der Aufl\u00f6sung zu erzielen, ist jedoch der Einsatz umfassender Simulationswerkzeuge unabdingbar. Deren einzelne Bestandteile werden in dieser Arbeit er\u00f6rtert.\nSo werden die ben\u00f6tigten Modelle, die den Lithographieprozess beschreiben, vorgestellt und diskutiert. Es wird gezeigt, dass die numerische und algorithmische Umsetzung aus einem Kompromiss zwischen Genauigkeit und Rechenzeit besteht. Beide Kriterien sind entscheidend bei der Entwicklung eines pr\u00e4diktiven und praktikablen Ansatzes. Eine weitere Komplikation ergibt sich aus der Multi-Skalen- und Multi-Physik-Eigenschaft pr\u00e4diktiver Prozessmodelle. Obwohl es bisweilen m\u00f6glich ist, reduzierte Modelle f\u00fcr ein spezielles Optimierungsproblem zu entwickeln, eignet sich ein solches Vorgehen im Allgemeinen nicht f\u00fcr die gleichzeitige Optimierung mehrerer Prozessaspekte.\nIn dieser Arbeit wird daher ein Ansatz untersucht, der die direkte Nutzung exakter Modelle erlaubt. Als Optimierungsverfahren werden dabei evolution\u00e4re Algorithmen (EA) entwickelt und verwendet. EAs bezeichnen probabilistische Verfahren, die evolution\u00e4re Mechanismen wie Selektion, Rekombination und Mutation imitieren und sich durch ein hohes Ma\u00df an Flexibilit\u00e4t auszeichnen. Da es zahlreiche EA-Varianten gibt, widmet sich ein Kapitel dieser Arbeit der Diskussion und Untersuchung verschiedener Darstellungsoptionen und genetischer Operatoren. Dabei wird insbesondere die f\u00fcr diese Arbeit getroffene Auswahl motiviert.\nDer lithographische Prozess umfasst nicht nur eine Vielzahl an Parametern, sondern bedarf auch der Bewertung hinsichtlich verschiedener Kriterien, von denen nicht wenige wechselseitig unvereinbar oder unvergleichbar sind. So sind beispielsweise Herstellbarkeit und Leistungsf\u00e4higkeit im Allgemeinen inkommensurabel. Daher wird ein multikriterieller genetischer Algorithmus (GA), der speziell auf die Suche nach Kompromissl\u00f6sungen zugeschnitten ist, implementiert und untersucht. Die Eigenschaften von Mehrzieloptimierung, insbesondere im Zusammenhang mit evolution\u00e4ren Algorithmen, werden in dieser Arbeit eingehend diskutiert.\nGenau so wenig wie andere Optimierer k\u00f6nnen EAs als universell bezeichnet werden: Sie zeichnen sich zwar durch hohe Flexibilit\u00e4t aus, sind aber anderen Verfahren bei der intensiven Ausnutzung lokaler Informationen oft unterlegen. Eine Kombination evolution\u00e4rer und lokaler Suchalgorithmen bietet sich deshalb an. Ein entsprechendes hybrides Verfahren wird in dieser Arbeit entwickelt, und dessen Leistungsf\u00e4higkeit wird mit Hilfe einer Reihe von Benchmark-Funktionen demonstriert.\nDie Mehrzahl lithographischer Optimierungsprobleme ist durch rechenintensive G\u00fcteauswertungen charakterisiert. Die Zahl der Auswertungen muss daher auf ein Minimum reduziert werden. Es wird zu dem Zweck ein Ansatz verfolgt, bei dem die Fitnessfunktion durch eine deutlich schneller auszuwertende Ersatzfunktion gen\u00e4hert wird. Dabei kommt ein k\u00fcnstliches neuronales Netz zum Einsatz, das die durch den GA erzeugte Population aus L\u00f6sungskandidaten als Trainingsinstanzen nutzt, um so ein Modell der Fitnessfunktion zu erzeugen. Dieses Modell wird dann f\u00fcr eine intensive lokale Suche verwendet, w\u00e4hrend die globale GA-Suche auf der urspr\u00fcnglichen, exakten Funktion durchgef\u00fchrt wird. Die Effizienz und die Machbarkeit dieses Ansatzes wird an einer Reihe von Vergleichstests nachgewiesen.\nDie f\u00fcr diese Arbeit entwickelten Algorithmen, Frameworks und Programme stehen im Rahmen der Fraunhofer IISB Lithographiesimulationsumgebung Dr.LiTHO als Software-Module zur Verf\u00fcgung. Der prinzipielle Aufbau der Recheninfrastruktur wird kurz diskutiert, insbesondere im Hinblick auf die entwickelten und verwendeten Verteilungs- und Parallelisierungsverfahren, ohne die praktikable Optimierungsl\u00e4ufe aufgrund der hohen Rechenzeiten nicht m\u00f6glich w\u00e4ren.\nEine Vielzahl von Anwendungsbeispielen zeigt die Vorteile der entwickelten Methoden. In einer Studie werden Beleuchtungsquellen/Photomasken-Optimierungsprobleme formuliert und gel\u00f6st. Im Gegensatz zu vergleichbaren Arbeiten, die zumeist auf vereinfachten, effizienten Modellen beruhen, wird hier ein direkter Ansatz verfolgt, der es erlaubt, exakte, in der Lithographiesimulation \u00fcbliche Modelle zu verwenden. Mehrere Darstellungsvarianten werden vorgestellt und anhand zahlreicher Ergebnisse diskutiert. Die Flexibilit\u00e4t des Ansatzes wird unter anderem durch die Ber\u00fccksichtigung von Maskentopographieeffekten demonstriert. Es wird ferner gezeigt, dass das Verfahren nicht auf die Auswertung von Luftbildern beschr\u00e4nkt ist, sondern auch andere Komponenten wie Prozessfenster oder D\u00fcnnfilmeffekte einbeziehen kann. Weitere Ergebnisse demonstrieren die Erweiterbarkeit des Verfahrens auf zuk\u00fcnftige Techniken zur Verbesserung der Aufl\u00f6sung, zum Beispiel, der Ausnutzung der Projektor-Aberrationskontrolle.\nZiel einer weiteren Reihe von Simulationsexperimenten ist die dreidimensionale Maskenoptimierung, in der zus\u00e4tzlich zur Quellen/Maken-Optimierung auch die Topographie und die Materialeigenschaften der Photomaske optimiert werden. Dabei k\u00f6nnen deutliche Verbesserungen im Vergleich zu Standardkonfigurationen erzielt werden. Optimierungsergebnisse sowohl f\u00fcr optische als auch f\u00fcr EUV-Lithographie werden pr\u00e4sentiert und diskutiert. Um alle Aspekte des Lithographieprozesses abzudecken, befasst sich der letzte Abschnitt der Arbeit mit dem Schichtsystem auf der Halbleiterscheibe. Als Beispiel wird die antireflektive Beschichtung auf der Unterseite des Photolackes optimiert. Diese Beschichtung wird eingesetzt, um eine Interferenz zwischen einfallendem und r\u00fcckreflektiertem Licht zu verhindern, die zu stehende Wellen f\u00fchrt. Verschiedene Anordnungen, darunter Einzel- und Zweischichtsysteme, werden untersucht und verbessert. Ziel dieser Studie ist es insbesondere, die Ver\u00e4nderungen des Schichtsystems unter heute h\u00e4ufig verwendeten Mehrfachbelichtungsverfahren exakt zu beschreiben und zu verbessern.\nDie Dissertation schlie\u00dft mit einer Diskussion sowohl der verschiedenen Optimierungsstrategien als auch der f\u00fcr diese Arbeit entwickelten Optimierungs- und Simulationsinfrastruktur. Vor- und Nachteile der Methodik werden hervorgehoben und m\u00f6gliche zuk\u00fcnftige Anwendungen und Erweiterungen vorgestellt.\n2014-08-15\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5068\nurn:nbn:de:bvb:29-opus4-50689\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50689\nhttps://opus4.kobv.de/opus4-fau/files/5068/DissFuehner.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5087\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nddc:610\nccs\nccs:D.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nRespiratory Self-Navigation for Whole-Heart Coronary Magnetic Resonance Imaging\nAtmungs-Selbstnavigation f\u00fcr die koronare Magnetresonanzbildgebung des gesamten Herzens\nPiccini, Davide\ncoronary\nMRI\nmotion\nddc:000\nddc:610\nAs the average life span of the world population increases, cardiovascular diseases firmly establish themselves as the most frequent cause of death in many of the developed countries. Coronary artery disease (CAD) is responsible for more than half of these cases and there is, hence, a strong need for a non-invasive and radiation-free test that could be reliably adopted for its assessment in clinical routine. Although coronary magnetic resonance imaging (MRI) has always been regarded with high expectations, it is still not considered for clinical assessment of CAD. This is mainly due to several limitations of current coronary MRI examinations. The complex anatomy of the coronary arteries requires extensive scout-scanning to precisely plan the actual data acquisition. The current speed limitations of the MRI scanners and the contribution of cardiac and respiratory motion do not allow the high resolution acquisitions to be performed within the fraction of a single heartbeat. Consequently, data acquisition must be split into multiple heartbeats and usually performed during free-breathing. At the same time, gating with respect to a consistent respiratory position is applied using an interleaved navigated scan which monitors the position\nof the subject's diaphragm. Major improvements in standard navigator-gated free-breathing coronary MRI have\nbeen achieved in recent years, but a number of important intrinsic limitations, such as the prolonged and unknown acquisition times, the non-linearity of the motion compensation, and the complexity of the examination setup have so far hindered the clinical usage of this technique. In contrast, a technique known as self-navigation, which performs motion detection and correction solely based on imaging data of the heart, promises a priori knowledge of the duration of the acquisition with improved accuracy of the motion compensation and requires minimal expertise for the planning of the examination.\nIn this work, a novel acquisition and motion correction strategy for free-breathing self-navigated whole-heart coronary MRA was introduced, analyzed and implemented to be entirely integrated in a clinical MR scanner. The proposed acquisition method consists of a novel interleaved 3D radial trajectory, mathematically constructed on the basis of a spiral phyllotaxis pattern, which intrinsically minimizes the eddy currents artifacts of the balanced steady state free-precessing acquisition, while ensuring a complete and uniform coverage of k-space. The self-navigated respiratory motion detection is performed on imaging readouts oriented along the superior-inferior axes and is based on a method for the isolation and automatic segmentation of the bright signal of the blood pool. Motion detection of the segmented blood pool is then\nperformed using a cross-correlation technique. This fully automated respiratory selfnavigated method offers an easy and robust solution for coronary MR imaging that can also be integrated into a regular clinical routine examination. The technique was tested in volunteers, compared to the standard navigator-gating approach, and, for the first time to the author's knowledge, allowed self-navigation to be positively applied to a large patient study in an advanced clinical setting.\nIm Zuge des Anstiegs der durchschnittlichen Lebenserwartung der Weltbev\u00f6lkerung sind kardiovaskul\u00e4re Krankenheiten heute in vielen der entwickelten L\u00e4nder zur h\u00e4ufigsten Todesursache geworden. Koronare Gef\u00e4\u00dferkrankungen sind dabei die Ursache f\u00fcr mehr als die H\u00e4lfte dieser Todesf\u00e4lle. Es besteht deshalb ein gro\u00dfer Bedarf an Diagnosetechniken, die nichtinvasiv und ohne Einsatz ionisierender Strahlung zuverl\u00e4ssig in die klinische Routine eingebunden werden k\u00f6nnen. Obwohl der koronaren Magnetresonanztomographie (MRT) seit langem hohe Erwartungen entgegengebracht werden, wird sie nach wie vor nicht zur klinischen Bewertung koronarer Gef\u00e4\u00dferkrankungen eingesetzt. Die Gr\u00fcnde daf\u00fcr sind vielf\u00e4ltig, r\u00fchren jedoch vor allem von verschiedenen technischen Unzul\u00e4nglichkeiten derzeitiger koronarer MRT-Untersuchungen.\nDie komplexe Anatomie der Koronararterien bedarf vieler zeitaufwendiger Voraufnahmen zur pr\u00e4zisen Planung der eigentlichen Untersuchung. Zudem behindern die Bildgebungsgeschwindigkeit heutiger MRT-Scanner in Verbindung mit den Herz- und Atmungsbewegungen die Aufnahme hochaufgel\u00f6ster Bilder innerhalb eines Herzschlags. Deshalb mu\u00df die Datenakquisition \u00fcber mehrere Herzschl\u00e4ge verteilt und typischerweise wegen ihrer Dauer auch bei freier Atmung durchgef\u00fchrt werden. W\u00e4hrend der eigentlichen Bildaufnahme wird immer wieder ein sehr schnelles Navigatorbild akquiriert, aus dem die Position des Diaphragmas und damit die Atmungsphase bestimmt werden kann. Im Zuge eines nachgeordneten \u201eGatings\u201c werden dann nur die Bildinformationen einer bestimmten Atemposition zur Weiterverarbeitung genutzt.\nDie Techniken zur navigatorgest\u00fctzten Aufnahme koronarer MRT-Bilder bei freiem Atmen haben in den letzten Jahren wesentliche Verbesserungen erfahren. Allerdings haben die ihnen immanenten Einschr\u00e4nkungen wie die wesentlich erh\u00f6hte und zudem im Vorhinein unbekannte Aufnahmezeit, die Nichtlinearit\u00e4t der Bewegungskompensation, sowie die Komplexit\u00e4t der Untersuchung eine klinische Nutzung bisher verhindert. Im Gegensatz dazu versprechen die sogenannten Selbstnavigations-Methoden, die Bewegungsdetektion und -korrektur ausschlie\u00dflich anhand der aufgenommenen Herzbilddaten ausf\u00fchren, eine im Vorhinein bekannte Aufnahmezeit mit verbesserter Genauigkeit der Bewegungskompensation sowie wesentlich weniger ben\u00f6tigte Expertise bei der Akquisitionsplanung.\nIn der vorliegenden Arbeit wird eine neuartige Aufnahmetechnik und Bewegungskorrektur f\u00fcr selbstnavigierte koronare Magnetresonanz-Angiographie des ganzen Herzens bei freier Atmung vorgestellt, analysiert und auf einem klinischen Kernspintomographen implementiert. Die vorgeschlagene Methode basiert auf einer neuartigen geschachtelten dreidimensionalen Radialtrajektorie, die mathematisch aus einem spiralen phylotaktischem Muster konstruiert wird. Ihre spezifischen Charakteristika garantieren eine intrinsische Minimierung von Wirbelstromartefakten der verwendeten Balanced-SSFP-Akquisition (Aufnahme bei symmetrisch ausgeglichener freier Pr\u00e4zession im station\u00e4ren Zustand) bei gleichzeitig stets uniformer Abdeckung des K-Raumes. Die selfnavigierte Atembewegungsdetektion wird anhand von Bildauslesez\u00fcgen in superiorer-inferiorer Richtung durchgef\u00fchrt und basiert auf einer Methode zur Isolierung und automatischen Segmentierung des hellen Blutsignales. Die eigentliche Bewegungskorrektur des segementierten Blutreservoirs wird dann mittels Kreuzkorrelation durchgef\u00fchrt.\nDiese vollautomatische selbstnavigierte Methode bietet eine einfache und robuste L\u00f6sung f\u00fcr die koronare MRT-Bildgebung, die auch in klinische Routineuntersuchungen integriert werdne kann. Die Technik wurde an Freiwilligen getestet und mit dem Gatingansatz als Referenz verglichen. Sie erlaubte, nach bestem Wissen des Autors, das erste Mal die Anwendung solcher Selbstnavigations-Techniken in einer gro\u00dfen Patientenstudie in einer erweiterten klinischen Umgebung.\n2014-08-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5087\nurn:nbn:de:bvb:29-opus4-50877\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-50877\nhttps://opus4.kobv.de/opus4-fau/files/5087/DavidePicciniDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5165\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:920\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:01-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Phil\nDer andere Kepler - Vom Aufstieg eines fr\u00fchneuzeitlichen Gelehrten mit Hilfe der Astrologie\nBauer, Katrin\nAstrologie\nddc:920\nDie Dissertation untersucht den Nutzen, den Johannes Kepler im Laufe seiner Karriere aus der Bearbeitung astrologischer Inhalte zog. Die Arbeit geht dabei in drei Schritten vor. Zun\u00e4chst wird analysiert, welchen Stellenwert astrologische Inhalte innerhalb von Kontaktaufnahmen f\u00fcr den Gelehrten spielten. Der zweite Abschnitt wendet sich der Erledigung von Auftragsarbeiten zu, wobei wiederum astrologische Fragestellungen bevorzugt in den Blick genommen werden. In einem dritten Hauptteil wird schlie\u00dflich die Positionierung Keplers in der wissenschaftlichen Welt thematisiert. Von besonderem Interesse sind dabei Rezensionen und gelehrter Austausch sowie eigene wissenschaftliche Werke.\nThe main topic of this work is Johannes Kepler's use of astrology for his own carreer. It contains three main parts. In the first part it is analized how Kepler commenced his contacts. The second main chapter gives an insight into the duties of an early modern mathematician. The last part emphasizes the importance of astrology for Kepler's standing in his coeval scientific field.\n2014-09-16\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5165\nurn:nbn:de:bvb:29-opus4-51657\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-51657\nhttps://opus4.kobv.de/opus4-fau/files/5165/DissertationKatrinBauer.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5296\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\nccs\nccs:B.\npacs\npacs:80.00.00\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Werkstoffwiss\nLaser Welding of Silicon Foils for Thin-Film Solar Cell Manufacturing\nLaserschwei\u00dfen von Siliziumfolien zur Herstellung von D\u00fcnnschicht-Solarzellen\nHe\u00dfmann, Maik\nLaserschwei\u00dfen\nSolarzelle\nD\u00fcnnschicht\nSilizium\nddc:600\nThin-film solar module manufacturing is one of the most promising recent developments in photovoltaic research and has the potential to reduce production costs. As the necessity for competitive prices on the world market increases and manufacturers endeavor to bring down the cost of solar modules, thin-film technology is becoming more and more attractive. In this work a special technique was investigated which makes solar cell manufacturing more compatible with an industrial roll-to-roll process. This technique allows the creation of the first monocrystalline band substrate by welding several monocrystalline silicon wafers together, so that the size restriction of float-zone grown wafers can be overcome. Currently the size is 8 inches in diameter. Float-zone grown material is well suited as feedstock for high efficiency solar cells and it has also been very intensively studied in the past. This makes it the perfect feedstock material for thin-film solar modules. Unfortunately this material is quite expensive and therefore it should only serve as feedstock to generate the band substrate. After this step the necessary silicon layers to produce solar cells are grown epitaxially on top of the band substrate using chemical vapor deposition. To produce solar cells a silicon layer is separated from the band substrate using a layer transfer process. Subsequently the band substrate can be repeatedly reused to produce an infinite amount of silicon layers without requiring any silicon ingot feedstock.\nThe linchpin for this technique is the welding step from single wafers to a band substrate. Thus, this work focuses on the investigation of the welding process. Welded samples were analyzed using micro-Raman and electron backscatter diffraction (EBSD). Moreover, the achievement of solar cells on top of 50 \u00b5m thick silicon foils and welded silicon foils are reported.\nDie Produktion von D\u00fcnnschicht-Solarmodulen ist eine der vielversprechendsten Entwicklungen in der Photovoltaik in der n\u00e4heren Vergangenheit, weil diese Technik geringe Produktionskosten verspricht. Wegen der Notwendigkeit von wettbewerbsf\u00e4higen Preisen an den Weltm\u00e4rkten und dem Bem\u00fchen der Hersteller die Produktionskosten zu senken ger\u00e4t die D\u00fcnnschicht-Technik immer mehr in den Fokus. In dieser Arbeit wird eine spezielle Technik untersucht, die die Herstellung von Solarzellen weiter an ein industrielles Rolle-zu\u2013Rolle-Verfahren ann\u00e4hern soll. Diese Technik erlaubt es, monokristalline Siliziumwafer miteinander zu dem ersten monokristallinen Bandsubstrat zu verschwei\u00dfen. Dadurch kann die Gr\u00f6\u00dfenrestriktion der Produktion von im Zonenschmelzverfahren hergestellten einkristallinen Silizium-Ingots \u00fcberwunden werden, die momentan einen Durchmesser von 8 Zoll haben. Da im Zonenschmelzverfahren gewonnenes Silizium als Ausgangsmaterial f\u00fcr Hochleistungssolarzellen ideal ist und auch schon intensiv untersucht wurde, ist es der perfekte Ausgangspunkt f\u00fcr D\u00fcnnschicht-Solarmodule. Allerdings ist der hohe Preis f\u00fcr dieses Material ein Problem. Darum soll das hochwertige und teure Silizium nur f\u00fcr die Herstellung des Ausgangsbandsubstrates verwendet werden. Danach soll mittels chemischer Gasphasenabscheidung eine Epitaxie-Schicht auf dem Band gewachsen werden und diese gewachsene Schicht mittels Transferprozess vom Ausgangsband getrennt werden, um damit Solarzellen herzustellen. Das Bandsubstrat wird wiederverwendet um eine endlose Anzahl von Siliziumschichten zu produzieren ohne die Notwendigkeit von Silizium-Ingots als Ausgangmaterial.\nF\u00fcr dieses Verfahren ist das Schwei\u00dfverfahren der Dreh- und Angelpunkt, daher wurde in dieser Arbeit der Fokus auf das Charakterisieren der Verschwei\u00dfung gelegt. Diese wurden mit Hilfe von Mikro-Raman und Electron backscatter diffraction (EBSD) untersucht. Au\u00dferdem wurden erfolgreich Solarzellen auf 50 \u00b5m d\u00fcnnen Siliziumfolien sowie Solarzellen auf verschwei\u00dften Siliziumfolien hergestellt.\n2014-10-14\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5296\nurn:nbn:de:bvb:29-opus4-52969\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-52969\nhttps://opus4.kobv.de/opus4-fau/files/5296/Dissertation%20Maik%20He%C3%9Fmann%20-%20Laser%20Welding%20of%20Silicon%20Foils%20for%20Thin-Film%20Solar%20Cell%20Manufacturing.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5363\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nPerformance Engineering of Numerical Software on Multi- and Manycore Processors\nPerformance Engineering von numerischer Software auf Mehr- und Vielkernprozessoren\nSt\u00fcrmer, Markus\nMehrkernprozessor\nGraphikkarte\nParallelisierung\nParallelization\nScientific computing\nWissenschaftliches Rechnen\nddc:004\nThe goal of performance engineering is to make the resource usage a controllable property of software. This thesis contributes to this field from the perspective of a computer scientist who is involved in scientific computing.\nFirst, the necessary basis is provided: Understanding of the development process of numerical software and of the emergence of performance at the hardware/software interface, further methods to model, predict, analyze, measure, assess, and optimize software performance.\nThe first study is about a port of a fluid simulation in complex geometries using a lattice Boltzmann method to the Cell Broadband Engine Architecture (CBEA). For this, a thorough adaption to the peculiarities of this architecture and subsequent low-level optimization are required.\nThe batch variant of the orthogonal matching pursuit algorithm allows to efficiently derive sparse representations for a large number of signals. Eventually, comparable performance is achieved on a dual-socket system with Intel processors, an IBM QS 20 blade, and on an NVIDIA GeForce GTX 480 graphics card. Each architecture, however, needs a different approach to achieve this.\nStencil computations on regular grids are often memory bound. They can then only be accelerated considerably by use of temporal blocking techniques, which are generally tedious to implement. A novel generic approach is proposed which can be used on cache-based architectures as well as on the CBEA and which can be strongly supported by a cross-platform framework. Corresponding implementations of two multigrid methods are described and analyzed.\nDesign and implementation of a software program which is able to simulate heat conduction within rolls in hot rolling mills in real-time with the aid of graphics cards concludes the list of examples. As this application requires outstanding run time performance, a holistic co-design approach is taken which matches model, numerical method, implementation, and target platform.\nDie Aufgabe von Performance Engineering ist es, den Ressourcenverbrauch von Software beherrschbar zu machen. Diese Arbeit tr\u00e4gt dazu aus der Sicht eines Informatikers im Umfeld des Computational Engineering bei.\nErst werden notwendige Grundlagen geschaffen: Verst\u00e4ndnis f\u00fcr den Entwicklungsprozess numerischer Software und das Entstehen von Software-Performance an der Hardware-Software-Schnittstelle, au\u00dferdem Methoden f\u00fcr ihre Modellierung, Vorhersage, Analyse, Messung, Bewertung und zuletzt Optimierung.\nDie erste Untersuchung betrachtet die Portierung einer Fluid-Simulation innerhalb komplexer Geometrien mit einer Lattice-Boltzmann-Methode auf die Cell Broadband Engine Architecture (CBEA). Hier ist eine grundlegende Anpassung an die dortigen Besonderheiten n\u00f6tig sowie eine maschinenorientierte Optimierung.\nDie Batch-Variante des Orthogonal-Matching-Pursuit-Algorithmus erlaubt es, einen gro\u00dfen Satz an Signalen als schwach besetzte Linearkombination eines Signalw\u00f6rterbuches darzustellen. Letztlich wird eine vergleichbare Performance auf einer Workstation, einem CBEA Blade und einer Grafikkarte erreicht. Daf\u00fcr sind aber jeweils ganz unterschiedliche Ans\u00e4tze n\u00f6tig.\nStencil-Berechnungen auf regul\u00e4ren Gittern sind oft durch die Speicherbandbreite beschr\u00e4nkt. Dann kann die Anwendung zeitlicher Blocking-Techniken Abhilfe schaffen. Ein Ansatz wird vorgestellt, der sowohl auf Cache-basierten Architekturen als auch der CBEA eingesetzt werden und gut mit einem Framework unterst\u00fctzt werden kann. Entsprechende Implementierungen zweier Mehrgitterverfahren werden beschrieben und analysiert.\nDen Abschluss bilden Entwurf und Implementierung einer Software, die die W\u00e4rmeausbreitung innerhalb einer Walze beim Warmwalzen mit Hilfe von Grafikkarten in Echtzeit berechnen kann. Diese hohen Anforderungen an die Rechengeschwindigkeit erfordern einen ganzheitlichen Ko-Design-Ansatz, bei dem Modell, numerisches Verfahren, Implementierung und Zielplattform aufeinander abgestimmt werden.\n2014-10-22\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5363\nurn:nbn:de:bvb:29-opus4-53635\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-53635\nhttps://opus4.kobv.de/opus4-fau/files/5363/MarkusStuermerDissertation-1.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5377\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:520\nccs\nccs:A.\npacs\npacs:90.00.00\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nCluster search for neutrinos from Active Galactic Nuclei\nCluster-Suche nach Neutrinos von\naktiven Galaxienkernen\nFritsch, Ulf\nNeutrinoastronomie; ANTARES Neutrino Teleskop\nddc:520\nDas ANTARES Neutrinoteleskop, welches sich im Mittelmeer nahe Toulon, Frankreich, befindet, ist derzeit das einzige Tiefseeteleskop zur Messung von hochenergetischen Neutrinos aus astrophysikalischen Quellen. Es besteht aus 885 Photomulitipliern, die an 500m langen flexiblen Kabeln befestigt sind. Die Kabeln werden am Boden mittels eines Ankers fixiert und durch eine Boje straff gehalten. Aufgrund von zeitlich ver \u0308anderlichen Unterwasserstro \u0308mungen werden die Kabel ausgelengt und die Detektorgeometrie wird zeitabha \u0308ngig. Da die Rekonstructionssoftware aus der Zeit- und Ortsinformation von Photomultipliersignalen eine Teilchenspur berechnet, wird eine alle zwei Minuten aktualisierte Detektorgeometrie ben \u0308otigt. Dazu habe ich die ben \u0308otigte Software entwickelt und getestet, welche sowohl fu \u0308r die Positionsberechnung an sich als auch fu \u0308r den Datenaustausch mit der zentralen ORACLE-Datenbank ver- antwortlich ist. Diese Arbeit ist auch wichtiger Bestandteil der folgenden ANATARES- Ver \u0308offentlichung [1]:\nhttp://iopscience.iop.org/1748-0221/7/08/T08002/.\nDas gro\u00dfe Ziel im Bereich der Neutrinoastronomie ist die Identifikation von Punktquellen am Himmel. Aufgrund theoretischer Vorhersagen und Messungen von hoch- energetischer Gamma-Strahlung wei\u00df man dass im Universum Objekte mit hadronischen Beschleunigungsmechanismen existieren, die mit Neutrinoemission einhergehen. Quellen wie Supernova-U \u0308berreste, Gamma-ray Bursts oder aktiven Galaxienkernen sind derartige Kandidaten. Aufgrund seiner Lage auf der no \u0308rdlichen Hemispha \u0308re ist das ANTARES Neutrinoteleskop fu \u0308r die Beobachtung der am Su \u0308dhimmel gelegenen TANAMI (Tracking Active Galactic Nuclei with Austral Milliarcsecond Interferome- try) AGNs, welche radio-laut und variabel sind, besonders geeignet. Daten des Large Area Teleskops auf dem Fermi Satelliten zeigen stark variable Photonenflu \u0308sse auf Zeitskalen von mehrere Wochen bis Monaten. Ich habe daher ein neues Softwarepaket zur Auswertung von zeitlich geclusterten Neutrinoevents fu \u0308r ANTARES entworfen und getestet. 24 TANAMI Quellen wurden fu \u0308r diese und kommende Analysen ausgewa \u0308hlt.\nUm die Analyse an echten Daten zu testen, wurde ein Subsample von acht Quellen fu \u0308r ein Unblinding ausgewa \u0308hlt, wobei darauf geachtet wurden, dass diese eine m \u0308oglichst homogene Verteilung am Himmel besitzen. Es konnte kein signifikanter Cluster von Events in den ANTARES Daten gefunden werden. Fu \u0308r die Quelle PKS0208-512 wur- den zwei Ereignisse gez \u0308ahlt, fu \u0308r 1954-388 eines. Limits auf den Fluss fu \u0308r die acht Quellen wurden berechnet.\nThe ANTARES neutrino telescope in the Mediterranean Sea near Toulon, France, is the world\u2018s only subsea Cherenkov telescope for the detection of astrophysical high- energy neutrinos. 885 photomultipliers are mounted on 500 m long flexible cables that are anchored on the seabed and pulled tight by submersed buoys. Due to underwa- ter sea currents the detector units are tilted and their positions are time dependent. As the reconstruction software tries to find particle tracks from time-stamped photo- multiplier hits in the detector, the detector alignment system is a crucial part of the calibration task of the telescope. I have developed, successfully tested and frequently improved the software for the position reconstruction including its data-I/O interface to the central ORACLE-database in Lyon. The software, the implemented algorithm and cross-checks with other ANTARES measurements are presented. The work is also summarized in the paper \u201cThe Positioning System of the ANTARES Neutrino Telescope\u201d [1] available from\nhttp://iopscience.iop.org/1748-0221/7/08/T08002/.\nThe main goal of neutrino astronomy using large-scale high-energy neutrino tele- scopes is the identification of neutrino point-sources in the sky. From theoretical predictions and measurements of high energetic gamma-rays, hadronic particle ac- celerators candidates in the universe have been identified. From these sources like supernova remnants, gamma ray bursts or active galactic nuclei neutrino emission is predicted. As ANTARES is located on the Northern Hemisphere and looks for neu- trinos from the southern sky, the TANAMI (Tracking Active Galactic Nuclei with Austral Milliarcsecond Interferometry) sample of radio-loud and variable AGN south of declination -30 degrees is an ideal target sample. Data from the Large Area Tele- scope on the Fermi satellite show variable photon fluxes on timescales of several weeks and months from many of these sources. I have developed an analysis to search for time-clustered neutrino events, including the full range from event handling up to cal- culation of statistical parameters. I have chosen 24 sources from the TANAMI sample to look at. For six of them, equally spread over the declination range, a dedicated flare simulation has been performed to test the code. The discovery potential as function of cone size and cuts on track reconstruction quality for the six sources is presented.\nIn order to test the analysis method on real data, for a small sub-sample of eight sources with a reasonable spread over the sky the data has been unblinded. No signif- icant cluster of neutrino events was found in the data. For the source PKS0208-512, two events were found, for 1954-388 one event. Consequently flux limits are presented for the unblinded sources.\n2014-10-23\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5377\nurn:nbn:de:bvb:29-opus4-53778\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-53778\nhttps://opus4.kobv.de/opus4-fau/files/5377/UlfFritschDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5395\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\nccs\nccs:J.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nDynamic Interventional Perfusion Imaging: Reconstruction Algorithms and Clinical Evaluation\nDynamische interventionelle Perfusionsbildgebung: Rekonstruktionsalgorithmen und klinische Evaluation\nManhart, Michael\nMedizinische Bildgebung\nComputertomographie\nSchlaganfall\nddc:600\nAcute ischaemic stroke is a major cause for death and disabilities with increasing prevalence in aging societies. Novel interventional stroke treatment procedures have the potential to improve the clinical outcome of certain stroke-affected patients. Certainly, prompt diagnosis and treatment are required. Brain perfusion imaging with computed tomography (CT) or magnetic resonance imaging (MRI) is a routine method for stroke diagnosis. However, in the interventional room usually only CT imaging with flat detector C-arm systems is available, which do not support dynamic perfusion imaging yet. Enabling flat detector CT perfusion (FD-CTP) in clinical practice could support optimized stroke management. By stroke diagnosis in the interventional room precious time until the start of treatment could be saved.\nRecently, first promising clinical results for FD-CTP imaging under laboratory conditions have been presented. Based on this work, this dissertation introduces and evaluates novel technical contributions for noise reduction, artifact reduction and dynamic reconstruction in FD-CTP. Furthermore, the feasibility of FD-CTP imaging in clinical practice is demonstrated for the first time using data acquired during interventional stroke treatments.\nCT perfusion imaging requires measurement of dynamic contrast agent attenuation over time. The contrast agent signal in the brain tissue is very low and noise is a major problem. Thus a novel computationally fast noise reduction technique for perfusion data is introduced.\nCurrently available C-arm systems have a comparably low rotation speed, which makes it challenging to reconstruct the dynamic change of contrast agent concentration over time. Therefore, a dynamic iterative reconstruction algorithm is proposed to utilize the high temporal resolution in the projection data for improved reconstruction of the contrast agent dynamics.\nNovel robotic C-arm systems (Artis zeego, Siemens Healthcare, Germany) provide a high speed rotation protocol (HSP) to improve the temporal acquisition of the contrast agent dynamics. However, the HSP suffers from angular under-sampling, which can lead to severe streak artifacts in the reconstructed perfusion maps. Thus a novel, computationally fast noise and streak artifact reduction approach for FD-CTP data is proposed. The feasibility of FD-CTP using the HSP is demonstrated with clinical data acquired during interventional treatment of two stroke cases.\nFurthermore, the design of a digital brain perfusion phantom for the thorough numerical evaluation of the proposed techniques is discussed.\nThe quality of the perfusion maps acquired and reconstructed using the introduced novel approaches suggests that FD-CTP could be clinically available in the near future.\nDer akute isch\u00e4mische Schlaganfall ist eine der Hauptursachen f\u00fcr Tod und Invalidit\u00e4t weltweit mit zunehmender Bedeutung in alternden Gesellschaften. Mit interventionellen Behandlungsmethoden ist es potentiell m\u00f6glich, die gesundheitliche Folgesch\u00e4den von Schlaganf\u00e4llen zu verringern. Daf\u00fcr ist allerdings eine zeitnahe Diagnose und Behandlung des Schlaganfalls notwendig. Zur Schlaganfallsdiagnose wird als Routineverfahren die Hirnperfusionsbildgebung mit Hilfe der Computertomographie (CT) oder der Magnetresonanztomographie (MRI) verwendet. \u00dcblicherweise ist im interventionellen Behandlungsraum nur CT Bildgebung mit Flachdetektor C-Bogen Systemen m\u00f6glich, welche bisher die dynamische Perfusionsbildgebung nicht unterst\u00fctzen. Flachdetektor CT Perfusionsmessung (FD-CTP) k\u00f6nnte zu einem verbesserten Schlaganfallsmanagement beitragen, wenn der Patient direkt im Interventionsraum untersucht und wertvolle Zeit bis zum Beginn der Behandlung gespart werden kann.\nUnl\u00e4ngst wurden erste vielversprechende klinische Ergebnisse von FD-CTP Messungen unter Laborbedingungen gezeigt. Aufbauend darauf werden in dieser Dissertation neue technische Verfahren zur Rauschreduktion, Artefaktreduktion und zur dynamischen Rekonstruktion f\u00fcr die FD-CTP vorgestellt und evaluiert. Des Weiteren wird zum ersten Mal die Durchf\u00fchrbarkeit von FD-CTP in der klinischen Praxis anhand von w\u00e4hrend interventioneller Schlaganfallbehandlungen aufgezeichneten Daten demonstriert,\nBei der CT Perfusionsbildgebung wird die dynamischer Kontrastmittelschw\u00e4chung \u00fcber die Zeit gemessen. Dabei ist das Signal des Kontrastmittels im Hirngewebe sehr gering. Daher wurde eine neue recheneffiziente Rauschreduktionsmethode f\u00fcr Perfusionsdaten entwickelt.\nAktuell verf\u00fcgbare C-Bogen Systeme haben eine relativ langsame Rotationsgeschwindigkeit. Dies macht die Rekonstruktion der zeitlich dynamischen Kontrastmittelkonzentration zur Herausforderung. F\u00fcr dieses Problem wurde ein dynamischer iterativer Rekonstruktionsalgorithmus entwickelt, der die hohe zeitliche Aufl\u00f6sung in den Projektionsdaten zur verbesserten Rekonstruktion der dynamischen Zeitkurven ausnutzt.\nRobotische C-Bogen Systeme (Artis zeego, Siemens Healthcare, Deutschland) verf\u00fcgen \u00fcber ein Aufnahmeprotokoll mit Hochgeschwindigkeitsrotation (HSP) zur verbesserten zeitlichen Aufnahme der Kontrastdynamik. Allerdings sind die HSP Aufnahmen in der Winkelschrittweite unterabgetastet, was zu starken Streifenartefakten in den rekonstruierten Perfusionskarten f\u00fchren kann. Deshalb wird ein neuer recheneffizienter Algorithmus zur Rausch- und Streifenartefaktreduktion in FD-CTP Daten vorgestellt. Die Durchf\u00fchrbarkeit der FD-CTP mit dem HSP in der klinischen Praxis wird anhand von Daten gezeigt, welche w\u00e4hrend der interventionellen Behandlung von zwei Schlaganfallsf\u00e4llen aufgezeichnet wurden.\nAu\u00dferdem wird das Design eines digitalen Perfusionsphantoms zur numerischen Evaluierung der vorgestellten Techniken diskutiert.\nDie Qualit\u00e4t der mit den neuen Techniken rekonstruierten Perfusionskarten zeigt, dass FD-CTP in absehbarer Zeit klinisch verf\u00fcgbar werden k\u00f6nnte.\n2014-10-29\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5395\nurn:nbn:de:bvb:29-opus4-53958\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-53958\nhttps://opus4.kobv.de/opus4-fau/files/5395/MichaelManhartDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5467\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nddc:610\nmsc\nmsc:94Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nHigh Performance Iterative X-Ray CT with Application in 3-D Mammography and Interventional C-arm Imaging Systems\nHochperformante Iterative Computertomographie mit Anwendung in der 3-D Mammographie und C-arm CT Bildgebung\nKeck, Benjamin\nCUDA <Informatik>\nComputertomographie\nddc:000\nddc:610\nMedical image reconstruction is a key component for a broad range of medical imaging technologies. For classical Computed Tomography systems the amount of measured signals per second increased exponentially over the last four decades, whereas the computational complexity of the majority of utilized algorithms has not changed significantly.\nA major interest and challenge is to provide optimal image quality at the fewest patient dose possible. One solution and active research field towards solving that problem, are iterative reconstruction methods. Their complexity is a multiple compared to the classical analytical methods which were used in nearly all commercially available systems. In this thesis the application of graphics cards in the field of iterative medical image reconstruction is investigated. The major contributions are the demonstrated fast implementations for off-the-shelf hardware as well as the motivation of graphics card usage in upcoming generations of medical systems. The first realization describes the implementation of a commonly used analytical cone-beam reconstruction method for C-arm CT, before covering iterative reconstruction methods. Both analytical as well as iterative reconstruction methods share the compute-intensive back-projection step. In addition iterative reconstruction methods require a forward-projection step with similarly high computational cost. The introduced Compute Unified Device Architecture (CUDA) builds the basis for the presented GPU implementation of both steps. Different realization schemes are presented by combining both steps and applying minor modifications. The implementations of the SART, SIRT as well as OS-SIRT illustrate the realization of algebraic reconstruction methods. Further, a realization for the more advanced statistical reconstruction methods is described, introducing a GPU accelerated implementation of a maximum likelihood reconstruction using a concave objective function.\nThe achieved reconstruction performance is based on different detailed optimizations and exploitation of various technical features. In addition the performance results are evaluated for different hardware platforms - like the CPU - and for the proposed algorithms. The results implicate that for all presented reconstruction methods a significant speedup compared to a CPU realization is achieved. In example, we achieve at least a speedup factor of 10 for the presented OS-SIRT comparing a NVIDIA QuadroFX 5600 graphics card with a workstation equipped with two Intel Xeon Quad-Core E5410 processors. This is additionally supported by the comparison of the presented implementations to the CUDA alternative OpenCL underpinning the performance lead of GPUs using CUDA.\nA further contribution of this thesis is the exemplary clinical application of the pro- posed algorithms to two different modalities: C-arm CT and 3-D mammography. These applications demonstrate the potential and importance of GPU accelerated iterative medical image reconstruction. This thesis is concluded with a summary and an outlook on the future of GPU accelerated medical imaging processing.\nEine wichtige Kernkomponente der medizinischen Bildgebung bildet die medizinische Bildrekonstruktion. Betrachtet man die Anzahl der gemessenen Signale pro Sekunde f\u00fcr die klassische Computertomographie in den letzten vier Jahrzehnten, so l\u00e4sst sich ein exponentielles Wachstum feststellen. Im Vergleich dazu ver\u00e4nderte sich die Berechnungskomplexit\u00e4t der verwendeten Algorithmen nicht merklich. Ein wichtiger Aspekt f\u00fcr die Computertomographie ist die Reduktion der applizierten Patientendosis auf ein Minimum unter der Voraussetzung weiterhin eine optimale Bildqualit\u00e4t zu erhalten. Ein m\u00f6glicher L\u00f6sungsansatz hierf\u00fcr und Gegen- stand aktueller Forschung sind iterative Rekonstruktionstechniken. Die Berechnungskomplexit\u00e4t dieser ist dabei allerdings ein Vielfaches gegen\u00fcber der \u00fcberwiegend kommerziell verwendeten analytischen Rekonstruktionstechniken.\nZiel dieser Arbeit ist es, den m\u00f6glichen Einsatz von Grafikkarten zur beschleunigten Berechnung iterativer Rekonstruktionstechniken zu untersuchen. Der Nachweis und die Erl\u00e4uterung zu den erzielten performanten Realisierungen mit Hilfe dieser Technologie z\u00e4hlt zu den wichtigsten Erkenntnissen. Des weiteren kann der Einsatz dieser breit verf\u00fcgbaren Technologie in zuk\u00fcnftigen medizinischen Bildgebungssystemen nahe gelegt werden. Zus\u00e4tzlich zu den Erl\u00e4uterungen bez\u00fcglich der iterativen Rekonstruktionstechniken wird zun\u00e4chst die Realisierung einer weit verbreitenden analytischen Rekonstruktionstechnik f\u00fcr die C-arm CT Bildgebung beschrieben. Ein Bestandteil dieser Rekonstruktionstechnik, die R\u00fcckprojektion, ist eine berechnungsintensive Kernkomponente die sowohl bei analytischen als auch bei iterativen Rekonstruktionstechniken ben\u00f6tigt wird. Im Unterschied zu den analytischen Methoden ben\u00f6tigen iterative Rekonstruktionstechniken zus\u00e4tzlich eine weitere Kernkomponente, die (Vorw\u00e4rts-)Projektion. Diese stellt dabei einen \u00e4hnlich hohen Berechnungsaufwand dar, wie die R\u00fcckprojektion. Die Basis f\u00fcr die Implementierungen beider Kernkomponenten bildet dabei die erl\u00e4uterte parallele Programmiertechnik f\u00fcr Grafikkarten namens CUDA. Durch geringe Modifikationen und geschickte Kombination beider Kernkomponenten werden verschiedene Realisierungen erzielt. Die Implementierung der Rekonstruktionstechniken SART, SIRT und OS-SIRT stellt die Gruppe der algebraischen Rekonstruktionsalgorithmen dar. Des weiteren wird ein Implementierungbeispiel f\u00fcr die Gruppe der statistischen Rekonstruktionstechniken anhand einer Maximum Likelihood Rekonstruktion auf Basis einer konkaven Zielfunktion erl\u00e4utert.\nDie erzielten Rekonstruktionsgeschwindigkeiten basieren auf verschiedenen dargelegten Optimierungstechniken und der Verwendung neuer technischer Funktionen. Zus\u00e4tzlich werden die Rekonstruktionsgeschwindigkeiten zwischen verschiedenen Hardware-Plattformen und den verschiedenen Algorithmen verglichen. Die Ergebnisse implizieren, dass f\u00fcr alle vorgestellten Rekonstruktionstechniken mittels Grafikkarten ein signifikanter Geschwindigkeitszuwachs im Vergleich zu einer CPU erzielt werden kann. Zum Beispiel ist die Implementierung der OS-SIRT auf einer NVIDIA QuadroFX 5600 mindestens Faktor 10 schneller als die CPU Implementierung auf einer Workstation mit zwei Intel Xeon Quad-Core E5410 Prozessoren. Dies wird durch den Vergleich der erl\u00e4uterten Implementierungen zur CUDA Alter- native OpenCL zus\u00e4tzlich bekr\u00e4ftigt, der die Geschwindigkeitsvorteile von Grafik- karten unter der Verwendung von CUDA untermauert.\n2014-11-19\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5467\nurn:nbn:de:bvb:29-opus4-54678\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-54678\nhttps://opus4.kobv.de/opus4-fau/files/5467/BenjaminKeckDissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5484\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nddc:610\nccs\nccs:B.4\npacs\npacs:40.00.00\nmsc\nmsc:94-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nDaten\u00fcbertragungskonzepte f\u00fcr ein Brust-CT\nSchilling, Harry\nDrehverbindung\nSchleifring\nddc:004\nddc:610\nHintergrund und Ziele\nEin am Institut f\u00fcr Medizinische Physik verfolgtes neuartiges Konzept f\u00fcr ein dediziertes CT der Brust zeigt in Simulationen und Experimenten erstmals auf der Basis von neuartigen und optimierten Bildgebungskomponenten die in der Diagnostik und im Screnning geforderten Leistungswerte hinsichtlich Bildqualit\u00e4t und Dosis. Damit scheint das Konzept geeignet, die Probleme der Mammographie, wie zum Beispiel eine reduzierte Sensitivit\u00e4t bei dichtem Brustgewebe, das Vorhanden sein von \u00dcberlagerungseffekten oder auch die h\u00e4ufig als schmerzhaft empfundene Kompression der Brust w\u00e4hrend der Untersuchung zu l\u00f6sen. Zur Umsetzung des Konzepts f\u00fcr das Brust-CT-System werden zwei alternative rotierende Tragwerkskonzepte f\u00fcr die Bildgebungseinheit favorisiert, welche beide zur Energie-, Signal- und Daten\u00fcbertragung einen sogenannten Dreh\u00fcbertrager ben\u00f6tigen.\nDas Ziel dieser Arbeit besteht in der Erforschung und Erarbeitung von unterschiedlichen \u00dcbertragungsprinzipien f\u00fcr die Energie-, Signal- und Daten\u00fcbertragung mit einem Dreh\u00fcbertrager. Dabei m\u00fcssen die Vorgaben aus der Spezifikation des Brust-CT-Systems eingehalten und umgesetzt werden.\nMethoden\nZun\u00e4chst wurden die beiden Tragwerkskonzepte und die Spezifikationsparameter des Brust-CT-Systems beschrieben, pr\u00e4zisiert und zusammengefasst.\nNach der Definition des Begriffs Dreh\u00fcbertrager wurde der Stand der Technik der \u00dcbertragungsprinzipien f\u00fcr die Energie-, Signal- und Daten\u00fcbertragung erkl\u00e4rt. Die \u00dcbertragungsprinzipien wurden untersucht und jeweils deren Vor- und Nachteile zusammengefasst. Im Ergebnis wurden die \u00dcbertragungsprinzipien ausgew\u00e4hlt und im Detail weiter erforscht, die f\u00fcr einen Dreh\u00fcbertrager f\u00fcr eines der beiden Tragwerkskonzepte als geeignet erschienen. Dazu wurden grundlegende Betrachtungen und Simulationen durchgef\u00fchrt und anschlie\u00dfend auch Versuchsaufbauten erstellt, um die praktische Anwendbarkeit des jeweiligen \u00dcbertragungsprinzips zu \u00fcberpr\u00fcfen. Danach erfolgte eine abschlie\u00dfende Auswahl der \u00dcbertragungsprinzipien f\u00fcr den oder die Dreh\u00fcbertrager des Brust-CT. Der Dreh\u00fcbertrager mit der besten Eignung f\u00fcr das Brust-CT wurde als Prototyp aufgebaut.\nErgebnisse und Beobachtungen\nEs zeigt sich, dass die 6Bildgebungskomponenten die gr\u00f6\u00dfte Herausforderung f\u00fcr den Dreh\u00fcbertrager darstellen.\nDie R\u00f6ntgenr\u00f6hre ben\u00f6tigt entweder eine Spannung von 60 KV bei einer Leistung von 3 KW, wenn der R\u00f6hrengenerator nicht mitrotiert, oder, falls der R\u00f6hrengenerator mitrotiert, eine Spannung von 400 V bei einer Leistung von 5 KW. Diese Spannungen und Leistungen lassen sich mit kontaktierenden Graphitb\u00fcrsten oder kontaktierenden Goldfederdr\u00e4hten \u00fcbertragen. Eine L\u00f6sung f\u00fcr eine direkte kontaktlose \u00dcbertragung der R\u00f6hrenspannung von 60 KV konnte nicht gefunden bzw. im Rahmen der Gesamtkosten nicht umgesetzt werden.\nDie Daten\u00fcbertragungsrate der Projektionsdaten des R\u00f6ntgendetektors von 28,6 GBit/s sind mit dem entwickelten und zum Patent angemeldeten einkanaligen optischen Dreh\u00fcbertrager auf der Rotationsachse problemlos direkt \u00fcbertragbar. Durch ein Wellenl\u00e4ngenmultiplex ist au\u00dferdem die \u00dcbertragung der Projektionsdaten mit drei 10 GBit Ethernet-Kan\u00e4len m\u00f6glich, jedoch f\u00fcr den spezifizierten Kostenrahmen zu teuer. F\u00fcr die Daten\u00fcbertragung mit einem Dreh\u00fcbertrager, welcher einen freien Innendurchmesser aufweist, gibt es unter der gegebenen Spezifikation deshalb f\u00fcr das Brust-CT keine L\u00f6sung.\nDes Weiteren ergab sich, dass f\u00fcr ein Brust-CT-System ein Dreh\u00fcbertrager mit kontaktierenden Goldfederdr\u00e4hten zur Leistungs- und Steuerdaten\u00fcbertragung am besten geeignet ist.\nPraktische Schlussfolgerungen\nDie durchgef\u00fchrten Untersuchungen zeigen, dass bei der entwickelten Ausf\u00fchrungen des Dreh\u00fcbertragers die spezifizierten Parameter f\u00fcr das Brust-CT mit einer U-Anordnung umgesetzt und mit den nach dem Stand der Technik zur Verf\u00fcgung stehenden \u00dcbertragungsprinzipien und Fertigungstechniken entsprechende realisiert werden kann.\nBackground and aims\nA novel concept for a dedicated CT of the breast tracked at the Institute of Medical Physics shows in simulations and experiments for the first time the required diagnostic and screnning performance values with respect to image quality and dose on the basis of new and optimized imaging components. Thus, the concept seems suitable for solving the problems of mammography, as a reduced sensitivity in dense breast tissue, superposition effects or even the often as painful perceived compression of the breast during the examination. In order to implement the concept of the Breast-CT-System, two alternative rotating structural concepts for the imaging unit are favored, which both for power, signal and data transmission require a so called rotary joint.\nThe aim of this work is to research and develop different transmission principles for power, signal and data transmission across the rotary joint. The requirements of the specifications of the Breast-CT-System must be respected and implemented.\nMethods\nAt first, the two structural concepts and the specification parameters of the Breast-CT-System have been described, clarified and summarized.\nAfter the definition of rotary joints of the prior art, the transmission principles for power, signal and data transmission were explained. The transfer technologies were studied and a resume was given for the advantages and disadvantages. As a result, those transfer principles were selected and further studied in detail, which appeared to suit best for a rotary joint for one of the two structural concepts. Basic considerations and simulations were carried out. In addition experimental built ups were made to verify the practicality of the transmission principles. This was followed by a final selection of the transfer technologies best qualified for alternative rotary joint setups of the Breast-CT. The rotary joint with the best appropriateness for the Breast-CT was built up as a prototype unit.\nResults and Observations\nIt turns out that the imaging components represent the greatest challenge for the rotary joint.\nThe X-ray tube will require either a voltage of 60 kV at a power of 3 kW, if the tube generator does not rotate, or, if the generator rotates, a voltage of 400 V and a power of 5 KW. These voltages and power can be transferred with contacting graphite brushes or contacting gold spring wires. A solution for a direct contactless transfer of tube voltage of 60 kV could not be found or is not implementable as part of the total cost.\nThe data transfer rate of projection data of the x-ray detector of 28.6 Gbit / s is easily transferable directly with the developed and patent pending single-channel optical rotary joint placed on the rotation axis. With a wavelength multiplexing the transfer of the projection data will be also possible, if three 10 Gbit Ethernet channels are used. But this is too expensive for the specified financial budget. For data transmission with a rotary joint, which has a free inner bore, there is under the given specification no solution.\nFurthermore, it was found that for a Breast-CT-System, a rotary joint with contacting gold spring wires for power and control data transmission is the most suitable.\nPractical conclusions\nThe investigations carried out showed that the specified parameters for the Breast-CT can be implemented in the developed rotary joint with an U-arrangement of the structural concept and also with the prior art available transmission principles and manufacturing techniques.\n2014-11-25\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5484\nurn:nbn:de:bvb:29-opus4-54843\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-54843\nhttps://opus4.kobv.de/opus4-fau/files/5484/HarrySchillingDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5503\n2018-11-12\ndoc-type:masterThesis\nbibliography:false\nddc\nddc:306\nddc:708\nccs\nccs:A.1\nccs:A.m\npacs\npacs:01.40.-d\nmsc\nmsc:00-02\nmsc:00Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Phil_Paedagogik\n\"Erfahrung Kunst\" - Eine qualitative Studie zur besucherorientierten Kunstvermittlung: Kunstgespr\u00e4che aus retrospektiver Sicht \u00e4lterer Museumsbesucher\nKiefer (geb. Kerner), Melanie\nAltenbildung\nBildbetrachtung\nBesucherforschung\nBesucherf\u00fchrung\nBesucher\nDrittes Lebensalter\nEmpirische P\u00e4dagogische Forschung\nEvaluation\nGermanisches Nationalmuseum\nInterview\nKunstbetrachtung\nKunstvermittlung\nKulturvermittlung\nKulturp\u00e4dagogik\nKunstkurs\nKunstgespr\u00e4ch\nKunst- und Kulturp\u00e4dagogisches Zentrum der Museen <N\u00fcrnberg>\nKPZ\nMuseum\nKunstmuseum\nMuseumsp\u00e4dagogik\nQualitative Inhaltsanalyse\nSenioren\nStaedtler Stiftung\nddc:306\nddc:708\nIn der Museumsp\u00e4dagogik ist der Trend zu besucherorientierten Angeboten erkennbar, wodurch auch die Besucherforschung zunehmend an Bedeutung gewinnt. Studien im museumsp\u00e4dagogischen Bereich beruhen vorwiegend auf quantitativen Erhebungen, die statistische Aussagen erm\u00f6glichen. Gezielte qualitative Untersuchungen zu museumsp\u00e4dagogischen Angeboten, in welchen spezifische Formate der Vermittlung aus der Sicht der jeweiligen Zielgruppe erforscht werden, sind selten, obwohl das Wissen um diese und deren Bed\u00fcrfnisse meist l\u00fcckenhaft ist. Dies betrifft auch Besuchergruppen, die zum sogenannten Stammklientel geh\u00f6ren, wie beispielsweise die Senioren,\ndie im Zuge des demografischen Wandels verst\u00e4rkt in den Fokus r\u00fccken. Didaktische Methoden zur Arbeit mit \u00c4lteren wurden anhand praktischer Erfahrungen entwickelt, welche es wissenschaftlich zu untermauern gilt. Die vorliegende Masterarbeit soll hierzu einen Beitrag leisten, indem der Frage nachgegangen wird: Inwiefern stellt das Kunstgespr\u00e4ch, im Sinne der Besucherorientierung, ein geeignetes Format der Kunstvermittlung f\u00fcr Museumsbesucher im dritten Lebensalter dar? Um eine Antwort auf diese Frage zu finden, werden die Interviews mit vier Senioren untersucht, die an einem Gespr\u00e4chskurs des Kunst- und Kulturp\u00e4dagogischen Zentrums der Museen in N\u00fcrnberg (KPZ), bestehend aus zehn Kunstgespr\u00e4chen, teilgenommen haben, welcher im Rahmen des Staedtler-Projektes \u201ePers\u00f6nlichkeitsbildung durch k\u00fcnstlerisches Gestalten\u201c durchgef\u00fchrt wurde. Die Kursteilnehmer gaben in leitfadengest\u00fctzten Interviews Auskunft zu ihren Erwartungen an den Kunstkurs, zu ihren Bed\u00fcrfnissen im Hinblick auf die Kunstvermittlung (bez\u00fcgl. d. Rahmenbedingungen, Methode, Kursgruppe, Kursleitung, Kursinhalte) und zu den subjektiv wahrgenommenen Wirkungen des Kursbesuchs. Die Interviews wurden nach der inhaltlich strukturierenden qualitativen Inhaltsanalyse ausgewertet (vgl. Udo Kuckartz 2012), wobei eine QDA-Software unterst\u00fctzend eingesetzt wurde. Die Interpretation der Ergebnisse erlaubt insgesamt eine Beantwortung der Forschungsfrage.\n2014-11-29\nmasterthesis\ndoc-type:masterThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5503\nurn:nbn:de:bvb:29-opus4-55036\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-55036\nhttps://opus4.kobv.de/opus4-fau/files/5503/KernerMelanieMasterarbeit.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-sa/3.0/de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5529\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:003\nddc:610\nccs\nccs:I.5\nmsc\nmsc:62G99\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nStatistische Modellierung, prototypische Implementierung und Evaluierung eines Systems zur Sturzrisikoprognose f\u00fcr Senioren\nStatistical modelling, prototypical implementation and evaluation of a system for fall-risk prognosis for elderly people\nBienk, Stefan\nInferenzstatistik\nMaschinelles Lernen\nTelemedizin\nddc:003\nddc:610\nVor dem Hintergrund intensiver einschl\u00e4giger Forschungsarbeiten wird in dieser Arbeit ein neuartiger Ansatz zum Sturzrisikomonitoring f\u00fcr Senioren entwickelt. Dabei werden pers\u00f6nliche mittelfristige Sturzrisikoprognosen anhand permanent im Alltag der Patienten erhobener Daten eines einzigen, in eine herk\u00f6mmliche Hausalarmuhr integrierten Akzelerometers automatisch mittels Software erstellt. Mit den damit einhergehenden pragmatischen Vorteilen gegen\u00fcber etablierten Ans\u00e4tzen ist jedoch ein erh\u00f6hter Schwierigkeitsgrad des Prognoseproblems verbunden: Einerseits erfolgt die Erhebung unter naturalistischen (d.h. unkontrollierten) Bedingungen, andererseits wird eine indirekte Messung vorgenommen, d.h. insbesondere ohne Sensorik an den unteren Extremit\u00e4ten. Diese Herausforderungen werden durch ein mehrstufiges Verfahren adressiert: Dieses berechnet (nach einer f\u00fcr jeden Patienten einmalig durchzuf\u00fchrenden supervisionierten Trainingsphase) regelm\u00e4\u00dfig einen bereits validierten Sturzrisikoindikator, die sog. Gangvariabilit\u00e4t. Zun\u00e4chst werden hierzu anhand der Beschleunigungsdaten Zeitintervalle ermittelt, w\u00e4hrend derer eine Person Gangstrecken absolvierte. F\u00fcr diese Intervalle werden dann die Schrittzeitpunkte rekonstruiert und hieraus die Gangvariabilit\u00e4t als Eingangsgr\u00f6\u00dfe f\u00fcr die Sturzprognose berechnet.\nHierbei kommen Methoden der Aktivit\u00e4tsklassifizierung und der Wavelet-Theorie zum Einsatz. W\u00e4hrend die letztere lediglich zur Anwendung gebracht wird, besteht ein Schwerpunkt der Arbeit in der theoretischen Analyse und Weiterentwicklung der etablierten heuristischen Methoden zur Aktivit\u00e4tsklassifizierung. Dies umfasst insbesondere die Untersuchung von technischen Fragen der Messbarkeit wie auch die Untersuchung der statistischen G\u00fcte der Inferenzmethoden. Durch die Einf\u00fchrung des abstrakten Konzepts des Pr\u00e4diktionsproblems mit allgemeinen separablen metrischen Zielr\u00e4umen wird dabei die blo\u00dfe \u00dcbertragung analoger Begriffe und Resultate aus der etablierten Klassifikationstheorie vermieden.\nDie Beschreibung der technischen Umsetzung dieser Ideen sowie eine erste Evaluierung bilden schlie\u00dflich den Abschluss der Arbeit.\nBased on extensive related work, in this thesis an alternative approach to fall-risk\nmonitoring of elderly persons is developed. Software components are provided\nwhich automatically establish personal middle-term risk prognostics, using as input\nonly data recorded in patients' everyday life employing one single accelerometer on board of\na common wristwatch (originally featuring only an emergency call function). Along with numerous pragmatic achievements compared to existing assessment tools, this\nentails a higher complexity of the prediction task: Data acquisition takes place in a\nnaturalistic (i.e. uncontrolled) setting and the measurement is of indirect nature, as\nno sensors are attached to the patients' lower extremities. These challenges are addressed\nby a multiple-stage procedure: After a supervised training to be performed\none time for each patient in order to personalize the system, an already established\nfall-risk indicator, the so called gait variability, can be periodically reconstructed by\nthe system. Firstly, time intervals during which monitored persons were walking are\ndetermined from the permanently recorded motion data. Secondly, for these intervals, gait\nevents are reconstructed and gait variability is calculated, serving as input for the\nfinal computation of fall-risk.\nOn the modelling level, methods from activity classification and the theory of wavelets\nare utilized. While the latter is simply applied, as a major contribution a\ntheoretical analysis and improvement of heuristic methods for activity classification\nis provided. This includes rather technical measurability issues as well as the\nanalysis of statistical notions of quality of the inferential methods. By introducing\nan abstract concept called \"prediction problem\", pure reproduction of analogous notions\nand results from established classification theory is avoided.\nThe thesis is completed by a brief sketch of the technical implementation of these\nideas and a first evaluation.\n2014-12-05\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5529\nurn:nbn:de:bvb:29-opus4-55293\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-55293\nhttps://opus4.kobv.de/opus4-fau/files/5529/StefanBienkDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5619\n2018-11-12\ndoc-type:MovingImage\nbibliography:false\nddc\nddc:006\nccs\nccs:I.6.4\npacs\npacs:45.70.Mg\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nCrystallization in a Granular Channel Flow Past an Obstacle\nPreclik, Tobias\nGranul\u00e4rer Stoff\nDirekte numerische Simulation\nKristallisation\nDreidimensionale Str\u00f6mung\nKanal\nHochleistungsrechnen\nParallelisierung\nddc:006\nThe video shows a granular flow through a channel of length 30cm, width 6.3cm and height 2.1cm. Gravitational acceleration acts towards the bottom. On the left end spherical particles of 1mm diameter are generated with an inflow velocity of 2m/s. An obstacle obstructs the flow 3cm downstream. It has a D-shaped cross section, is 2cm long and wide, extends to the full height of the channel, and is placed directly in the middle of the channel. The setup was simulated for 5s simulation time with time steps of length 10us. In each time step 10 iterations of a parallel non-smooth contact dynamics (NSCD) method were performed. The simulation ran for 2 hours on 64 nodes of the Emmy cluster at the regional computing centre in Erlangen (RRZE). To properly resolve the dynamics visual scene descriptions were written to disk with a rate of 600 frames per second. The final video is slowed down by a factor of 20. The formation of a dead zone behind the obstacle with a triangular cross section can be observed as well as the formation of crystalline clusters with occasional defects.\n2014-12-22\nmovingimage\ndoc-type:MovingImage\nvideo/mp4\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5619\nurn:nbn:de:bvb:29-opus4-56192\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56192\nhttps://opus4.kobv.de/opus4-fau/files/5619/5619_crystalline-channelflow.mp4\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5621\n2018-11-12\ndoc-type:MovingImage\nbibliography:false\nddc\nddc:006\nccs\nccs:I.6.4\npacs\npacs:45.70.Mg\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nGranular Channel Flow Past an Obstacle\nPreclik, Tobias\nGranul\u00e4rer Stoff\nHochleistungsrechnen\nDirekte numerische Simulation\nKanal\nDreidimensionale Str\u00f6mung\nParallelisierung\nStation\u00e4re Str\u00f6mung\nddc:006\nThe video shows a granular flow through a channel of length 15cm, width 6.3cm and height 2.1cm. Gravitational acceleration acts towards the bottom. On the left end spherical particles of 1mm diameter are generated with an inflow velocity of 1m/s. An obstacle obstructs the flow 3cm downstream. It has a D-shaped cross section, is 2cm long and wide, extends to the full height of the channel, and is placed directly in the middle of the channel. The setup was simulated for 10s simulation time with time steps of length 10us. In each time step 10 iterations of a parallel non-smooth contact dynamics (NSCD) method were performed. The simulation ran for 50 minutes on 32 nodes of the Emmy cluster at the regional computing centre in Erlangen (RRZE). To properly resolve the dynamics visual scene descriptions were written to disk with a rate of 300 frames per second. The final video is slowed down by a factor of 10. The average maximum penetration in 50 samples was 1.8% of the particle diameter. The formation of a steady-state flow with minor local fluctuations as well as the formation of recirculation regions behind the obstacle can be observed.\n2014-12-22\nmovingimage\ndoc-type:MovingImage\nvideo/mp4\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5621\nurn:nbn:de:bvb:29-opus4-56215\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56215\nhttps://opus4.kobv.de/opus4-fau/files/5621/5621_channelflow.mp4\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5622\n2018-11-12\ndoc-type:MovingImage\nbibliography:false\nddc\nddc:006\nccs\nccs:I.6.4\npacs\npacs:45.70.Mg\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nSize Segregation of Sharp-edged Granular Matter in a Horizontal Shaker\nPreclik, Tobias\nGranul\u00e4rer Stoff\nHochleistungsrechnen\nDirekte numerische Simulation\nParallelisierung\nEntmischung\nInstation\u00e4re Str\u00f6mung\nddc:006\nThe video shows a large-scale horizontal shaker filled with 864000 sharp-edged particles ranging in size between 0.25mm to 2mm. The shaking frequency is set to 4 Hz and the amplitude to 3cm. The size of the particles is color-coded. To properly resolve the dynamics visual scene descriptions were written to disk with a rate of 1000 frames per second. The final video is slowed down by a factor of 10. The simulation ran for 27 hours on 64 nodes of the Emmy clusters located at the regional computing centre in Erlangen (RRZE). The setup was simulated for 2.8s simulation time with time steps of length 10us. In each time step 10 iterations of a parallel non-smooth contact dynamics (NSCD) method were performed. An emerging size segregation can be observed: The smallest particles with beige color collect at the bottom right below the larger bluish particles.\n2014-12-22\nmovingimage\ndoc-type:MovingImage\nvideo/mp4\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5622\nurn:nbn:de:bvb:29-opus4-56220\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56220\nhttps://opus4.kobv.de/opus4-fau/files/5622/shaker.mp4\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5623\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nccs\nccs:I.6\npacs\npacs:45.70.-n\nmsc\nmsc:70E55\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nModels and Algorithms for Ultrascale Simulations of Non-smooth Granular Dynamics\nModelle und Algorithmen f\u00fcr ultraskalige Simulationen nichtglatter granularer Dynamiken\nPreclik, Tobias\nGranul\u00e4rer Stoff\nHochleistungsrechnen\nDirekte numerische Simulation\nParallelisierung\nNichtglatte Mechanik\nKontaktmechanik\nRegularisierung\nddc:000\nThe macroscopic behaviour of granular matter in contrast to fluids and solids is often insufficiently understood not least because poor visual accessibility hinders the analysis. Computer simulations are capable of revealing the internal dynamics not directly accessible in the experiment but are computationally extremely expensive since no unified model equations exist homogenizing the dynamics of the individual particles. Unfortunately, the time and length scales of relevant problems are quickly beyond the reach of today's simulation tools. This thesis aims to address these issues by providing models and algorithms efficiently harnessing the power of today's supercomputers and at the same time extending the reachable scales by hiding the collision micro-dynamics and improving the convergence rate of the iterative solver and time coherence of the solutions. To hide the collision micro-dynamics hard contact models are employed leading to non-smooth trajectories and discontinuous velocities of the particles. The convergence rate and time coherence are impeded by the non-uniqueness of the contact reactions. Hence, a new friction model is introduced that can be viewed as a regularization of the Coulomb friction model exhibiting unique one-contact solutions. Another regularization, stiffening contact compliances in multi-contact problems, identifies the contact reactions of weighted minimum norm as physically meaningful unique solutions. Finally, an iterative solver based on the non-smooth contact dynamics method is parallelized for distributed-memory architectures. The parallelization uses a sophisticated protocol supporting the processes in deciding robustly upon responsibilities such as contact treatment and position integration while minimizing communication overhead. The parallel efficiency is assessed in strong- and weak-scaling experiments on three clusters including Juqueen and SuperMUC, two of today's largest supercomputers. The simulations are of unprecedented scale: in the order of 10 billion non-spherical particles and contacts. Aside from scaling experiments the versatility is demonstrated using the examples of a large-scale horizontal shaker filled with sharp-edged granular matter and rapid granular channel flows. Though the limits of non-smooth granular matter simulations are pushed considerably, the dependence of the convergence rate of the non-smooth contact dynamics method on the number of unknowns remains an open problem.\nDas makroskopische Verhalten granularer Materie ist - im Gegensatz zu dem von Fluiden und Festk\u00f6rpern - h\u00e4ufig nur unzureichend verstanden. Dies r\u00fchrt nicht zuletzt daher, dass die Analyse durch die schlechte visuelle Zug\u00e4nglichkeit erschwert wird. Computersimulationen verm\u00f6gen die im Experiment nicht direkt zug\u00e4ngliche interne Dynamik preiszugeben, sind aber extrem rechenaufwendig, da keine allgemeing\u00fcltigen Modellgleichungen existieren, welche die Dynamik der einzelnen Partikel homogenisieren. Allerdings liegen die Zeit- und L\u00e4ngenskalen relevanter Probleme schnell au\u00dferhalb der Erreichbarkeit heutiger Simulationsprogramme. Ziel der Arbeit ist es, diese Aspekte durch die Entwicklung von Modellen und Algorithmen anzugehen, welche die Rechenleistung heutiger Supercomputer nutzbar machen und zugleich die erreichbaren Skalen erweitern. Dies soll realisiert werden, indem die Mikrodynamik der Kollisionen ausgeblendet wird und die Konvergenzrate des iterativen L\u00f6sers sowie die zeitliche Koh\u00e4renz der L\u00f6sungen verbessert werden. Um die Mikrodynamik der Kollisionen auszublenden, werden harte Kontaktmodelle eingesetzt, welche auf nichtglatte Trajektorien und unstetige Geschwindigkeiten der Partikel f\u00fchren. Konvergenzrate und zeitliche Koh\u00e4renz werden durch die Uneindeutigkeit der Kontaktreaktionen verschlechtert. Daher wird ein neues Reibungsmodell eingef\u00fchrt, welches als Regularisierung der Coulombschen Reibung betrachtet werden kann und eindeutige Einkontaktl\u00f6sungen aufweist. Eine weitere Regularisierung, welche Kontaktnachgiebigkeiten in Mehrkontaktproblemen verfestigt, identifiziert die Kontaktreaktion mit minimaler gewichteter Norm als physikalisch bedeutsame eindeutige L\u00f6sung. Abschlie\u00dfend wird ein iteratives L\u00f6sungsverfahren, welches auf der \"non-smooth contact dynamics\"-Methode basiert, f\u00fcr Architekturen mit verteiltem Speicher parallelisiert. Die Parallelisierung verwendet ein durchdachtes Protokoll, welches die Prozesse darin unterst\u00fctzt, Zust\u00e4ndigkeiten - wie beispielsweise Kontaktbehandlung oder Positionsintegration - robust zu entscheiden bei gleichzeitiger Reduktion des Kommunikationsaufwandes. Die parallele Effizienz wird in starken und schwachen Skalierungsexperimenten auf drei Clustern - einschlie\u00dflich Juqueen und SuperMUC, zwei der gr\u00f6\u00dften heutigen Supercomputer - ausgewertet. Die Gr\u00f6\u00dfenordnung der Simulationen erreicht ein bislang ungekanntes Ausma\u00df: 10 Milliarden nicht-sph\u00e4rische Partikel und Kontakte. Abgesehen von den Skalierungsexperimenten wird die Vielseitigkeit anhand eines gro\u00dfskaligen horizontalen R\u00fcttlers demonstriert, welcher mit scharfkantiger granularer Materie gef\u00fcllt ist und anhand rapider granularer Kanalstr\u00f6mungen. Obwohl die Simulationsm\u00f6glichkeiten nichtglatter granularer Materie deutlich erweitert werden, bleibt die Abh\u00e4ngigkeit der Konvergenzrate der \"non-smooth contact dynamics\"-Methode von der Anzahl der Unbekannten ein ungel\u00f6stes Problem.\n2014-12-22\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5623\nurn:nbn:de:bvb:29-opus4-56232\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56232\nhttps://opus4.kobv.de/opus4-fau/files/5623/dissertation_preclik_tobias.pdf\neng\nhttps://creativecommons.org/licenses/by-sa/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5697\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:92-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nDie Modulation immunologisch relevanter endogener Antigene und ihre prognostische Bedeutung in Speicheldr\u00fcsenkarzinomen\nMueller, Maximilian\nSpeicheldr\u00fcsenkarzinome\nddc:610\nA Hintergrund und Ziele\nDamit ein Tumor entstehen, wachsen und \u00fcberleben kann, sind molekulare Dys-regulationen notwendig, die einerseits die zellul\u00e4re Proliferation und Differenzierung be-einflussen und andererseits eine Immunantwort des K\u00f6rpers unterdr\u00fccken. Den Protei-nen \u03b22-Mikroglobulin, Calnexin, Calreticulin, ERp57, HLA-A, LMP-2, LMP-7, TAP-1, TAP-2, Tapasin und HC-10 kommt beim Entkommen der Immunabwehr (\u201eimmune escape\u201c) eine entscheidende Rolle zu, da sie die Fremdantigenexpression an Zellober-fl\u00e4chen und die anschlie\u00dfende Erkennung durch T-Zellen steuern. Ziel der vorliegenden Arbeit war die Untersuchung des Expressionsprofils dieser Proteine in Speicheldr\u00fcsen-karzinomen, um zu analysieren, ob \u201eImmun escape\u201c-Mechanismen eine Rolle spielen, es innerhalb der verschiedenen Entit\u00e4ten diesbez\u00fcglich Unterschiede gibt und ob eine prognostische Bedeutung abzuleiten ist.\nB Methoden\nF\u00fcr die Untersuchung wurden Proben von paraffineingebetteten Tumor- und korres-pondierenden Normalgeweben der Speicheldr\u00fcse aus einem Kollektiv von 288 Patien-ten, die seit 1984 in den Universit\u00e4tskliniken Regensburg, Erlangen und am Klinikum N\u00fcrnberg behandelt worden waren, herangezogen. Die klinischen Daten wurden durch das Erlanger Speicheldr\u00fcsenregister (Leiter: Prof. Dr. H. Iro) in Zusammenarbeit mit den Tumorzentren Regensburg und Erlangen-N\u00fcrnberg erhoben und zur Verf\u00fcgung gestellt. Durch eine besondere Stanztechnik wurden Multi-Gewebe-Bl\u00f6cke (\u201eTissue microarrays\u201c) erstellt, die eine immunhistochemische Untersuchung des Kollektivs er-m\u00f6glichten. Mit Hilfe einer Polymerkonjugatf\u00e4rbemethode (EnVisionTM) wurden die immunhistochemischen Objekttr\u00e4ger zu den oben genannten Markern angefertigt. Die weitere statistische Analyse (SPSS 18.0) st\u00fctzte sich auf Punktewerte, in die F\u00e4rbean-teil und \u2013intensit\u00e4t einflossen. Dadurch konnten Abweichungen zwischen Normal- und Tumorgewebe (mit dem t-Test f\u00fcr unabh\u00e4ngige Variablen mit p<0,05) und diesbez\u00fcgliche \u00dcberlebensunterschiede der Patienten retrospektiv analysiert werden (univariat mit der Kaplan-Meier-Analyse mit p<0,05, multivariat mit Hilfe einer r\u00fcckw\u00e4rts bedingten Cox-Regression mit schrittweiser Elimination bei p<0,05 und Konfidenzintervall 95 %).\nC Ergebnisse und Beobachtungen\nHinsichtlich des Geschlechts der Patienten, der Tumorlokalisation und der Karzinom-typen (Entit\u00e4ten) konnte ein, verglichen mit den von der Weltgesundheitsorganisation ver\u00f6ffentlichten Daten, repr\u00e4sentatives Fallkollektiv zusammengestellt werden. Bei den h\u00e4ufigsten Karzinomen lie\u00dfen sich statistisch signifikante \u00dcber- und Unterexpressionen der untersuchten Marker dokumentieren. Dabei zeigen sich unter Ber\u00fccksichtigung der Tumorhistogenese Parallelen der Expressionsmuster bei Karzinomen mit derselben Entstehungslokalisation. Weiterhin wurde die \u00c4nderung der Expression einzelner immu-nologisch relevanter Parameter in ihrem Einfluss auf das \u00dcberleben der Patienten un-tersucht und im Kontext etablierter klinischer Parameter multivariat analysiert. In der univariaten Analyse aller 288 Tumoren waren Tumoren mit Unterexpressionen von nukle\u00e4rem LMP-7 (p=0,005), \u00dcberexpressionen von \u03b22-Mikroglobulin (p=0,024), HLA-A (p<0,001), TAP-1 (p=0,01) und Tapasin (p<0,001) mit einem schlechteren \u00dcberleben der Patienten assoziiert. Bezogen auf einzelne Entit\u00e4ten lie\u00dfen sich hier hingegen Ab-weichungen dokumentieren. Hinsichtlich klinischer Parameter zeigten Patienten mit einem Alter \u00fcber 60 Jahre (p<0,001), m\u00e4nnlichem Geschlecht (p=0,002), einer Lokali-sation des Tumors in der Gl. parotis (p=0,005) und hinsichtlich histopathologischer Fak-toren wie einem hohen pT/N- und UICC-Stadium (p<0,001) sowie mit einem hohen Malignit\u00e4tsgrad G2/3 (p<0,001), Residualtumoren R1/2 (p<0,001) und innerhalb einzel-ner Entit\u00e4ten ein schlechteres \u00dcberleben. In der multivariaten Analyse beeinflussten ein hoher Malignit\u00e4tsgrad (Hazard Ratio, HR 4,1), hohes Tumorstadium T3/4 (HR 1,46) sowie Fernmetastasierung (HR 1,95), eine \u00dcberexpressionen von Tapasin (HR 1,51) sowie von HLA-A (HR 1,95) negativ das \u00dcberleben, eine \u00dcberexpression von LMP-7 in der nukle\u00e4ren Auswertung (HR 0,58) stellte einen positiven \u00dcberlebensfaktor dar.\nD Praktische Schlussfolgerung\nDie Expression immunologisch bedeutsamer Proteine differiert zwar bei den biologisch sehr unterschiedlichen Karzinomtypen der Speicheldr\u00fcse erheblich, dennoch lie\u00dfen sich einzelne Marker (HLA-A, LMP-7 und Tapasin) als auch etablierter klinisch-pathologische Parameter als prognostisch bedeutsam herausstellen. Diese Erkenntnis-se k\u00f6nnten gegenw\u00e4rtig als zus\u00e4tzliche Entscheidungshilfen bei der Festlegung der indi-viduellen Tumortherapie in der Adjuvanz (Strahlentherapie, Chemotherapie) fungieren, ob auf der Basis der Untersuchungen zuk\u00fcnftig auch immunologische Therapieans\u00e4tze verfolgt werden k\u00f6nnen, ist hingegen auch von funktionellen Studien abh\u00e4ngig.\nA Background and Aims\nFor a tumor to develop, grow and survive, molecular dysregulations which not only af-fect cellular proliferation and differentiation but also suppress an immunologic response of the organism are considered a prerequisite. With the proteins \u03b22-Microglobulin, Calnexin, Calreticulin, ERp57, HLA-A, LMP-2, LMP-7, TAP-1, TAP-2, Tapasin and HC-10 regulating antigen presentation on cell surfaces and their subsequent discovery by T-cells, these molecules play a decisive role in immune escape. In this context, the purpose of this investigation was to study the expression profile of these proteins in salivary gland carcinomas and to determine, whether immune escape mechanism ap-pear in these carcinomas, if there are differences between subtypes and to analyse a prognostic impact.\nB Methods\nParaffin embedded samples of tumor and corresponding healthy salivary gland tissue obtained from 288 patients who had been treated at the universities of Regensburg and Erlangen as well as at Nuremberg hospital served as a basis for this investigation. Clinical data were obtained and provided by the salivary gland registry Erlangen (Head: Prof. Dr. H. Iro) in cooperation with the cancer centers Erlangen-Nuremberg and Regensburg. Applying a special punch technique, tissue microarrays were fabricated and subsequently stained by means of a polymer conjugate staining method (EnVi-sionTM) for visualizing the above mentioned markers. Point scores based on the degree of staining and staining intensity served as a basis for statistical analysis (SPSS 18.0). This approach allowed for a retrospective analysis of both, differences between regular tissue and tumor tissue (t-test for independent variables \u03b1 = 0.05) and expres-sion profile related differences in survival rate of the patients (univariate Kaplan-Meier analysis, \u03b1 = 0.05; multivariate backwards Cox-regression with stepwise elimination at \u03b1 = 0.05 and confidence interval, CI 95%).\nC Results and observations\nIn terms of patient gender, tumor localisation and tumor entity, a representative patient population comparable to the data reported by the world health organization could be compiled. In the most frequent carcinomas statistically significant above und below average expressions of the markers examined were found. Taking into account the histogenesis of the tumors, carcinomas from identical localisations showed comparable expression profiles. Furthermore, the effect of changes in the expression of specific immunologically relevant parameters on patient survival was studied and subject to multivariate analysis in context with already established clinical parameters. Univariate analysis of all of the 288 tumors revealed that tumors with below average expression of LMP-7 in nuclear analysis (p=0.005) as well as above average expression of \u03b22-Microglobulin (p=0.024) and HLA-A (p<0.001), TAP-1 (p=0.01) and Tapasin (p<0.001) were associated with a lower survival rate. However, in some entities, deviations from these findings were found. In terms of clinical parameters, patients aged above 60 years (p<0.001), male patients (p=0.002), patients with tumors in the parotid gland (p=0.005) as well as patients with histopathologic factors such as high pT/N- and UICC-stage (p<0.001) high level of malignity G2/3 (p<0.001), residual tumors R1/2 (p<0.001) and entities among each other showed lower survival rates. Multivariate analysis revealed that a high level of malignity (Hazard Ratio, HR 4.1), advanced staging of local tumour (HR 1.3) or distant metastasis (HR 1.95), above average expression of Tapasin (HR 1.7) and HLA-A (HR 1.95) had a negative effect on the relative risk of death, a below average expression of LMP-7 in nuclear analysis seems to be a positiv factor of survival (HR 0.58).\nD Conclusion\nAlthough the expression of immunologically important proteins differs greatly between the biologically different types of salivary gland carcinomas, the prognostic value of specific markers (HLA-A, LMP-7 and Tapasin), also in context with established clinical and pathologic parameters, could be shown. Currently, these findings may be used as additional determinants in developing a patient specific adjuvant tumortherapy (radiation therapy, chemotherapy). Functional studies should be conducted to clarify whether immunologic therapies are feasible based on the current findings.\n2015-01-10\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5697\nurn:nbn:de:bvb:29-opus4-56979\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-56979\nhttps://opus4.kobv.de/opus4-fau/files/5697/Korrigiert%20Abschluss%20Erlangen%20Bibliothek.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5816\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:E.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nEGFR-\u00dcberexpression als m\u00f6glicher Parameter zur Einsch\u00e4tzung des malignen Potentials von Leukoplakien der Mundh\u00f6hle\nBechtold, Moritz\negfr\nbechtold\nleukoplakie\nmaligne\nddc:610\n1. Hintergrund und Ziele: Die Leukoplakie der Mundh\u00f6hle (LPM) stellt die h\u00e4ufigste potentiell maligne Mundschleimhautver\u00e4nderung dar. Der Bewertung ihres Potentials zur malignen Transformation kommt eine Schl\u00fcsselrolle in der Fr\u00fchdiagnostik von Kopf-Hals-Karzinomen, insbesondere des Plattenepithelkarzinoms der Mundh\u00f6hle (PECM), zu. In Fr\u00fchstadien therapiert, weist das PECM eine 5-Jahres-\u00dcberlebensrate von etwa 80 % auf, diese sinkt in den fortgeschrittenen Stadien auf etwa 30 %. Da die Mehrzahl der F\u00e4lle erst in den h\u00f6heren Stadien diagnostiziert wird, sind differenzierte Methoden f\u00fcr die Fr\u00fchdiagnostik von h\u00f6chster Wichtigkeit.\nDie histopathologische Einstufung des Grades der Dysplasie geh\u00f6rt zur Routine in der Fr\u00fchdiagnostik, um benigne Proben von solchen mit potentieller Malignit\u00e4t zu unterscheiden. Diese Methode ist jedoch subjektiv und kann h\u00e4ufig keine zuverl\u00e4ssige Aussage dar\u00fcber treffen, ob das Gewebe maligne entarten k\u00f6nnte. Daher sucht man heute nach molekularbiologischen Markern, deren Expression in Zusammenhang mit Ver\u00e4nderungen in Zellproliferation und Differenzierung steht. Diese Marker k\u00f6nnen mit der malignen Entartung eines Gewebes assoziiert sein.\nZiel dieser Studie ist die Pr\u00fcfung, ob zwischen der Expressionsrate des Proteins Epidermal Growth Factor Receptor (EGFR) in Leukoplakien der Mundh\u00f6hle und deren zeitnaher maligner Entartung eine Korrelation besteht.\n2. Material und Methoden: Das historische Kollektiv umfasste 148 Gewebeproben von insgesamt 127 Patienten, diese wurden in drei Gruppen unterteilt. Gruppe I bildeten 53 pr\u00e4maligne Leukoplakieproben, die im Beobachtungszeitraum von 5 Jahren (Follow-Up) ein PECM entwickelten. Bei 21 dieser 53 Patienten wurde der korrespondierende Tumor immunhistochemisch nachuntersucht. Gruppe II umfasste 45 Leukoplakien (LP), die im Beobachtungszeitraum von 5 Jahren kein PECM entwickelten. Gruppe III diente als Kontrollgruppe und beinhaltete 29 Proben gesunder Mundschleimhaut.\nBei den untersuchten Schnitten handelte es sich um formalinfixierte Paraffinproben. Vorab erfolgte die Bestimmung des Dysplasiegrades anhand einer H\u00e4matoxylin-Eosin-F\u00e4rbung (HE). Um die Expression von EGFR einzusch\u00e4tzen, wurde das Protein in weiteren Schnitten immunhistochemisch nachgewiesen und anschlie\u00dfend geeignete Bereiche der Proben (regions of interest, ROI) semi-quantitativ mittels Cell-Counting ausgez\u00e4hlt. F\u00fcr die Auswertung wurde die Expression in den einzelnen Gruppen und in den jeweiligen Dysplasiegraden betrachtet und bez\u00fcglich signifikanter Unterschiede bewertet. Hierzu diente der Mann-Whitney-U-Test mit einem Signifikanzniveau von p \u2264 0,05.\n3. Ergebnisse: Die Analyse der EGFR-Expression in den einzelnen Gruppen zeigte signifikante Unterschiede in der Gruppe der entartenden (Gruppe I) und nicht-entartenden (Gruppe II) Leukoplakien (p = 0,017). Vergleicht man die einzelnen Dysplasiegrade in Gruppe I und II miteinander, so waren die Unterschiede bei D0 (p = 0,013) und D1 (p = 0,049) signifikant. F\u00fcr die h\u00f6heren Dysplasiegrade (D2 und D3) zeigten sich keine signifikanten Zusammenh\u00e4nge. F\u00fcr D2 lag der p-Wert bei 0,43. D3 lag in Gruppe II nicht vor und hatte in Gruppe I die geringste Probenzahl (n = 11). Beim Vergleich der Kontrollgruppe (Gruppe III) mit den zusammengefassten D0- und D1-Proben der Gruppe I zeigte sich ein hochsignifikanter Unterschied (p = 0,003). Dieser war auch noch beim Zusammenfassen der D0-, D1- und D2-Proben der Gruppe I gegen\u00fcber Gruppe III signifikant (p = 0,05). Fasst man dagegen die D0- und D1-Proben der Gruppe II zusammen, ergibt sich kein signifikanter Unterschied zur Kontrollgruppe (p = 0,26).\nUm zu beurteilen, ob eine pr\u00e4maligne L\u00e4sion als benigne einzustufen ist oder erh\u00f6htes Potential zur malignen Entartung besitzt, wurde ein Schwellenwert (cut off point, COP) im Sinne einer kritischen EGFR-\u00dcberexpression mithilfe der ROC-Kurve (receiver operating characteristic) ermittelt. Der COP, ab dem eine maligne Entartung als wahrscheinlich anzusehen ist, liegt nach dem errechneten Youden-Index bei 44,7 %.\nZudem wurde das Quotenverh\u00e4ltnis (odds ratio, OR) bestimmt. Dieses liegt bei 4,83. L\u00e4sionen, die den COP der EGFR-Expression \u00fcberschreiten haben demzufolge eine rund 5-mal h\u00f6here Chance zu entarten. Betrachtet man das 95%-Konfidenzintervall der OR, so ist diese bei \u00dcberschreiten des COP mindestens doppelt und maximal 12-fach erh\u00f6ht gegen\u00fcber einer L\u00e4sion mit unterschwelliger Expression.\n4. Praktische Schlussfolgerung: Die Bestimmung der EGFR-Expression in LPM kann ein wichtiges zus\u00e4tzliches diagnostisches Hilfsmittel sein, um Gewebever\u00e4nderungen mit erh\u00f6htem Risiko f\u00fcr eine maligne Entartung \u2013 \u00fcber die Bewertung des Dysplasiegrades hinaus - sicherer zu identifizieren.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5816\nurn:nbn:de:bvb:29-opus4-58168\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-58168\nhttps://opus4.kobv.de/opus4-fau/files/5816/Dissertation_Bechtold_Moritz_2015.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:5920\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:J.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nHuman epidermal growth factor receptor 2 und Topoisomerase II \u03b1 als pr\u00e4diktive Marker f\u00fcr das Ansprechen auf eine neoadjuvante, anthrazyklinhaltige Chemotherapie beim Mammakarzinom und deren Bedeutung als prognostische Faktoren\nHuman epidermal growth factor Rezeptor 2 and Topoisomerase II \u03b1 as prediktive marker for a response to neoadjuvant, anthracyclinebased Chemotherapy on breast cancer and their relevance as prognostic factor\nPimenta, Daniela Andrea\nOnkogen HER-2/neu\nddc:610\n1.1. Hintergrund und Ziele\nBrustkrebs stellt nach wie vor die h\u00e4ufigste b\u00f6sartige Tumorerkrankung der\nFrau dar und ist neben dem Lungenkarzinom f\u00fcr die Mehrheit der\nKrebstodesf\u00e4lle verantwortlich. Es ist bekannt, dass Brustkrebs ein\nheterogenes Krankheitsbild darstellt, das aus verschiedenen Entit\u00e4ten mit\nunterschiedlichen genetischen Aberrationen besteht, z.B. Amplifikationen\nvon HER2 und TOP2A. Die neoadjuvante Chemotherapie stellt eine klinische\nBehandlungsm\u00f6glichkeit von Brustkrebspatientinnen dar, die nicht nur die\nChancen auf eine brusterhaltende Operation erh\u00f6ht, sondern auch Einblicke\nin das Verhalten des Tumors im Sinne eines in-vivo-Experiments gibt. Wird\ndie Therapie im Rahmen von Studien durchgef\u00fchrt, ist das Studiendesign\nideal, um pr\u00e4diktive Faktoren f\u00fcr das Ansprechen auf eine neoadjuvante\nChemotherapie zu identifizieren.\nZiel dieser Arbeit war es, den HER2- und TOP2A-Status von\npr\u00e4therapeutisch stanzbioptisch gewonnenem Tumormaterial mit dem\nAnsprechen auf die Chemotherapie in einem neoadjuvant behandelten\nKollektiv von Mammakarzinompatientinnen zu korrelieren.\n1.2. Methoden\nZwischen 2000 und 2008 konnten 142 Patientinnen mit einem prim\u00e4ren\nMammakarzinom ermittelt werden, die alle mit 4 Zyklen neoadjuvanter\nChemotherapie mit Epirubicin und Cyclophosphamid behandelt worden sind.\nNach Formalinfixation und Paraffineinbettung wurden die entsprechenden\nTumorproben mittels Fluoreszenz-in-situ-Hybridisierung (FISH) auf den\nHER2- und TOP2A-Amplifikationstatus und Assoziation mit diversen\nklinischen Variablen hin untersucht. Schlie\u00dflich wurden die komplette\npathologische Remission und der \u00dcberlebensstatus der Patientinnen\nausgewertet.\n3\n1.3. Ergebnisse und Beobachtungen\nBei ca. einem Drittel der untersuchten Mammakarzinome konnte eine HER2-\nAmplifikation nachgewiesen werden. Darunter waren mehr als die H\u00e4lfte mit\neiner TOP2A-Koamplifikation assoziiert. Eine isolierte TOP2A-Amplifikation\nkonnte in keinem einzigen Fall ermittelt werden, so dass die Vermutung\nnaheliegt, dass eine HER2-Amplifikation die Voraussetzung f\u00fcr eine TOP2AAmplifikation\ndarstellt. Die immunhistologische Expressionsanalyse von\nHER2/neu zeigte eine signifikante Assoziation mit der entsprechenden\nFluoreszenz-in-situ-Hybridisierung von HER2 als auch von TOP2A. W\u00e4hrend\nin diesem kleinen Studienkollektiv weder HER2 noch TOP2A mittels FISH als\nPr\u00e4diktiv- bzw. Prognosefaktor nachgewiesen werden konnte, zeigten\nfolgende Biomarker eine signifikante Assoziation mit einer kompletten\npathologischen Remission: Nodalstatus, Graduierung, \u00d6strogen- und\nProgesteronrezeptorstatus.\n1.4. Praktische Schlussfolgerungen\nEine \u00dcberexpression von TOP2A konnte in dieser Studie nicht mit dem\nAnsprechen auf eine anthrazyklinhaltige Chemotherapie in Verbindung\ngebracht werden. Eine TOP2A-Amplifikation scheint jedoch von einer HER2-\nAmplifikation abh\u00e4ngig zu sein, so dass eine kausale Beziehung auf\nmolekularer Ebene in weiteren Studien genauer analysiert werden sollte. Ziel\nzuk\u00fcnftiger Studien wird es sein, weitere pr\u00e4diktive bzw. prognostische\nBiomarker zu identifizieren, um neue therapeutische Strategien in der\nBehandlung des Mammakarzinoms zu erm\u00f6glichen.\nBackground\nBreast cancer continues to be the most frequent cancer in women and is\nbesides lung cancer also responsible for the majority of deaths from cancer.\nThe individual lifetime risk is influenced by several risk factors. The\nneoadjuvant chemotherapy is one treatment option for breast cancer patients\nwith an indication for a cytotoxic therapy. It increases the probability of a\nbreast conserving surgery and additionally gives the opportunity to study the\ntumor in vivo during chemotherapy. The data that can be obtained from\nneoadjuvant chemotherapy studies in breast cancer is ideal to identify\npredictive and prognostic factors for specific breast cancer therapies.\nPatients and methods\nBetween 2000 and 2008 a total of 142 patients could be identified who have\nbeen treated with neoadjuvant chemotherapy of 4 cycles of epirubicin and\ncyclophosphamide. After formalin-fixed paraffin-embedding and fluorescence\nin situ hybridization the corresponding samples of tumor were analysed for\nHER2 and TOP2A amplification and association with diverse clinical\nvariables. Finally the complete pathological remission and the overall survival\nof the patients were determined.\n5\nResults\nUp to one third of the analyzed breast carcinomas showed an amplification of\nHER2. More than half of them had in parallel a TOP2A coamplification. An\nisolated TOP2A amplification was not detected suggesting a HER2\namplification as a precondition for TOP2A amplification. There was a\nsignificant association between immunhistochemistry and fluorescence-insitu-\nhybridization for both HER2 and TOP2A. In this small collective neither\nHER2 nor TOP2A could be shown to be a predictive or prognostic factor after\nneoadjuvant chemotherapy. In contrast the following factors showed a\nsignificant association with the complete pathological remission: status of\nlymph node, grading, expressio of the oestrogen and progesterone\nreceptors.\nConclusion\nA high TOP2A amplification could not be correlated with the complete\npathological remission of anthracycline chemotherapy. However a TOP2A\namplification seems to depend on a HER2 amplification. This causal\ncorrelation should be precisely analysed on molecular level by further\nstudies. Furthermore future studies should identify additional predicitve and\nprognostic factors to provide new therapeutical opportunities for the\ntreatment of breast cancer.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/5920\nurn:nbn:de:bvb:29-opus4-59209\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-59209\nhttps://opus4.kobv.de/opus4-fau/files/5920/PROMOTION_Daniela%20Pimenta.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6024\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Elektrotechnik\nRepetition-based Structure Analysis of Music Recordings\nWiederholungsbasierte Strukturanalyse von Musikaufnahmen\nJiang, Nanzhu\nMusic\nStructure\nAnalysis\nddc:004\nMusic Information Retrieval (MIR) is a current area of research which aims at providing techniques and tools for searching, organizing, processing and interacting with music data. In order to extract musically meaningful information from audio recordings, one requires methods from various fields such as digital signal processing, music theory, human perception, and information retrieval. One central research topic within MIR is referred to as music structure analysis, where an important goal is to divide a music recording into temporal segments and to group these segments into musically meaningful categories. The extracted structural information can be used for a variety of other MIR tasks including music navigation, audio thumbnailing, audio summarization, and chord recognition.\nThe structure of a music recording depends on various principles such as temporal order, repetition, contrast, variation and homogeneity. Based on these principles, many approaches for music structure analysis have been proposed in the literature. However, it remains difficult to perform music structure analysis in a fully automated fashion. One reason is that music structure can be considered on different temporal levels so that even music experts may disagree on how to structure a given piece of music. Furthermore, the task of music structure analysis is complex when analyzing audio recordings due to possible acoustic variations across different musical sections.\nIn this thesis, we focus on repetition-based approaches for music structure analysis. As one main contribution, we introduce a novel fitness-based method that extracts repetitive structures from audio recordings. First, using signal processing techniques, the given audio recording is converted into a feature sequence that captures harmonic and melodic aspects. Next, using the concept of similarity matrices, the feature sequence is analyzed with respect to recurring patterns. In particular, we discuss various enhancement techniques to cope with musical variations such as tempo differences and transpositions. Using alignment techniques related to Dynamic Time Warping (DTW), we introduce a novel fitness measure that assigns a fitness value to each segment. Each fitness value expresses how much and how well the respective segment explains the repetitive structure of the entire recording. This fitness measure serves as the main basis for several other contributions made in this thesis.\nFirst of all, we deal with a subproblem of music structure analysis called audio thumbnailing with the goal to determine the audio segment that best represents a given music recording. We show that our fitness measure is useful in detecting suitable audio thumbnails by considering segments of high fitness. Then, we present a novel scape plot representation that makes it possible to visualize repetitive structures of the entire music recording in a hierarchical, compact, and intuitive way. This visualization does not only indicate the benefits and limitations of our methods, but also yields interesting musical insights into the data. As an application within musicology, we show how our techniques can be applied for analyzing and segmenting music recordings in sonata form. To this end, we adapted our repetition-based approach for detecting the coarse structure of a sonata (exposition, development, recapitulation) and introduced a rule-based approach measuring local harmonic relations for analyzing finer substructures. Furthermore, we discuss how the fitness-based structure analysis can be extended for deriving more general musical structures that consist of several groups of repeating segments. As a further technical contribution, we show how the computational efficiency of our structure analysis approach can be improved significantly by using multi-resolution strategies.\nDas aktuelle Forschungsgebiet des Music Information Retrieval (MIR) befasst sich mit der Bereitstellung von Techniken und Werkzeugen zum Suchen, Organisieren, Verarbeiten sowie zur Interaktion mit Musikdaten. Um musikalisch sinnvolle Informationen aus Audioaufnahmen zu extrahieren, werden Methoden aus vielen Bereichen, darunter digitale Signalverarbeitung, Musiktheorie, menschliche Wahrnehmung und dem Information Retrieval eingesetzt. Ein zentrales Forschungsgebiet im MIR ist die Musikstrukturanalyse, bei der ein Musikst\u00fcck in zeitliche Segmente zerlegt wird und diese anschlie\u00dfend in musikalisch sinnvolle Kategorien gruppiert werden. Die extrahierte Strukturinformation kann f\u00fcr eine Vielzahl anderer MIR-Aufgabenstellungen wie Musiknavigation, Audio Thumbnailing, Audiozusammenfassung und Akkorderkennung verwendet werden.\nDie Struktur einer Musikaufnahme h\u00e4ngt von verschiedenen Aspekte wie der zeitlichen Reihenfolge, von Wiederholungen, Kontrasten, Variationen und Homogenit\u00e4t ab. In der Literatur finden sich viele Ans\u00e4tze zur Musikstrukturanalyse, die auf diese Prinzipien aufbauen. Allerdings erweist es sich als schwierig, eine Musikstrukturanalyse vollst\u00e4ndig automatisiert durchzuf\u00fchren. Ein Grund daf\u00fcr ist, dass Musikstruktur auf verschiedenen zeitlichen Stufen betrachtet werden kann, sodass selbst Musikexperten dar\u00fcber streiten m\u00f6gen, wie ein konkretes Musikst\u00fcck zu strukturieren sei. Weiterhin ist die Musikstrukturanalyse von Audioaufnahmen eine schwierige Aufgabe aufgrund m\u00f6glicher akustischer Unterschiede \u00fcber verschiedene musikalische Passagen hinweg.\nIn dieser Arbeit konzentrieren wir uns auf wiederholungsbasierte Ans\u00e4tze zur Musikstrukturanalyse. Als Hauptbeitrag stellen wir eine neuartige, fitness-basierte Methode vor, welche geeignet ist, Wiederholungsstrukturen in einer Audioaufnahme zu detektieren. Hierzu wird die Aufnahme zuerst mittels Methoden der Signalverarbeitung in eine Merkmalsdarstellung \u00fcberf\u00fchrt, die harmonische und melodische Eigenschaften abbildet. Durch die Verwendung von Selbst\u00e4hnlichkeitsmatrizen wird diese Merkmalsdarstellung bez\u00fcglich wiederholt auftretender Muster analysiert. Insbesondere diskutieren wir verschiedene Verbesserungsstrategien, um musikalische Variationen wie Tempo\u00e4nderungen und Transpositionen abzudecken. Durch Alinierungsmethoden \u00e4hnlich dem Dynamic Time Warping (DTW) f\u00fchren wir ein neuartiges Fitnessma\u00df ein, welches jedem Segment einen sogenannten Eignungswert zuordnet. Jeder dieser Werte gibt an, wie gut und zu welchem Anteil das jeweilige Segment die Wiederholungsstruktur der kompletten Aufnahme erkl\u00e4rt. Dieses Ma\u00df dient als Ausgangspunkt f\u00fcr einige weitere Beitr\u00e4ge dieser Arbeit.\nZuerst besch\u00e4ftigen wir uns mit einem Teilproblem der Musikstrukturanalyse namens Audio Thumbnailing, welches zum Ziel hat, das Audiosegment zu bestimmen, welches ein Musikst\u00fcck am besten beschreibt. Wir zeigen, dass unser Fitnessma\u00df zur Bestimmung von sinnvollen Audio Thumbnails durch Betrachtung von Segmenten mit hoher Fitness geeignet ist. Anschlie\u00dfend pr\u00e4sentieren wir eine neue Scape-Plot-Darstellung, welche die Visualisierung von Wiederholungsstrukturen des gesamten Musikst\u00fcckes auf eine hierarchische, kompakte und intuitive Weise erm\u00f6glicht. Diese Visualisierung zeigt nicht nur die M\u00f6glichkeiten und Grenzen unserer Methoden auf, sondern f\u00fchrt auch zu interessanten musikalischen Einblicken in die Daten. Als Anwendung in der Musikwissenschaft zeigen wir, wie unsere Techniken zur Analyse und Segmentierungen von Audioaufnahmen in der Sonatenhauptsatzform verwendet werden k\u00f6nnen. Hierzu wird unser wiederholungsbasierter Ansatz zur Ermittlung der Grobstruktur einer Sonate (Exposition, Durchf\u00fchrung, Reprise) angepasst und ein regelbasierter Ansatz zum Messen lokaler harmonischer Beziehungen eingef\u00fchrt. Weiterhin diskutieren wir, wie die fitness-basierte Strukturanalyse erweitert werden kann, um allgemeinere musikalische Strukturen bestehend aus mehreren Gruppen wiederholter Segmente aufzufinden. Als einen weiteren technischen Beitrag zeigen wir, wie die Rechenzeit f\u00fcr unseren Strukturanalyse-Ansatz durch die hierarchische Betrachtung mehrerer Aufl\u00f6sungsstufen signifikant verringert werden kann.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6024\nurn:nbn:de:bvb:29-opus4-60245\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60245\nhttps://opus4.kobv.de/opus4-fau/files/6024/NanzhuJiang_PhDThesis_Library.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6029\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:330\nccs\nccs:A.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Wiwi\nLogistikorientierte Ersatzteilvarianten-Reduzierung am Beispiel der Automobilindustrie: Reduzierungsstrategien und ressourcenorientierte Variantenkostenanalyse der Wertsch\u00f6pfungskette zur Begegnung der Proliferation der Ersatzteilvarianten\nWirz, Markus\nlogistik\nreduzierung\nvarianten\nvielfalt\nersatzteile\nressourcenorientierte\nbewertung\nautomobilindustrie\nvariantenmanagement\nersatzteilversorgung\nersatzteilwesen\nersatzteilwirtschaft\nersatzteillagerhaltung\nvariantenreich\nkostenanalyse\nwertsch\u00f6pfugskette\nddc:330\nDie vorliegende Arbeit besch\u00e4ftigt sich mit nachtr\u00e4glichen Reduzierung von Ersatzteilvarianten am Beispiel der Automobilindustrie. Insbesondere mit der Suche von Reduzierungsoptionen und der Entwicklung eines Bewertungsmodells f\u00fcr die Wertsch\u00f6pfungskette.\nIn der Arbeit wird Ziel-Strategie-Alternativen-Portfolio (ZSA-Portfolio) erarbeitet, dass die M\u00f6glichkeiten der nachtr\u00e4glichen Variantenreduzierung strukturiert abbildet.\nIm Weiteren wird die ressourcenorientierte Variantenkostenanalyse (RVKA) entwickelt. Das Bewertungsmodell erlaubt dabei die Variantenkostenbewertung der gesamten logistischen Wertsch\u00f6pfungskette. Zudem wird der lange Bewertungshorizont anhand einer adaptierten dynamischen Investitionskostenrechnung ber\u00fccksichtigt.\nZwei Fallbeispiele aus dem automobilen Umfeld dienen der Erprobung der Bewertungsmethode. Zugleich zeigen die Fallbeispiele den Rahmen des wirtschaftlichen Erfolges der Ersatzteilvariantenreduzierung.\n2014\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6029\nurn:nbn:de:bvb:29-opus4-60290\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60290\nhttps://opus4.kobv.de/opus4-fau/files/6029/MarkusWirzDissertation.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6042\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:E.\npacs\npacs:00.00.00\nmsc\nmsc:28-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nPr\u00e4diktoren des Bildrauschens bei einer CT-Koronarangiographie\nPatient-specific Predictors of Image Noise\nSchaefer, Marcella\nBildrauschen\nddc:610\nPatientenspezifische Vorwert, der eine Aussage dar\u00fcber macht, ob man ein Niedrig-Dosen-Protokoll verwenden kann\nwhich parameter may be the optimal patient-specific discriminator to determine the use of reduced tube voltage\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6042\nurn:nbn:de:bvb:29-opus4-60422\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60422\nhttps://opus4.kobv.de/opus4-fau/files/6042/Pr%C3%A4diktoren%20des%20Bildrauchens%20letzteVersion09.2013.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6044\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:006\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nBoosting Methods for Automatic Segmentation of Focal Liver Lesions\nBoosting-Verfahren zur automatischen Segmentierung fokaler Leberl\u00e4sionen\nMilitzer, Arne\nBildsegmentierung\nLebertumor\nMaschinelles Lernen\nBoosting\nComputertomographie\nddc:006\nOver the past decades, huge progress has been made in treatment of cancer, decreasing fatality rates despite a growing number of cases. Technical achievements had a big share in this development.\nWith modern image acquisition techniques, most types of tumors can be made visible.\nAutomatic processing of these images to support diagnosis and therapy, on the other hand, is still very basic. Marking lesions for volume measurements, intervention planning or tracking over time requires a lot of manual interaction, which is both tedious and error prone.\nThe work at hand therefore aims at providing tools for the automatic segmentation of\nliver lesions. A system is presented that receives a contrast enhanced CT image of the liver as input and, after several preprocessing steps, decides for each image voxel inside the liver whether it belongs to a tumor or not. That way, tumors are not only detected in the image but also precisely delineated in three dimensions. For the decision step, which is the main target of this thesis, we adopted the recently proposed Probabilistic Boosting Tree. In an offline learning phase, this classifier is trained using a number of example images. After training, it can process new and previously unseen images.\nSuch automatic segmentation systems are particularly valuable when it comes to monitoring tumors of a patient over a longer period of time. Therefore, we propose a method for\nlearning a prior model to improve segmentation accuracy for such follow-up examinations.\nIt is learned from a number of series of CT images, where each series contains images of\none patient. Two different ways of incorporating the model into the segmentation system are investigated. When acquiring an image of a patient, the system can use the model to calculate a patient specific lesion prior from images of the same patient acquired earlier\nand thus guide the segmentation in the current image.\nThe validity of this approach is shown in a set of experiments on clinical images. When comparing the points of 90% sensitivity in these experiments, incorporating the prior improved the precision of the segmentation from 82.7% to 91.9%. This corresponds to a\nreduction of the number of false positive voxels per true positive voxel by 57.8%.\nFinally, we address the issue of long processing times of classification based segmentation systems. During training, the Probabilistic Boosting Tree builds up a hierarchy of AdaBoost classifiers. In order to speed up classification during application phase, we modify this hierarchy so that simpler and thus faster AdaBoost classifiers are used in higher levels. To this end, we introduce a cost term into AdaBoost training that trades off discriminative\npower and computational complexity during feature selection. That way the\noptimization process can be guided to build less complex classifiers for higher levels of the tree and more complex and thus stronger ones for deeper levels. Results of an experimental evaluation on clinical images are presented, which show that this mechanism can reduce\nthe overall cost during application phase by up to 76% without degrading classification accuracy.\nIt is also shown that this mechanism could be used to optimize arbitrary secondary\nconditions during AdaBoost training.\nIn der Krebstherapie gab es in den vergangenen Jahrzehnten gro\u00dfe Fortschritte zu verzeichnen.\nW\u00e4hrend die Zahl der Krebsf\u00e4lle weiter ansteigt, konnte die Sterblichkeit verringert werden. Einen gro\u00dfen Anteil an dieser Entwicklung hatte der technische Fortschritt.\nBildgebende Verfahren k\u00f6nnen heute die meisten Tumoren sichtbar machen. Die automatische Verarbeitung dieser Bilder f\u00fcr Diagnose und Therapie dagegen beschr\u00e4nkt sich weiterhin zumeist auf sehr einfache Verfahren. L\u00e4sionen k\u00f6nnen nur mit viel Handarbeit\nf\u00fcr Volumenbestimmung, Operationsplanung, oder zeitliche \u00dcberwachung eingezeichnet werden. Dieses Vorgehen ist nicht nur sehr m\u00fchsam sondern auch anf\u00e4llig f\u00fcr Fehler.\nDie vorliegende Arbeit soll deshalb neue Methoden f\u00fcr die automatische Segmentierung\nvon Leberl\u00e4sionen aufzeigen. Ein System wird vorgestellt, das in einem kontrastverst\u00e4rkten CT-Bild der Leber nach einigen Vorverarbeitungsschritten f\u00fcr jeden Bildpunkt innerhalb der Leber entscheidet, ob er Teil eines Tumors ist oder nicht. So werden die Tumoren nicht nur detektiert sondern gleichzeitig auch in allen drei Raumrichtungen vom\numliegenden Gewebe abgegrenzt. Der Entscheidungsschritt macht dabei den Kern dieser\nArbeit aus. Als Klassifikator wurde hierf\u00fcr der Probabilistic Boosting Tree gew\u00e4hlt. Er wird in einer separaten Lernphase vor seinem eigentlichen Einsatz anhand einer Reihe von\nBeispielbildern trainiert. Nach erfolgreichem Training kann er in der Anwendungsphase\nauch zuvor nicht gesehene Bilddaten verarbeiten.\nDerartige automatische Segmentierungsverfahren sind besonders dann hilfreich, wenn\nTumoren \u00fcber einen l\u00e4ngeren Zeitraum \u00fcberwacht werden sollen. Wir pr\u00e4sentieren daher\nan dieser Stelle ein Verfahren, mit dem ein a priori Modell f\u00fcr Tumorwahrscheinlichkeiten\nerstellt werden kann. Das Modell wird aus einer Reihe von zeitlichen Serien von CTBildern gewonnen, wobei eine Serie jeweils Bilder eines Patienten enth\u00e4lt. Ziel ist es, die Qualit\u00e4t der Segmentierungsergebnisse f\u00fcr Folgeuntersuchungen zu verbessern. Es werden\nzwei verschiedene Methoden untersucht, wie das Modell in das bestehende Segmentierungssystem zu integrieren ist. Anschlie\u00dfend kann das System die Segmentierung bei einer Folgeuntersuchung steuern, indem es mit Hilfe des Modells aus fr\u00fcheren Aufnahmen\ndesselben Patienten berechnet, wo L\u00e4sionen zu erwarten sind.\nDie Wirksamkeit dieses Vorgehens wird anhand einer Reihe von Experimenten mit\nklinischen Aufnahmen belegt. Vergleicht man in diesen Experimenten jeweils den Punkt, an dem 90% der Tumorpunkte erkannt wurden, stellt man fest, dass sich der positive Vorhersagewert durch das a priori Modell von 82,7% auf 91,9% verbessert. F\u00fcr die Zahl der falsch positiv klassifizierten Voxel je korrekt positiv klassifiziertem Voxel entspricht das einer Verbesserung um 57,8%.\nSchlie\u00dflich widmen wir uns der Problematik der Rechenzeit von klassifikationsbasierten\nSegmentierungsverfahren. In der Lernphase baut der Probabilistic Boosting Tree eine Hierarchie von AdaBoost-Klassifikatoren auf. Um die Klassifikation im Produktiveinsatz zu beschleunigen ver\u00e4ndern wir diese Hierarchie dahingehend, dass auf den oberen\nStufen einfachere und damit schnellere AdaBoost-Klassifikatoren verwendet werden. Zu diesem Zweck wird ein Kostenfaktor in das Lernverfahren f\u00fcr AdaBoost eingef\u00fchrt, der w\u00e4hrend der Merkmalsauswahl die Komplexit\u00e4t eines Merkmals gegen seinen Nutzen f\u00fcr\ndie Entscheidung abw\u00e4gt. Auf dieseWeise kann das Lernverfahren gezwungen werden, f\u00fcr die oberen Ebenen der Hierarchie einfachere und f\u00fcr die tieferen Ebenen komplexere Klassifikatoren\nzu erzeugen. Die Ergebnisse einer experimentellen Auswertung mit klinischen\nBildern belegen, dass diese Methode die Gesamtkosten der Entscheidung in der Anwendungsphase um bis zu 76% verringern kann ohne die Genauigkeit der Segmentierung zu beeintr\u00e4chtigen. DesWeiteren wird in den Experimenten gezeigt, dass AdaBoost auf diese\nArt nicht nur die Laufzeit sondern beliebige Nebenbedingungen optimieren kann.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6044\nurn:nbn:de:bvb:29-opus4-60445\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60445\nhttps://opus4.kobv.de/opus4-fau/files/6044/DissertationArneMilitzer.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6061\n2018-01-26\ndoc-type:workingPaper\nbibliography:false\nddc\nddc:003\nccs\nccs:C.0\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nAn Experiment in Wait-Free Synchronisation of Priority-Controlled Simultaneous Processes: Guarded Sections\nDrescher, Gabor\nSchr\u00f6der-Preikschat, Wolfgang\n-\nddc:003\nWait-free synchronisation gives any process in the system strong progress\nguarantees, irrespective of number and behaviour of other processes\nsimultaneously competing for shared resources (i.e., data structures and code\nsections). It ensures completion of any operation in a finite number of steps\nand, thus, provides the basis to derive bounded above or even constant\nexecutions times for non-sequential programs. This characteristic is of special\nmeaning for time-dependent processes typical for real-time (embedded) systems.\nBut wait-free synchronisation against the background of especially arbitrary\ndata and code structures is no bed of roses.\nThis paper is about organising non-sequential programs to the benefit of\nwait-free synchronisation. Conventional critical sections are designed as so\ncalled guarded sections. Unlike critical sections, preferential processes never\nblock at entrance to a guarded section though only one process at a time is\nallowed to pass through. Competing processes are forced into bypass but, if\nnecessary and by using futures, they can synchronise on concurrent state\nchanges inside the respective section. In consequence of this measure, the\nexecution model of guarded sections constrains the overlapping pattern of\ninteracting (simultaneous) processes. Thereby, efficient wait-free\nsynchronisation of the \"guarding operations\" is a gratifying by-product.\nFirst experiments on a 80-way multi-core system show that non-blocking\nwait-free synchronised guarded sections outperform lock-based protection\nschemes such as MCS-locks.\n2015\nworkingpaper\ndoc-type:workingPaper\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6061\nurn:nbn:de:bvb:29-opus4-60612\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60612\nhttps://opus4.kobv.de/opus4-fau/files/6061/cs-2015-01.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6092\n2018-01-26\ndoc-type:report\nbibliography:false\nddc\nddc:000\nccs\nccs:D.2.9\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nInner Source in Platform-Based Product Engineering\nRiehle, Dirk\nCapraro, Maximilian\nKips, Detlef\nHorn, Lars\nOpen source\nInner source\nddc:000\nInner source is an approach to collaboration across intra-organizational boundaries for the creation of shared reusable assets. Prior project reports on inner source suggest improved code reuse and better knowledge sharing. Using a multiple-case case study research approach, we analyze the problems that three major software development organizations were facing in their platform-based product engineering efforts. We find that a root cause, the separation of product units as profit centers from a platform organization as a cost center, leads to delayed deliveries, increased defect rates, and redundant software components. All three organizations assume that inner source can help solve these problems. The article analyzes the expectations that these companies were having towards inner source and the problems they were experiencing or expecting in its adoption. Finally, the article presents our conclusions on how these organizations should adapt their existing engineering efforts.\n2015\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6092\nurn:nbn:de:bvb:29-opus4-60923\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-60923\nhttps://opus4.kobv.de/opus4-fau/files/6092/TR%20CS-2015-02%20Inner%20Source%20Full%20Web.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6127\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:005\nddc:621\nccs\nccs:J.2\npacs\npacs:42.55.Rz\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nAnalyse der spannungsinduzierten Doppelbrechung in Laserkristallen und deren Einfluss auf Laserresonatoren\nGraupeter, Thomas\nKristalloptik\nOptikmodellierung\nSpannungsoptik\nPolarisation\nFestk\u00f6rperlaser\nLaserkristall\nComputersimulation\nMechanik\nFinite-Elemente-Methode\nPhotoelastizit\u00e4t\nddc:005\nddc:621\nIn dieser vorliegenden Dissertation wird mit Hilfe von numerischen Simulationen der Einfluss der spannungsinduzierten Doppelbrechung auf die Eigenpolarisationszust\u00e4nde von Festk\u00f6rperlasern untersucht. Es werden die grundlegenden physikalischen Zusammenh\u00e4nge zwischen den thermischen, mechanischen und elektromagnetischen Feldern behandelt, die durch die nicht homogen verteilte Absorption des Pumplichtes im Laserkristall auftreten. Die durch diesen photoelastischen Effekt dominierten Eigenpolarisationszust\u00e4nde im Resonator werden untersucht. Als eine Anwendung werden die Eigenpolarisationszust\u00e4nde, die Ausgangsleistung, die Strahlqualit\u00e4t und die Polarisationsreinheit eines Laserresonators mit Brewsterplatten in Abh\u00e4ngigkeit von den Kristallschnitten des Nd:YAG-Laserkristalls erforscht. In einem zweiten Beispiel wird mit Hilfe des polarisationsabh\u00e4ngigen Stabilit\u00e4tsverhaltens eines Laserresonators mit einem Nd:YAG-Kristall ein Ausgangsstrahl einer bestimmten zirkularen Polarisation ohne zus\u00e4tzliche optische Elemente erzeugt. Dabei wird ebenfalls der Einfluss der Kristallschnitte auf die Stabilit\u00e4tsbereiche, die Ausgangsleistung und die Strahlqualit\u00e4t untersucht.\nKapitel 2 befasst sich mit den physikalischen Grundlagen der Mechanik und Optik, welche in dieser Arbeit angewendet werden. Dabei wird ein \u00dcberblick \u00fcber verschiedene Festk\u00f6rperlasermaterialen und deren Einteilung in Kristallklassen gegeben. Es wird auf die Modellierung des mechanischen Steifigkeitsverhaltens des Kristalls mit Hilfe von anisotropen und isotropen Materialmodellen eingegangen. Als weiterer physikalischer Effekt im Kristall wird die Kopplung zwischen den mechanischen und optischen Feldern sowie deren mathematische Handhabung beschrieben.\nKapitel 3 behandelt die Eigenpolarisationszust\u00e4nde in Festk\u00f6rperlaserresonatoren mit Nd:YAG-Kris-tallen aufgrund thermisch induzierter Doppelbrechung. Es wird darin auf Eigenpolarisationszust\u00e4nde im Resonator und ihre Berechnung durch den Jonesformalismus eingegangen. Die Eigenmoden des elektromagnetischen Feldes innerhalb eines Resonators werden kurz erw\u00e4hnt. F\u00fcr die Berechnung der Ausgangsleistung und Strahlqualit\u00e4t eines Resonators wird die Dynamische Multimode Analyse kurz vorgestellt. Um m\u00f6gliche Stabilit\u00e4tsuntersuchungen von Festk\u00f6rperlasern durchf\u00fchren zu k\u00f6nnen, wird der parabolische Fit der Brechungsindizes, die ABCD-Matrixmethode und der polarisationsabh\u00e4ngige Brechungsindex mit dessen Auswirkung auf die Resonatorstabilit\u00e4t beschrieben.\nIm Kapitel 4 wird eine Untersuchung zur Verbesserung der Strahlqualit\u00e4t und Ausgangsleistung durch Verwendung eines Polarisationsfilters in einem Resonator mit einem Nd:YAG-Kristall durchgef\u00fchrt. Darin werden nach dem Vorstellen des numerischen Modells die auftretende Temperaturverteilung und die auftretenden mechanischen Spannungen im Kristall aufgrund des absorbierten Pumplichtes dargestellt. Anschlie\u00dfend wird die spannungsinduzierte Doppelbrechung gezeigt. Die resultierenden Eigenpolarisationszust\u00e4nde im Resonator mit und ohne Brewsterplatten werden mit dem Jonesformalismus berechnet. Abschlie\u00dfend werden die erzielten Ausgangsleistungen, die Strahlqualit\u00e4t und die Polarisationsreinheit in Abh\u00e4ngigkeit von den Kristallschnittrichtungen verglichen. Dabei werden der [100]-Schnitt mit einer Rotation des Kristalls um seine eigene L\u00e4ngsachse um 0\u00b0 und 90\u00b0, der [110]-Schnitt mit einer Rotation um 0\u00b0, 45\u00b0 und 90\u00b0 und der [111]-Schnitt ohne Variation des Rotationswinkels betrachtet.\nEin Beispiel zur Erzeugung eines bestimmten Polarisationszustandes mittels unterschiedlichen Stabilit\u00e4tsverhalten durch spannungsinduzierte Doppelbrechung im Resonator wird im Kapitel 5 gezeigt. Dazu werden nach dem Vorstellen des zugrunde liegenden Modells der Temperaturverlauf und die auftretenden mechanischen Spannungen im Kristall untersucht. Des Weiteren werden die resultierenden, von der Schnittrichtung des Kristalls abh\u00e4ngige Brechungsindizes und Doppelbrechungsmuster berechnet. Dabei werden die Schnittrichtungen und Rotationen des Laserkristalls wie im vorhergehenden Kapitel variiert. Es werden die auftretenden Eigenmodeprofile gezeigt. Durch die unterschiedliche Auspr\u00e4gung der Doppelbrechung entstehen bei Variation der L\u00e4nge des Resonators bestimmte Bereiche, in denen nur der radiale oder nur der azimutale Eigenpolarisationszustand stabil ist. Diese Bereiche werden in Abh\u00e4ngigkeit vom Kristallschnitt und -drehung quantitativ berechnet. In diesen Bereichen werden die Strahlradien der Eigenmoden der ersten Ordnung, die Ausgangsleistung und die Strahlqualit\u00e4t ermittelt und miteinander verglichen.\nIm letzten Kapitel 6 werden abschlie\u00dfend die Erkenntnisse dieser Arbeit zusammengefasst, auf Einschr\u00e4nkungen der erzielten Ergebnisse eingegangen und ein m\u00f6glicher Ausblick gezeigt.\nZusammenfassend l\u00e4sst sich aussagen, dass sich mit Hilfe dieser numerischen Methoden genaue Analysen der Eigenpolarisationszust\u00e4nde in einem Festk\u00f6rperlaserresonator aufgrund der spannungsinduzierten Doppelbrechung durchf\u00fchren lassen, was mit den bisherigen auf analytischen Formeln aufbauenden Methoden nur eingeschr\u00e4nkt oder gar nicht m\u00f6glich war. Mit den hier vorgestellten Methoden lassen sich effektiv Laserresonatoren simulieren und entwickeln, welche bestimmte Polarisationseigenschaften aufweisen sollen.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6127\nurn:nbn:de:bvb:29-opus4-61272\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-61272\nhttps://opus4.kobv.de/opus4-fau/files/6127/Diss_Graupeter.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6141\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:535\npacs\npacs:42.50.-p\npacs:89.70.-a\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat_Physik\nSecurity of practical quantum key distribution systems\nSicherheitsanalyse von praktischen Quantenschl\u00fcsselverteilungs-Systemen\nJain, Nitin\nQuantenkommunikationsprotokoll\nQuanten-Kommunikation\nKryptographie\nDatenverschl\u00fcsselung\nQuantenkryptographie\nQuantum key distribution\nddc:535\nThis thesis deals with practical security aspects of quantum key distribution (QKD) systems. At the heart of the theoretical model of any QKD system lies a quantum-mechanical security proof that guarantees perfect secrecy of messages \u2013 based on certain assumptions. However, in practice, deviations between the theoretical model and the physical implementation could be exploited by an attacker to break the security of the system. These deviations may arise from technical limitations and operational imperfections in the physical implementation and/or unrealistic assumptions and insufficient constraints in the theoretical model. In this thesis, we experimentally investigate in depth several such deviations. We demonstrate the resultant vulnerabilities via proof-of-principle attacks on a commercial QKD system from ID Quantique. We also propose countermeasures against the investigated loopholes to secure both existing and future QKD implementations.\nDiese Arbeit besch\u00e4ftigt sich mit in der Praxis wichtigen Sicherheitsaspekten von Quantenschl\u00fcsselverteilungs-Systemen (QKD-Systemen). Grundlage des theoretischen Modells eines QKD-Systems ist ein quantenmechanischer Sicherheitsbeweis, der die perfekte Geheimhaltung von Nachrichten garantiert \u2013 basierend auf bestimmten Annahmen. Allerdings\nk\u00f6nnen in der Praxis Abweichungen zwischen dem theoretischen Modell und der physikalischen Implementierung von einem Angreifer ausgenutzt werden, um die Sicherheit des Systems\nzu kompromittieren. Diese Abweichungen k\u00f6nnen ihren Grund in technischen Einschr\u00e4nkungen und in den nicht idealen Betriebsbedingungen der physikalischen Implementierung haben\noder sie treten aufgrund unrealistischer Annahmen und unzureichender Begrenzungen im theoretischen Modell auf. In dieser Arbeit werden mehrere solcher Abweichungen in der Implementierung experimentell untersucht. Die daraus resultierenden Schwachstellen werden durch experimentelle Machbarkeitsnachweise entsprechender Angriffe auf ein kommerzielles QKD-System von ID Quantique nachgewiesen. Au\u00dferdem werden Gegenma\u00dfnahmen gegen die untersuchten Schlupfl\u00f6cher vorgeschlagen, um sowohl bestehende als auch zuk\u00fcnftige QKDImplementierungen in der Praxis sicher zu machen.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6141\nurn:nbn:de:bvb:29-opus4-61417\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-61417\nhttps://opus4.kobv.de/opus4-fau/files/6141/PhDthesisNitinJain.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6197\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:530\nddc:600\nccs\nccs:I.6.4\npacs\npacs:89.20.Bb\nmsc\nmsc:65Z05\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nSimulation and Modeling of Silicon Carbide Devices\nSimulation und Modellierung von Siliziumkarbid-Bauelementen\nUhnevionak, Viktoryia\nWide-gap-Halbleiter, Siliciumcarbid, Simulation, n-Kanal-FET, Hall-Beweglichkeit, Haftstelle\nddc:530\nddc:600\nIn den letzten Jahren wurde Siliziumkarbid (SiC) ein attraktives Material f\u00fcr die Leistungselektronik und \u00f6ffnete wegen seiner \u00fcberlegenen Materialeigenschaften neue Perspektiven auf diesem Gebiet. Der hohe Bandabstand, die hohe thermische Leitf\u00e4higkeit und\ndie hohe Durchbruchfeldst\u00e4rke machen SiC zum Material der Wahl f\u00fcr Leistungs-MOSFETs.Die Verwendung von SiC MOSFETs in Leistungswandlern erlaubt zum Beispiel eine Verringerung deren Gewicht und Gr\u00f6\u00dfe. Das kann ein gro\u00dfer Vorteil f\u00fcr viele Anwendungen inklusive Elektroautos sein. Die F\u00e4higkeit von SiC Bauelementen, auch bei hohen Temperaturen zu funktionieren, vereinfacht das W\u00e4rmemanagement von elektrischen Systemen. Die kommerzielle Nutzung von SiC MOSFETs ist derzeit jedoch durch technologische Probleme\nbegrenzt, die sich in Form von niedrigen Kanalbeweglichkeiten und hohen Einschaltspannungen manifestieren.\nDer Zweck dieser Doktorarbeit war, durch numerische Simulationen den Mechanismus zu verstehen und zu erkl\u00e4ren, der in SiC MOSFETs die Kanalbeweglichkeit bestimmt und eine selbstkonsistente Simulationsmethodologie zur Beschreibung der elektrischen Eigenschaften von\nSiC MOSFETs zu entwickeln. F\u00fcr den technologischen Fortschritt, sowie f\u00fcr die Entwicklung und Optimierung von Halbleiterbauelementen ist der rechnergest\u00fctzte Entwurf von elektronischen Bauelementen und ihrer Herstellung (TCAD \u2013 Technology Computer Aided\nDesign) zu einem zunehmend wichtigen Untersuchungswerkzeug geworden. TCAD-Simulationen f\u00fcr SiC-Bauelemente sind aktuell jedoch eine gro\u00dfe Herausforderung. Die meisten Simulationsmodelle wurden f\u00fcr Silizium entwickelt und k\u00f6nnen deshalb die Transporteigenschaften von SiC-Bauelementen nicht ad\u00e4quat beschreiben. Dar\u00fcber hinaus ist die Grenzschicht zwischen Siliziumkarbid und Gateoxiden durch eine hohe Konzentration von Haftstellen charakterisiert, die die Kanalbeweglichkeit in SiC-MOSFETs stark degradieren.Deshalb ist ein genaues Modell f\u00fcr die Haftstellen an der Grenzschicht von vorrangiger Bedeutung f\u00fcr die Simulation.\nIm Rahmen des Projekts MobiSiC (Mobility Engineering for SiC Devices) wurden laterale n-Kanal 4H-SiC MOSFETs hergestellt und elektrisch durch Strom-Spannungs-und Halleffektmessungen charakterisiert. Die Effekte von Temperatur und Substratdotierung auf die\nTransporteigenschaften im Kanal von SiC MOSFETs wurden untersucht. Die Interpretation sowohl der Strom-Spannungskennlinen (ID(VG)) als auch der aus den Halleffektmessungen abgeleiteten Schichtladungstr\u00e4gerkonzentrationen und Kanalbeweglichkeiten (ninv(VG),\u03bc(VG)) wurden in dieser Arbeit mit Hilfe numerischer Simulationen mit Sentaurus Device von Synopsys durchgef\u00fchrt.\nZur genauen Analyse der Halleffektmessungen wurde eine neue Methode der Berechnung des Hallfaktors entwickelt. Sie beruht auf der Tatsache, dass sowohl der Hallfaktor als auch die Beweglichkeit von denselben Mechanismen bestimmt werden, durch die die Ladungstr\u00e4ger\ngestreut werden. Die Berechnungsmethode ber\u00fccksichtigt alle Streumechanismen im aktiven Bereich der Bauelemente. Auf diese Weise ist es zum ersten Mal m\u00f6glich, einen genauen Wert f\u00fcr den Hallfaktor f\u00fcr den Kanal von MOSFETs zu berechnen und f\u00fcr die Korrektur der Halleffektmessungen zu verwenden.\nExperimentelle Daten, z. B. von Halleffektmessungen, werden oft zur Charakterisierung der Dichte von Haftstellen an der Grenzschicht verwendet. In dieser Arbeit wird eine neue Methode vorgeschlagen, die eine genauere Charakterisierung erlaubt. In einem ersten Schritt werden Haftstellendichten als Funktion der Energie (DIT(ET)) von Halleffekt- und Kapazit\u00e4ts-Spannungsmessungen auf konventionelle Art extrahiert. Danach werden sie in Sentaurus Device eingegeben und numerisch optimiert um die Abweichungen zwischen den simulierten Kennlinien (ID(VG), ninv(VG) und \u03bc(VG)) und den Messungen zu minimieren. Die numerische Simulation erlaubt, Effekte wie z.B. die Potentialverteilung zwischen Source und Drain sowie die Fermi-Dirac-Verteilung der Elektronen zu ber\u00fccksichtigen. Solche Effekte bleiben bei der konventionellen Extraktion der Haftstellendichte unber\u00fccksichtigt. Es ist deshalb zu erwarten, dass die neue Methode physikalisch schl\u00fcssigere Ergebnisse f\u00fcr Haftstellendichte DIT(ET)liefert.Basierend auf den experimentellen Ergebnissen und den Simulationen werden Ursprung und Natur der Grenzfl\u00e4chenhaftstellen diskutiert.\nDie Simulationsmethodologie, in die die Methode der Berechnung von Hallfaktoren und die Methode der Extraktion der Haftstellendichte\neingingen, konnte konsistent die Temperaturabh\u00e4ngigkeit sowie die Konzentrationsabh\u00e4ngigkeit der Transporteigenschaften der\nin dieser Arbeit betrachteten SiC MOSFETs beschreiben. Auf der Basis der guten \u00dcbereinstimmung zwischen Simulationen und Messungen konnten die Streumechanismen im Kanal von SiC MOSFETs mit unterschiedlichen Dotieratomkonzentrationen und bei unterschiedlichen Temperaturen umfassend interpretiert werden. Eine der Haupterkenntnisse dieser Arbeit ist, dass eine Verringerung der Grenzfl\u00e4chenhaftstellendichte nicht der einzige Faktor zur Verbesserung der Eigenschaften von SiC MOSFETs ist. Ihre Eigenschaften k\u00f6nnen z.B. auch durch eine Verringerung der Kanaldotierung erheblich verbessert werden. Weiterhin wurde gefunden, dass die Konzentration der Kanaldotierung die Temperaturabh\u00e4ngigkeit der Kanalbeweglichkeit beeinflusst: Bei hochdotierten MOSFETs steigt sie mit der Temperatur w\u00e4hrend sie sich bei niedrig dotierten MOSFETs verringert.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6197\nurn:nbn:de:bvb:29-opus4-61975\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-61975\nhttps://opus4.kobv.de/opus4-fau/files/6197/Thesis-UhnevionakViktoryia.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6215\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:000\nccs\nccs:J.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nPrivacy-Preserving Email Forensics\nArmknecht, Frederik\nDewald, Andreas\nComputerforensik\nddc:000\nIn many digital forensic investigations, email data needs to be analyzed. However, this poses a threat to the privacy of the individual whose emails are being examined and in particular becomes a problem if the investigation clashes with privacy laws. This is commonly addressed by allowing the investigator to run keyword searches and to reveal only those emails that contain at least some of the keywords. While this could be realized with standard cryptographic techniques, further requirements are present that call for novel solutions: (i) for investigation-tactical reasons the investigator should be able to keep the search terms secret and (ii) for efficiency reasons no regular interaction should be required between the investigator and the data owner.\nWe close this gap by introducing a novel cryptographic scheme that allows to encrypt entire email boxes before handing them over for investigation. The key feature is that the investigator can non-interactively run keyword searches on the encrypted data and decrypt those emails (and only those) for which a configurable number of matches occurred.\nOur implementation as a plug-in for a standard forensic framework confirms the practical applicability of the approach.\n2015\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6215\nurn:nbn:de:bvb:29-opus4-62152\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62152\nhttps://opus4.kobv.de/opus4-fau/files/6215/report.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6217\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:539\nccs\nccs:A.\npacs\npacs:04.60.Pp\nmsc\nmsc:81Q20\nmsc:83C45\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nOn the relation of canonical and covariant formulations of Loop Quantum Gravity\nZipfel, Antonia\nQuantengravitation\nQuasiklassische N\u00e4herung\nddc:539\nLoop Quantum Gravity (LQG) is a background independent approach towards a quantum theory of gravity that splits into a canonical and a covariant branch the latter of which is also often called spin foam model. The spin foam model can only be derived formally from a constrained BF-theory that is discretized prior to quantization so that the resulting quantum theory is not continuous while canonical LQG rests on a true representation of the continuum canonical commutation relations at the kinematical level. The covariant approach therefore raises a lot of questions, for example, the faith of Dirac observables is unknown to a large extend and it is neither settled whether all constraints are implemented in the spin foam model nor how to take the continuum limit. In contrast, it is possible to even solve the quantum dynamics of deparametrized models coupled to a scalar field within the canonical approach. However, the full physical Hilbert space of canonical LQG can still not be determined satisfactorily even though a non-anomalous quantization of the Hamiltonian constraint is known. It is widely believed although not proven that spin foams could be a useful tool in order to solve this issue. A comparison of the two approaches could therefore give valuable insights for both branches of LQG.\nSemiclassical techniques provide an important tool to check the consistency of a model in the absence of experimental data. In canonical LQG such a limit is realized through so-called complexifier coherent states. These states could for example be used in the context of deparametrized models, e.g. dust models, where the Hamiltonian is not a constraint but defines a true evolution, in order to compare the semiclassical dynamics of the canonical theory with the prediction of a (to-be-defined) spin foam model coupled to dust. Since spin foams should already incorporate the dynamics it is crucial for such a comparison that the coherent states defined in the canonical theory stay (approximately) coherent throughout the evolution. In the first part of the thesis a stability criterion for finite systems is derived an analyzed. Although some important insights can be gained, the formalism developed turns out to be too restrictive to be applied to LQG. Despite some promising hints a further investigation of these issues would go beyond the scope of this thesis.\nInstead it is examined whether spin foam techniques can be directly applied to construct a projector onto the physical Hilbert space of the canonical theory. The fundaments for this ansatz where laid in a seminal work by Kaminski, Kisielowski and Lewandowski [1\u20133] who extended the definition of the (Euclidean) spin foam model to arbitrary boundary graphs. This finally allows to make contact to the canonical formulation whose Hilbert space contains all these graphs. The KKL-model used in the thrid part of the dissertation to build a spin foam operator Z[\u03ba] based on abstract 2-complexes \u03ba that acts on the kinematical Hilbert space H0 of LQG by identifying the spin nets induced on the boundary graph of \u03ba with states in H0. Following a common heuristic argument, a rigging map is then postulated by summing over all possible spin foams \u03ba including a possible weight that is designed such that it does not violate a certain gluing property. In the analysis of the resulting object it is possible to identify a spin foam transfer matrix that allows to generate any finite foam as a finite power of the transfer matrix.\nThat the would-be rigging map has the chance to define a projector is demonstrated in a simplified situation where the class of foams is restricted to a certain type with only one internal vertex and where the weight and Barbero-Immirzi parameter are set to one. The so-obtained sum indeed annihilates the Euclidean constraint whose matrix elements are calculated here as well.\nDespite these certainly encouraging results, the further analysis transpires that the full sum over all \u03ba, as written, does not define a projector into the physical Hilbert space. This statement is independent of the concrete spin foam model and Hamiltonian constraint. However, the transfer matrix potentially contains the necessary ingredient in order to construct a proper rigging map in terms of a modified transfer matrix.\nThere are several hints that the failure of the sum to define a rigging map is caused by the fact that the simplicity constraint used in current spin foam models does not only admit gravitational solutions but includes all Plebanski sectors. A similar effect also causes problems in the asymptotic expansion of the Euclidean 4-simplex amplitude but can be cured by an additional constraint as shown in [4\u20137]. These results are extended to the Lorentzian spin foam model in the last part.\nSchleifenquantengravitation (SQG) ist ein hintergrundunabha\u0308ngiger Zugang zur Quantengravitation und teilt sich in zwei Ansa\u0308tze, den kanonischen und den kovarianten. Letzterer wird auch als Spin- schaummodell bezeichnet. Wa\u0308hrend die kanonische SQG auf einer korrekten Darstellung der kontinuier- lichen kanonischen Vertauschungsrelationen aufbaut, kann das Spinschaummodell nur formal von einer BF-Wirkung, deren Name sich von ihrer Form ableitet, mit zusa\u0308tzlichen Zwangsbedingungen hergeleitet werden. Dabei wird bereits die klassische Theorie diskretisiert, so dass die daraus resultierende Quan- tentheorie im Gegensatz zur kanonischen SQG keine kontinuierlichen Freiheitsgrade aufweist. Des Weiteren ist zum Beispiel die Behandlung von Dirac Observablen im kovarianten Ansatz weitgehend unverstanden und es ist nicht bewiesen, ob alle Zwangsbedingungen tatsa\u0308chlich implementiert sind. In der kanonischen SQG kann hingegen sogar die Quantendynamik im Rahmen deparametrisierter Modelle mit gekoppelten Skalarfeldern gelo\u0308st werden. Jedoch ist der physikalische Hilbertraum der vollen Theorie bisher nur unzureichend bestimmt, obwohl seit La\u0308ngerem eine anomaliefreie Quantisierung der Hamiltonschen Zwangsbedingung bekannt ist. Es wird ha\u0308ufig angenomen, das Spinschaummodell ko\u0308nnte ein nu\u0308tzliches Mittel darstellen, dieses Problem zu lo\u0308sen. Ein Vergleich der kovarianten und kanonischen SQG kann daher wertvolle Erkenntnisse fu\u0308r beide Ansa\u0308tze liefern.\nDa bisher experimentelle Daten fehlen, ist eine semiklassische Analyse gut geeignet, die Konsistenz der Theorien zu pru\u0308fen. Im kanonischen Ansatz wird ein solcher Limes mit Hilfe spezieller koha\u0308renter Zusta\u0308nde definiert. Solche Zusta\u0308nde ko\u0308nnen zum Beispiel dazu benutzt werden die semiklassische Dynamik deparametrisierter Modelle wie z.B. Staubmodelle, in welchen die Hamiltonfunktion eine echte Evolution generiert und keine Zwangsbedingung ist, mit den Vorhersagen eines hypothetische Staub-Spinschaummodells zu vergleichen. Weil Spinschaumamplituden definitionsgema\u0308\u00df die Dynamik implementieren sollten, ist es fu\u0308r einen Vergleich der beiden Ansa\u0308tze wichtig, dass die koha\u0308renten Zusta\u0308nde in der kanonischen Theorie (approximativ) stabil unter der Zeitentwicklung sind. Im ersten Teil der Ar- beit wird ein Stabilita\u0308tskriterium fu\u0308r endlich dimensionale Systeme hergeleitet und untersucht. Obwohl einige wichtige Erkenntnisse gewonnen werden ko\u0308nnen, ist der hier entwickelte Formalismus zu restriktiv um in der SQG angewendet werden zu ko\u0308nnen. Trotz vielversprechender Hinweise kann die obige Problematik hier leider nicht weiterverfolgt werden, da sie zu weit vom Thema der Arbeit abfu\u0308hren wu\u0308rde.\nAnstatt dessen wird im Hauptteil der Dissertation untersucht, ob sich Spinschaumtechniken nutzen lassen, um einen Projektor auf den physikalischen Hilbertraum der kanonischen Theorie zu bauen. Der Grundstein hierfu\u0308r wurde in der wegweisenden Arbeit von Kaminski, Kisielowski und Lewandowski [1\u20133] gelegt, da es das KKL-Modell erlaubt auch Spinscha\u0308ume mit allgemeineren Randgraphen, wie sie in der kanonischen Theorie vorkommen, zu definieren. Indem die auf dem Rand eines 2-Komplexes \u03ba induzierten Spinnetze mit Elementen im kinematischen Hilbertraum H0 der kanonischen Theorie iden- tifiziert werden, kann im dritten Teil ein Spinschaumoperator Z[\u03ba] auf H0 konstruiert werden. Einer gela\u0308ufigen heuristischen Argumentation folgend, wird anschlie\u00dfend eine sogenannte \u2018Riggingabbildung\u2019 postuliert, indem alle mo\u0308glichen Spinschaumoperatoren aufsummiert werden. Die einzelnen Summanden Z[\u03ba] ko\u0308nnen dabei auch unterschiedlich gewichtet werden, um die Summe zu regularisieren, solange das Gewicht nicht eine gewisse Klebeeigenschaft verletzt. In der weiteren Analyse des resultierenden Objekts ist es schlie\u00dflich mo\u0308glich eine spezifische Spinschaumtransfermatrix zu identifizieren, mit der jeder beliebige Spinschaum generiert werden kann.\nDass die postulierte Riggingabbildung tatsa\u0308chlich ein Projektor auf den physikalischen Hilbertraum sein ko\u0308nnte, wird anhand eines vereinfachenden Beispiels gezeigt. Wenn die Klasse der 2-Komplexe auf spezielle Komplexe mit nur einem internen Vertex eingeschra\u0308nkt wird und das Gewicht und der Barbero- Immirzi Parameter gleich eins gewa\u0308hlt werden, dann anihiliert das resultierende Objekt tatsa\u0308chlich die Euklidische Hamiltonsche Zwangsbedingung, deren Matrixelemente im zweiten Teil berechnet werden.\nTrotz dieses ermutigenden Resultats stellt sich im weiteren Verlauf heraus, dass die vollsta\u0308ndige Summe u\u0308ber alle 2-Komplexe keinen Projektor auf den physikalischen Hilbertraum liefert. Diese Tatsache ist unabha\u0308ngig von den gewa\u0308hlten Parametern innerhalb des Spinschaummodells und der konkreten Quantisierung der Hamiltonschen Bedingung. Jedoch ko\u0308nnten diese Probleme eventuell durch eine modifizierte Spinschaumtransfermatrix behoben werden.\nEs gibt mehrere Hinweise, dass das Fehlverhalten der postulierten Riggingabbildung darauf zuru\u0308ckzufu\u0308hren ist, dass die Simplizita\u0308tsbedingungen, die momentan im Spinschaummodell implementiert werden, alle sogenannten Plebanskisektoren mit einschliessen. Ein a\u0308hnliches Problem taucht auch im asymptotischen Limes der Euklidischen 4-Simplexamplitude auf und kann durch eine zusa\u0308tzliche Bedingung vermieden werden, wie in [4\u20137] gezeigt wurde. Dieses Resultat wird im letzten Teil auf das Lorentzsche Modell erweitert.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6217\nurn:nbn:de:bvb:29-opus4-62172\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62172\nhttps://opus4.kobv.de/opus4-fau/files/6217/Phdthesis.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6219\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:629\nccs\nccs:A.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nBildungsmechanismen bei der Herstellung von CIGSe Solarzellenabsorbern aus nanopartikul\u00e4ren Pr\u00e4kursorschichten\nM\u00f6ckel, Stefan\nKupferindiumselenid\nSolarzelle\nNanopartikel\nKristallisation\nPr\u00e4kursor\nddc:629\nDas Ziel der vorliegenden Arbeit war die Entwicklung und Herstellung eines nanopartikul\u00e4ren Pr\u00e4kursors f\u00fcr die Herstellung von CISe Absorberschichten, die Untersuchung der Bildungsmechanismen von CISe aus dieser Art von Pr\u00e4-kursor und der Vergleich mit dem in der Industrie verwendetem SEL Pr\u00e4kursor. Hierf\u00fcr wurden verschiedene Nanopartikel synthetisiert und in Hinblick auf die Anforderungen als Pr\u00e4kursoren optimiert.\nPr\u00e4kursorschichten wurden aus Dispersionen dieser Nanopartikel hergestellt und untersucht. Drei unterschiedliche Pr\u00e4kursortypen wurden hergestellt aus A) Cu-In Nanopartikeln, abgedeckt mit einer thermisch verdampften Se Schicht, B) Cu-In Nanopartikeln und Se Nanopartikeln und C) Cu-In Nanopartikeln, Se Na-nopartikel und Cu-Ga Pulver. Der Schwerpunkt der Charakterisierung in der vorliegenden Arbeit lag auf in-situ Beobachtungen der CISe Phasenbildung mit-tels in-situ XRD und DSC und wurde durch weitere Methoden zur Untersu-chung der Phasenzusammensetzung und Morphologie der Proben erg\u00e4nzt.\nDie Charakterisierungen zeigten, dass die vollst\u00e4ndige Umsetzung der nano-partikul\u00e4ren Pr\u00e4kursoren bei gleichen Prozessparametern schneller abl\u00e4uft als bei herk\u00f6mmlichen SEL Pr\u00e4kursoren. Hierbei ist die Zusammensetzung der Pr\u00e4-kursoren vor Prozessbeginn von entscheidender Bedeutung. Herstellungsbedingt wiesen Typ A Pr\u00e4kursoren bei Prozessbeginn neben Se und verschiedenen Cu-In Phasen auch In2O3 auf. Durch dieses Oxid wurde die Phasenbildung ge-hindert und konnte erst zwischen 290 \u00b0C und 390 \u00b0C ablaufen.\nIm Vergleich dazu l\u00e4uft die Bildung von CISe beim Typ B Pr\u00e4kursor schnel-ler ab. Zwei Gr\u00fcnde k\u00f6nnen daf\u00fcr angef\u00fchrt werden. Einerseits wurde bei diesem Pr\u00e4kursortyp Se als Nanopartikel zugegeben, was die Durchmischung der Materialien verbesserte und das por\u00f6se Netzwerk der Cu-In Nanopartikelschicht nicht von fl\u00fcssigem Se infiltriert werden musste. Andererseits wurde kein Pro-zessschritt durchgef\u00fchrt um einen Dispergator zu entfernen, wodurch sich kein Oxid bildete, das als Diffusionsbarriere zwischen Cu-In und Se fungieren kann. Die Ergebnisse der Untersuchungen am Typ B Pr\u00e4kursor zeigen eine Vollst\u00e4n-dige Umsetzung des Pr\u00e4kursors bis zu einer Temperatur von 315 \u00b0C.\nIn beiden F\u00e4llen l\u00e4uft die Reaktion schneller ab, als bei herk\u00f6mmlichen se-quentiellen Pr\u00e4kursoren. Der Grund daf\u00fcr wird in der Strukturgr\u00f6\u00dfe der nano-partikul\u00e4ren Pr\u00e4kursoren gesehen. Es kann angenommen werden, dass der Me-chanismus, welcher der Reaktion des sequentiellen metallischen Pr\u00e4kursors zu-grunde liegt, derselbe ist und mit der Korrosion von Metallen verglichen werden kann. Die intermetallischen Cu-In Phasen werden in bin\u00e4re Selenide umgesetzt, an deren Grenzfl\u00e4che sich CISe bildet. Aufgrund der um Gr\u00f6\u00dfenordnungen kleineren Strukturen und der homogeneren Durchmischung der nanopartikul\u00e4ren Pr\u00e4kursoren findet die Umsetzung der bin\u00e4ren Selenide deutlich schneller statt. Daher steht bei sequentiellen Pr\u00e4kursoren die Thermodynamik der intermedi\u00e4ren Phasen im Vordergrund, w\u00e4hrend aufgrund der geringen Diffusionswege bei nanopartikul\u00e4ren Pr\u00e4kursoren die Kinetik des Prozesses dominiert. Aufgrund der hohen Diffusionswege in sequentiellen Pr\u00e4kursoren ist eine schnelle Reaktion nur dann m\u00f6glich, wenn auch Phasen vorliegen, die eine hohe Diffusionsrate erlauben, wie Cu2-xSe. Diese Phase ist jedoch nur bei bestimmten thermodyna-mischen Bedingungen stabil.\nDie Bildung grobk\u00f6rnigen CISe Gef\u00fcges ist ebenfalls stark mit den Reakti-onsbedingungen verkn\u00fcpft. F\u00fcr die vorliegende Arbeit wurden Proben unter ver-schiedenen Bedingungen getempert. Die Typ A Pr\u00e4kursoren wurden in einerstabile Se Atmosph\u00e4re um den Pr\u00e4kursor getempert, welche eine Se Schmelze stabilisiert, die Pr\u00e4kursoren des Typs B in einem por\u00f6sen Graphitautoklaven wodurch zwar st\u00f6chiometrisches CISe gebildet wurde, aber dennoch \u00fcber keinen lange Zeitraum eine stabile Se Atmosph\u00e4re gew\u00e4hrleistet werden konnte. Nur bei Typ A Pr\u00e4kursoren wurde eine Bildung von einigen K\u00f6rnern im Bereich von wenigen \u03bcm beobachtet werden. Die Korngr\u00f6\u00dfe aus Typ B Pr\u00e4kursoren liegt im Bereich von einigen 100 nm. In der Literatur wird zwar von dichten Oberfl\u00e4-chen, grobk\u00f6rnigem CISe aus vergleichbaren Pr\u00e4kursoren berichtet [51,52,55,57], jedoch bildet sich in diesen F\u00e4llen immer eine Doppel-schichtstruktur aus mit einem grobk\u00f6rnigen Absorber an der Oberfl\u00e4che und ei-nem feink\u00f6rnigem Absorber an der Grenzfl\u00e4che zum R\u00fcckkontakt. Die feink\u00f6r-nige Morphologie ist vergleichbar mit den Ergebnissen aus den Typ B Pr\u00e4kurso-ren der vorliegenden Arbeit. Es wird daher davon ausgegangen, dass sowohl ei-ne die richtige Phasenzusammensetzung, als auch eine stabile Versorgung mit Se n\u00f6tig sind, um grobk\u00f6rniges CISe aus nanopartikul\u00e4ren Pr\u00e4kursoren herzu-stellen.\nF\u00fcr die Zukunft k\u00f6nnten zwei Strategien zur Bildung eines grobk\u00f6rnigen Ge-f\u00fcges aus nanopartikul\u00e4ren Pr\u00e4kursoren verfolgt werden:\n- Die Prozessierung von homogen durchmischten Pr\u00e4kursoren des Typs B bei Bedingungen, welche die Bildung einer fl\u00fcssigen Se Phase gestatten, um \u00e4hnlich dem \u201eFl\u00fcssigphasensintern\u201c eine hohe Diffusion zu erreichen.\n- Die Herstellung von Pr\u00e4kursoren aus Cu2-xSe und In-Se Nanopartikeln die in einer stabilen Se Atmosph\u00e4re getempert werden. Hierbei m\u00fcssen die Prozessparameter so gew\u00e4hlt werden, dass keine Se-reicheren Cu-Se Phasen gebildet werden k\u00f6nnen.\nThe objective of the present work CISe was the development and formation of a nanoparticulate Precursor for CISe absorber layers. The Phase formation using nanoparticulate precursors was studied and compared to state-of-the-art SEL industrial process.. Different types of nanoparticles were synthesised and opti-mised for their use as precursors for this purpose.\nThree different types of precursor layers were deposited from dispersions of these nanoparticles. A) Cu-In nanoparticles covered with thermally evaporated Se, B) Cu-In nanoparticles mixed with Se nanoparticles and C) Cu-In nanoparti-cles mixed with Se nanoparticles and Cu-Ga powder. In-situ characterisation of the CISe phase formation using in-situ XRD and DSC was focused in this work complemented by other characterisation methods for phase composition and morphology.\nThe result show, that a complete consumption of the precursor and conversion to CISe takes place much faster than reported for state-of-the-art SEL precursors using the same process parameters. The Composition if the precursor is essen-tial. Due to their preparation type A precursors consist beneath Se and Cu-In phases also In2O3. This oxide acts as a diffusive barrier and shifts the formation reaction to temperatures between 290 \u00b0C and 390\u00b0C.\nCompared to type A precursors the formation of type B precursors stars earli-er, which is caused by two reasons. On the one hand type B precursors consist of Se nanoparticles mixed homogeneously with the Cu-In nanoparticles improving the intermixing of the elements. The porous network of the Cu-In nanoparticles did not have to be infiltrated by liquid selenium. On the other hand no dispersing agents were removed by temperature from these precursors so no oxide, acting a diffusive barrier, was formed before the annealing. The result of the type B pre-cursor characterisation shows a complete transformation below 315 \u00b0C.\nIn both cases the reaction takes place faster compared to state-of-the-art SEL precursors. The reason therefore is meant to be the structure size of the nanopar-ticulate precursor. The basic formation mechanism of CISe is most probable the same for both the SEL precursor and the nanoparticulate precursor and can be compared to the corrosion of metals. The intermetallic phases are transformed to binary selenides. At the interface of these selenides CISe if formed. Due to the small structures of nanoparticulate precursors the formation of CISe takes place much faster. Therefore one can say, that for SEL precursors the thermodynamic of intermediate phases is the dominant factor, while, due to the short diffusions paths, for nanoparticulate precursors is kinetics. As the diffusion paths in SEL precursors are comparably long a fast reaction can only take place in the pres-ence of phases allowing a high diffusion of elements like Cu2-xSe. This phase is just stable under certain thermodynamic conditions.\nThe formation of coarse grain is also closely connected to the reaction envi-ronment. For the present work samples were annealed under various conditions. Type A precursors were annealed in a stable Se atmosphere, stabilising a Se melt, while Type B precursors were annealed in a porous graphite autoclave where the formation of stoichiometric CISe was possible, but the stability of the atmosphere could be ensured just over a short time period. Only precursors of the type A showed some grains in the range of few \u03bcm. For annealed type B precursors grain sizes in the range of few 100 nm could be observed. Literature also reports closed surfaces of coarse grained CISe [51,52,55,57], however, in those cases always a double layer is forming consisting of coarse grained CISe on top and fine grained CISe on the absorber/back contact interface. The mor-phology of the fine grained layer is comparable to the morphology of anneald type B precursors in the present work. Since it is assumed that the correct phase composition of the precursors as well as a stable Se environment are crucial for the growth of coarse grained CISe.\nFor the future two strategies for the formation of coarse grained CISe are rec-ommended:\n- The processing of homogeneously mixed type B precursors in conditions stabilising a liquid Se phase. This phase could improve the diffusion be-tween the nanoparticles, compared to liquid phase sintering, thus improv-ing grain gwoth.\n- The formation of precursors consisting of Cu2-xSe and In-Se nanoparticles annealed in a stable Se atmosphere. Here the annealing conditions have to be chosen in that way, that Cu-Se with more Se content than Cu2-xSe can-not be formed.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6219\nurn:nbn:de:bvb:29-opus4-62197\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62197\nhttps://opus4.kobv.de/opus4-fau/files/6219/Dissertationsschrift%20M%C3%B6ckel%202.3.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nd/3.0/de/\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6246\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:005\nccs\nccs:D.1.0\nccs:D.2.13\nccs:D.4.7\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_ohneAngabe\nTailorable System Software\nMa\u00dfschneiderbare Systemsoftware\nLohmann, Daniel\nBetriebssystem\nMa\u00dfschneidern\nddc:005\nSystem software, such as the operating system, provides no business value of its own. Its sole purpose is to serve the concrete application's needs -- that is, to map the functional and nonfunctional requirements efficiently to the functional and nonfunctional properties of the hardware.\nEfficiency calls for specific, tailored system software; reusability demands generic solutions. To overcome this dilemma, most system software provides built-in static variability: It can be tailored at compile time with respect to a specific application\u2013hardware use case.\nIn the case of Linux v3.2, this static variability is reflected by nearly 12000 configurable features that control the inclusion and exclusion of 28000 source files with 84000 conditional (#ifdef) blocks.\nVariability by means of thousands of features imposes challenges for both system-software developers, who have to implement and maintain variability, as well as application developers/administrators, who have to understand the impact of all these features in order to configure a tailored variant.\nOver the last four years, my research has focused on methods and techniques to improve the design, implementation, and maintenance of static variability in highly tailorable system software. My central contributions in this respect are:\n(a) The CiAO approach, which employs language techniques to achieve excellent up-tailorability of embedded system software (towards the requirements of a specific application).\n(b) The Sloth approach, which employs generative techniques to achieve down-tailorability of embedded system software (towards better exploitation of modern commodity hardware).\n(c) The VAMOS approach, which employs cross-language analysis techniques and holistic variability modeling to improve on the long-term maintainability of multi-paradigmatic variability implementations in existing large-scale system software, such as Linux.\nThis research has been carried out in collaboration with seven doctoral researchers and master students from my research group, four of which have already defended.\n2014\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6246\nurn:nbn:de:bvb:29-opus4-62464\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62464\nhttps://opus4.kobv.de/opus4-fau/files/6246/habil.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6272\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nccs\nccs:H.4.3\ninstitutes\ninstitutes:Wiwi\nSocial Media Einsatz in projektorientierten Organisationen - Eine Fallstudie zur Identifikation und Analyse der Erfolgsfaktoren in einem deutschen Gro\u00dfunternehmen\nFrank, Sophia\nSoziale Software\nComputerunterst\u00fctzte Kommunikation\nProjektmanagement\nChange Management\nFallstudie\nErfolgsfaktor\nddc:000\nDurch zunehmende Wissensintensit\u00e4t und Komplexit\u00e4t von Projekten, eine Verbreitung von Social Media in Organisationen und eine ver\u00e4nderte Erwartungshaltung von Mitarbeitern, Partnern oder Kunden, wird der Einsatz neuer Arbeitsweisen im Projektmanagement immer wichtiger. In dieser Arbeit wird anhand einer Fallstudie in einem deutschen Gro\u00dfunternehmen untersucht, welche Rolle Social Media f\u00fcr Projektmanager spielen, wie sie genutzt werden und welche Erfolgsfaktoren f\u00fcr einen Social Media Einsatz in projektorientierten Organisationen bestehen. Projekte werden dabei als soziotechnische Systeme betrachtet, in denen das soziale und das technische System gemeinsam optimiert werden. Der gew\u00e4hlte Ansatz kombiniert qualitative und quantitative Forschungsmethoden. Die Ergebnisse beinhalten die Entwicklung und Bewertung von 14 projektspezifischen Anwendungsszenarien und acht Erfolgsfaktoren f\u00fcr einen Social Media Einsatz in projektorientierten Organisationen. Diese werden mit Handlungsempfehlungen f\u00fcr Unternehmen, Project Management Offices und Projektmanager erg\u00e4nzt.\nToday, projects are becoming more and more skill-intensive and complex while companies are adopting Social Media. Employees, partners and customers have developed new ways of working. That is why the use of new communication media in project management becomes more and more important. This thesis presents a case study in a big German enterprise. The author analyzes, which role Social Media play for project managers, how they use Social Media and which success factors exist for the use of Social Media in project oriented organizations. Projects are viewed as socio-technical systems, where the social and the technical system are optimized together. The chosen approach combines qualitative and quantitative research methods: The results contain the development and evaluation of 14 project specific use cases as well as eight success factors for the use of Social Media in project oriented organizations with recommended actions for companies, Project Management Offices and project managers.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6272\nurn:nbn:de:bvb:29-opus4-62723\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62723\nhttps://opus4.kobv.de/opus4-fau/files/6272/dissertationsophiafrank.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-sa/3.0/de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6275\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:536\nddc:608\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nPhysical assessment of a novel concept for computed tomography with a stationary source ring of fixed X-ray anodes for medical imaging\nKellermeier, Markus\nCT physics Fact Fixed anode CT Thermal load capacity X-ray anode\nddc:536\nddc:608\nBackground and Aims:\nThe potential performance of current technology for computed tomography (CT) is basically limited by the thermal load capacity of the X-ray anode. In addition, the gantry rotation time is limited, because the centrifugal force can become destructive. Focusing primarily on the thermal load capacity, this work intends to show a novel CT with fixed anodes (Fixed Anodes Computed Tomography: FACT), which has basically no mechanical constraints. For comparison, a common clinical CT was used as Reference CT.\nMaterial and Methods:\nMonte Carlo simulations, with the free software combination GAMOS/GEANT4, were used to determine the energy deposition of accelerated electrons in the so-called thermal focal spot of an X-ray anode. The thermal energy thus obtained was used to determine the temperature distribution, using the finite element method, where the commercial software COMSOL Multiphysics was used. Based on the typical sizes of the anode, a basic simulation model has been developed for fixed anodes, which comprises a thin tungsten layer embedded in a copper block, and is transmitted for comparison to a model for a rotating anode. The operating temperature of the X-ray focal spot in the tungsten target of the anode was set at a maximum permissible temperature of 2500 K. For cooling, a heat bath at 300 K has been defined at the distal copper block end. Simulations for the fixed anode model were performed under the assumption of a short thermal load till up to the beginning of long thermal load (1 \u03bcs \u2013 1 s) at the respective performances of the maximum thermal focal spots. Subsequently, numerous parameters were changed for carrying out the studies of the thermal model of the fixed anode in order to investigate the validity of modelling. Long-term exposure of a single fixed anode was examined for continuous pulse operation at any FACT. A virtual model for FACT has been created in order to analyse different temporal sequences in the X-ray source ring, forming a circular array of 1160 single fixed anodes. Under the assumption of known detector characteristics, which have been transferred to the much lower integration times at FACT, the image quality was determined using the CT image reconstruction library ROTLib at the Institute of Medical Physics in Erlangen, Germany. With the definition of a Relative Performance (RP ), the relation to the Reference CT was made.\nResults:\nSimulations for thermal focal spot power density (P_th/A) of anodes were in good agreement with literature results and vendor data with regard to their absolute values and trends. Studies on continuous pulse operation showed that with fixed anodes, the FACT can be operated, in principle, in an unlimited manner. The virtual model for FACT showed that around 60 gantry rotations per second are required to achieve the reference CT, according to the X-ray power (at RP = 1). Un- der this condition, an equivalent image quality in FACT can be obtained by superposition. The optimal duration of each projection direction was 10 \u03bcs. With a beam pause of 1 \u03bcs between the projections 78.4 rounds per second at successive X-ray source activities were possible, resulting in an RP of 1.3 at the same focal spot size, in comparison with the Reference CT.\nConclusions:\nAs a stationary system with fixed anodes, FACT has no focal spot blurring of the X-ray source during projection. With an RP > 1, a shorter scan time can be achieved, while maintaining radiation exposure and image quality. Based on the high number of rounds at each low dose, we can conclude that FACT can support a high image frame rate and very thin slices, which could be of significant advantage in a wide range of medical diagnostic as well as technical applications.\nHintergrund und Ziele:\nDie potentiale Leistungsf\u00e4higkeit eines aktuellen Computertomographen (CT) ist im Wesentlichen durch die thermische Belastbarkeit der R\u00f6ntgenanode begrenzt. Zus\u00e4tzlich ist die Gantry-Umlaufzeit limitiert, da mit wachsender Zentrifugalkraft eine destruktive Wirkung einhergeht. Mit Blick auf die thermische Belastbarkeit ist in dieser Arbeit ein neuartiges CT mit Festanoden (engl.: Fixed Anodes Computed Tomography: FACT) vorgestellt worden, was vom grundlegenden Konzept aus keine mechanische Einschr\u00e4nkung aufweist. Zum Vergleich wurde ein g\u00e4ngiges klinisches CT als Referenz-CT herangezogen.\nMaterial und Methoden:\nMithilfe der freien Softwarekombination GAMOS/GEANT4 wurden Monte-Carlo-Simulationen durchgef\u00fchrt, um die Energiedeposition von beschleunigten Elektronen in dem sogenannten thermischen Brennfleck einer R\u00f6ntgenanode zu bestimmen. Die so erhaltene thermische Energie diente zur Ermittlung der Temperaturverteilung unter Anwendung der Finite\u2013Elemente-Methode, wobei die kommerzielle Software COMSOL Multiphysics zum Einsatz kam.\nBasierend auf typische Abmessungen f\u00fcr Anoden wurde ein grundlegendes Simulationsmodell f\u00fcr Festanoden, die aus einer d\u00fcnnen Wolframschicht, eingebettet in einen Kupferblock, aufgebaut sind, entwickelt und zum Vergleich auf ein Modell zur Drehanode \u00fcbertragen. Die Betriebstemperatur des R\u00f6ntgenanodenbrennflecks im Wolframtarget wurde auf eine maximal zul\u00e4ssige Temperatur von 2500 K festgelegt. Zur K\u00fchlung wurde ein W\u00e4rmebad mit 300 K am entfernten Kupferblockende definiert. Simulationen zum Festanodenmodell wurden unter der Annahme einer kurzen bis in den Beginn einer langen thermischen Belastung (1 \u03bcs \u2013 1 s) bei der jeweiligen maximalen thermischen Brennfleckleistung durchgef\u00fchrt. Im Anschluss wurden zahlreiche Parameter zum thermischen Studienmodell der Festanode ver\u00e4ndert, um die Modellierung auf seine G\u00fcltigkeit untersuchen zu k\u00f6nnen. Die Langzeitbelastung einer einzelnen Festanode wurde hinsichtlich eines kontinuierlichen Impulsbetriebs bei einem FACT untersucht. Es wurde ein virtuelles Modell zur FACT erstellt, um verschiedene zeitliche Abl\u00e4ufe im R\u00f6ntgenquellring, der eine kreisf\u00f6rmige Anordnung von 1160 einzelnen Festanoden bildet, analysieren zu k\u00f6nnen. Unter der Annahme von bekannten Detektoreigenschaften, die auf die deutlich kleineren Integrationszeiten beim FACT \u00fcbertragen worden sind, wurde die Bildqualit\u00e4t mithilfe der CT-Bildrekonstruktionsbibliothek ROTLib aus dem Institut f\u00fcr Medizinische Physik in Erlangen, Deutschland, ermittelt. Mit der Definition einer Relative Performance (RP) wurde ein Bezug zum Referenz-CT hergestellt.\nErgebnisse:\nSimulationsstudien mit den definierten Anodenmodellen zur thermischen Brennfleck-Leistungs- dichte (P_th/A) waren in guter \u00dcbereinstimmung in den absoluten Werten sowie deren Trends zu Literatur- und Hersteller-Angaben. Studien zum kontinuierlichen Impulsbetriebs zeigten, dass mit Festanoden das FACT prinzipiell unbegrenzt betrieben werden kann. Aus dem virtuellen Modell zum FACT ging hervor, dass rund 60 Gantry-Uml\u00e4ufe pro Sekunde erforderlich sind, um die R\u00f6ntgenleistung (bei RP = 1) entsprechend dem Referenz-CT zu erreichen. Unter dieser Bedingung konnte eine \u00e4quivalente Bildqualit\u00e4t beim FACT durch Superposition erzielt werden. Die optimale Projektionsdauer aus jeder Projektionsrichtung lag bei 10 \u03bcs. Mit einer Strahl-Pause von 1 \u03bcs zwischen den Projektionen waren 78,4 Uml\u00e4ufe pro Sekunde bei aufeinander folgenden R\u00f6ntgenquellenaktivit\u00e4ten m\u00f6glich, was in einer RP von 1,3 resultierte bei gleicher Brennfleckabmessung im Vergleich zum Referenz-CT.\nSchlussfolgerungen:\nAls station\u00e4res System mit Festanoden zeigt FACT keine Brennfleck-Verschmierung w\u00e4hrend der Projektion. Mit einer RP >1 kann eine k\u00fcrzere Aufnahmezeit erreicht werden, bei gew\u00f6hnlicher Strahlenexposition und Bildqualit\u00e4t. Basierend auf der hohen Anzahl von Uml\u00e4ufen bei jeweils niedriger Dosis kann mit einem FACT eine hohe Bildwiederholungsrate und sehr d\u00fcnne Bildschichten erzielt werden, was f\u00fcr ein breites Spektrum von medizinisch-diagnostischen sowie technischen Anwendungen einen deutlichen Fortschritt darstellen k\u00f6nnte.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6275\nurn:nbn:de:bvb:29-opus4-62751\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62751\nhttps://opus4.kobv.de/opus4-fau/files/6275/dissertation_200dpi.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6288\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:006\npacs\npacs:31.15.es\nmsc\nmsc:35-02\nmsc:65-02\ninstitutes\ninstitutes:Tech_Informatik\nMultilevel Adaptive Techniques for Higher-Order Finite Differences in Elliptic Problems with Interfaces\nAdaptive Multilevel-Techniken f\u00fcr Finite Differenzen h\u00f6herer Ordnung in Elliptischen Problemen mit Grenzfl\u00e4chen\nRitter, Daniel\nmultigrid\nadaptive\nmolecular dynamics\nddc:006\nHermitian methods, also known as Mehrstellenverfahren (MSV), are a class of finite-difference (FD)discretization schemes for partial differential equations (PDEs) that have fourth\nor higher error order and lead to compact stencil operators. Their high order is achieved by forcing the fulfillment of the governing PDE also at neighboring locations of the current grid point, i.e., by averaging the right-hand side of the PDE.\nMultigrid (MG) methods are efficient, iterative solvers for sparse linear systems, such as those resulting from a PDE that is discretized with an MSV. Based on the combination of\nlocal smoothing methods, e.g. Gauss-Seidel iterations, with a coarse-grid error correction, MG methods achieve efficient error elimination over all frequencies.\nThe focus of this thesis is on the derivation of MSV schemes and their application to elliptic PDEs. Out of the various fields where these PDEs occur, examples from quantum electro-chemistry are chosen. Here, interfaces are of special interest: These can be found in the model equation, leading to a transition in the conductivity coefficient of the PDE that can be either modeled by a smooth variation or by a jump, as well as in the discretization, if hierarchical refined grids are coupled. For both cases, the combination of MSV schemes with MG solvers leads to an efficient and precise solution of the PDE.\nFor interfaces in the model equation, different MSV schemes are developed for both the smooth and the discontinuous coefficient, and experiments are performed to compare these approaches. A systematic stencil generation algorithm is introduced, based on Taylor expansions and least-squares optimization. This algorithm can produce both variants.\nBesides a comparison of the MSV scheme with linear finite elements, enhanced by Tau-extrapolation, the smooth and discontinuous schemes are compared before different variants of MSVs are integrated into the RSDFT software. Within that framework they are applied for the solution of the potential equation and for the calculation of electron-electron interactions. The resulting systems of equations are solved by MG, in particular, by applying V-cycles. Convergence properties, run times of different solver configurations, and errors in the potential and energy terms are analyzed for different MSV approaches.\nA staggered coefficient MSV variant that is very efficient to establish competes with the aligned coefficient MSV in all means except the error in the potential.\nFor interfaces between grids, the fast adaptive composite (FAC) grid approach is used to simulate open boundary conditions in the potential equation. Hence, grids of different mesh widths are coupled, i.e., coarser grids expand the original domain. Error analysis shows that the\noriginal FAC coupling does not preserve the error order of the discretization method, if the latter is not adapted at the interfaces between two grids. Therefore, a slight change in the algorithm is made, introducing higher-order interpolation as the prolongation operator.\nThis approach can restore the original error order, demonstrated for a seven-point FD as well as\nfor the corresponding MSV discretization.\nHermitesche Methoden bzw. Mehrstellenverfahren (MSV) bezeichnen eine Klasse von Diskretisierungen f\u00fcr partielle Differentialgleichungen (PDEs) mittels finiter Differenzen (FD), die vierte oder h\u00f6here Fehlerordnung besitzen und zu kompakten Stencil-Operatoren f\u00fchren. Die hohe Ordnung wird dadurch erreicht, dass die Erf\u00fcllung der PDE auch an benachbarten Gitterpunkten erzwungen wird, indem die rechte Seite der PDE gemittelt wird. Mehrgittermethoden (MG) z\u00e4hlen zu den effizientesten iterativen L\u00f6sungsverfahren f\u00fcr\nschwachbesetzte lineare Systeme, wie sie bei der Diskretisierung einer PDE mit MSV entstehen. Durch die Kombination eines lokalen Gl\u00e4tters, wie z.B. der Gau\u00df-Seidel-Iteration, mit der Fehlerkorrektur auf einem gr\u00f6beren Gitter k\u00f6nnen MG Fehler \u00fcber alle Frequenzen eliminieren.\nIm Mittelpunkt dieser Arbeit steht die Herleitung von MSV-Stencils sowie deren Anwendung auf elliptische PDEs. Es werden Beispiele aus der Quanten-Elektrochemie betrachtet, einer von vielen Disziplinen, in denen diese Differentialgleichungen Verwendung finden.\nGrenzfl\u00e4chen, bzw. Interfaces, sind hier von herausragender Bedeutung: Sie k\u00f6nnen sowohl in den Modellgleichungen selbst auftreten, wobei sie zu einem variablen Koeffizienten f\u00fchren,\nder entweder durch einen glatte Ver\u00e4nderung oder einen Sprung modelliert werden kann, als auch in der Diskretisierung zwischen gekoppelten, hierarchisch verfeinerten Gittern vorkommen. In beiden F\u00e4llen k\u00f6nnen MSV mit MG kombiniert werden, um PDEs effizient und pr\u00e4zise\nzu l\u00f6sen.\nF\u00fcr Grenzfl\u00e4chen in der Modellgleichung werden MSV sowohl f\u00fcr glatte als auch f\u00fcr springende Koeffizienten entwickelt und anschlie\u00dfend experimentell miteinander verglichen.\nBasierend auf Taylor-Entwicklungen und der Methode der kleinsten Quadrate wird ein Algorithmus zur systematischen Synthese von MSV-Stencils vorgestellt, der f\u00fcr beide Varianten verwendet werden kann.\nDas Fehlerverhalten des MSV wird mit dem der Tau-Extrapolation verglichen. Dar\u00fcber hinaus werden die Schemata f\u00fcr glatte und springenden Koeffizienten einander gegen\u00fcber gestellt, bevor die Integration verschiedener Varianten von MSV in die Software RSDFT beschrieben wird.\nHier werden die Diskretisierungen f\u00fcr die L\u00f6sung der Potentialgleichung und somit f\u00fcr die Berechnung von Elektron-Elektron-Wechselwirkungen verwendet. Die resultierenden linearen Gleichungssysteme werden dann mit einem MG gel\u00f6st, indem V-Zyklen ausgef\u00fchrt werden. F\u00fcr die verschiedenen MSV werden Konvergenzraten, Laufzeiten f\u00fcr verschiedene Konfigurationen des L\u00f6sers sowie die Fehler im Potential und der Energie miteinander verglichen. Es zeigt sich, dass sich ein effizient zu berechnendes MSV mit gestaffelten (staggered) Koeffizienten sich, bis auf den Fehler des Potentials, genauso verh\u00e4lt wie die aufw\u00e4ndigere Variante, in der die Koeffizienten auf einem ausgerichteten (aligned) Gitter diskretisiert sind.\nF\u00fcr Grenzfl\u00e4chen bei der Kopplung von Gittern wird der sogenannte Fast Adaptive Composite (FAC) Ansatz verwendet, um offene Randbedingungen in der Potentialgleichung zu simulieren. Daf\u00fcr w\u00e4hlt man eine Gitterhierarchie, in der die gr\u00f6beren Gitter das Simulationsgebiet vergr\u00f6\u00dfern, und koppelt diese miteinander. Die Fehleranalyse zeigt, dass die Fehlerordnung einer Diskretisierung mit dem urspr\u00fcnglichen FAC ohne eine Anpassung der Diskretisierungsoperatoren an den Grenzfl\u00e4chen zwischen zwei Gittern nicht erhalten werden kann. Jedoch kann unter Verwendung von Interpolationsverfahren h\u00f6herer Ordnung zur Prolongation die\nurspr\u00fcngliche Fehlerordnung wiederhergestellt werden. Dies wird in Tests sowohl f\u00fcr den Sieben-Punkt-Stern als auch f\u00fcr das entsprechende MSV gezeigt.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6288\nurn:nbn:de:bvb:29-opus4-62886\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-62886\nhttps://opus4.kobv.de/opus4-fau/files/6288/DissDanielRitter.pdf\neng\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6302\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:A.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nW\u00e4hrend der Lagerung freigesetzte und intrazellul\u00e4r verbliebene Zytokine TGF\u03b21 und sCD40L in Thrombozytapheresekonzentraten von den Zellseparatoren Trima und Amicus\nVajko, Melinda\nWachstumsfaktor\nThrombozyt\nTransforming Growth Factor beta 1\nApherese\nddc:617\nHintergrund und Ziele:\nThrombozytenkonzentrate werden heute am h\u00e4ufigsten zur Prophylaxe oder Therapie von Blutungen bei Patienten mit hyporegenerativen Thrombozyto\u00acpenien w\u00e4hrend der Be\u00achand\u00aclung maligner Neoplasien eingesetzt. Neben anderen Inhaltsstoffen werden Zytokine, die sich in thrombozyt\u00e4ren \u03b1-Granula befinden, w\u00e4hrend der Lagerung von Thrombozytenkonzentraten freigesetzt. Dazu geh\u00f6ren auch mehrere Zytokine, die explizit als angiogen und onkogen gelten, unter anderem TGF(transforming growth factor)\u03b21 und der l\u00f6sliche CD40-Ligand (sCD40L). Die Freisetzung dieser Zytokine k\u00f6nnte einen \u00dcberlebensnachteil f\u00fcr Patienten zur Folge haben. Die vorliegende Arbeit untersucht, wie viel TGF\u03b21 und sCD40L w\u00e4hrend der Lagerungszeit von Thrombozytapheresekonzentraten von zwei Zellseparatoren in den plasmatischen Anteil freigesetzt wird, welcher intrazellul\u00e4re Verlust mit der Freisetzung einhergeht und ob eine Neusynthese dieser Proteine stattfindet.\nMaterial und Methoden:\n32 Thrombozytenapheresekonzentrate von freiwilligen Spendern wurden im Rahmen der Studie zur einen H\u00e4lfte am Zellseparator Terumo Trima Accel, zur ande\u00acren H\u00e4lfte am Zellseparator Fenwal Amicus gewonnen. Den Throm\u00acbo\u00ac\u00aczytapheresekonzentraten wurden anschlie\u00dfend Proben entnom\u00acmen, aus denen einerseits CTAD-Plasma, andererseits mittels Zugabe des Detergens Triton X-100 Lysat hergestellt wurde. Sowohl im zellfreien Plasma\u00fcberstand der Thrombozytapheresekonzentrate als auch im Lysat wurden TGF\u03b21 und sCD40L mittels ELISA-Technik untersucht. Die Messungen erfolgten am Entnahmetag sowie an den darauf folgenden Tagen +1, +3 sowie +5.\nErgebnisse:\nBedingt durch die h\u00f6here Thrombozytenkonzentration in den Konzentraten vom Zellseparator Amicus waren die Konzentrationen von TGF\u03b21 und sCD40L in den Lysaten dieser Konzentrate h\u00f6her als in denen vom Zellseparator Trima Accel. Bezogen auf 105 Thrombozyten bestanden keine Unterschiede. W\u00e4hrend der Lagerung nimmt der sCD40L-Gehalt in den Lysaten signifikant um ca. ein Drittel ab, w\u00e4hrend der Gehalt des TGF\u03b21 weitgehend konstant bleibt. In den \u00dcberst\u00e4nden steigen die Konzentrationen beider Zytokine w\u00e4hrend der Lagerung an, wobei der \u00fcberwiegende Anteil beider Zytokine bis zum Tag +5 intrazellul\u00e4r verbleibt.\nSchlussfolgerungen:\nDie zwei in dieser Studie eingesetzten Zellseparatoren Trima Accel und Amicus weisen Unterschiede in ihrem Funktionsprinzip auf, was be\u00ackann\u00acter\u00acma\u00dfen zu einer etwas st\u00e4rkeren Thrombozytenaktivierung in Pr\u00e4paraten vom Zellseparator Amicus f\u00fchrt. Dieser Effekt beeinflusst die nachweisbaren Konzentrationen der Zytokine TGF\u03b21 und sCD40L in den Lysaten und den \u00dcberst\u00e4nden jedoch kaum. Im Wesentlichen sind die Unterschiede auf die unterschiedliche Konzentration der Thrombozyten in den Pr\u00e4paraten zur\u00fcckzuf\u00fchren. Die Messungen zeigten ein Ausbleiben signifikanter Ver\u00e4nderungen der intrazellul\u00e4ren TGF\u03b21-Menge w\u00e4hrend der f\u00fcnft\u00e4gigen Lagerung. Dies kann als Hinweis auf eine kontinuierliche Nach\u00acsynthese w\u00e4hrend den Tagen der Lagerung gewertet werden. Dagegen nimmt der intrazellul\u00e4re Gehalt an sCD40L innerhalb der f\u00fcnft\u00e4gigen Lagerung um etwa ein Drittel ab. Inwieweit diese Befunde tats\u00e4chlich klinische Signifikanz f\u00fcr die Empf\u00e4nger von Thrombozytapherese\u00ackonzentraten haben, kann man derzeit nicht beantworten.\nBackground\nPlatelet concentrates are most commonly used for prophylaxis or treatment of bleeding in patients with hyporegenerative thrombocytopenia due to leukemia, lymphoma or other malignant neoplasms. In addition to other ingredients of platelet \u03b1-granules, cytokines are released during storage of platelet concentrates. This includes several cytokines, which are explicitly considered as angiogenic and oncogenic, including TGF-(transforming growth factor-)\u03b21 and soluble CD40 ligand (sCD40L). The release of these cytokines could cause a survival disadvantage for recipients of platelet concentrates. The present study examined how much of TGF\u03b21 and sCD40L is released during the storage period of plateletapheresis concentrates from two different cell separators. In addition the measurements elucidate whether the release of these cytokines is accompanied by an intracellular loss or is compensated by de novo synthesis of these proteins.\nStudy Design and Methods\n32 plateletapheresis concentrates were obtained from volunteer donors to one half by cell separator Terumo Trima Accel and to one half by the Fenwal Amicus cell separator. From samples of these plateletapheresis concentrates, CTAD plasma was obtained by centrifugation. In addition, lysates were prepared by means of addition of the detergent Triton X-100. The cell-free supernatants as well as the lysates were examined for their content of TGF\u03b21 and sCD40L by an ELISA technique. The measurements were made on the day of collection and on the days +1, +3 and +5 thereafter.\nResults\nDue to the higher platelet concentration in concentrates from the Amicus cell separator, the concentrations of TGF\u03b21 and sCD40L in the lysates of these concentrates were higher than in those from the Trima Accel cell separator. In relation to 105 platelets there were no differences. During storage, the sCD40L-content increases in the lysates significantly by about one third, while the content of TGF\u03b21 remains largely constant. In the supernatants, the concentrations of both cytokines increase during storage. Nevertheless, the majority of both cytokines remains intracellularly until day +5.\nConclusions\nThe two cell separators Trima Accel and Amicus, which were used in this study, show differences in their operating principle, which leads \u2013 as is known - to a slightly higher platelet activation in preparations from the cell separator Amicus. This effect, however, hardly affected the detectable concentrations of the cytokines TGF\u03b21 and sCD40L in the lysates and supernatants. Essentially, the measured differences are due to the different concentration of platelets in the concentrates from the two different cell separators. The measurements did not show any significant changes in the intracellular TGF\u03b21 concentrations during the five-day storage. This could indicate a continuous synthesis of TGF\u03b21 by stored platelets. In contrast, the intracellular content of sCD40L decreases within the five-day storage by about one third. To what extent these findings actually are of any clinical importance for the recipient of platelet concentrates, can not yet be answered.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6302\nurn:nbn:de:bvb:29-opus4-63025\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63025\nhttps://opus4.kobv.de/opus4-fau/files/6302/Vajko_Dissertation.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6334\n2018-01-26\ndoc-type:conferenceObject\nbibliography:false\nddc\nddc:000\nccs\nccs:H.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nA multiple case study of small free software businesses associal entrepreneurships\nBarcomb, Ann\nopen source\nsocial entrepreneurship\nddc:000\nFree/libre and open source software are frequently described as a single community or movement. The difference between free software and open source ideology may influence founders, resulting in different types of companies being created. Specifically, the relationship between free/libre software ideology and social entrepreneurships is investigated. This paper presents seven case studies of businesses, five of which were founded by people who identify with the free/libre software movement. The result is a theory that small businesses founded by free/libre software advocates have three characteristics of social entrepreneurships. First, social benefit is prioritized over wealth creation. Second, the business\u2019s social mission is not incidental but is furthered through its for-profit activities, rather than supported by the company\u2019s profits. Third, the company\u2019s success is defined in part by the success of its social mission Free/libre software entrepreneurs who recognize their activities as social entrepreneurships can benefit from the existing literature on the unique challenges faced by socially-oriented businesses.\n2015\nconferenceobject\ndoc-type:conferenceObject\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6334\nurn:nbn:de:bvb:29-opus4-63344\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63344\nhttps://opus4.kobv.de/opus4-fau/files/6334/barcomb-2015-multiple.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6348\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:92-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nGelenkfunktion nach Bikondyl\u00e4rer Knie-Endoprothese: Prospektiv vergleichende Studie bei Patienten mit Gonarthrose und Rheumatoider Arthritis\nStolle, Jeska\nKniegelenksendoprothese\nrheumatoide Arthritis\nGonarthrose\nddc:617\n1.1 Hintergrund\nM\u00f6gliche Unterschiede zwischen Patienten mit Rheumatoider Arthritis (RA) und Gonarthrose (OA) bez\u00fcglich der rein subjektiven Einsch\u00e4tzung der Gelenkfunktion nach Knie-TEP sind bis dato nicht untersucht.\n1.2 Material und Methoden\nProspektive klinische Studie mit 128 konsekutiv erfassten Patienten (OA n=92; RA n=36) mit der Indikation zur hybriden bikondyl\u00e4ren Knie-TEP. Die Kniefunktion wurde pr\u00e4operativ (T0) und nach sechs Monaten (T1) mittels Knee Injury and Osteoarthritis Outcome Score (KOOS) und Oxford Knee Score (OKS) erfasst.\n1.3 Ergebnisse\nOKS und KOOS zeigen eine statistisch signifikante Verbesserung f\u00fcr die OA- und RA-Patientenkohorte bei T1 (6 Monate) verglichen mit T0 (pr\u00e4operativ).\n1.4 Schlussfolgerung\nSowohl bei OA-Patienten als auch bei RA-Patienten wurde sechs Monate nach Implantation eine Besserung der Knie-Funktion beobachtet. Offen bleibt, ob die postoperative Datenauswertung nach 12 Monaten und mehr deutliche Unterschiede zwischen den Subgruppen OA und RA aufzeigen kann.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6348\nurn:nbn:de:bvb:29-opus4-63480\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63480\nhttps://opus4.kobv.de/opus4-fau/files/6348/Promotion_JeskaStolle.VersionBib.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6399\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:616\nccs\nccs:E.0\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nDer Einfluss der H\u00e4ufigkeit von Papillenrandblutungen auf die Progressionsrate bei chronischem Offenwinkelglaukom\nThe influence of the frequency of peripapillary bleedings on the progression of chronic open angle glaucoma\nBilger, Angelika\nhttp://d-nb.info/gnd/4021210-5Link\nddc:616\nHintergrund und Ziele\nDas Glaukom ist eine Erkrankung, die meist in Zusammenhang mit einem erh\u00f6hten Augeninnendruck auftritt und progredient aufgrund einer Sehnervenzerst\u00f6rung zu einer Sehverschlechterung f\u00fchrt.\nIm Zusammenhang mit dem Glaukom kann man im Laufe der Erkrankung h\u00e4ufig Papillenrandblutungen beobachten. Diese sind sehr spezifisch f\u00fcr das Glaukom, da sie bei fast keinen anderen Erkrankungen auftreten. Allerdings treten sie nicht bei jedem Patienten, der an einem Glaukom leidet, auf. Oft sind Papillenrandblutungen ein Zeichen einer fortschreitenden Optikusatrophie im Rahmen eines Offenwinkelglaukoms und gehen meistens im Verlauf der Erkrankung mit einer Sehverschlechterung einher.\nDa Papillenrandblutungen im Allgemeinen mit einer Sehverschlechterung assoziiert sind, soll in dieser Arbeit untersucht werden, ob die H\u00e4ufigkeit von Papillenrandblutungen Einfluss auf den Verlauf nimmt. Es wird in dieser medizinischen Dissertation untersucht, ob das geh\u00e4ufte Autreten von Papillenrandblutungen im Vergleich zu einer einmaligen Blutung, zu einer schnelleren Progression der Erkrankung f\u00fchrt oder ob auch beim nur einmaligen Auftreten einer Papillenrandblutung die Erkrankung gleich schnell fortschreitet.\nPatienten und Methoden\nRetrospektiv wurden 50 Patienten beziehungsweise 60 Augen von Patienten, die an einem Glaukom leiden und bei denen im Verlauf der Erkrankung Papillenrandblutungen auftraten, untersucht.\nDie Patienten wurden in 2 Gruppen unterteilt. Bei Gruppe 1 wurde nur eine Papillenrandblutung beobachtet und bei Gruppe 2 konnte mehr als eine Blutung festgestellt werden. Die Patienten wurden nach dem Auftreten der ersten dokumentierten Butung beobachtet. Die Patienten wurden w\u00e4hrend des Beobachtungszeitraums j\u00e4hrlich zur Kontrolle einbestellt, sodass durchschnittlich \u00fcber 10 Jahre im Zeitraum von 1992 bis 2011 der Verlauf der Erkrankung beobachtet werden konnte. Bei einigen Patienten konnte auch der Gesichtsfeldverlauf vor der ersten Blutung untersucht werden.\nZur Beurteilung der Krankheitsprogression wurden fundoskopische Bilder und perimetrische Befunde genutzt. Der Zeitpunkt, zu dem man einen Beginn einer Progression beobachtete, wurde dokumentiert und mittels Kaplan-Maier-Kurve und Log-Rank-Test f\u00fcr die unterschiedlichen Progressionsmethoden die Signifikanz ermittelt und zwischen Gruppe 1 und 2 verglichen.\nMittels eines Gesichtsfeldbefundungsprogrammes, PeriData, konnte die Progression im gesamten Gesichtsfeld in dB/Jahr trendbasiert nach Auftreten der ersten Blutung gemittelt werden. Bei einigen Patienten war dies auch vor Auftreten der ersten Blutung m\u00f6glich. Ebenso wurde trendbasiert in dB/Jahr die Progression im betroffenen Sektor der ersten Papillenrandblutung vor und nach Auftreten der Blutung ausgewertet. Zudem wurde eine globale eventbasierte Progressionsanalyse durchgef\u00fchrt. Eine globale Progression wurde dann dokumentiert, wenn perimetrisch 3 nebeneinander liegende Punkte ermittelt werden konnten, die sich signifikant (p=0,05) verschlechtert hatten. Zudem musste dies in 3 Folgeuntersuchungen best\u00e4tigt werden. Mithilfe von fundoskopischen Bildern wurden Nervenfaserrandsaumverluste ausfindig gemacht und als Progression gewertet.\nErgebnisse und Beobachtungen\nInnerhalb der Gruppen 1 und 2 konnte ein Unterschied in der Progression festgestellt werden. In dieser konnte Arbeit gezeigt werden, dass bei Gruppe 2 eine schnellere Progression als bei Gruppe 1 auftritt. Allerdings erwies sich dieser Unterschied in der statistischen Auswertung nicht als signifikant.\nBei der Beurteilung von Nervenfaserrandsaumverlusten zeigt sich in der Kaplan-Meier-Kurve graphisch eine schnellere Verschlechterung bei Gruppe 2, was jedoch im Logrank-Test nicht signifikant ist (p=0,191). Bei der globalen eventbasierten Progressionsanalyse zeigt sich graphisch eine schnellere Verschlechterung bei Gruppe 2, was im Logrank-Test wiederum nicht signifikant ist (p=0,06). Bei der trendbasierten Auswertung im gesamten Gesichtsfeld in dB/Jahr zeigt sich vor der ersten Blutung bei Gruppe 1 eine Verschlechterung um -0,144 dB/Jahr und bei Gruppe 2 eine Verbesserung um +0,233 dB/Jahr. Im unabh\u00e4ngigen t-Test ist dieser Unterschied jedoch nicht signifikant (p=0,237). Bei der trendbasierten Auswertung im gesamten Gesichtsfeld nach der ersten Blutung konnte bei Gruppe 1 eine mittlere Verschlechterung um -0,2dB/Jahr und bei Gruppe 2 eine Verschlechterung um -0,465 dB/Jahr ermittelt werden. Auch hier ist der Unterschied im unabh\u00e4ngigen t-Test nicht signifikant (p=0,096). Bei der Progression im Sektor der ersten Blutung zeigt sich nach dem Auftreten der ersten Blutung bei Gruppe 1 eine Verschlechterung um -0,222 dB/Jahr und bei Gruppe 2 eine Verschlechterung um -0,520 dB/Jahr. Im unabh\u00e4ngigen t-Test ist dieser Unterschied nicht signifikant (p=0,121).\nPraktische Schlussfolgerungen\nSowohl bei Gruppe 1 als auch bei Gruppe 2 verl\u00e4uft die Erkrankung chronisch progredient.\nEine schnellere Sehverschlechterung bei Patienten der Gruppe 2, bei denen mehr als eine Papillenrandblutung auftrat, spricht f\u00fcr einen Zusammenhang multipler Papillenrandblutungen als Risikofaktor f\u00fcr eine schnellere Progression. Bei allen unseren Untersuchungen zeigte sich nach dem Auftreten der ersten Papillenrandblutung eine schnellere Sehverschlechterung bei Patienten der Gruppe 2. Nur bei der trendbasierten Auswertung im gesamten Gesichtsfeld in dB/Jahr vor der ersten Papillenrandblutung konnten bei Gruppe 2 bessere Werte im Vergleich zu Gruppe 1 ermittelt werden. Allerdings konnten wir in keiner unserer Untersuchungen das Signifikanzniveau (p = 0,05) erreichen, sodass weitere Studien notwendig sind, um den Zusammenhang n\u00e4her zu untersuchen.\nDa, wie bereits erw\u00e4hnt bei beiden Gruppen die Erkrankung chronisch progredient verl\u00e4uft, ist es notwendig, dass weiterhin bei beiden Patientengruppen regelm\u00e4\u00dfige Kontrolluntersuchungen stattfinden und die Patienten ad\u00e4quat therapiert werden.\nNur aufgrund der Tatsache, dass nur eine Blutung auftrat, darf keine Herabsetzung der Anzahl der Kontrollbesuche erfolgen. Diese sollten in gleicher Anzahl stattfinden wie bei Patienten der Gruppe 2, die mehr Blutungen aufweisen.\nBackground and Objectives\nGlaucoma is a disease which is correlated with a higher intraocular pressure. It is a chronical progressive disease leading to an irreversible damage of the optic nerve and a detoriation of the vision.\nIn glaucoma eyes peripapillary bleedings could be observed very often. They are very specific for glaucoma and are rarely seen in other diseases. Peripapillary bleedings do not occur with every eye suffering glaucoma, though. If they are seen in patients with glaucoma it is often a sign for progression of atrophy of nervus opticus and a sign of detoriation of the visus.\nAs it is already known that peripapillary bleedings in general could be a sign of detoriation of the visus, in this dissertation it is analysed if multiple peripapillary bleedings lead to a quicker progression of the disease in comparison with a single bleeding.\nPatients and Methods\nThis retrospective study has the aim to observe the occurring of peripapillary bleedings by comparing and analyzing 60 eyes of 50 patients suffering from eye glaucoma.\nPatients were divided into two groups. Group 1 had only one single peripapillary bleeding and in group 2 several bleedings could be seen. We started the investigation of each patient when the first bleeding was found. Patients were seen annually after their first bleeding and we had a follow-up of around 10 years.\nTo evaluate the disease`s progression, we used fundoscopic photographs and perimetric findings. The time of progression was documented and analyzed with Kaplan-Meier curve and Log-Rank-test. Consequently we could test the level of significance between the two groups.\nThe time of progression was analyzed with several methods. We used a programme called PeriData to average the progression trend-based in dB/year within the whole visual field before and after the first bleeding could be registered. In addition, the sector of the visual field in which the first bleeding occured was tested in the same way. Another method to analyze the progression was to look for 3 points next to each other on the visual field which became worse in a significance level of p = 0,05 and could be confirmed by three following examinations. In fundoscopic photographs a loss of the nerve fiber layer was considered as progression.\nResults\nA difference in progression could be seen. Group 2 had a faster progression than group 1. The significance niveau (p=0,05) could not be reached in any of our analysis, though.\nA faster loss of the nerve fiber layer could be seen in group 2 but there was no significant difference (p=0,191). In the global eventbased progression analysis measured with three points next to each other on the visual field, which became worse in a significance level of p=0,05, a faster progression could be seen in group 2. The Log-Rank-test was not significant, eather (p=0,06). Before the first bleeding occured a detoriation of -0,144dB/year could be observed in the average trend-based progression in dB/year in group 1. Group 2 showed an amelioration of +0,233 dB/year. In the t-test there was no significant difference (p=0,237). In the average trend-based progression after the first bleeding there was a mean detoriation of -0,2dB/year and a mean detoriation of -0,465 dB/year in group 2. The t-test was not significant (p=0,096). In the sector of the visual field in which the first bleeding occured a progression of -0,222dB/year could be observed in group 1 and a progression of -0,520 dB/year in group 2. This difference is not significant when tested by the t-test (p=0,121).\nConclusions\nIn both groups a chronical progression of the disease was seen.\nThe faster progression in group 2 shows that multiple bleedings could be a risk factor for a worse course of disease. As the significance niveau couldn`t be reached further studies are warranted to analyze this correlation.\nAs glaucoma is a chronical progressive disease regular check-ups are needed for every patient to optimize therapy.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6399\nurn:nbn:de:bvb:29-opus4-63998\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-63998\nhttps://opus4.kobv.de/opus4-fau/files/6399/Abgabe.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6456\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nKlinische Ergebnisse nach laparoskopischer Operation bei Morbus Crohn mit und ohne Komplikationen\nClinical results after laparoscopic surgery in Crohn's disease with and without complications\nF\u00f6rtsch, Line\nMorbus Crohn\nddc:617\nZusammenfassung\n1. Hintergrund und Ziele\nDer Morbus Crohn geh\u00f6rt zu den chronisch-entz\u00fcndlichen Darmerkrankungen und kann den gesamten Magen-Darm-Trakt befallen. Die Ursache der Erkrankung ist sehr komplex und noch nicht vollst\u00e4ndig gekl\u00e4rt. Die Erkrankung ist langj\u00e4hrig und rezidivierend. Sie kann milde oder mit diversen Komplikationen wie Fisteln, Abszessen oder Perforationen verlaufen, was zu unterschiedlicher Beschwerdesymptomatik f\u00fchrt. Da es bisher keine Heilung gibt, ist das Ziel der Therapie die Reduktion der Entz\u00fcndung und die Symptomlinderung. Die Therapie besteht einerseits aus einer immunsupprimierenden und antiinflammatorischen Medikation, andererseits stehen operative Verfahren zur Verf\u00fcgung, die vornehmlich bei konservativem Therapieversagen oder schweren intestinalen Komplikationen Anwendung finden. Besondere Bedeutung gewinnt hierbei die Laparoskopie.\nZiel dieser Arbeit war es herauszufinden, ob es einen Unterschied im klinischen Verlauf bei laparoskopisch operierten Patienten, die sich pr\u00e4operativ bez\u00fcglich Komplikationen unterschieden, gibt. Es soll in vorliegender Studie nachgegangen werden, ob Voroperationen oder Komplikationen wie Fisteln, Abszesse, Perforationen und Peritonitis zu vermehrten intra- und postoperativen Komplikationen, erh\u00f6hter Mortalit\u00e4tsrate und einem l\u00e4ngeren postoperativen Krankenhausaufenthalt f\u00fchren.\n2. Methoden\nDer Beobachtungszeitraum dieser Arbeit erstreckt sich vom Jahr 1999 \u2013 2012. Insgesamt wurden 79 Patienten mit Morbus Crohn laparoskopisch operiert. Dieses Patientenkollektiv wurde in \u201eunkomplizierte\u201c und \u201ekomplizierte\u201c F\u00e4lle unterteilt. Unkomplizierte F\u00e4lle wiesen lediglich Stenosen und Entz\u00fcndung auf und die komplizierten F\u00e4lle hatten zus\u00e4tzliche Komplikationen wie Fisteln, Abszesse, Perforationen oder eine Peritonitis. Die Daten wurden anhand eines Erhebungsbogens retrospektiv erfasst und mit Hilfe des Statistikprogramms SPSS statistisch ausgewertet.\n3. Ergebnisse und Schlussfolgerung\nBeide Gruppen zeigten keine signifikanten Unterschiede in der Verteilung der Patienteneigenschaften. Sie unterschieden sich nicht im M\u00e4nner-Frauen-Verh\u00e4ltnis, im Alter zum Zeitpunkt der Erstdiagnose bzw. der OP, in der Anamnesedauer, in Laborparametern, im BMI oder in der ASA-Klassifikation. Ausserdem lagen in beiden Gruppen \u00e4hnlich viele Rezidive und Erstdiagnosen des Morbus Crohn vor.\nIn der Gruppe der unkomplizierten F\u00e4lle gaben Patienten wesentlich h\u00e4ufiger Schmerzen an als Patienten der komplizierten F\u00e4lle. Ebenso war in der unkomplizierten Gruppe das Ileum signifikant h\u00e4ufiger betroffen und Stenosen traten in der Gruppe der unkomplizierten F\u00e4lle \u00f6fter auf als in der Gruppe der komplizierten F\u00e4lle. Bez\u00fcglich der Stenosenlokalisation war der D\u00fcnndarm bzw. der \u00dcbergang von D\u00fcnndarm zum Dickdarm bei unkomplizierten Patienten signifikant h\u00e4ufiger befallen \u2013 mit konsekutiv signifikant mehr Ileoz\u00f6kalresektionen - und Steroide wurden von unkomplizierten Patienten vermehrt eignenommen.\nBez\u00fcglich intraoperativer Komplikationen zeigte sich in dieser Arbeit kein signifikanter Unterschied zwischen beiden Gruppen. Ebenso war keine Signifikanz hinsichtlich postoperativer Komplikationen und der postoperativen Krankenhausverweildauer erkennbar.\nZusammenfassend kann diese Arbeit darlegen, dass es keinen signifikanten Unterschied bez\u00fcglich des klinischen Verlaufes zwischen pr\u00e4operativ komplizierten und komplikationslosen Patienten bei laparoskopischer Operationsmethode gibt.\nSummary\n1.Background and objectives\nCrohn's disease is an inflammatory bowel disease and can affect the entire gastrointestinal tract. The cause of the disease is very complex and not yet fully understood. The disease is long-standing and recurrent. There are mild symptoms as well as severe ones with various complications such as: fistulas, abscesses or perforations, which then lead to different symptomatology. Since there is no cure yet, the objective of the therapy is to reduce the inflammation and to relieve the symptoms. On one hand, the therapy consists of an immunosuppressive and anti-inflammatory medication, on the other hand there are surgical procedures available, which are mainly used when conservative treatment fails or severe intestinal complications occur. In this case, laparoscopy is particularly important.\nThe objective of this dissertation was to find out whether there is a difference in the clinical symptoms of patients undergoing laparoscopic resection, which differed with respect to preoperative complications. In the study presented here, an investigation will be made into understanding if previous surgeries or complications such as fistulas, abscesses, perforations and peritonitis have led to increased intra-and postoperative complications, increased mortality rate and a longer postoperative hospital stay.\n2. Methods\nThe observation period of this study extends from 1999 \u2013 2012. A total of 79 patients with Crohn's disease underwent laparoscopic surgery. This patient population was divided into \"uncomplicated\" and \"complicated\" cases. Uncomplicated cases showed only stenosis and inflammation whereas complicated ones had additional difficulties such as fistulas, abscesses, perforation or peritonitis. The data were recorded retrospectively using a written questionnaire and were then statistically analyzed using the statistics software SPSS.\n3 Results and Conclusions\nBoth groups showed no significant differences in patient characteristics. They did not differ in the male-female ratio, age at diagnosis or surgery, in the duration of anamnesis, in laboratory parameters, in BMI or in the ASA classification. In addition, there was a similar amount of recurrences and initial diagnoses of Crohn's disease in both groups.\nIn the group of uncomplicated cases patients reported pain significantly more frequently than patients in the complicated cases. Likewise, in the group of uncomplicated cases, the ileum was affected significantly more often and stenoses occurred far more frequently than in the group of complicated cases. Regarding the location of stenosis the small intestine or the transition from the small intestine to the colon was affected significantly more often in the group of uncomplicated patients \u2013 consecutively with significantly more ileo-caecal-resections, \u2013 also steroids were taken more often in this group.\nRegarding intraoperative complications, no significant difference between the two groups was found. Also no significance in regards to postoperative complications and postoperative hospital stay was apparent.\nIn conclusion, this study demonstrates that there is no significant difference in the clinical course between preoperatively complicated and uncomplicated patients, who had laparoscopic surgery.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6456\nurn:nbn:de:bvb:29-opus4-64565\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-64565\nhttps://opus4.kobv.de/opus4-fau/files/6456/Klinische%20Ergebnisse%20nach%20laparoskopischer%20Operation%20bei%20Morbus%20Crohn%20mit%20und%20ohne%20Komplikationen.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6552\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:610\nccs\nccs:A.\npacs\npacs:00.00.00\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nMultimodale Bildgebungsstrategien zur optimierten Tumorabgrenzung am experimentellen Gliommodell\nImaging strategies glioma\nPitann, Patrick\nGliom\nGlioblastom\nddc:610\nHinsichtlich der Tumorabgrenzung haben sich die DCE - bzw. die DTI -Sequenzen als \u00e4u\u00dferst n\u00fctzlich erwiesen. Tierexperimentell w\u00fcrde sich durch die Definition von Grenzwerten, unter Ber\u00fccksichtigung der Grenzwertvariation, eine schnelle (automatisierte) Darstellung des Glioms mit seiner diffusen Infiltration erreichen lassen. Insbesondere die, aus den FA - und KTrans - Karten berechneten Grenzwerte und die sich daraus ergebenden Volumina zeigten eine hohe \u00dcbereinstimmung mit der Histologie als Goldstandard.\nLetztlich k\u00f6nnen mit den oben genannten Sequenzen pr\u00e4zisere Aussagen hinsichtlich malignisierter Areale und insbesondere eine Abgrenzung des Tumors bez\u00fcglich seiner R\u00e4nder, anhand der, aus den berechneten FA - und KTrans - Karten definierten Grenzwerten, gemacht werden. Somit kann in Bezug auf die Malignit\u00e4t des Tumors ein Surrogat - Parameter erstellt werden, an dem sich ein m\u00f6gliches Ansprechen auf eine Therapie zeigen l\u00e4sst. Ebenfalls von Vorteil w\u00e4re diese Methode bei der chirurgischen Versorgung des Glioblastoms. Hier h\u00e4tte der Neurochirurg die M\u00f6glichkeit, unter Verwendung der genannten Technik, eine exaktere pr\u00e4operative Planung durchzuf\u00fchren [84].\nZusammenfassend l\u00e4sst sich tierexperimentell, mit Hilfe des funktionellen MRTs und unter Verwendung definierter Grenzwerte, in guter \u00dcbereinstimmung zur Histologie eine pr\u00e4zise Abgrenzung des Glioblastoms erzielen.\nWith regard to tumor differentiation, the DCE have - or DTI sequences proved extremely useful. In animal experiments would be attained by the definition of limit values, taking into account the variation limit, a fast (automated) Presentation of glioma with its diffuse infiltration. In particular, from the FA - and Ktrans - calculated card limits and volumes resulting showed high correlation with histology as the gold standard.\nUltimately, with the sequences above precise statements regarding malignisierter areas and in particular a definition of the tumor with respect to its edges, on the basis of, from the calculated FA - be defined card limits - and Ktrans. Thus, a surrogate can with respect to the malignancy of the tumor - parameters are created at which a possible response can point to a therapy. Also of advantage would this method in the surgical treatment of glioblastoma. Here the neurosurgeon would have the ability to carry out using the above technique, a more accurate preoperative planning [84].\nIn summary, in animal experiments, achieve using the functional MRI and using defined limits, in good agreement for histology precise delineation of glioblastoma\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6552\nurn:nbn:de:bvb:29-opus4-65528\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-65528\nhttps://opus4.kobv.de/opus4-fau/files/6552/PatrickPitanndissertation.pdf\ndeu\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6554\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:D.4.7\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nDesign, Implementation and Evaluation of the ULIX Teaching Operating System\nDesign, Implementation und Evaluation des Lehrbetriebssystems ULIX\nE\u00dfer, Hans-Georg\nBetriebssystem\nLehre\nUNIX\nProgrammdokumentation\nddc:004\nWe have implemented and documented Ulix, a Unix-like instructional operating system whose kernel sources consist of 7750 lines of C and Assembler code. The system supports concurrent processes and threads, implements a Round-Robin scheduler, a virtual filesystem with support for hard and floppy disks, the logical Minix filesystem and a /dev filesystem, and it provides mutexes and semaphores. In addition, a user mode library gives access to the system calls via typical Unix functions. Ulix can be executed in the Qemu PC emulator.\nWhile there are several other instructional operating systems with similar features, e. g., Minix, the novelty of our approach lies in using Knuth's Literate Programming technique which puts the focus on documentation with embedded code, rather than code with embedded documentation. The literate program is a book - in this case an introduction to operating system principles which presents the full Ulix source code in a way that follows didactical considerations.\nBased on the Ulix source code and the book we developed a \"Design and implementation of operating systems\" course with a collection of implementation exercises and evaluated it in a course setting.\nPart I of the thesis summarizes the research carried out while conceptually designing and implementing Ulix as well as evaluating the Ulix-based course. Part II is the Ulix book.\nWir haben Ulix, ein Unix-\u00e4hnliches Lehrbetriebssystem, implementiert und dokumentiert. Die Kernel-Sourcen bestehen aus 7750 Zeilen C- und Assembler-Code. Das System unterst\u00fctzt die parallele Ausf\u00fchrung von Prozessen und Threads, implementiert einen Round-Robin-Scheduler, ein virtuelles Dateisystem, das Festplatten- und Floppy-Laufwerke sowie das logische Minix-Dateisystem und ein /dev-Dateisystem unterst\u00fctzt,\nund es stellt Mutexe und Semaphore zur Verf\u00fcgung. Erg\u00e4nzend erlaubt eine User-Mode-Bibliothek \u00fcber typische Unix-Funktionen Zugriff auf die\nSystem-Calls. Ulix l\u00e4uft im PC-Emulator Qemu. Zwar gibt es bereits zahlreiche weitere Lehrbetriebssysteme mit \u00e4hnlichen Features, wie z. B. Minix, doch unser Ansatz unterscheidet sich wesentlich von ihnen, da wir die Programmiertechnik Literate Programming von D. E. Knuth eingesetzt haben, welche den Fokus auf Dokumentation mit integriertem Code (statt auf Code mit integrierter Dokumentation) legt. Das\nLiterate Program ist ein Buch: in diesem Fall eine Einf\u00fchrung in Betriebssystemkonzepte, die den vollst\u00e4ndigen Ulix-Quellcode didaktisch aufbereitet pr\u00e4sentiert.\nBasierend auf dem Ulix-Quellcode und dem Buch haben wir eine Vorlesung \"Betriebssystem-Entwicklung mit Literate Programming\"\nmit einer Reihe von Implementierungsaufgaben konzipiert, gehalten und evaluiert.\nTeil I der Arbeit fasst die Forschungsergebnisse, die sich aus dem konzeptuellen Design und der Implementierung von Ulix ergeben haben, zusammen und evaluiert den Einsatz in der Lehre. Teil II ist das Ulix-Buch.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6554\nurn:nbn:de:bvb:29-opus4-65543\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-65543\nhttps://opus4.kobv.de/opus4-fau/files/6554/HansGeorgEsserDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6593\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:620\nccs\nccs:B.m\npacs\npacs:78.67.Ch\nmsc\nmsc:00-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nPolymer-Selected Carbon Nanotubes for Light-Emitting Field-Effect Transistors\nPolymerselektierte Kohlenstoffnanor\u00f6hren f\u00fcr lichtemittierende Feldeffekttransistoren\nJakubka, Florian\nSWNT\nFET\nddc:620\nCarbon nanotubes hold remarkable optoelectronic properties. Beside their high conductivity\nand field-effect mobility, which predetermines their use in future electronic applications,\nthey feature a direct bandgap that enables luminescence in the near-infrared. Yet, carbon\nnanotubes still suffer from heterogeneous source material that commonly consists of various\nnanotube species with different metallicity, diameter and emission wavelength. This dissertation\ninvestigates the selective sorting of carbon nanotubes by polyfluorene-polymers as\nwell as the application of these polymer/nanotube dispersions in light-emitting field-effect\ntransistors.\nSome polyfluorene-polymers show a striking selectivity for the dispersion of certain nanotube\nspecies that is not yet completely understood. In this work, the influence of the solvent\ntype and the polymer molecular weight on the selective dispersion behavior is analyzed, to\nproduce nanotube/polymer dispersions with tailored composition and yield. It was found\nthat meta- or non-stable nanotube species are stabilized by a low diffusion constant. Therefore\na low dispersion viscosity leads to high selectiveness, however at at the expense of\noverall yield.\nCarbon nanotube dispersions can be used with high excess of polymer as semiconducting\npolymer layers with improved injection behavior. The application of these films in lightemitting\nfield-effect transistors reveals an improved charge injection from the metal source\nand drain contacts. As shown in electrostatic simulations, the one-dimensionally confined\nstructure of carbon nanotubes leads to an enhancement of the applied electric field at the\ntube-tips. The nanotube doping reduces threshold voltages for both electron and holes and\nincreases ambipolar currents and light emission. Hence, it represents an easy-to-achieve\nperformance improvement for polymer transistors where charge injection might be an issue.\nA further purification of the dispersion and removal of the excess polymer leads to solution\nprocessable nanotube inks with near-monochiral distribution. Their performance\nin nanotube-network field-effect transistors with polymer dielectrics and electrolyte-gated\ntransistors has been evaluated. While monochiral and narrowband excitonic light emission\nwas obtained from transistors with polymer dielectric, the high charge carrier densities in\nion-gel gated devices revealed electrically stimulated trion emission and low voltage operation\nwith the nanotube network films. The dependence of exciton and trion emission\non charge carrier density has been shown, effectively creating a photoluminescence emitter\nwith voltage-controllable emission wavelength.. Defined emission in the near-infrared\nand good electrical performance promotes the use of polymer-selected carbon nanotubes as\na bridge between electric and optical signal transmission, especially for upcoming generations\nof printable and flexible electronics.\nKohlenstoffnanor\u00f6hren besitzen bemerkenswerte optoelektronische Eigenschaften.\nNeben hoher Leitf\u00e4higkeit und Feldeffekt-Mobilit\u00e4t erm\u00f6glicht eine direkte Bandl\u00fccke Lumineszenz\nim nahen infraroten Spektrum. Das heterogene Ausgangsmaterial der Kohlenstoffnanor\u00f6hrenherstellung\nerweist sich allerdings als Nachteil f\u00fcr optoelektronische Anwendungen,\nda es in der Regel verschiedene Spezies mit leitenden oder halbleitenden Eigenschaften\nsowie unterschiedlichen Durchmessern und Emissionwellenl\u00e4ngen enth\u00e4lt. Diese\nDissertation behandelt das selektive Dispergieren von Nanor\u00f6hren mit Polyfluoren-Polymeren\nund die Anwendung dieser Dispersionen in lichtemittierenden Feldeffekttransistoren.\nBestimmte Polyfluorene zeigen eine noch nicht eindeutig verstandene Selektivit\u00e4t f\u00fcr spezifische\nNanor\u00f6hren-Arten. Diese Arbeit analysiert hierbei den Einfluss des L\u00f6sungsmittels\nund des Polymer-Molekulargewichts auf das selektive Dispergierverhalten, mit dem Ziel,\nma\u00dfgeschneiderte Nanor\u00f6hren-Zusammensetzungen herzustellen. Die Ergebnisse zeigen\neine Stabilisierung von halb- und instabilen R\u00f6hren durch niedrige Diffusionskonstanten.\nNiedrigviskose Dispersionen zeigen deshalb eine h\u00f6here Selektivit\u00e4t, allerdings verbunden\nmit einer niedrigeren Gesamtausbeute.\nErzeugte Nanor\u00f6hren-Dispersionen k\u00f6nnen mit hohem Polymer\u00fcberschuss als halbleitende\nPolymerfilme benutzt werden. Die Anwendung dieser Schichten zeigt ein verbessertes\nInjektionsverhalten der Ladungstr\u00e4ger. Elektrostatische Simulationen deuten auf eine\nErh\u00f6hung der angelegten elektrischen Felder um die R\u00f6hrenspitzen, verursacht durch die\neindimensionale Struktur der Kohlenstoffnanor\u00f6hren, hin. Dies erleichtert das Tunneln von\nLadungstr\u00e4gern durch die Schottkybarrieren an Source- und Drain-Kontakten, was zu einer\nVerringerung der Schwellspannung und erh\u00f6hten ambipolaren Str\u00f6men f\u00fchrt.\nMit einem weiteren Zentrifugierschritt kann die selektive Dispersion von \u00fcbersch\u00fcssigem\nPolymer gereinigt werden. Dies erzeugt l\u00f6sungsprozessierbare Nanor\u00f6hren-Formulierungen\nmit fast monochiralen Zusammensetzungen. Diese wurden in Nanor\u00f6hrennetzwerken\nals Halbleiter f\u00fcr Feldeffekttransistoren getestet. W\u00e4hrend mit Polymerdielektrikas\neine exzitonische und schmalbandige Emission erreicht werden kann, zeigen Elektrolytkontaktierte\nTransitoren weitere Emmisionsb\u00e4nder bei niedrigerer Energie, die der Emission\nvon Trionen zugeordent werden kann. Es wird die Abh\u00e4ngigkeit bei Exzitonen- und\nTrionenbildung von der Ladungstr\u00e4gerdichte aufgezeigt, was effektiv zu einem spannungsgesteuerten\nPhotolumineszenzemitter mit wechselbarer Wellenl\u00e4nge f\u00fchrt. Die schmalbandige\nInfrarotlumineszenz und die guten elektrischen Eigenschaften erm\u00f6glichen den\nEinsatz der Polymer-selektierten Nanor\u00f6hren als Br\u00fccke zwischen elektrischer und optischer\nSignal\u00fcbermittlung, insbesondere in neuen Anwendungsfeldern wie druckbarer und\nflexibler Elektronik.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6593\nurn:nbn:de:bvb:29-opus4-65938\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-65938\nhttps://opus4.kobv.de/opus4-fau/files/6593/Thesis_Florian_Jakubka_2015.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6664\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nVergleich gebr\u00e4uchlicher Parallelrechensysteme f\u00fcr die Beschleunigung evolution\u00e4rer Algorithmen demonstriert f\u00fcr den Entwurf miniaturisierter optischer und elektronischer Bauelemente\nLimmer, Steffen\nOptimierung , Cluster\nddc:004\nEvolutionary algorithms are one of the most popular forms of optimization algorithms. They are comparatively easy to use and were successfully employed for a wide variety of practical applications. However, frequently it is necessary to execute them in parallel in order to reduce the runtime. There are a number of different approaches for the parallelization of evolutionary algorithms and\nvarious hardware platforms can be used for the parallel execution. However, not every platform is equally suitable for any kind of parallelization of evolutionary algorithms. In addition, it also depends on properties of the concrete\noptimization problem to be solved and on the used evolutionary algorithm,\nwhich platform is best suited for the execution.\nThis work deals with the question of the circumstances under which an architecture is better suited for executing a parallel evolutionary algorithm than\nanother, and examines the common forms of parallelization of evolutionary algorithms on the most widely used parallel computing systems (multi-core\nCPUs, grids, clusters and graphics cards).\nFurthermore, the solution of four real-world examples from the field of the design of miniaturized optical and electronic devices with help of parallel evolutionary algorithms is described and the results of the previous studies to the\nparallelization forms and parallel platforms are discussed on these concrete\nexamples.\nIn addition, a framework for the execution of distributed evolutionary algorithms on grid platforms is presented.\nEvolution\u00e4re Algorithmen z\u00e4hlen zu einer der popul\u00e4rsten Formen von Optimierungsalgorithmen. Sie sind vergleichsweise leicht in der Handhabung und konnten bereits f\u00fcr eine Vielzahl praktischer Anwendungen erfolgreich eingesetzt werden. In etlichen F\u00e4llen weisen sie jedoch eine lange Laufzeit auf, sodass eine Beschleunigung \u00fcber eine parallele Ausf\u00fchrung notwendig ist. F\u00fcr die Parallelisierung evolution\u00e4rer Algorithmen existieren eine Reihe unterschiedlicher Ans\u00e4tze und es kommen unterschiedliche Hardware-Plattformen f\u00fcr die parallele Ausf\u00fchrung infrage. Nicht jede Plattform eignet sich jedoch gleich gut f\u00fcr jede Form der Parallelisierung von evolution\u00e4ren Algorithmen. Zus\u00e4tzlich h\u00e4ngt es auch von Eigenschaften des konkret zu l\u00f6senden Optimierungsproblems beziehungsweise des eingesetzten evolution\u00e4ren Algorithmus ab, welche\nPlattform sich am besten f\u00fcr die Ausf\u00fchrung eignet.\nDiese Arbeit besch\u00e4ftigt sich mit der Fragestellung, unter welchen Umst\u00e4nden eine Architektur besser als eine andere Architektur zur Ausf\u00fchrung eines\nparallelen evolution\u00e4ren Algorithmus geeignet ist, und untersucht die g\u00e4ngigen\nParallelisierungsformen von evolution\u00e4ren Algorithmen auf den gebr\u00e4uchlichen\nParallelrechensystemen (Multicore-CPUs, Grids, Cluster und Grafikkarten).\nWeiterhin wird die L\u00f6sung von vier Realweltbeispielen aus dem Bereich des Entwurfs miniaturisierter optischer und elektronischer Bauelemente mittels paralleler evolution\u00e4rer Algorithmen beschrieben und die Ergebnisse der vorangegangenen Untersuchungen werden an diesen konkreten Beispielen diskutiert.\nDar\u00fcber hinaus wird ein Framework vorgestellt, welches der Ausf\u00fchrung\nverteilter evolution\u00e4rer Algorithmen auf Grid-Plattformen dient.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6664\nurn:nbn:de:bvb:29-opus4-66642\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-66642\nhttps://opus4.kobv.de/opus4-fau/files/6664/SteffenLimmerDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6684\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:600\npacs\npacs:87.61.Tg\nmsc\nmsc:97M60\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nReconstruction Techniques for Dynamic Radial MRI\nRekonstruktionstechniken f\u00fcr dynamische, radiale MR-Bildgebung\nGrimm, Robert\nMRT\n3D-Rekonstruktion\nMedizinische Bildgebung\nPositronen-Emissions-Tomographie\nOnkologie\nddc:600\nToday, magnetic resonance imaging (MRI) is an essential clinical imaging modality and routinely used for orthopedic, neurological, cardiovascular, and oncological diagnosis.\nThe relatively long scan times lead to two limitations in oncological MRI.\nFirstly, in dynamic contrast-enhanced MRI (DCE-MRI), spatial and temporal resolution have to be traded off against each other.\nSecondly, conventional acquisition techniques are highly susceptible to motion artifacts. As an example, in DCE-MRI of the liver, the imaging volume spans the whole abdomen and the scan must take place within a breath-hold to avoid respiratory motion. Dynamic imaging is achieved by performing multiple breath-hold scans before and after the injection of contrast agent. In practice, this requires patient cooperation, exact timing of the contrast agent injection, and limits the temporal resolution to about 10 seconds.\nThis thesis addresses both challenges by combining a radial k-space sampling technique with advanced reconstruction algorithms for higher temporal resolution and improved respiratory motion management.\nA novel reconstruction technique, golden-angle radial sparse parallel MRI (GRASP),\nenables performing DCE-MRI at simultaneously high spatial and temporal resolution.\nIterative gradient-based and alternating optimization techniques were implemented and evaluated.\nGRASP is based on a single, continuous scan during free breathing, allowing for a simplified and more patient-friendly examination workflow.\nThe technique is augmented by an automatic detection of the contrast agent bolus arrival and by incorporating variable temporal resolution.\nThese proposed extensions reduce the number of generated image volumes, resulting in faster reconstruction and post-processing.\nThe radial trajectory also allows to extract a respiratory signal directly from the scan data. This self-gating property can be used for dynamic imaging in such a way that different, time-averaged phases of respiration are retrospectively reconstructed from a free-breathing scan. Automated algorithms for deriving, processing, and applying the self-gating signal were developed.\nThe clinical relevance of self-gating was demonstrated by generating a motion model to correct for respiratory motion in a simultaneous positron emission tomography (PET) examination on hybrid PET/MRI scanners. This approach reduces the motion blur and, thus, improves tracer uptake quantification in moving lesions, while avoiding an increased noise level as it would be the case for conventional gating techniques.\nIn conclusion, the presented advanced reconstruction techniques help to improve the spatio-temporal resolution as well as the robustness with respect to motion of dynamic radial MRI. The effectiveness of the proposed methods was supported by numerous studies in patient settings, showing that non-Cartesian k-space sampling can be advantageous in a variety of applications.\nDie Magnetresonanztomographie (MRT) ist heute eine unverzichtbare klinische Bildgebungsmodalit\u00e4t und wird in der Routine zur Diagnose von orthop\u00e4dischen, neurologischen, kardiovaskul\u00e4ren und onkologischen Fragestellungen angewendet.\nIn der Onkologie f\u00fchrt die relativ lange Untersuchungsdauer zu zwei Einschr\u00e4nkungen.\nZum einen m\u00fcssen, besonders in der dynamischen kontrastmittelgest\u00fctzten MRT (DCE-MRI), r\u00e4umliche und zeitliche Aufl\u00f6sung gegeneinander abgewogen werden.\nZum anderen sind konventionelle Abtastverfahren des k-Raums anf\u00e4llig gegen\u00fcber Bewegungsartefakten.\nBeispielsweise muss das Bildgebungsvolumen bei DCE-MRI der Leber das gesamte Abdomen abdecken und die Messung muss dennoch innerhalb eines Atemstopps erfolgen, um Atembewegung zu vermeiden. Dynamische Bildgebung wird durch mehrere derartige Messungen vor und nach der Injektion des Kontrastmittels erzielt.\nIn der Praxis ist bei diesem Vorgehen eine Zeitaufl\u00f6sung von maximal etwa 10 Sekunden erreichbar, w\u00e4hrend die Kooperation des Patienten und genaue zeitliche Abstimmung der Kontrastmittelgabe unabdingbar sind.\nIn der vorliegenden Arbeit werden beide Aspekte durch die Kombination von radialer k-Raum-Abtastung und erweiterten Rekonstruktionstechniken adressiert.\nEine neue Rekonstruktionstechnik f\u00fcr radiale Perfusionsbildgebung, GRASP, erlaubt es, gleichzeitig sowohl hohe Orts- als auch Zeitaufl\u00f6sungen zu erreichen.\nIterative gradientenbasierte und alternierende Optimierungsverfahren wurden f\u00fcr GRASP implementiert und evaluiert.\nBei GRASP basiert die Akquisition auf einer einzigen, zusammenh\u00e4ngenden Messung bei freier Atmung, was die Untersuchung vereinfacht und auch patientenfreundlicher gestaltet.\nDas Verfahren wird durch eine neuartige automatische Detektion der Kontrastmittelanflutung und durch die M\u00f6glichkeit einer variablen Zeitaufl\u00f6sung erg\u00e4nzt.\nDiese Erweiterungen reduzieren die Anzahl der erzeugten Volumina und erlauben dadurch schnellere Rekonstruktion und Nachverarbeitung der Bilder.\nDas radiale Abtastschema erlaubt es au\u00dferdem, ein Atemsignal aus den k-Raum-Rohdaten abzuleiten.\nDiese auch als Self-Gating bezeichnete Eigenschaft kann zur dynamischen Bildgebung genutzt werden, indem unterschiedliche, zeitlich gemittelte Atemphasen retrospektiv aus einer Messung in freier Atmung abgebildet werden. In dieser Arbeit werden automatische Algorithmen pr\u00e4sentiert, um das Self-Gating-Signal zu extrahieren, zu verarbeiten und anzuwenden. Der praktische Nutzen wird durch die Erzeugung eines Bewegungsmodells demonstriert, das die Kompensation von Bewegungsartefakten einer zeitgleich ausgef\u00fchrten Positronen-Emissions-Tomographie (PET) bei hybriden PET/MRT-Ger\u00e4ten erm\u00f6glicht. Dieser Ansatz reduziert Bewegungsunsch\u00e4rfe und f\u00fchrt dadurch zu einer genaueren Quantifizierung der Tracer-Aufnahme in beweglichen L\u00e4sionen und vermeidet dabei aber ein erh\u00f6htes Rauschniveau, wie es durch konventionelle Gating-Technik herbeigef\u00fchrt w\u00fcrde.\nDie pr\u00e4sentierten erweiterten Bildgebungstechniken helfen so, die Orts- und Zeitaufl\u00f6sung sowie die Bewegungsunempfindlichkeit von dynamischer, radialer MRT zu verbessern. Die Wirksamkeit der vorgeschlagenen Verfahren wird durch mehrere Studien mit klinischen Patienten gest\u00fctzt und zeigt, dass unterschiedliche Applikationen von den Vorteilen nicht-kartesischer k-Raum-Abtastung profitieren k\u00f6nnen.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6684\nurn:nbn:de:bvb:29-opus4-66842\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-66842\nhttps://opus4.kobv.de/opus4-fau/files/6684/Diss_Grimm_FINAL.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6722\n2018-11-12\ndoc-type:report\nbibliography:false\nddc\nddc:000\nccs\nccs:D.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech\nInterrupt Handling in Linux\nRothberg, Valentin\nRothberg, Valentin\nInterrupt\nLINUX\nBetriebssystem\nddc:000\nAn interrupt is an event that alters the sequence of instructions executed by a processor and requires immediate attention. When the processor receives an interrupt signal, it may temporarily switch control to an inter- rupt service routine (ISR) and the suspended process (i.e., the previously running program) will be resumed as soon as the interrupt is being served. The generic term interrupt is oftentimes used synonymously for two terms, interrupts and exceptions. An exception is a synchronous event that occurs when the processor detects an error condition while executing an instruction. Such an error condition may be a devision by zero, a page fault, a protection violation, etc. An interrupt, on the other hand, is an asynchronous event that occurs at random times during execution of a pro- gram in response to a signal from hardware. A proper and timely handling of interrupts is critical to the performance, but also to the security of a computer system.\nIn general, interrupts can be emitted by hardware as well as by software. Software interrupts (e.g., via the INT n instruction of the x86 instruction set architecture (ISA)) are means to change the execution context of a program to a more privileged interrupt context in order to enter the kernel and, in contrast to hardware interrupts, occur synchronously to the currently running program. Consequently, such instructions are gates to the privileged operating-system kernel (e.g., ring 0 on x86) and thus are used to request services from the kernel (i.e., system calls). A hardware-triggered interrupt indicates that some device requires attention of the processor, and hence implements a means of communication between devices and the operating system. Interrupts from hardware can greatly improve a system\u2019s performance when devices send interrupts (e.g., a keystroke on a keyboard) instead of expensive polling of devices (e.g., periodically polling a keyboard for stroked keys). Furthermore, hardware-emitted interrupts by timers are used for timing and time measurement in general, but also for time sharing as ticks can be used to schedule another task.\nAfter arrival of an interrupt, the processor executes the interrupt ser- vice routine that is associated with this interrupt. The ISR is also referred to as the interrupt request (IRQ) handler. But as interrupts are disabled during execution of the IRQ handler, not all kinds of code can or should be executed in this IRQ context. For instance, if such routine goes to sleep with interrupts disabled, the system is likely to freeze. On the other hand, active waiting and blocking should be avoided since other interrupts with potentially urgent needs remain disabled and, hence, cannot be served. Furthermore, acquiring locks is likely to cause deadlocks. As a consequence, the Linux kernel offers various mechanisms and application programming interfaces (APIs) to implement interrupt handling in order to meet certain functional and non-functional requirements.\nThis report focuses on how the Linux operating-system kernel handles interrupts on the software side and aims to give brief background infor- mation as well as implementation details. Detailed information about exceptions and exception handling in the x86 architecture can be found in the CPU manual \u201dIntel 64 and IA-32 Architectures Software Developer\u2019s Manual\u201d. Notice that the report does not aim for completeness, neither does it target to introduce the general concept of interrupts. It rather tries to provide information for developers, researchers and students, familiar with operating systems and operating-system concepts, how Linux handles interrupts on the software side.\n2015\nreport\ndoc-type:report\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6722\nurn:nbn:de:bvb:29-opus4-67221\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67221\nhttps://opus4.kobv.de/opus4-fau/files/6722/report.pdf\neng\nhttps://creativecommons.org/licenses/by-nc/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6724\n2018-07-11\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:004\nccs\nccs:H.\npacs\npacs:00.00.00\nmsc\nmsc:68-XX\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nEntwicklung und Anwendung eines Modells zur Untersuchung soziotechnischer Faktoren bei der Einf\u00fchrung neuer Informationssysteme im klinischen Bereich\nA framework to evaluate human and technical concerns when introducing IT systems to healthcare clinics\nVollmer, Anne-Maria\nMensch-Computer-Interaktion\nSoziotechnik\nddc:004\nHintergrund und Ziele\nViele gro\u00dfe IT-Implementierungen im klinischen Bereich scheitern, es wird von einer Rate von bis zu 70% ausgegangen [273]. Die oftmals kommerziellen Systeme enthalten formale und standardisierte Konzepte, welche nicht der Arbeit im spezifischen klinischen Umfeld entsprechen. Zum Teil sind die Systeme f\u00fcr andere organisatorische Rahmenbedingungen, Anwendungsf\u00e4lle und Gesetzgebungen entwickelt und entsprechen nicht der Organisation-/einheit in der sie um Einsatz kommen. Als Folge k\u00f6nnen diese Systeme schwer implementiert werden, die klinischen Anwender wie \u00c4rzte und Pflegekr\u00e4fte lehnen die Nutzung ab. Die vorliegende Arbeit entwickelt ein soziotechnisches Evaluationsmodell um die zugrunde liegenden technischen, organisatorischen und anwenderbezogenen Einflussfaktoren schon w\u00e4hrend der IT-Implementierungen zu identifizieren. Mithilfe der konstanten Evaluation w\u00e4hrend der Einf\u00fchrung sollen Erfolg bzw. Misserfolg der Implementierung erkl\u00e4rt und valide Daten f\u00fcr die Weiterentwicklung der IT sowie der Organisationsstrukturen geliefert werden. Das Modell wurde in einer empirischen Studie zur Evaluation der Umstellung auf eine elektronische Pflegedokumentation am Universit\u00e4tsklinikum (UK) Erlangen eingesetzt.\nMethoden\nDas Modell wurde aufbauend auf einer Literaturstudie entwickelt. Die gefundene Literatur wurde in drei Kategorien klassifiziert: Theorien, welche beschreiben wie Technologie im klinischen Bereich implementiert werden, Studien, welche die Anforderungen f\u00fcr erfolgreiche Implementierungen und potentielle Hindernisse aufzeigen und Studien, welche Methoden f\u00fcr die Evaluation von IT-Implementierungen liefern. Die verschiedenen Aspekte wurden in einem neuen Evaluationsmodell zusammengef\u00fchrt, welches Prinzipien f\u00fcr die prozessbegleitende Evaluation erschlie\u00dft. Das Modell wurde eingesetzt, um den Ver\u00e4nderungsprozess von der papiergest\u00fctzten hin zu einer elektronisch gef\u00fchrten Pflegedokumentation am UK Erlangen zu untersuchen. Die Evaluation erfolgte \u00fcber einen Zeitraum von zwei Jahren auf den f\u00fcnf beteiligten Pilotstationen und im Rahmen der Projektarbeit mithilfe qualitativer und quantitativer Methoden.\nErgebnisse\nDas entwickelte Modell beantwortet die Fragen des \u201dWann soll evaluiert werden?\u201d, \u201dWelche Faktoren k\u00f6nnen potenziell Einfluss nehmen und untersucht werden?\u201d, \u201dWelche Sichtweisen k\u00f6nnen evaluiert werden?\u201d und \u201dWelche Methoden stehen zur Verf\u00fcgung?\u201d. Die empirische Studie am UK Erlangen hat gezeigt, dass der Erfolg oder Misserfolg von neuer IT im klinischen Bereich sich nicht mit wenigen Faktoren erkl\u00e4ren l\u00e4sst. Was die Einf\u00fchrung misslingen oder gelingen l\u00e4sst, wird von multiplen Faktoren bestimmt, die sich w\u00e4hrend der Einf\u00fchrung \u00e4ndern: Einstellungen und Erwartungen der Pflegekr\u00e4fte, die Konzeption der Benutzeroberfl\u00e4che, fehlende Funktionalit\u00e4ten,unzureichende Umsetzung von Anwenderanforderungen, fachspezifische Abl\u00e4ufe und viele andere. Die im Modell vorgesehene breite Untersuchung des Zusammenspiels von Anwender, Arbeitsorganisation und IT, die kontinuierliche Evaluation, sowie der Einsatz der methodischen Triangulation haben sich in der Studie bewiesen, um das Nutzungsverhalten der Anwender zu erkl\u00e4ren.\nPraktische Schlussfolgerungen\nIn der Vergangenheiten haben viele Forschungsarbeiten versucht, den Erfolg bzw. die Nutzung von klinischen IT-Systemen mit einem definierten Set an Einflussfaktoren vorauszusagen. Es wurde beispielsweise die \u201dAngst Fehler zu machen\u201d als Einflussfaktor untersucht und quantifiziert,\nwie stark dieser einzelne Faktor auf die Systemakzeptanz wirkt. Nach den Erfahrungen in unserer Studie ist dieses Vorgehen nicht zielf\u00fchrend. Wir empfehlen die umfassende Analyse der Arbeitsorganisation mit einem wie von uns entwickelten Rahmenwerk. Die Einflussfaktoren sind zu vielf\u00e4ltig, kontextspezifisch und unvorhersebar. Nach der Untersuchung anhand des entwickelten Rahmenwerkes, welches die wirkenden Einflussfaktoren offen legt, k\u00f6nnen diese Faktoren im weiteren Studienverlauf auf ihre Einflussst\u00e4rke hin untersucht werden. Als Schlussfolgerung sehen wir zudem, dass die prozessbegleitende Evaluation ausreichend Personalressourcen oder sich weniger aufwendigen Forschungsmethodiken bedienen muss, um ein zeitnahes Feedback zu gew\u00e4hrleisten.\nDie Evaluation der technischen Produktentwicklung sehen wir als essenziell. Das eingekaufte Softwareprodukt wurde in unserer Studie stark hausintern angepasst. Nach dieser Erfahrung muss zur Interpretation des Erfolgs und des Nutzungsverhaltens die tats\u00e4chliche Ausgestaltung des Produkts bekannt sein. In der Studie der elektronischen Pflegedokumentation wurde deutlich, dass die Dokumentation flexibel gestaltet sein muss. Die Dokumentation der verschiedenen Schritte muss an die Patientenbed\u00fcrfnisse und die Expertise der Pflegekraft anpassbar sein. Das System\nbeispielsweise sieht eine Planung von allen pflegerischen Ma\u00dfnahmen vor, dies entspricht oft nicht der pflegerischen Wirklichkeit. Bekommt ein Patient pl\u00f6tzlich Fieber, m\u00fcssen Ma\u00dfnahmen auch ungeplant dokumentiert werden. Die angebotenen Inhalte m\u00fcssen fachbereichsspezifisch oder sogar\nstationsspezifsch angepasst sein und die Benutzeroberfl\u00e4che muss den Tagesablauf der Pflege widerspiegeln, um die Benutzerfreundlichkeit zu gew\u00e4hrleisten.\nBackground and Objectives\nNearly 70% of major IT implementations in clinics fail [273]. These mostly commercial IT systems are built on requirements that are distanced from practical scenarios in a clinic. They are often developed for different organizational contexts and use cases, or are beholden to laws that do not apply to the organization in which they are deployed. In addition to being difficult to implement,\nphysicians and nurses often do not use them. We develop a socio-technical model that evaluates technical, organizational, and user factors during an IT deployment. Through constant evaluation, it can detect early problems and risks, and glean data to explain an eventually successful or unsuccessful deployment. The model has been validated with this objective in an empirical study in evaluating how nurses document processes electronically at Erlangen University Hospital.\nMethods\nWe created the model by classifying literature into three categories: studies showing how technology is implemented in the clinic, studies that show the requirements for successful deployment and possible pitfalls, and studies that show methods for evaluating a deployment\u2019s outcome. The model is based on aspects found in the literature which describe the process and methods of evaluating IT deployments. We used the model to evaluate how nurses shifted towards documenting their processes electronically. Our work was conducted at Erlangen University Hospital over two years at five participating wards, using qualitative and quantitative methods.\nResults\nThe model we created is grounded in extensive questions: when to evaluate a process, what methods to use in evaluation, which of a myriad of factors to evaluate, and whose perspectives to consider about a new process. Through the Erlangen study, we found that it is impossible to determine why an IT deployment succeeds or fails in a clinic when only considering a few factors. What makes or breaks a deployment varies wildly: attitudes and expectations of nurses, poor user interfaces, malfunctioning software, an inability to implement user requirements, and a fear of and lack of support from the IT department, among others. In the Erlangen study, our methodology allowed us to predict how nurses would use the system.\nPractical Conclusions\nIn the past, researchers have attempted to describe why IT deployments in the clinic fail or succeed by hypothesizing about a handful of factors. They might choose \u201cfear of making mistakes,\u201d as one factor, and take measurements to quantify how much it contributes or detracts from a deployment\u2019s success. They restrict themselves to be able to compare results with other studies scientifically. We believe this approach is ineffective; instead, we recommend a catch-all evaluative framework like we have developed. Influencing factors are simply too numerous and unpredictable to prune before a deployment. After the model has been used to show which factors are and are not important, a finer research study can delve into how much the now-identified factors contribute to a project\u2019s success.\nStaff resources should be increased, so evaluators can iterate quickly on factors that prove themselves useful or useless during the evaluation. During deployment, software grows dissimilar from its original state as it conforms to user requirements, so describing the transformation is essential to interpreting the results. In the Erlangen study, we found that nursing process documentation must be flexible. Which steps to document depends on the nurse\u2019s expertise and the specific patient. IT systems often force nurses into a planned day of care for a patient, but this contradicts reality. A patient may suddenly catch fever, and an unplanned event like this must also be documented. The user interface must likewise reflect the daily care routine. Finally, documentation catalogs should be pared down by department, as the verbosity is otherwise overwhelming to nurses.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6724\nurn:nbn:de:bvb:29-opus4-67243\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67243\nhttps://opus4.kobv.de/opus4-fau/files/6724/Dissertation_final_Vollmer.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6753\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:629\npacs\npacs:47.11.Df\nmsc\nmsc:76-04\ninstitutes\ninstitutes:Tech_Chemieing\nEntwicklung und Anwendung eines semi-impliziten Kopplungsverfahrens zur numerischen Berechnung der Fluid-Struktur-Wechselwirkung in turbulenten Str\u00f6mungen mittels Large-Eddy Simulationen\nDevelopment and application of a semi-implicit coupling approach for the numerical computation of fluid-structure interaction in turbulent flows using large-eddy simulations\nM\u00fcnsch, Manuel\nFluid-Struktur-Wechselwirkung\nddc:629\nIn vielen Bereichen der Ingenieurwissenschaften ist das Mehrfeldproblem der Fluid-Struktur-Wechselwirkung von gro\u00dfer Bedeutung, wobei die\nStr\u00f6mungsfelder zumeist von turbulenter Natur sind. Zur Berechnung dieser Str\u00f6mungsfelder ist das Verfahren der Large-Eddy\nSimulation in Verbindung mit einem expliziten Pr\u00e4diktor-Korrektor-Verfahren ein zu bevorzugender Ansatz, welcher\ngeeignet mit dem Strukturfeld zu koppeln ist. Das \u00fcbergeordnete Ziel der vorliegenden Arbeit ist die Entwicklung\neines neuartigen Kopplungsansatzes zur effektiven Berechnung der Fluid-Struktur-Wechselwirkung in turbulenten Str\u00f6mungen mittels\nLarge-Eddy Simulationen (LES).\nIn der vorliegenden Arbeit erfolgt zun\u00e4chst eine Einf\u00fchrung in die Thematik der Fluid-Struktur-Wechselwirkung. Dazu werden die grundlegenden Anregemechanismen\nanhand von Fallbeispielen beschrieben und ein Literatur\u00fcberblick \u00fcber Modellierungs- und Berechnungsans\u00e4tze gegeben.\nAusgehend von den Erhaltungsgleichungen der Kontinuumsmechanik erfolgt eine Vorstellung der hier ber\u00fccksichtigten Erhaltungsgleichungen\nf\u00fcr jedes der Teilfelder, d.h. der Str\u00f6mungs- und Strukturmechanik. Hinsichtlich der Str\u00f6mungsmechanik wird in\ndiesem Zusammenhang das Verfahren der Large-Eddy Simulation basierend auf einer impliziten Filterung detailliert vorgestellt.\nEin wesentlicher Gesichtspunkt ist die Vereinbarkeit der zugrundeliegenden ALE-Formulierung\n(Arbitrary Lagrangian-Eulerian) bzw. eines bewegten Rechengitters und dem Verfahren der LES. Die Bewegung des\nRechengitters induziert eine r\u00e4umlich und zeitliche Variation der Filterweite, was zu zus\u00e4tzlichen\nCommutation-Fehlern und einer Reduktion der Qualit\u00e4t der Str\u00f6mungsvorhersage f\u00fchrt und im weiteren Verlauf der\nArbeit am Beispiel eines Testfalls untersucht wird.\nDie Berechnung der Str\u00f6mung basiert hierbei auf dem Str\u00f6mungsl\u00f6ser FASTEST-3D.\nDer Str\u00f6mungsl\u00f6ser basiert auf der dreidimensionalen Finite-Volumen-Methode und wird im Rahmen dieser Arbeit f\u00fcr den Fall der\nexpliziten Zeitdiskretisierung um eine ALE-Formulierung der Erhaltungsgleichung unter Ber\u00fccksichtigung\ndes Raumerhaltungsgesetzes erweitert. Im Rahmen des Pr\u00e4diktor-Korrektor-Verfahrens zweiter Ordnung wird die\nPr\u00e4diktion einer intermedi\u00e4ren Str\u00f6mungsgeschwindigkeit \u00fcber ein explizites, low-storage Runge-Kutta-Verfahren durchgef\u00fchrt,\nwelches die Impulserhaltungsgleichung in der Zeit integriert. Die Korrektur\nder intermedi\u00e4ren Str\u00f6mungsgeschwindigkeit und des Druckes zur Erf\u00fcllung der Massenerhaltung erfolgt \u00fcber eine Poisson-Gleichung,\nwas im Detail in der vorliegenden Arbeit beschrieben wird.\nZur Berechnung des Strukturfeldes wird auf die Methode der Finite-Elemente f\u00fcr die Berechnung einer flexiblen Struktur zur\u00fcckgegriffen. Die eigentliche Berechnung\nwird \u00fcber das Programmpaket Carat (LS Statik, TU M\u00fcnchen) durchgef\u00fchrt. Die zeitliche Integration erfolgt in diesem Fall\n\u00fcber eine Formulierung des Newmark-Verfahrens f\u00fcr nicht-lineare F\u00e4lle.\nEinige der anvisierten Testf\u00e4lle lassen sich auf Seiten der Struktur auf Starrk\u00f6rperbewegungen reduzieren. Die zeitliche Integration\nwird \u00fcber ein explizites Runge-Kutta-Verfahren oder ebenso ein nicht-lineares Newmark-Verfahren realisiert. Die im Rahmen dieser Arbeit\nimplementierten L\u00f6sungsans\u00e4tze werden anhand einer Modellgleichung validiert.\nZur Kopplung von Str\u00f6mungs- und Strukturfeld werden zun\u00e4chst die Kopplungsbedingungen und m\u00f6gliche L\u00f6sungsans\u00e4tze\ndiskutiert. Der Hauptgesichtspunkt ist hierbei die Entwicklung eines neuartigen partitionierten Kopplungsalgorithmus,\nwelcher die Vorteile eines expliziten Pr\u00e4diktor-Korrektor-Verfahrens f\u00fcr das Fluidfeld mit den\nStabilit\u00e4tseigenschaften impliziter Kopplungsans\u00e4tze zur Berechnung der Fluid-Struktur-Wechselwirkung verbindet. Das resultierende semi-implizite\nKopplungsverfahren ist dadurch charakterisiert, dass der Korrektor-Schritt, d.h. die Poisson-Gleichung, des Fluidfeldes\nim Rahmen einer Subiterationsschleife mit dem Strukturl\u00f6ser gekoppelt wird. Hierbei wird so lange iteriert, bis ein vorgeschriebenes\nKonvergenzkriterium unterschritten wird. Hierdurch wird die Anforderung nach einem expliziten Zeitschrittverfahren im Kontext der LES\nund einem stabilen Kopplungsalgorithmus erf\u00fcllt. Weitere Aspekte der Kopplung sind hierbei die Pr\u00e4diktion und Unterrelaxation der Verschiebungen zur\nStabilisierung und Beschleunigung der Berechnung und die die verwendeten Konvergenzkriterien. Aufgrund unterschiedlicher r\u00e4umlicher\nDiskretisierungen der Teilfelder sind des Weiteren Lasten und Verschiebungen an der Strukturoberfl\u00e4che zwischen den jeweiligen\nRechengittern zu interpolieren. Ein weiterer diskutierter Gesichtspunkt stellt\ndie informationstechnische Realisierung der Kopplung zwischen Str\u00f6mung und Struktur bzw. zwischen zwei entsprechenden\nBerechnungsprogrammen, hier FASTEST-3D und Carat, dar. Der Datentransfer und die Dateninterpolation wird\nhier \u00fcber die Kopplungsschnittstelle CoMA (LS Statik, TU M\u00fcnchen) realisiert, welche im Kern auf eine parallele\nProzessierung mittels MPI (Message Passing Interface) aufbaut. Dies erm\u00f6glicht die Nutzung von Mehrkern-Architekturen oder HPC-Systemen.\nNachfolgend befasst sich die Arbeit mit der Berechnung von Testf\u00e4llen. Zun\u00e4chst richtet sich hierbei der Fokus auf die Qualit\u00e4t der\nLES auf bewegten Rechengittern, wie bereits oben beschrieben. Der Einfluss des induzierten Commutation-Fehlers wird hierzu\nam Testfall einer Kanalstr\u00f6mung bei einer auf die Wandschubspannungsgeschwindigkeit bezogenen Reynolds-Zahl von 590 untersucht. Zun\u00e4chst\nwerden hierzu Berechnungen ohne eine aufgepr\u00e4gte Gitterbewegung durchgef\u00fchrt. F\u00fcr die untersuchten LES-Feinstrukturmodelle zeigt\nsich eine gute \u00dcbereinstimmung hinsichtlich der Str\u00f6mungsgeschwindigkeit, der Wandschubspannungsgeschwindigkeit und den Statistiken\nzweiter Ordnung mit DNS- und LES-Referenzdaten, was als erfolgreiche Validierung des Pr\u00e4diktor-Korrektor-Verfahrens und der\nimplementierten LES-Feinstrukturmodelle zu sehen ist. Bei einer aufgepr\u00e4gten, sinusf\u00f6rmigen Gitterbewegung mit vorgegebener Amplitude\nund Periode zeigt sich, dass der induzierte Gesamtfehler hinsichtlich der gemittelten Str\u00f6mungsgeschwindigkeit und der Statistiken zweiter Ordnung f\u00fcr\nzunehmende Amplituden und kleiner werdende Perioden im Vergleich zum unbewegten Referenzfall zunimmt. F\u00fcr moderate Gitterbewegungen liegen\ndie Abweichungen in einem tolerablen Bereich, was somit grunds\u00e4tzlich die Anwendung der LES im Kontext der Fluid-Struktur-Wechselwirkung unter Ber\u00fccksichtigung\nder ALE-Formulierung der Erhaltungsgleichungen erm\u00f6glicht. Ausgehend von dieser Feststellung, wird der entwickelte semi-implizite Kopplungsansatz im\nRahmen von drei numerischen und zwei experimentellen Testf\u00e4llen f\u00fcr laminare und\nturbulente Str\u00f6mungsregime untersucht und validiert. Die Reynolds-Zahl reicht hierbei von 200 bis 68000. Die Fluid-Struktur-Wechselwirkung\nist f\u00fcr die Testf\u00e4lle durch den Mechanismus der bewegungs-induzierten oder der instabilit\u00e4ts-induzierten Anregung dominiert.\nF\u00fcr den semi-impliziten Kopplungsalgorithmus zeigen sich insbesondere an den Ergebnissen zum Benchmark-Testfall FSI3\nder DFG Forscherguppe 493 sehr gute \u00dcbereinstimmungen mit den Referenzwerten hinsichtlich der ermittelten Schwingungsamplituden und Frequenzen der\nStruktur. Am Beispiel experimenteller Testf\u00e4lle von Gomes et al. werden des Weiteren Parameterstudien zum Einfluss der Pr\u00e4diktion der Verschiebungen im Rahmen der Kopplung,\nder LES-Feinstrukturmodelle, der Modellkonstanten und der Randbedingungen im Fluidfeld auf die L\u00f6sung des Mehrfeldproblems durchgef\u00fchrt.\nHinsichtlich des wichtigen Aspektes der Feinstrukturmodellierung wird gezeigt, dass Frequenz und Amplitude der resultierenden Strukturbewegung von\nden LES-Feinstrukturmodelle bzw. der Wahl der Modellkonstanten abh\u00e4ngen. Dieses Verhalten wird urs\u00e4chlich\nauf die Beeinflussung des Umverteilungsprozesses der Wirbelst\u00e4rke zur\u00fcckgef\u00fchrt, was die St\u00e4rke der Druckmaxima und -minima, induziert durch\nan der Struktur abschwimmende Wirbel, beeinflusst und somit Auswirkungen auf die resultierende Strukturbewegung hat.\nFluid-structure interaction (FSI) plays an important role in many fields of engineering\nsciences. In addition, most of the technically relevant flows are dominated by turbulence.\nFor the prediction of a turbulent flow field in the context of FSI, large-eddy\nsimulation (LES) is a preferential method in combination with an\nexplicit predictor-corrector scheme. The overall objective of the current thesis is the\ndevelopment of a novel coupling approach to enable the efficient prediction of FSI in turbulent\nflows via LES.\nStarting point of the present thesis is an introduction to the fluid-structure interaction phenomenon\ndescribing the excitation mechanisms and giving a literature overview on modeling and numerical\napproaches. Based on the conservation equations of continuum mechanics the formulation of the\ngoverning equations for each particular field, here the fluid and the structure, is presented.\nRegarding the simulation of turbulent flows the concept of large-eddy\nsimulation by the use of implicit filtering is introduced. The underlying ALE (Arbitrary Lagrangian-Eulerian)\nformulation of the governing equations is an important issue for LES relying on implicit filtering.\nMoving grids are causing a variation of the filter width in time and space leading to additional commutation\nerrors. This issue is addressed by a test case described later on.\nFor the computation of the flow field the in-house code FASTEST-3D is used. The code relies on a three-dimensional\nfinite-volume formulation for the spatial discretization of the governing equations\nwhich is extended towards an ALE formulation in the case of explicit time marching\nwithin the present developments. A predictor-corrector scheme of second order is basically used\nto compute the flow field. Within the predictor step an explicit low storage Runge-Kutta scheme\npredicts intermediate flow velocities by advancing the momentum equation in time. The corrector-step\nupdates the intermediate flow velocities and the pressure via a Poisson equation to fulfill mass conservation,\nwhich is described in detail in the current thesis.\nAlso the numerical approaches to solve the governing equations for the structure are described in detail.\nSpatial discretization via finite-elements is used to compute the behaviour of a complex, i.e. a flexible\nstructure. Here, the temporal integration is performed via a non-linear formulation of the Newmark scheme.\nThe computation is done by using the highly developed finite-element solver Carat (Chair of Structural Analysis, TU Munich).\nFor particular test cases the structure can be described by a rigid body approach. The underlying equations are integrated\nin time via an explicit Runge-Kutta scheme or a non-linear implementation of the Newmark scheme. The implemented methods\nare validated against the results of a model equation.\nFor the coupling of the fluid and the structure coupling conditions and solution methods are presented. Here,\nthe main objective is the development of a novel partitioned coupling approach which combines the features of an\nexplicit predictor-corrector scheme for the estimation of the fluid field with the stability properties known\nfrom fully implicit coupling approaches. The resulting semi-implicit coupling approach relies on a coupling\nthe corrector-step of the fluid field and the governing equations for the structure within a sub-iteration loop.\nHere, sub-iterations are performed until a prescribed convergence criterion is fulfilled. Thus, the requirements\nof explicit time-marching favorable for LES on the one hand side and stable coupling on the other hand are met.\nFurther coupling related aspects tackled in this context are named as the prediction and underrelaxation of displacements\nto stabilize and accelerate convergence and the applied convergence criterion. Furthermore interpolation of loads\nand displacements is required since both fields are spatially discretized by different approaches (FEM versus FVM) leading\nto non-matching meshes on the surface of the structure. In addition, the technological realization and implementation of the\ncoupling of two codes, here FASTEST-3D and Carat, has to be considered. For that purpose the coupling interface\nCoMA (Chair of Structural Analysis, TU Munich) is used whereas the data transfer is done via MPI\n(Message Passing Interface). Thus, multi-core or high-performance computing is enabled to solve the multi-field problem.\nIn the following chapter test cases are presented and discussed in detail. The impact of moving grids on the quality of\nLES predictions is investigated by considering a turbulent channel test case at friction velocity based Reynolds number of 590.\nInitial computations of the classical test case without any grid movement deliver satisfactory results for the implemented\nLES models and the implemented predictor-corrector scheme in comparison with DNS and LES reference data sets.\nThe impact of moving grids is addressed by forcing a sinusoidal displacement of the grid points with specific amplitudes\nand periods. With increasing amplitudes and decreasing periods the induced deviations of the mean velocity and the second-order\nmoments increase compared to the fixed grid case. For moderate grid movement the impact on the flow field turns out to be\nless significant. Thus, the application of LES in the context of FSI and ALE formulation of the governing equations\nis possible for moderate grid movements. Starting from this results, the behaviour of the developed semi-implicit coupling\napproach is evaluated basing on three numerical and two experimental test cases\nin laminar as well as in turbulent flow regimes. The Reynolds number for this cases ranges from 200 up to 68000 and the\nFSI is dominated by the moving induced or instability induced excitation mechanism.\nThe semi-implicit coupling algorithm delivers satisfactory results, especially for the benchmark test case FSI3\nof DFG Research Group 493, regarding oscillation amplitudes and frequencies of the structures compared to reference\nvalues. The impact of the FSI predictor within the coupling scenario, the LES models and\nmodel constants and the boundary conditions on the solution of the multi-field problem\nis investigated basing on experimental test cases of Gomes et al..\nA core aspect of the current thesis is the LES modeling in the framework of FSI. Basing on the present investigations LES models and model constants\nhave a strong influence on the resulting oscillation amplitudes and frequencies of the structure. The behaviour is explained due to the\nrelation between LES modeling and the process of vorticity redistribution caused by vortex tilting. This influences the induced pressure\nzones of separating vortices. These pressure zones are in turn responsible for the movement of the structure.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6753\nurn:nbn:de:bvb:29-opus4-67533\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67533\nhttps://opus4.kobv.de/opus4-fau/files/6753/150521_DissMT.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-nd/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6756\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:005\nccs\nccs:D.m\nccs:E.m\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nSemantikerhaltende Anfrageverteilung in f\u00f6derierten Datenstromumgebungen\nSemantics-Preserving Query Distribution in Federated Data Stream Environments\nLauterwald, Frank\nDatenstrommanagementsystem\nddc:005\nDatenstromsysteme (DSSe) haben in den vergangenen Jahren ihren Weg aus der Forschung\nin den betrieblichen Einsatz gefunden. Dabei sind bisher nur geringe Anstrengungen zu\nderen Standardisierung erkennbar - und noch weniger Erfolge. Somit ist davon auszuge-\nhen, dass der Einsatz verschiedener DSSe mittelfristig Realit\u00e4t bleibt. Allerdings ist die\nIntegration verschiedener DSSe f\u00fcr ein spezielles Einsatzszenario sehr arbeitsintensiv.\nEine L\u00f6sungsm\u00f6glichkeit sind f\u00f6derierte Datenstromsysteme, die eine Abstraktionsschicht\noberhalb der realen Systemimplementierungen darstellen und somit deren Unterschiede\nvor dem Anwender verbergen. Eine solche F\u00f6deration wird dadurch erschwert, dass sich\ndie heute verf\u00fcgbaren Datenstromsysteme nicht nur durch die Syntax ihrer Anfragespra-\nchen, sondern auch hinsichtlich ihrer Verarbeitungslogik unterscheiden. Das zeigt sich\ndarin, dass f\u00fcr vermeintlich gleiche Anfragen unterschiedliche Ergebnisse erzeugt wer-\nden bzw. Ergebnisstr\u00f6me unterschiedliches zeitliches Verhalten aufweisen. Der m\u00f6glichen\nAbweichungen muss sich der Anwendungsentwickler bewusst sein und er muss vorgeben\nk\u00f6nnen, welche davon er in Kauf nehmen will. Diese Arbeit beschreibt einen Ansatz, der\nes dem Anwendungsentwickler wahlweise erlaubt, pr\u00e4zise zu definieren, wie eine Anfra-\nge verarbeitet werden soll oder dem System bestimmte Teilaspekte freizustellen, um so\nOptimierungspotentiale zu nutzen.\nDiese Arbeit orientiert sich an den konkreten Anforderungen des Data Stream Application\nManager (DSAM)-Projektes und greift dessen grunds\u00e4tzliches Einsatzszenario auf. Insbe-\nsondere ist dies die automatische Verteilung globaler Datenstromanfragen auf ein Netzwerk\nheterogener Datenstromsysteme. Die entwickelten Verfahren wurden daher weitgehend in\nDSAM integriert.\nIn recent years Stream-Processing Systems (SPSs) made their way from research to\ncommercial use, yet standardization still lacks behind. Consequently, we expect to see\na variety of different SPSs in larger organizations. The integration of multiple SPSs is\ncurrently a labor-intensive task.\nFederated Data-Stream Systems provide a possible solution to this problem. They con-\nstitute an abstraction layer on top of individual SPSs\u2019 interfaces. This abstraction layer\nhides the differences between different SPSs from users. Federation is hindered by the fact\nthat existing data-stream systems differ not only in their query languages but also in their\nexecution logic. Queries that seem equivalent may produce different results or results with\ndifferent temporal behavior. Application developers need to be aware of these differences.\nAdditionally, they must be enabled to specify they expectations of a querie\u2019s behavior.\nThis thesis describes an approach that allows users to describe data-stream queries in\na platform-independent way. Application developers may either precisely describe the\nexpected behavior of a query or allow the federation to make some choices itself. Leaving\nchoices to the federation allows to employ certain performance optimizations.\nThis thesis is part of the DSAM-project which develops a federated data-stream system.\nDSAM\u2019s primary usecase is the automatic distribution of global data-stream queries to\na network of heterogeneous data-stream systems. Consequently, most of the proposed\ncomponents were prototypically integrated into DSAM.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6756\nurn:nbn:de:bvb:29-opus4-67566\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67566\nhttps://opus4.kobv.de/opus4-fau/files/6756/LauterwaldDissertation2015.pdf\nhttps://opus4.kobv.de/opus4-fau/files/6756/TitelseiteLauterwald.pdf\ndeu\nhttps://creativecommons.org/licenses/by-nc-sa/3.0/de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6769\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:000\nccs\nccs:I.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nSystem-Level Power and Performance Estimation in Early SoC Design Phases\nLeistungsverbrauchs- und Performance-Sch\u00e4tzung auf Systemebene in fr\u00fchen Phasen der System-on-Chip Entwicklung\nXu, Yang\nsystem-level power modeling\nsystem-level performance modeling\nSoC design\nvariation-aware leakage modeling\nddc:000\nIn the last decade, System-on-Chip (SoC) centralized system design has essentially become a fundamental methodology in modern electronic system design and manufacturing industry. This methodology enables manufacturers to save Bill of Material (BoM) and to shorten the increasing competitive Time-to-Market (TTM) significantly. At early SoC design phases, System-level SoC design methodologies are often applied to evaluate different system architecture options and consequently to identify optimal system architecture options. However, at such early design phases, there exist a lot of uncertainties coming from both SoC design and manufacturing. How to handle these uncertainties during architecture evaluation at early design phases still remains a big challenge for modern system-level SoC design methodologies. This work proposes several novel methods to handle such uncertainties. Specifically, it first proposes a system-level task-accurate power-state-based power modeling methodology to enable efficient evaluation of power and performance for different dynamic power management policies and architecture options. This method can be applied at very early SoC design phases, even before detailed hardware and software design information is available. To use task-accurate performance estimation method to guide also time critical SoC designs, a robust task-accurate performance estimation method is proposed next. To incorporate the effects of process variations in system-level SoC power estimation, a hierarchical statistical leakage analysis method is subsequently proposed. Finally, to integrate the hierarchical statistical leakage analysis into a floorplan optimization framework, a method to accelerate leakage yield calculation is proposed.\nAufgrund kontinuierlicher Forschritte der Mikroelektronik in der letzten Dekade geh\u00f6ren Systementwurfsmethoden f\u00fcr Systems-on-Chip (SoC), in denen alle Systemkomponenten in einer einzigen integrierten Schaltung implementiert werden, mittlerweile zu den Schl\u00fcsseltechnologien f\u00fcr moderne elektronische Systeme. Effiziente Entwurfsmethoden erm\u00f6glichen es den Systemherstellern, sowohl die Produktkosten (Bill of Material \u2013 BoM) zu senken als auch die Produktentwicklungszyklen (Time to Market \u2013 TTM) zu verk\u00fcrzen und somit ihre Wettbewerbsf\u00e4higkeit zu erh\u00f6hen. In fr\u00fchen Phasen der SoC Entwicklung werden verschiedene Systemarchitekturvarianten oft mit Hilfe von Entwurfsmethoden auf Systemebene evaluiert, um die im Hinblick auf bestimmte Kenngr\u00f6\u00dfen, wie Leistungsverbrauch oder Performance, optimale Systemarchitektur zu finden. In fr\u00fchen Phasen der Entwicklung gibt es naturgem\u00e4\u00df viele Unbekannte, die im Detail erst in sp\u00e4teren Phasen gekl\u00e4rt werden, insbesondere der quantitative Einfluss der SoC Implementierung oder der Halbleiterfertigungsprozesse auf diese Kenngr\u00f6\u00dfen. Diese Unbekannten stellen in fr\u00fchen Phasen der SoC Entwicklung, bei der Exploration von Systemarchitekturoptionen, nach wie vor eine gro\u00dfe Herausforderung f\u00fcr moderne SoC Entwurfsmethoden auf Systemebene dar. Diese Arbeit stellt mehrere neuartige Verfahren zur Handhabung dieser Unsicherheiten vor. Im ersten Teil wird eine neue Task-genaue Methodik zur Modellierung des Leistungsverbrauchs auf Systemebene eingef\u00fchrt, die auf der Abstraktionsebene von Systemleistungszust\u00e4nden aufsetzt. Sie erlaubt sowohl den effizienten Vergleich des Leistungsverbrauchs und der System-Performance bei verschiedenen Verfahren zur dynamischen Leistungssteuerung als auch die Analyse unterschiedlicher Architekturvarianten f\u00fcr jedes dieser Verfahren. Diese Methodik kann bereits in sehr fr\u00fchen Phasen der SoC Entwicklung angewandt werden, bevor Hardware- oder Software-Implementierungsdetails verf\u00fcgbar sind. Im zweiten Teil wird diese Methodik um eine robuste Task-genaue Absch\u00e4tzung der System-Performance erweitert, die dar\u00fcber hinaus f\u00fcr SoCs mit strengen Echtzeitanforderungen eingesetzt werden kann. Eine neue hierarchische und statistische Methodik zur Analyse der Leckstr\u00f6me wird im dritten Teil vorgestellt. Mit ihrer Hilfe l\u00e4sst sich der Einfluss der Streuung der Halbleiterfertigungsprozesse bei der SoC Leistungsverbrauchs-Sch\u00e4tzung auf Systemebene ber\u00fccksichtigen. Abschliessend wird eine verbesserte, schnelle Berechnung der Produktionsausbeute unter Ber\u00fccksichtigung der Streuung der Leckstr\u00f6me vorgeschlagen, welche die Integration der neuen hierarchischen und statistischen Leckstrom Analyse zur Optimierung des Floorplans in einen ansonsten etablierten SoC Implementierungsablauf erm\u00f6glicht.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6769\nurn:nbn:de:bvb:29-opus4-67692\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67692\nhttps://opus4.kobv.de/opus4-fau/files/6769/YangXuDissertation.pdf\neng\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6771\n2018-01-26\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:617\nccs\nccs:E.\npacs\npacs:00.00.00\nmsc\nmsc:00Axx\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Med\nKnochenneubildung und -remodelling bei der Verwendung von biofunktionalisierten Implantatoberfl\u00e4chen\nKnochenneubildung und -remodelling bei der Verwendung von biofunktionalisierten Implantatoberfl\u00e4chen\nKranich, Lena\nbiofunctional\nddc:617\n1 Zusammenfassung/ Abstract\n1.1 Zusammenfassung\n1.1.1 Hintergrund und Ziele\nZiel biofunktionalisierter Implantatoberfl\u00e4chen ist es, sich beschleunigend auf die Einheilphase der Implantate auszuwirken. In dieser Studie wurden Hydroxylapatit (HA)-beschichtete und durch Korundbestrahlung und Hochtemperatur-\u00c4tzen mikro-strukturell ver\u00e4nderte Titanimplantate in Bezug auf die Osseointegration und ihren Einfluss auf die Ausrichtung der Kollagenfasern untersucht.\n1.1.2 Material und Methode\nAufgrund des dem Menschen vergleichbaren Knochenstoffwechsels wurde f\u00fcr diese Studie das adulte Hausschwein als Versuchstier gew\u00e4hlt. Es wurden insgesamt 540 Implantate untersucht. Hierf\u00fcr wurden 45 Tieren jeweils 12 Implantate in das Os frontale inseriert. Verwendet wurden hierbei modifizierte Ankylos-A8-Implantate mit einer FRIADENT\u00ae-plus-Oberfl\u00e4che, sowohl unbeschichtet als auch mit einer bio-funktionalisierten Implantatoberfl\u00e4che. Als Opferungszeitpunkte wurden 7, 14 und 30 Tage gew\u00e4hlt. Um Aussagen \u00fcber die Osseointegration treffen zu k\u00f6nnen, wurden die gewonnenen Knochenproben histomorphometrisch und immunhistochemisch untersucht. Da viele Quellen einen Zusammenhang zwischen Kraftvektor und Kol-lagenfaserorientierung beschreiben, wurde in dieser Studie die Ausrichtung dieser Fasern entlang unbelasteter Implantate analysiert.\n1.1.3 Ergebnisse\nDie Implantate mit Hydroxylapatitbeschichtung erreichten in Bezug auf die Osteozy-tendichte, Kollagenfaserorientierung und Osteoidbildung die h\u00f6chsten Werte. Ledig-lich die Neoangiogenese konnte durch die Verwendung der FRIADENT\u00ae-plus-Oberfl\u00e4che positiv beeinflusst werden. Die unbelasteten Implantate zeigten \u00fcberwie-gend transversale Kollagenfaserverl\u00e4ufe (61,96%- 100%).\n1.1.4 Schlussfolgerung\nDie Osseointegration von Implantaten kann durch die Verwendung einer Hydroxyla-patitbeschichtung verbessert werden. Die Biofunktionalisierung von Implantaten stellt somit eine zukunftsweisende Ma\u00dfnahme zur Optimierung von Einheilvorg\u00e4ngen dar. Ob diese histologisch ermittelten Ergebnisse allerdings eine klinische Relevanz besit-zen, ist jedoch fraglich und sollte in weiteren Studien in einem geeigneten Tiermodell \u00fcberpr\u00fcft werden.\n1.2 Abstract\n1.2.1 Background and objectives\nBiofunctionalized implant surfaces aim to accelerate the initial healing period follow-ing implant placement. In this study, hydroxyapatite-coated titanium implants, the microstructure of which was changed by corundum blasting and high-temperature etching, were examined with respect to osseointegration and with respect to their effect on the orientation of the collagen fibers.\n1.2.2 Material and method\nFor this study, the adult domestic pig was selected as the experimental animal be-cause the bone metabolism of the domestic pig is comparable to that of human be-ings. A total of 540 implants were examined. To that end, 12 implants were inserted in the os frontale of 45 animals. The implants used were modified Ankylos-A8 im-plants with a FRIADENT\u00ae plus surface, both uncoated as well as provided with a biofunctionalized implant surface. The selected examination times were 7, 14 and 30 days. The bone samples were examined histomorphometrically and immunohisto-chemically so as to be able to draw conclusions regarding the osseointegration. As many sources describe the correlation between force vector and collagen fiber ori-entation, in this study the direction of these fibers along unloaded implants was ana-lysed.\n1.2.3 Results\nThe implants with a hydroxyapatite coating obtained the highest values with respect to osteocyte density, collagen fiber orientation, and osteoid formation. The use of the FRIADENT\u00ae plus surface merely had a positive effect on the neoangiogenesis. The unloaded implants primarily displayed transverse collagen fiber directions (61,96%- 100%).\n1.2.4 Conclusion\nThe osseointegration of implants can be improved by applying a hydroxyapatite coating. The biofunctionalization of implants thus constitutes an advanced measure for optimizing the initial healing process. However, whether these histologically de-termined results actually have a clinical relevance is questionable and should be veri-fied through further studies, using a suitable animal model.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6771\nurn:nbn:de:bvb:29-opus4-67715\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-67715\nhttps://opus4.kobv.de/opus4-fau/files/6771/LenaKranichDissertation.pdf\ndeu\nhttp://www.gesetze-im-internet.de/urhg/index.html\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6814\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:540\nccs\nccs:A.\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Nat\nSynthesis of Bimane oligomers\nSynthese von Bimane Oligomeren\nSynth\u00e8se d'oligom\u00e8res \u00e0 base de bimanes\nOerthel, Vincent\nbimane\nddc:540\nThe work described in this thesis is based on a synthetic method for the synthesis of bimane oligomers. This synthetic method relies on the Sonogashira-Hagihara Pd-catalyzed cross-coupling reaction of terminal acetylene with \u03b1-iodobimanes. The main topic of this thesis describes the development of a divergent synthesis allowing control of the elongation growth with the uses of synthetic strategies supported by building blocks with reactive sites.\nChapter 1 offers an introduction to conjugated organic materials, focused on polymers and oligomers. Subsequently, the commonly used polymerization reactions are introduced, and examples of some recent polymers and oligomers are described. Previous synthetic efforts to prepare model compounds for bimane building blocks and oligomers are also discussed.\nChapter 2 describes the synthesis of the bimane building blocks, mainly focused on derivatives decorated with phenyl and propyl-substituents that are used in the course of this thesis. Through the use of these building blocks in the Sonogashira-Hagihara cross-coupling reactions, oligomers of two series with different linkers is realized.\nChapter 3 describes characterizations of the building blocks and the related oligomers, which should provide insight into the behavior of bimane oligomers toward determining the effective conjugation lengths. These molecules are characterized using standard methods, including 1H and 13C NMR, UV-vis, fluorescence, and IR spectroscopies, mass spectrometry, and cyclic voltammetry (CV). The characterization of several building blocks by X-ray crystallography is also discussed.\nChapter 4 concludes this thesis work with a brief summary, an outlook for the synthesis of bimane oligomers, and a description of how bimane might be of interest for the development of useful materials in the synthesis of other oligomers.\nChapter 5 documents the experimental details and data for the precursors and target molecules. Chapter 6 comprises 1H and 13C NMR spectra of selected compounds.\nDie in folgender Dissertation beschriebene Arbeit basiert auf einer Synthesemethode zur Herstellung von Bimanen-Oligomeren. Diese Methode greift zur\u00fcck auf die Sonogashira-Hagihara Pd-katalysierte Kreuzkupplungsreaktion eines terminalen Acetylens mit \u03b1-iodobimanen. Das Hauptaugenmerk dieser Dissertation beschreibt die Entwicklung einer divergenten Synthesemethode, die eine kontrollierbare Verl\u00e4ngerung der Oligomeren erlaubt, basierend auf der Verwendung von Synthesestrategien von Bausteinen mit reaktiven Positionen.\nKapitel 1 beinhaltet eine Einf\u00fchrung in konjugierte organische Materialien und ihre Anwendung in der Photovoltaik mit einem Schwerpunkt auf Poly- und Oligomeren. Folglich werden Polymerisierungsreaktionen sowie einige Beispiele k\u00fcrzlich ver\u00f6ffentlichter Poly- und Oligomere beschrieben. Bisherige Synthesemethoden zur Herstellung von Modellverbindungen f\u00fcr Bimanebausteine und \u2013oligomere werden ebenfalls veranschaulicht.\nKapitel 2 beschreibt die Synthese von Bimanebausteinen, die haupts\u00e4chlich auf Derivaten mit Phenyl- und Propylsubstituenten basieren. Durch die Verwendung dieser Bausteine in der Sonogashira-Hagihara Kreuzkupplungsreaktion wird die Synthese von Oligomeren mit zwei verschiedenen Verkn\u00fcpfungsverbindungen pr\u00e4sentiert.\nKapitel 3 beschreibt die Charakterisierungen der hergestellten Bausteine und der verwandten Oligome re, welche Aufschluss \u00fcber das Verhalten von Bimaneoligomeren bez\u00fcglich der effektiven Konjugationsl\u00e4nge geben. Diese Verbindungen sind mit g\u00e4ngigen Charakterisierungsmethoden, darunter 1H-, 13C-NMR-, IR- und UV-Vis Spektroskopie, Fluoreszenzmessungen, Massenspektrometrie sowie Cyclovoltammetrie (CV) charakterisiert worden. Die Charakterisierung einiger Bausteine mittels R\u00f6ntgenkristallstrukturanalyse wird ebenfalls diskutiert.\nKapitel 4 fasst diese Arbeit kurz zusammen und gibt einen Ausblick auf die Synthese von Bimanoligomeren. Des Weiteren wird das gro\u00dfe Interesse von Bimanen in der Entwicklung von n\u00fctzlichen Materialien in der Synthese anderer Oligomere veranschaulicht.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6814\nurn:nbn:de:bvb:29-opus4-68142\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-68142\nhttps://opus4.kobv.de/opus4-fau/files/6814/Synthesis_of_bimane_oligomers.pdf\neng\nhttps://creativecommons.org/licenses/by/3.0/de/deed.de\ninfo:eu-repo/semantics/openAccess\noai:ub.uni-erlangen.de-opus:6836\n2018-11-12\ndoc-type:doctoralThesis\nbibliography:false\nddc\nddc:006\nccs\nccs:G.1\nmsc\nmsc:65N30\nmsc:65N50\nmsc:65N55\nmsc:65Y05\nmsc:68W10\nopen_access\nopen_access:open_access\ninstitutes\ninstitutes:Tech_Informatik\nData Structures and Algorithms for the Optimization of Hierarchical Hybrid Multigrid Methods\nDatenstrukturen und Algorithmen zur Optimierung hierarchisch-hybrider Mehrgittermethoden\nGradl, Tobias\nMehrgitterverfahren\nGitterverfeinerung\nFinite-Elemente-Methode\nTetraedrische finite Elemente\nHochleistungsrechnen\nBranch-and-Bound-Methode\nddc:006\nMultigrid methods are among the theoretically most efficient\nalgorithms in numerical simulation. They solve certain classes of\nequations - e. g., those arising from finite element (FE)\ndiscretizations - with optimal complexity. Practically relevant for\nlarge-scale simulations, however, are only algorithms that\nexploit the massive parallelism that characterizes today\u2019s\nhigh-performance computing landscape. Implementing multigrid\nmethods efficiently on massively parallel computers is\nchallenging, because for some of the core algorithms the\ndistribution of the numerical operations to many processors is\nnot straightforward.\nBergen et al. proved with the Hierarchical Hybrid Grids (HHG)\nsoftware framework that it is possible to solve FE simulations\nefficiently with multigrid methods on supercomputers. The\ncentral concept of HHG is to discretize the simulated domains\ninto patch-wise structured meshes. It facilitates the\ndistribution of the computational work to many processors, but it\nalso restricts HHG\u2019s flexibility regarding the types of numerical\nproblems it can be applied to.\nThis thesis presents performance studies for FE simulations with\nup to 3 \u00d7 10\u00b9\u00b9 degrees of freedom that demonstrate short time to\nsolution and good scalability of HHG on up to 16384 processor\ncores. We describe the modifications to the initial version of\nHHG - e. g., in the build system and the performance measurement\nmethods - that were necessary in order to execute and study HHG\non systems of this size.\nThe central chapter of the thesis is dedicated to adaptive mesh\nrefinement (AMR). The technique makes HHG applicable to a new\nclass of problems, which is characterized by a strong variance in\nthe required mesh resolution across the domain, e. g., the\nsimulation of room acoustics or turbulent flows. AMR allows for\nthe FE mesh to be tailored flexibly to the simulation\u2019s\ncharacteristics. The multigrid solver can thus spend the\ncomputer\u2019s resources - memory and processor cycles - on areas\nwhere the simulation requires a high resolution. In consequence,\nthe time to solution decreases and the problem size that can be\nhandled increases. When implementing AMR for HHG, it was\nimportant to maintain the numerical and software engineering\nconcepts that are crucial for HHG\u2019s performance and\nscalability. We describe how the algorithms and data structures\nwere extended in order to achieve this goal.\nThere are many other techniques for optimizing the distribution\nof computational resources in multigrid algorithms. As a contrast\nto AMR, we present a technique that was developed in joint work\nwith Thekale et al. A branch and bound search is used to find the\noptimal number of V-cycles on each level of a full multigrid\nalgorithm. By performing V-cycles on the levels where they yield\nthe best ratio between error reduction and cost, the time to\nsolution of a full multigrid run in a realistic scenario was\nreduced by 35%.\nMehrgittermethoden geh\u00f6ren zumindest theoretisch zu den effizientesten\nAlgorithmen in der Numerischen Simulation. Einige Klassen von Gleichungen,\nz.B. die bei der Disktretisierung mit Finiten Elementen (FE) entstehenden\nGleichungssysteme, sind damit unter bestimmten Voraussetzungen in optimaler\nKomplexit\u00e4t l\u00f6sbar. F\u00fcr die Anwendung im High-Performance-Computing sind jedoch\nnur Methoden relevant, die den extremen Parallelismus aktueller Supercomputer\nausnutzen k\u00f6nnen. Mehrgittermethoden effizient f\u00fcr Parallelrechner zu\nimplementieren ist eine Herausforderung, weil f\u00fcr einige der zentralen\nAlgorithmen das Verteilen der numerischen Operationen auf viele Prozessoren\nnicht trivial ist.\nMit dem Software-Framework Hierarchical Hybrid Grids (HHG) zeigten Bergen et\nal., da\u00df effiziente FE-Simulationen mit Mehrgittermethoden auf Supercomputern\nm\u00f6glich sind. Das zentrale Konzept von HHG ist die Diskretisierung des\nsimulierten Gebiets in abschnittsweise strukturierte Gitter. Das erleichtert\ndas Verteilen der Rechenoperationen auf viele Prozessoren, schr\u00e4nkt allerdings\nauch die Anwendbarkeit von HHG auf bestimmte numerische Probleme ein.\nDiese Arbeit demonstriert mit Performance-Studien auf bis zu 16384\nProzessorkernen und FE-Simulationen mit bis zu 3 \u00d7 10\u00b9\u00b9 Freiheitsgraden die\nhohe Effizienz und Skalierbarkeit von HHG. Um HHG auf Systemen dieser Gr\u00f6\u00dfe\nausf\u00fchren und analysieren zu k\u00f6nnen, waren \u00c4nderungen an der urspr\u00fcnglichen\nHHG-Version n\u00f6tig, z.B. am Build-System und an den Methoden zur\nPerformance-Messung.\nDas zentrale Kapitel der Arbeit widmet sich der adaptiven Gitterverfeinerung\n(AMR, von adatpive mesh refinement). Diese Technik erweitert den\nAnwendungsbereich von HHG auf Probleme mit starker r\u00e4umlicher Varianz in der\nben\u00f6tigten Gitterweite, z.B. die Simulation von Raumakustik oder von\nturbulenten Str\u00f6mungen. AMR erm\u00f6glicht eine flexible Anpassung des FE-Gitters\nan die Simulationscharakteristika. So k\u00f6nnen die Ressourcen des Computers \u2013\nSpeicher und Prozessorzyklen \u2013 gezielt dort eingesetzt werden, wo eine hohe\nGitteraufl\u00f6sung n\u00f6tig ist, und damit die Rechenzeit verringert und die l\u00f6sbare\nProblemgr\u00f6\u00dfe erh\u00f6ht werden. Bei der Implementierung von AMR in HHG war es\nwichtig, die Konzepte aus der Numerik und aus dem Software-Engineering, die f\u00fcr\ndie Performance und Skalierbarkeit von HHG entscheidend sind, zu erhalten. Die\nArbeit beschreibt, wie die Algorithmen und Datenstrukturen erweitert wurden, um\ndieses Ziel zu erreichen.\nEine weitere Methode zur Optimierung von Mehrgittermethoden wurde in\nZusammenarbeit mit Thekale et al. entwickelt. Mit einer Branch-And-Bound-Suche\nwird die Verteilung von V-Zyklen im Full-Multigrid-Algorithmus\noptimiert. V-Zyklen werden gezielt auf den Leveln ausgef\u00fchrt, wo sie das beste\nVerh\u00e4ltnis aus Fehlerreduktion und Kosten erzielen. Mit dieser Methode wurde in\neinem realistischen Szenario eine Verringerung der Laufzeit von Full Multigrid\num 35% erreicht.\n2015\ndoctoralthesis\ndoc-type:doctoralThesis\napplication/pdf\nhttps://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/6836\nurn:nbn:de:bvb:29-opus4-68368\nhttps://nbn-resolving.org/urn:nbn:de:bvb:29-opus4-68368\nhttps://opus4.kobv.de/opus4-fau/files/6836/Gradl_Dissertation.pdf\neng\nhttps://creativecommons.org/licenses/by-nd/3.0/de/\ninfo:eu-repo/semantics/openAccess\n169836141400000", "language": null, "image": null, "pagetype": null, "links": []}