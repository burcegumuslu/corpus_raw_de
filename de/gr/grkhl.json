{"title": "GRIN - Die F\u00f6rderung von Schulkindern mit auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rungen (AVWS)", "author": "Winter; Alexandra", "url": null, "hostname": null, "description": "Die F\u00f6rderung von Schulkindern mit auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rungen (AVWS) - P\u00e4dagogik - Examensarbeit 2006 - ebook 34,99 \u20ac - GRIN", "sitename": null, "date": "2006-09-21", "id": null, "license": null, "body": null, "comments": "Kommentare", "commentsbody": null, "raw_text": null, "text": "Leseprobe\nInhalt\n1 Einleitung S.\n2 Begriffsbestimmung S.\n2.1 Auditive Verarbeitung und Wahrnehmung S.\n2.1.1 Akustische Signale S.\n2.1.2 Der H\u00f6rvorgang S.\n2.1.3 Entwicklung der auditiven Verarbeitung und Wahrnehmung S.\n2.1.4 Teilfunktionen der auditiven Verarbeitung und Wahrnehmung S.\n2.2 St\u00f6rungen der auditiven Verarbeitung und Wahrnehmung S.\n2.2.1 Definition der auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rung (AVWS) S.\n2.2.2 \u00c4tiologie und Pathogenese S.\n2.2.3 \u00dcberblick \u00fcber ausgew\u00e4hlte diagnostische Verfahren S.\n3 F\u00f6rderung von Schulkindern mit AVWS S.\n3.1 Beratung und Elternarbeit S.\n3.1.1 Auswirkungen der St\u00f6rung im pers\u00f6nlichen und schulischen Bereich S.\n3.1.2 Interdisziplinarit\u00e4t in der Erkennung und Behandlung von AVWS S.\n3.1.3 Konkrete Empfehlungen f\u00fcr Eltern von Schulkindern mit AVWS S.\n3.2 Ausgew\u00e4hlte Therapieans\u00e4tze und ihre Einsetzbarkeit in Schule und Unterricht S.\n3.2.1 \u00dcbende Therapie mit sprachlichen \u00dcbungen S.\n3.2.1.1 Therapiekonzept nach ANDREAS BURRE S.\n3.2.1.2 Therapiekonzept nach NORINA LAUER S.\n3.2.1.3 Therapiekonzept nach DOLORES HEBER und JUTTA BURGER-GARTNER S.\n3.2.1.4 Amerikanische Programme S.\n3.2.1.5 Psychomotorische Entwicklungsf\u00f6rderung - Auditive Wahrnehmung und Sprache nach INGRID OLBRICH S.\n3.2.1.6 Therapie auditiver Wahrnehmungsst\u00f6rungen nach ERWIN BREITENBACH S.\n3.2.1.7 F\u00f6rderung der phonologischen Bewusstheit nach SANDRA K\u00dcSPERT und WOLFGANG SCHNEIDER S.\n3.2.1.8 H\u00f6rtraining als Komponente der F\u00f6rderung bei Lese- Rechtschreib-St\u00f6rung nach MARIA KLATTE und UTE DINGEL S.\n3.2.1.9 Psycholinguistisch orientierte Phonologie Therapie (P. O. P. T.) nach ANETTE V. FOX S.\n3.2.1.10 Computergest\u00fctzte Therapieprogramme S.\n3.2.1.10.1 Audio 1 S.\n3.2.1.10.2 AudioLog 3, AudioLog Home S.\n3.2.1.10.3 Fast ForWord S.\n3.2.1.10.4 Earobics Step 1, Earobics Step 2 und Sound Smart S.\n3.2.2 H\u00f6rtraining/Klangtherapie S.\n3.2.2.1 Audio-Psycho-Phonologie nach ALFRED TOMATIS S.\n3.2.2.2 Lateraltraining nach FRED WARNKE S.\n3.2.2.3 AUDIVA S.\n3.2.3 Kompensation durch Technische Hilfsmittel und Raumakustik S.\n3.2.4 \u201eH\u00f6rerziehung\u201c im Unterricht S.\n3.2.5 \u00dcbersicht zu durch die vorgestellten Konzepte abgedeckten F\u00f6rderbereichen S.\n4 Zusammenfassung S.\n5 Verzeichnisse S.\n5.1 Literaturverzeichnis S.\n5.2 Internetquellenverzeichnis S.\n5.3 Abbildungsverzeichnis S.\n5.4 Tabellenverzeichnis S.\n6 Anlagen S.\n6.1 Verzeichnis der genannten Therapiematerialien und Software S.\n6.2 Spielvorschl\u00e4ge f\u00fcr zu Hause und einige Hinweise f\u00fcr Eltern S.\n1 Einleitung\nW\u00e4hrend eines vierw\u00f6chigen Blockpraktikums an einer Lernbehindertenschule lernte ich Johanna kennen. Sie war zu dem Zeitpunkt 8 Jahre und 4 Monate alt und besuchte die Klasse 1.2 der Diagnose- und F\u00f6rderstufe. Vom Wesen her war Johanna hilfsbereit, freundlich und sowohl anderen Menschen als auch neuen Inhalten im Unterricht gegen\u00fcber sehr aufgeschlossen. Im Rechnen mit Rechenmaterial oder mit Hilfe von Bilddarstellungen sowie beim Rechnen vorgegebener Aufgaben auf einem Arbeitsblatt war sie im Vergleich zur Klasse gut und machte schnelle Fortschritte. Auch beim Erarbeiten von Inhalten anderer Fachbereiche im Sitzkreis brachte sie sich gut ein. Schwierigkeiten hatte sie allerdings beim Erwerb der Schriftsprache. Das Schreiben von lauttreuen W\u00f6rtern gelang ihr selten fehlerfrei, die Analyse der einzelnen im Wort vorkommenden Laute war ihr nicht m\u00f6glich. Auch das Erlesen von W\u00f6rtern und einfachen S\u00e4tzen und die zu Grunde liegende Synthese fielen ihr schwer. Trotz ihres Alters substituierte Johanna noch immer [g] und [k] mit [d] und bei der Aussprache von [z] und [ ] fehlte ihr die notwendige artikulatorische Sch\u00e4rfe. Ihre Stimmlage war ungew\u00f6hnlich hoch und nasal gef\u00e4rbt. W\u00e4hrend meiner Beobachtungen in Unterrichtssituationen fiel mir auf, dass Johanna im Morgenkreis, wenn die Klasse regelm\u00e4\u00dfig auf das Ticken der Klassenzimmeruhr lauschte, um zur Ruhe zu kommen, das Ger\u00e4usch nahezu immer entweder als letztes von 10 Kindern wahrzunehmen schien oder es gar nicht h\u00f6rte. In entsprechenden Spielsituationen konnte sie die Richtung, aus der ein Ger\u00e4usch kam, nicht bestimmen. W\u00e4hrend des Spiels \u201eFl\u00fcsterpost\u201c war es Johanna nicht m\u00f6glich, die geh\u00f6rten W\u00f6rter ad\u00e4quat wiederzugeben. Wenn die Klasse ein Lied sang, fiel immer wieder auf, dass Johanna sich den Text nicht eingepr\u00e4gt hatte. In Einzelf\u00f6rderstunden mit Johanna stellte ich au\u00dferdem fest, dass sie nicht mehr als drei lautsprachlich vorgegebene Einheiten im Kurzzeitged\u00e4chtnis behalten konnte und zum Teil bei der Wiedergabe deren Reihenfolge ver\u00e4nderte. Erkl\u00e4rte ich ihr eine Aufgabe, die ich mit ihr zusammen l\u00f6sen wollte, so sah sie mich dabei sehr konzentriert an, wobei sie ihr Augenmerk auf meine Lippenbewegungen zu legen schien. Demonstrierte ich ihr eine Beispielaufgabe, so kam es des \u00f6fteren vor, dass Johanna, meine Lippen nicht aus den Augen lassend, laut und zeitlich leicht verz\u00f6gert mitsprach. W\u00e4hrend dieses Praktikums begann ich, mit Johanna \u00dcbungen aus dem Programm H\u00f6ren, lauschen, lernen von Sandra K\u00dcSPERT und Wolfgang SCHNEIDER durchzuf\u00fchren, wodurch sich ihre Analyse- und Synthesef\u00e4higkeiten durchaus verbesserten. Nach meinem Praktikum wurde das Programm von einer Lehrerin an der Praktikumsschule weitergef\u00fchrt.\nBei der Begegnung mit Johanna wurde mir bewusst, wie wenig ich doch \u00fcber die Wahrnehmung und Verarbeitung von Sprache wei\u00df, obwohl diese doch offensichtlich eine der wichtigsten Basisf\u00e4higkeiten im Hinblick auf den Spracherwerb und Schriftspracherwerb ist. Obwohl ich Johanna mit den \u00dcbungen tats\u00e4chlich f\u00f6rdern konnte, war ich mit meinem Wissensstand zu den theoretischen Hintergr\u00fcnden \u00e4u\u00dferst unzufrieden und wollte herausfinden, welche anderen M\u00f6glichkeiten zur Verf\u00fcgung gestanden h\u00e4tten. Im Anschluss an dieses Blockpraktikum absolvierte ich ein weiteres Praktikum in einer logop\u00e4dischen Praxis. Auch dort traf ich auf mehrere Kinder, die Defizite in der auditiven Verarbeitung und Wahrnehmung aufwiesen und mit Hilfe von unterschiedlichen Therapiematerialien behandelt wurden. Auf die theoretische Grundlegung dieser Behandlung angesprochen, erkl\u00e4rte mir eine der dort besch\u00e4ftigten\nLogop\u00e4dinnen, dass die Existenz der auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rungen umstritten sei und in den Behandlungsm\u00f6glichkeiten wenige Therapiekonzepte zur Verf\u00fcgung standen und gab mir die beiden B\u00fccher zum Thema, die sie besa\u00df, Zentral-auditive Verarbeitungsst\u00f6rungen im Kindesalter von NORINA LAUER und Zentrale H\u00f6rwahrnehmungsst\u00f6rungen von REGINA LEUPOLD. Darin fanden sich zwar interessante Aspekte, aber f\u00fcr meinen Bedarf war das Thema nicht umfassend erschlossen.\nDie vorliegende Arbeit befasst sich deshalb mit der F\u00f6rderung von Schulkindern mit auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rungen (AVWS).\nZun\u00e4chst wird der Begriff der auditiven Verarbeitung und Wahrnehmung erl\u00e4utert. Dabei soll zun\u00e4chst ein \u00dcberblick \u00fcber die f\u00fcr Kommunikation wohl wichtigsten auditiven Stimuli der Lautsprachproduktion eingegangen werden, bevor die Aufnahme der akustischen Reize und ihre Weiterleitung und Verarbeitung, also der Prozess der auditiven Verarbeitung und Wahrnehmung (AVW), dargestellt wird.\nIm Anschluss werden Teilfunktionen der AVW benannt und erkl\u00e4rt sowie kurz ihre Einordung in Modelle der Sprachwahrnehmung und -verarbeitung angerissen. Nach einem \u00dcberblick \u00fcber aktuelle Definitionen der AVWS und bisherigen Erkenntnissen zu \u00c4tiologie und Pathogenese sollen ausgew\u00e4hlte Tabellen einen \u00dcberblick \u00fcber diagnostische Verfahren zur Erfassung verschaffen.\nIm Hauptteil der Arbeit schlie\u00dflich sollen m\u00f6gliche Zusammenh\u00e4nge zwischen AVWS und anderen St\u00f6rungsbildern, die sich ebenfalls auf den schulischen Erfolg betroffener Kinder auswirken k\u00f6nnen als Grundlage einer beratenden Unterst\u00fctzung f\u00fcr betroffene Eltern beleuchtet werden. M\u00f6glichkeiten und Beispiele der interdisziplin\u00e4ren Zusammenarbeit in der Erfassung und Behandlung von AVWS sollen allgemein dargestellt werden sowie ausgew\u00e4hlte internationale Therapiekonzepte und F\u00f6rdermaterialien zu unterschiedlichen Bereichen der auditiven Verarbeitung und Wahrnehmung vorgestellt und sowohl im Hinblick auf ihre Wirksamkeit als auch ihrer Einsetzbarkeit im Unterricht oder Gruppen- sowie Einzelf\u00f6rderstunden diskutiert werden. Da die Arbeit ihren Schwerpunkt auf der F\u00f6rderung von Schulkindern mit AVWS hat, wird sich die Darstellung von F\u00f6rdermaterialien auf solche beschr\u00e4nken, die f\u00fcr den Altersbereich von der Einschulung bis zur Pubert\u00e4t, also von ca. 6 - 12 Jahren geeignet sind.\n2 Begriffsbestimmung\n2.1 Auditive Wahrnehmung und Verarbeitung\n2.1.1 Akustische Signale\nUm den Vorgang des H\u00f6rens nachzuvollziehen, sollte man sich zun\u00e4chst dar\u00fcber im Klaren sein, was da eigentlich auf das Ohr trifft. Man bezeichnet es gemeinhin als Schall oder akustischen Reiz (acoustic stimulus). Ein akustischer Reiz kann einfach oder komplex sein, es kann sich dabei um das Ger\u00e4usch handeln, das ein vorbeifahrendes Auto oder eine sich die Krallen wetzende Katze erzeugt. Es kann sich aber auch um ein Musikst\u00fcck handeln, in dem gleichzeitig mehrere Stimmen und Instrumente zu h\u00f6ren sind oder um gesprochene Sprache, die aus einer komplexen Aneinanderreihung verschiedener akustischer Signale besteht, doch dazu sp\u00e4ter mehr. Im Englischen wird dieser akustische Reiz auch als sound bezeichnet (vgl. z.B. MOORE 1994, SCHOUTEN 1992, GOLDSTEIN 1999), was \u00fcbersetzt sowohl Klang als auch Ger\u00e4usch bedeutet. Auf die Unterscheidung von Klang und Ger\u00e4usch werde ich im Laufe der Arbeit ebenfalls differenzierter eingehen. Bis dahin werde ich mit der \u00dcbersetzungsvariante Schall arbeiten, f\u00fcr die sound ebenfalls stehen kann.\nPhysikalisch gesehen ist dieser Schall Druck. Er entsteht durch Vibrationen eines K\u00f6rpers, die sich auf die ihn umgebende Luft, Wasser oder andere elastische Medien \u00fcbertragen. Im Fall von Sprache, um deren auditive Verarbeitung und Wahrnehmung es in dieser Arbeit gehen soll, versetzt bei physiologisch richtigem Sprechmuster der betreffende Mensch auf Grund einer Sprechabsicht seine Stimmlippen mit Hilfe von ca. 1,5 bis 2,5 Litern Ausatemluft pro Zyklus in Schwingung. Dabei werden durch entsprechenden Tonus auf den sich in Phonationsstellung befindlichen Stimmlippen austretende Luftmolek\u00fcle zur\u00fcckgehalten, wobei sich die Dichte der Luft unter den Stimmlippen und damit auch der Luftdruck unterhalb der Stimmlippen vergr\u00f6\u00dfert, bis die entsprechende Luft schlie\u00dflich entweichen kann. Nachdem sich dadurch der subglottische Druck reduziert hat, kommt der sogenannte Bernouilli-Effekt zum Tragen und unter der Glottis (hier: Stimmlippen und Stimmritze) bildet sich kurzzeitig ein Unterdruck. Dieser f\u00fchrt in Zusammenwirken mit den elastischen R\u00fcckstellkr\u00e4ften der Aryknorpel zu einem erneuten Schluss der Stimmlippen. Durch den abwechselnden \u00d6ffnungs- und Schlie\u00dfvorgang der Stimmlippen wird der Expirationsstrom in Luftimpulse von ansteigendem und abfallendem Druck umgeformt. Die Stimmlippen schwingen dabei ca. 100 - 400 Mal pro Sekunde indem sie durch die muskul\u00e4ren und elastischen Kr\u00e4fte innerhalb der Stimmlippen und die Kr\u00e4fte der Luft, die durch die Stimmritze str\u00f6mt bewegt (vgl. STORCH 2002, S. 19 - 29) werden. Dies entspricht auch der myoelastischen aerodynamischen Phonationstheorie nach JOHANNES M\u00dcLLER (vgl. WIRTH 1995, S. 90).\nDie erw\u00e4hnten Luftimpulse kann man sich als abwechselnde \u00dcber- und Unterdruckfelder in der Atemluft vorstellen, da benachbarte Luftmolek\u00fcle einander beeinflussen (vgl. GOLDSTEIN 1999, S. 312).\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 1: Schematische Darstellung der Kippschwingung in den Stimmlippen (KOLLMEIER 20051, S. 97, Online im Internet)\nBei dem entstandenen Muster an Luftdruckver\u00e4nderung, das sich von seiner Quelle aus durch Luft mit 340 m/sec fortbewegt, handelt es sich um eine Schallwelle. Die Qualit\u00e4ten dieser Schallwelle, die vom menschlichen H\u00f6rer unterschieden werden, n\u00e4mlich Tonh\u00f6he und Lautst\u00e4rke sind von der Frequenz und der Amplitude der Schallwelle abh\u00e4ngig. An einem Reinton dargestellt, handelt es sich bei der Amplitude um die h\u00f6chste gemessene Abweichung des Luftdrucks von seinem Ausgangszustand. Am Beispiel des gemessenen Reintons ergibt sich eine Sinuswelle. Der \u00dcberdruck in der Luft f\u00fchrt zu dem Ausschlag nach oben, der nachfolgende Unterdruck zum Ausschlag nach unten. Um den Luftdruck des Schalls zu messen, wird die Einheit Dezibel verwendet, die sich wie folgt berechnet: dB = 20 log p/p0, wobei p f\u00fcr den Schalldruck des Tons steht, w\u00e4hrend p0 der Standardschalldruck ist, der bei 20 Micropaskal liegt. Die Messung der Amplitude in der Einheit dB erm\u00f6glicht eine \u00fcbersichtlichere Darstellung der Druckunterschiede.\nMit der Frequenz wird mittels Spektrogrammen gemessen, wie oft in der Sekunde es in der Luft zum Zyklus des \u00dcberdrucks mit folgendem Unterdruck kommt. Ein Hertz (Hz) entspricht dabei einem Zyklus pro Sekunde. Das menschliche Ohr ist sensibel f\u00fcr Frequenzen zwischen ca. 20 und 20000 Hz (vgl. GOLDSTEIN 1999, S. 312-315). Der Frequenzbereich menschlicher Lautsprache wird unterschiedlich definiert. GOLDSTEIN siedelt ihn zwischen 400 Hz und 5000 Hz (vgl. GOLDSTEIN, 1999, S. 351) an, nach NIEMEYER liegt er zwischen 125 Hz und 1200 Hz (vgl. NIEMEYER 1978 S. 22), was dem durch das Audiogramm erfassbaren Bereich entspricht (vgl. MINNING 20051, S.1, Online im Internet; vgl. auch Abb. 8, S. 17). Der Frequenzbereich von Sprache und Musik wird im AUDIVA-Informationspapier Grundlagen/Verfahren der H\u00f6rwahrnehmung mit 40Hz bis 1600Hz angegeben (vgl. MINNING 20051, S. 1, Online im Internet). Nach WIRTH liegt das Klangspektrum menschlicher Sprache zwischen 50 und 1500 Hz (vgl. WIRTH 1995, S. 88). M\u00f6glicherweise gehen diese Unterschiede darauf zur\u00fcck, dass nicht jede Lautsprache sich im selben Frequenzbereich bewegt. Nach TOMATIS liegt zum Beispiel der Frequenzbereich des Franz\u00f6sischen zwischen 1000 und 2000 Hz, w\u00e4hrend der des Englischen erst bei ca. 1000 Hz beginnt. Der Frequenzbereich des Deutschen beinhaltet demnach auch sehr viele tiefe T\u00f6ne, was die Abweichungen in den unteren Grenzen erkl\u00e4ren w\u00fcrde. Spanisch bewegt sich nach TOMATIS ausschliesslich in tiefen Frequenzen, w\u00e4hrend slawische Sprachen und das Portugiesische bis zu elf Oktaven umfassen (vgl. TOMATIS 2004, S. 45).\nDie von den Stimmlippen erzeugte Schallwelle wird auch \u201eprim\u00e4rer Kehlkopfklang\u201c (WIRTH 1995, S. 88) genannt. Im Gegensatz zum Reinton setzt er sich aus mehreren Frequenzen zusammen, einer Grundfrequenz, die auch \u201e1. Partialton\u201c (WIRTH 1995, S. 85) genannt wird und weiteren Partialt\u00f6nen, die entweder Obert\u00f6ne oder Harmonien der Grundfrequenz sind. Es handelt sich dabei um ein eher schmales Frequenzband. Durch Resonanz im Ansatzrohr, die durch die entsprechenden Artikulationsorgane im Vokaltrakt ver\u00e4ndert werden kann, werden jeweils zwei Partialt\u00f6ne verst\u00e4rkt, w\u00e4hrend die \u00fcbrigen Partialt\u00f6ne ged\u00e4mpft werden (vgl. WIRTH 1995, S.85 - 88). Durch die unterschiedliche H\u00f6he der Amplituden der Partialt\u00f6ne wird die Klangfarbe bestimmt (vgl. SCHELLBERG 1998, S. 9 f.). Das Vokalspektrum der Lautsprache entsteht also durch Maximierung der Amplitude von jeweils zwei Partialt\u00f6nen, den Formanten.\nBei KOLLMEIER ist die Grundfrequenz von M\u00e4nnern mit 100 bis 400 Hz und die von Frauen mit 200 bis 800 Hz angegeben. Der Frequenzbereich des ersten Formanten F1 liegt demnach bei 300 bis 1000 Hz, der des zweiten Formanten F2 bei 600 bis 2500 Hz und der des dritten Formanten F3 bei 1500 bis 2500 Hz (vgl. KOLLMEIER 2005 1, Online im Internet, S. 102). F\u00fcr die Vokalerkennung sind nur die beiden untersten Formanten zwingend erforderlich. Der F3 und weitere Formanten beeinflussen die Klangfarbe von Vokalen dennoch (vgl. WIRTH 1995, S. 88f.). F\u00fcr die einzelnen Vokale im Deutschen ergeben sich charakteristische Lagebereiche innerhalb der Formanten wie im Vokaldreieck (Abb. 3) demonstriert wird.\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 2: Vokaldreieck (KOLLMEIER 20051, S. 103, Online im Internet)\nDie Klangfarbe der einzelnen Vokale wird im Wesentlichen durch die Lage der Zunge und die Kiefer\u00f6ffnungsweite bestimmt. Vokale sind vom Wesen her den Kl\u00e4ngen zuzuordnen. Vokale werden lang (z.B. [i:]), kurz (z.B. [ ]), betont (alle Vokale in betonten Silben) oder unbetont (Schwalaut [ ], silbisches r [ ] und unsilbisches r [ ]) gesprochen. Diphtonge werden ebenfalls den Vokalen zugeordnet. Sie sind Gleitlaute von einem Vokal zum n\u00e4chsten. Im Deutschen kommen drei Diphtonge vor (vgl. STORCH 2002, S. 33 - 50). Neben den Vokalen werden in der Lautsprache auch Konsonanten artikuliert. Diese sind nur zum Teil stimmhaft. Nach DOBSLAFF (2005) sind sie als Ger\u00e4usche zu bezeichnen. Konsonanten sind im Spektrogramm zwar erkennbar, aber schwer zu unterscheiden (vgl. KOLLMEIER 2005 1, S. 105, Online im Internet). Der Unterschied in der lautlichen Realisation von Konsonanten wird durch den Artikulationsmodus, den \u00dcberwindungsmodus und die Sonorit\u00e4t bestimmt. M\u00f6gliche Artikulationsmodi sind Verschluss (Verschlusslaute, z.B. [k]), Enge (Engelaute, z.B. [\u00e7]), Nasen\u00f6ffnung (Nasallaute, z.B. [m]) oder unterbrochener Verschluss (Schwinglaute, z.B. [R]). Beim \u00dcberwindungsmodus kann es sich um Sprengung (Plosivlaute, z.B. [p]), Reibung (Frikative, z.B. [f]), Flie\u00dfen (Liquide, z.B. [l]) und Schwingen, Flattern oder Rollen (Vibranten, z.B. [R]) handeln. In der Sonorit\u00e4t geht es schlie\u00dflich darum, ob ein Konsonant stimmhaft oder stimmlos artikuliert wird (vgl. STORCH 2002, S. 51 - 53).\nAbbildung in dieser Leseprobe nicht enthalten\nTab. 1: \u00dcbersicht \u00fcber deutsche Sprachlaute (nach KLEINER, ANDREAS: Atlas deutscher Sprachlaute 2005, Online im Internet); IPA=International Phonetics Alphabet\nIm Hinblick auf die Sprachperzeption ist dar\u00fcberhinaus das Koartikulationsprinzip zu\nbeachten. Beim Sprechen werden die einzelnen Laute nicht gesondert artikuliert, sondern \u201edie Bewegungen der beteiligten Organe\u201c gehen \u201egleitend ineinander \u00fcber\u201c und wirken \u201ewechselseitig aufeinander ein\u201c (KRECH/KURKA/STELZIG/STOCK/ST\u00d6TZER/TESKE 1982, S. 69). Gleichzeitig setzen die \u201eeinem Laute vorbestimmten Bewegungen meistens schon ein[...], bevor die der vorangehenden Lautung noch beendet sind, bzw. die Bewegungen der fr\u00fcheren Lautung noch anhalten, w\u00e4hrend die der sp\u00e4teren bereits im Gange sind\u201c (ESSEN 1979, S. 117).\nDie Laute werden also zu Sprecheinheiten zusammengefasst und erzeugen auf Grund der Koartikulation leicht ver\u00e4nderte Frequenzen.\n2.1.2 Der H\u00f6rvorgang\nNachdem ich mich im vorhergegangenen Teil darum bem\u00fcht habe, die akustischen Merkmale der vom Menschen produzierten Lautsprache m\u00f6glichst kurz, \u00fcbersichtlich und unabh\u00e4ngig vom menschlichen H\u00f6ren darzustellen, soll es nun darum gehen, was passiert, wenn der Schall schlie\u00dflich auf das menschliche Ohr trifft und damit zum auditiven Reiz wird.\nIn der Umgangssprache wird mit dem Begriff Ohr das bezeichnet, was an den Seiten des Kopfes zu sehen ist, n\u00e4mlich die Ohrmuschel. Abgesehen von Brillentr\u00e4gern k\u00f6nnten Menschen auf diesen Teil des Ohres jedoch verglichen mit den Bereichen, die sich im Inneren des Kopfes befinden, am ehesten verzichten, ohne dadurch allzugro\u00dfe Beeintr\u00e4chtigungen ihres Geh\u00f6rs auf sich zu nehmen. Interessanter im Bezug auf die Weiterleitung und Verarbeitung des akustischen Reizes sind die Teile des Ohrs, die sich dahinter verbergen.\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 3: Aufbau des Ohrs (KOLLMEIER 20052, S. 1, Online im Internet)\nZun\u00e4chst trifft der Schall also auf die Ohrmuschel, einen Teil des Au\u00dfenohrs und wird dadurch in den Geh\u00f6rgang geleitet. Dieser ist ca. 3 cm lang und hat im Wesentlichen die Aufgabe, das an seinem anderen Ende befindliche empfindliche Trommelfell vor gef\u00e4hrlichen Umwelteinfl\u00fcssen zu sch\u00fctzen und die Temperatur des Mittelohrs einigerma\u00dfen konstant zu halten (vgl. GOLDSTEIN 1999, S. 318f.). Eine weitere Funktion des Au\u00dfenohrs ist die B\u00fcndelung von Schall und damit eine Trichterwirkung bei hohen Frequenzen sowie die richtungsabh\u00e4ngige Verformung (Filterung) von akustischen Signalen. Da die Klangf\u00e4rbung derselben je nach Einfallswinkel unterschiedlich ist, kann sie zur Ortung von Ger\u00e4uschen genutzt werden (vgl. KOLLMEIER 20052, S. 2, Online im Internet).\nDer Schall versetzt nun das Trommelfell in Bewegung. Diese Vibrationen werden auf das Mittelohr \u00fcbertragen, das sich in der luftgef\u00fcllten Paukenh\u00f6hle befindet und \u00fcber die Eustachische R\u00f6hre mit dem Nasen-Rachenraum verbunden ist. Dort treffen sie auf die Geh\u00f6rkn\u00f6chelchen. Der Hammer, der mit dem Trommelfell direkt verbunden ist, beginnt als erstes zu vibrieren und \u00fcbertr\u00e4gt die Schwingungen auf den Ambo\u00df, der wiederum mit dem Steigb\u00fcgel verbunden ist (vgl. GOLDSTEIN 1999, S. 319). Hammerkopf und Ambo\u00df sind durch ein Gleitreibungsgelenk miteinander verbunden, das bei starken statischen Auslenkungen nachgibt und den optimalen \u201eArbeitspunkt\u201c des Mittelohrs festlegt (vgl. KOLLMEIER 2005 2, S. 3, Online im Internet). Vom Steigb\u00fcgel aus werden die Schwingungen \u00fcber das ovale Fenster auf die Cochlea (auch: Geh\u00f6rschnecke) \u00fcbertragen. Am Steigb\u00fcgelfu\u00dfst\u00fcck befindet sich der Musculus Stapedius, der bei hohem Schalldruck in einer Art Schutzreflex reagiert und eine Ver\u00e4nderung in der Mechanik des Mittelohrs bewirkt. Die Cochlea ist wesentlicher Bestandteil des Innenohrs und mit einer\nFl\u00fcssigkeit gef\u00fcllt. Dadurch l\u00e4sst sich auch die Funktion des Mittelohrs als Impendanzanpassung zwischen den wellenf\u00f6rmigen Bewegungen der Luft, in der kleine Auslenkungskr\u00e4fte eine hohe Auslenkung der Luftelemente bewirken und der sehr hohen Impendanz der Innenohrfl\u00fcssigkeit erkl\u00e4ren (vgl. KOLLMEIER 20052, S. 4, Online im Internet).\nDie Cochlea besteht aus drei schlauchf\u00f6rmigen R\u00e4umen, der Scala vestibuli, der Scala mediae und der Scala tympani. Die Scala vestibuli und die Scala tympani sind mit Perilymphe gef\u00fcllt, die Scala mediae mit Endolymphe. Die Scala mediae liegt zwischen den beiden anderen R\u00e4umen und wird durch die ionendurchl\u00e4ssige Reissner'sche Membran von der Scala vestibuli und durch die Basiliarmembran von der Scala tympani abgegrenzt. Auf der Basiliarmembran in der Scala mediae befindet sich das Cortische Organ, welches \u00fcber Haarzellen mit den H\u00f6rnerven verbunden ist. Man unterscheidet die inneren Haarzellen, die durchg\u00e4ngig einreihig angeordnet sind und die \u00e4u\u00dferen Haarzellen, die dreireihig angeordnet sind und deren Zahl zum spitz zulaufenden Inneren der Cochlea hin zunimmt (vgl. LAUER 2001, S. 2f.). An dieser Spitze sind die Scala vestibuli und die Scala tympani \u00fcber das Helicotrema verbunden (vgl. ROSEN/HOWELL 1991, S. 245). \u00dcbertr\u00e4gt nun der Steigb\u00fcgel die Vibrationen \u00fcber das ovale Fenster auf die Endolymphe in der Cochlea, so werden diese nur bedingt \u00fcber das Helicotrema und durch die Scala tympani zum runden Fenster an deren Ende \u00fcbertragen. Der Druck wirkt mehr auf die Scala mediae ein und setzt damit die elastische Basiliarmembran in Bewegung (vgl. GOLDSTEIN 1999, S. 321). Da die Endolymphe nicht komprimierbar ist, wird die entstehende Bewegung au\u00dferdem durch eine Ausw\u00e4rtsbewegung der Membran im runden Fenster ausgeglichen. Bewegen sich Steigb\u00fcgel und Membran des ovalen Fensters wieder ausw\u00e4rts, so wird die Membran des runden Fensters nach innen bewegt (vgl. ROSEN/HOWELL 1991, S. 245f.).\nDie Wanderwelle der Basiliarmembran wiederum \u00fcbersetzt sich auf das Cortische Organ, welches sich ebenfalls auf- und abw\u00e4rts bewegt und sorgt au\u00dferdem daf\u00fcr, dass sich die Tectorialmembran, die das Cortische Organ bedeckt, sich \u00fcber den Flimmerh\u00e4rchen der Haarzellen vor- und r\u00fcckw\u00e4rts bewegt. Dadurch beugen sich die inneren Haarzellen auf Grund ihrer Bewegungen gegen die umgebende Perilymphe, w\u00e4hrend die \u00e4u\u00dferen Haarzellen durch die Tectorialmembran gebeugt werden. Nicht jedes akustische Signal wird dabei auf der Basiliarmembran gleich weitergeleitet. Unterschiedliche Frequenzen erzeugen unterschiedliche Wellenbewegungen auf der Basiliarmembran und wirken somit auf unterschiedliche innere Haarzellen ein (vgl. GOLDSTEIN 1999, S. 322 - 330).\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 4: Schematischer Aufbau des Innenohrs: Cochlea, Querschnitt durch die Cochlea und Cortisches Organ (nach KOLLMEIER 20052, S. 5, Online im Internet und GOLDSTEIN 1999, S. 322f.)\nEine tonotopische \u00dcbersicht \u00fcber die Lage der Frequenzbereiche auf der Cochlea zeigt Abb. 7. Allgemein werden in Abh\u00e4ngigkeit vom Schallwellenwiderstand der Innenohrfl\u00fcssigkeit hohe Frequenzen n\u00e4her am ovalen Fenster \u00fcbertragen, w\u00e4hrend tiefe Frequenzen eher in der N\u00e4he des Helicotrema \u00fcber die Haarzellen weitergeleitet werden. Die Wellen auf der Basiliarmembran verlaufen nur bis zu ihrer frequenzspezifischen Stelle, hinter der die Basiliarmembran in Ruhe ist. (vgl. LAUER 2001, S. 3)\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 5: Schematische \u00dcbersicht \u00fcber die Lage von sensiblen Haarzellen f\u00fcr einzelne Frequenzbereiche auf der Cochlea (GOLDSTEIN 1999, S. 330)\nDie Bewegung der inneren Haarzellen erzeugt ein elektrisches Signal, das auf den H\u00f6rnerv (VIII. Hirnnerv: Nervus vestibulocochlearis in LAUER 2001, S. 3 oder Nervus statoacusticus in WIRTH 2000, S. 645) \u00fcbertragen wird. Hier wird auch die Verbindung zwischen Gleichgewicht und H\u00f6ren deutlich. Bewegen sich die Zilien in eine Richtung, werden sie entladen, bewegen sie sich in die andere Richtung, werden sie aufgeladen. Dies f\u00fchrt dazu, dass Neurotransmitter ausgesch\u00fcttet werden und der H\u00f6rnerv zum feuern gebracht wird (vgl. GOLDSTEIN 1999, S. 330). Die inneren Haarzellen werden jeweils von mehreren afferenten Axonen des H\u00f6rnervs innerviert, w\u00e4hrend ein afferentes Axon jeweils f\u00fcr mehrere \u00e4u\u00dfere Haarzellen zust\u00e4ndig ist (vgl. LAUER 2001, S.3). Der H\u00f6rnerv besteht zum gr\u00f6\u00dften Teil aus afferenten Nervenfasern, von denen aber nur ca.\n5 % von den \u00e4u\u00dferen Haarzellen ausgehen. Sie haben ihren Zellkern im Spiralganglion im Zentrum der Schnecke sowie eine Synapse an den Wurzeln der inneren Haarzellen und enden im Nucleus cochlearis dorsalis (NCD) im Hirnstamm. Die efferenten Fasern des H\u00f6rnervs entspringen gr\u00f6\u00dftenteils der oberen Olive, kreuzen dann \u00fcber Sagittal die Seite und innervieren \u00fcber das Spiralganglion die \u00e4u\u00dferen Haarzellen. Von den inneren Haarzellen beeinflussen sie lediglich die afferenten Synapsen (vgl. KOLLMEIER 20053, S. 72, Online im Internet). An dieser Stelle wird vermutlich die Sensitivit\u00e4t der inneren Haarzellen reguliert, um ein zu schnelles Erreichen der maximalen Antwortquote bei steigender Schallintensit\u00e4t zu verhindern. Es ist auch denkbar, dass hier Hintergrundger\u00e4usche reduziert werden k\u00f6nnen und die Aufmerksamkeit auf ein bestimmtes Ger\u00e4usch gelenkt werden kann (vgl. GOLDSTEIN 1999, S. 341). \u00dcber die Aufgabe der \u00e4u\u00dferen Haarzellen herrscht in der Literatur Uneinigkeit. Es wird vermutet, dass sie auf Grund von positivem elektromechanischen Feedback als Verst\u00e4rker der Frequenzaufl\u00f6sung wirken (vgl. ZWISLOCKI 1992, S. 4), indem sie durch \u201eeine Art aktive R\u00fcckkopplung die Sensitivit\u00e4t und gleichzeitig die Frequenzspezifit\u00e4t der Basiliarmembran\u201c erh\u00f6hen (KOLLMEIER 20052, S. 6, Online im Internet).\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 6: Physiologie der zentral-auditiven Verarbeitung (o. A.: Auditory Pathway 2005, Online im Internet)\n\u00dcber die Abgrenzung von peripherem und zentralem H\u00f6ren finden sich in der Literatur unterschiedliche Ansichten. So beginnt bei LAUER (2001, S. 2) in Anlehnung an B\u00d6HME der \u201ezentrale[..] Teil\u201c des H\u00f6rens, der auch die Wahrnehmung mit einschlie\u00dft, an der \u201eEintrittsstelle des N. Vestibulocochlearis in den Hirnstamm\u201c und die \u201ezentrale Verarbeitung [...] beim Spiralganglion\u201c (LAUER 2001, S. 4), w\u00e4hrend andere Autoren die Ansicht vertreten, dass bereits die Cochlea bzw. die im Cortischen Organ befindlichen Zilien Teil des zentralen H\u00f6rsystems bzw. der H\u00f6rbahn (auditory pathway) sind (vgl. z.B. KOLLMEIER 20053, S. 70, Online im Internet; GOLDSTEIN 1999, S. 321; GAAB 2004, S.13, Online im Internet). Im Konsensus-Statement aus den Bereichen Phoniatrie und P\u00e4daudiologie wird die Cochlea nur dann als Teil des zentralen H\u00f6rens angesehen, wenn ihre Bestandteile intakt sind (vgl. M. PTOK, R. BERGER, Chr. von DEUSTER, M. GROSS, A. LAMPRECHT-DINNESEN, A. NICKISCH, H. J. RAD\u00dc und V. UTTENWEILER 2005, Online im Internet, S. 6).\nNun aber zur\u00fcck zum H\u00f6rnerv: Er f\u00fchrt durch den inneren Geh\u00f6rgang zum Hirnstamm und m\u00fcndet dort in den Nucleus cochlearis ventralis und den Nucleus cochlearis dorsalis. Niederfrequente Schallwellen werden dabei in die ventralen, rostralen und lateralen Teile der Nuclei weitergeleitet, w\u00e4hrend h\u00f6here Frequenzen in den dorsalen, kaudalen und medialen Abteilungen der Nuclei landen. Von diesen Hirnnervenkernen aus f\u00fchrt die Verbindung zu den oberen Oliven (superior olivary complex), die sich ebenfalls in der Medulla oblongata (verl\u00e4ngertes R\u00fcckenmark) befinden. In jeder oberen Olive treffen Projektionen beider cochlearer Nuclei ein. Auf Grund der gr\u00f6\u00dferen Projektionen der jeweiligen Gegenseite kommen auf dem prim\u00e4ren auditorischen Cortex haupts\u00e4chlich die Informationen von dem Ohr auf der Gegenseite des Kopfes an (vgl. GAAB 2004, S. 14f., Online im Internet). Dar\u00fcberhinaus werden auch Signale zum Nucleus accessorius und zu den seitlichen Schleifenkernen (Nuclei lemnisci laterales) \u00fcbertragen (vgl. KOLLMEIER 20052, S. 8, Online im Internet). Von der oberen Olive aus steigen Bahnen durch den Lemniscus lateralis Trakt zum Colliculus inferior (unteres Vierh\u00fcgelpaar) auf. Von dort aus leiten zwei unterschiedliche Verbindungen die Signale zu den ventralen und dorsalen Regionen des Corpus geniculatum mediae im Thalamus (graue Substanz). Vom dorsalen Bereich aus werden Signale (also u. a. h\u00f6here Frequenzen) an den sekund\u00e4ren auditorischen Cortex weitergeleitet, w\u00e4hrend die Signale aus den ventralen Regionen (also u. a. tiefere Frequenzen) zum prim\u00e4ren auditorischen Cortex weitergeleitet werden. Der Nucleus mediale steht auch mit subkortikalen Strukturen im Frontallappen, der dorsalen Amygdala und dem Neostriatum posterior in Verbindung, so dass ein Zusammenhang zu emotionalen Reaktionen auf auditive Stimuli vermutet werden kann (vgl. GAAB 2004, S. 14f., Online im Internet).\nDie Aufgabe der H\u00f6rbahn ist es also, akustische Informationen in neuronalen Erregungsmustern und Strukturen zu kodieren und zu verarbeiten. Im H\u00f6rnerv wird die Schallintensit\u00e4t zu jedem Zeitpunkt verschl\u00fcsselt, indem die akustische Information durch die Erh\u00f6hung bzw. Synchronisation der Entladungsrate verschiedener Nervenfasern bei Stimulation der zugeh\u00f6rigen Haarzelle kodiert wird. In der oberen Olive im Hirnstamm erfolgt bereits ein Schritt zur Schalllokalisation, indem eine Auswertung der Zeit- und Intensit\u00e4tsunterschiede des akustischen Stimulus (Modulationsfrequenzen) erfolgt. Unterst\u00fctzt wird dies durch die Analyse der zeitlichen Schwankungen der akustischen Energien im Colliculus inferior (vgl. KOLLMEIER 20052, S. 9, Online im Internet).\nAllgemein lassen sich in der gesamten H\u00f6rbahn tonotope Organisationsstrukturen nachweisen, benachbarte akustische Frequenzen f\u00fchren also zu Nervenerregungen an benachbarten Orten im ZNS. \u00c4hnlich verh\u00e4lt es sich mit der spatiotopen Abbildung der r\u00e4umlichen Anordnung von Schallquellen und der periodotopen Abbildung der aufgespaltenen Modulationsfrequenzen (vgl. KOLLMEIER 2005 2, S. 9, Online im Internet).\nDie H\u00f6rbahn m\u00fcndet in den auditorischen Cortex, der in der sylvischen Spalte im oberen Temporallappen liegt. Er wird in die drei Teilbereiche prim\u00e4rer, sekund\u00e4rer und terti\u00e4rer auditorischer Cortex unterteilt.\nDas System des prim\u00e4ren auditorischen Cortex umfasst Neuronen, die auf Frequenzen zwischen 20 Hz und 20 000 Hz reagieren und die Merkmale Lautst\u00e4rke, Tonh\u00f6he und Klangfarbe registrieren. Die Hirnwindung, in der er liegt, wird h\u00e4ufig als Heschl'scher Gyrus bezeichnet. Um den prim\u00e4ren auditorischen Cortex herum befindet sich der sekund\u00e4re auditorische Cortex, der Projektionen vom prim\u00e4ren empf\u00e4ngt und mit einem anderen Bereich des Gehirns, dem Planum temporale, korrespondiert, das f\u00fcr sprachliche Funktionen aber auch allgemeine Schall- und Musikwahrnehmung zust\u00e4ndig ist. Der prim\u00e4re und der sekund\u00e4re auditorische Cortex entsprechen in ihrer Gesamtheit dem Wernicke-Zentrum.\nMit terti\u00e4rer auditorischer Cortex werden die auditorischen Assoziationsfelder bezeichnet, die lateral und ventral zum prim\u00e4ren auditorischen Cortex auf dem Gyrus frontalis inferior liegen, also dort, wo man auch das Broca-Zentrum vermutet.\nAbbildung in dieser Leseprobe nicht enthalten\nAbb 7: links: Prim\u00e4rer und sekund\u00e4rer auditorischer Cortex (GAAB 2004, S. 17, Online im Internet) rechts: Ansicht des Gehirns von lateral, zus\u00e4tzliche Kennzeichnung der Bereiche des terti\u00e4ren auditorischen Cortex (nach Wikipedia 2006, Online im Internet)\nZusammenfassend ist zu sagen, dass eine genaue tonotopische Zuordnung von Sprachstimuli und Gehirnregionen bis heute nicht eindeutig m\u00f6glich ist, da nachgewiesenerma\u00dfen gleiche Hirnareale bei verschiedensten T\u00e4tigkeiten aktiv sind (vgl. SCH\u00d6NWIESNER 2004, PRICE/THIERRY/GRIFFITHS 2005, SCOTT 2005). Vermutete Gr\u00fcnde sind zum Beispiel, dass die entsprechenden Messverfahren nicht genau genug sind, dass sich Gehirne anatomisch unterscheiden (vgl. SCH\u00d6NWIESNER 2004, S. 29f.) oder dass die funktionelle Spezialisierung auf funktionellen Verbindungen zwischen\nauditiven Assoziationen und h\u00f6heren Netzwerken basiert (vgl. PRICE/THIERRY/GRIFFITHS 2005, S. 274; KAISER/WALKER/LEIBERG/ LUTZENBERGER 2005, S. 587).\nDie neuere Forschung zeigt au\u00dferdem, dass an der auditiven Wahrnehmung noch weitere Bereiche beteiligt sind, so \u00fcbernimmt zum Beispiel das Cerebellum Aufgaben im Bereich der rein sensorischen auditorischen Weiterleitung (vgl. PETACCHI/LAIRD/FOX/BOWER 2005, S. 118).\nDass Lautst\u00e4rken und Tonh\u00f6hen, zeitliche Verh\u00e4ltnisse akustischer Reize, Musik und Sprache im auditiven System verarbeitet und Schallquellen lokalisiert werden, steht jedoch fest.\nIm Bezug auf Amplitude und Tonfrequenz ist das zentrale H\u00f6rfeld des Menschen eingegrenzt. Die Angaben im Bezug auf das Sprachfeld und den Frequenzbereich des menschlichen H\u00f6rens unterscheiden sich je nach Quelle leicht, was vermutlich, wie schon auf Seite 4 kurz angesprochen, auf die unterschiedlichen Muttersprachen der jeweiligen Studien sowie auf anatomische und damit auch auditive interindividuelle Unterschiede zur\u00fcckzuf\u00fchren ist. Abbildung 10 zeigt eine Darstellung der menschlichen H\u00f6rschwelle.\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 8: Menschliches H\u00f6rfeld (MINNING 20051, S. 1, Online im Internet)\n2.1.3 Entwicklung der auditiven Verarbeitung und Wahrnehmung\nBereits im Mutterleib entwickeln sich die Grundlagen f\u00fcr die auditive Verarbeitung und Wahrnehmung. Die Cochlea und mit ihr das Cortische Organ sind bereits in der 20. Schwangerschaftswoche ausgeformt (vgl. HOLTZ 1994, S. 45). Zu h\u00f6ren beginnt der F\u00f6tus ab der 27. Woche, was an der Reaktion in Form einer ver\u00e4nderten Herzschlagrate gemessen wurde (vgl. HENNON/HIRSH-PASEK/MICHNICK GOLINKOFF 2000, S. 51). Von Geburt an kann der S\u00e4ugling sprachliche von nicht-sprachlichen Lauten unterscheiden. Er pr\u00e4feriert au\u00dferdem die Stimme der Mutter (vgl. GRIMM 2002, S. 28). Im ersten Lebensmonat zeigt er auditive Aufmerksamkeit und beruhigt sich bei kontinuierlichen leiseren Ger\u00e4uschen aus unmittelbarer Umgebung. Zwischen dem dritten und dem sechsten Lebensmonat versucht er, Schallquellen zu lokalisieren und reagiert unterschiedlich auf verschiedene Ger\u00e4usche. Er erkennt die Stimmen beider Eltern (vgl. THIEL 2000, S. 19). Bereits im Alter von vier Monaten unterscheiden Babys stimmhafte und stimmlose Laute und diskriminieren zunehmend weitere phonetisch relevante Merkmale (vgl. LAUER 2001, S. 11). Dies zeigt sich auch w\u00e4hrend der 1. Lallperiode zwischen dem 2. und 6. Lebensmonat. Diverse Studien haben nachgewiesen, dass bei normalh\u00f6renden Kindern in der ersten Lallphase \u201elautliche Strukturen und Entwicklungsverl\u00e4ufe in den Sprachproduktionen bestehen\u201c (SENDLMEIER/R\u00d6HR- SENDLMEIER 1997, S. 218). Demnach k\u00f6nnen zumindest Kinder mit schwergradiger H\u00f6rst\u00f6rung durch Abweichungen in der Lautproduktion in dieser Phase von normalh\u00f6renden Kindern abgegrenzt werden (vgl. SENDLMEIER/R\u00d6HR-SENDLMEIER 1997, S. 218). Bei der Analyse von Sprache achten Babys vor allem auf die Prosodie, was in diversen Studien immer wieder nachgewiesen wurde. Mit 9 Monaten erkennen sie auf Grund von Prosodie und Phonemkonstellationen bereits, welche sprachlichen Muster ihrer Muttersprache zuzuordnen sind (vgl. HENNON/HIRSH-PASEK/MICHNICK GOLINKOFF 2000, S. 52 - 59). Das Kind versteht au\u00dferdem \u201ebekannte W\u00f6rter und einfache Aufforderungen im situativen Kontext\u201c (THIEL 2000, S.19). Im 10. Lebensmonat dreht es den Kopf zum benannten Gegenstand und zwischen dem 11. und 15. Lebensmonat befolgt es bereits einfache Ge- und Verbote (vgl. THIEL 2000, S. 19). Die F\u00e4higkeit zur auditiven Lokalisation von oben und unten entwickelt sich zwischen dem 15. und dem 18. Lebensmonat. Die Ausreifung der H\u00f6rbahn schlie\u00dft nach WIRTH in etwa zwischen dem 12. und dem 18. Lebensmonat ab, so dass z.B. Paukenerg\u00fcsse mit daraus resultierender Schalleitungsschwerh\u00f6rigkeit w\u00e4hrend dieser Zeit eine \u201ezentrale auditive Perzeptionsst\u00f6rung\u201c (WIRTH 2000, S. 195) bewirken k\u00f6nnen. H\u00f6rst\u00f6rungen sollten demnach vor Beginn der zweiten Lallperiode im 6. bis 9. Monat, also im g\u00fcnstigsten Fall im Alter von 3 - 4 Monaten erkannt werden (vgl. WIRTH 2000, S. 195f.). Nach B\u00d6HME ist die H\u00f6rbahnreifung jedoch erst mit dem 15. Lebensjahr vollkommen abgeschlossen, w\u00e4hrend im 1. Lebensjahr lediglich die Myelinzunahme sehr ausgepr\u00e4gt ist und die Markscheidenreifung der entsprechenden Nervenfasern stattfindet (vgl. B\u00d6HME 2006, S. 24). Daraus l\u00e4sst sich schlussfolgern, dass im Schulkindalter die H\u00f6rbahn noch nicht vollst\u00e4ndig ausgereift ist und sich durch therapeutische Intervention zumindest in Ans\u00e4tzen auf Grund der bereits abgeschlossenen Myelinisierungsphase noch beeinflussen lassen m\u00fcsste.\n2.1.4 Teilfunktionen der auditiven Verarbeitung und Wahrnehmung\nEin weiterer Punkt, der mir im Hinblick auf die F\u00f6rderung von Schulkindern mit auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rungen wichtig erscheint, ist eine Darstellung der Aufgaben, die die auditive Verarbeitung und Wahrnehmung im Bezug auf das Individuum zu erf\u00fcllen hat, also der Teilfunktionen auditiver Verarbeitung und Wahrnehmung.\nIm Konsensusstatement der American Speech-Language-Hearing Association von 1996 werden die auditorischen Prozesse als Mechanismen des auditorischen Systems bezeichnet, die f\u00fcr die Verhaltensph\u00e4nomene \u201esound localization and lateralization, auditory discrimination, auditory pattern recognition, temporal aspects of audition, including temporal resolution, temporal masking, temporal integration and temporal ordering, auditory performance decrements with competing acoustic signals and auditory performance decrements with degraded acoustic signals\u201c (vgl. FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 2, Online im Internet).\nAbbildung 11 zeigt das Modell der zentral-auditiven Verarbeitung (ZAV) nach LAUER (links) sowie das Model of the speech-processing chain nach DODD (rechts). Das Modell von DODD verdeutlicht den Einfluss auditiver Wahrnehmung und Verarbeitung auf den Kommunikationsvorgang, w\u00e4hrend das Modell von LAUER den Prozess der auditiven Verarbeitung und Wahrnehmung detailliert darstellt (vgl. Abb. 9).\nAbbildung in dieser Leseprobe nicht enthalten\nAbb. 9: Modell der zentral-auditiven Verarbeitung (LAUER 2001, S. 14) und Model of the speechprocessing chain (DODD 1995, S. 67)\nIn der auditiven Verarbeitung und Wahrnehmung spielen sogenannte top-down sowie bottom-up Prozesse eine Rolle. Hierbei handelt es sich um die unter 2.1.2 erw\u00e4hnten neuronalen Leitungsprozesse, die zum einen afferent (aufsteigend), aber auch efferent (absteigend) sein k\u00f6nnen. Da insbesondere die efferenten Prozesse nur in Tierexperimenten sicher nachgewiesen wurden, wird diese Art der Leitung im Konsensusstatement vernachl\u00e4ssigt (vgl. PTOK / BERGER / von DEUSTER / GROSS / LAMPRECHT-DINNESEN / NICKISCH / RAD\u00dc /UTTENWEILER 2005, S. 4, Online im Internet).\nAuditive F\u00e4higkeiten, die auf diesen Prozessen basieren, sind Aufmerksamkeit, Speicherung und Sequenz, Lokalisation, Diskrimination, Selektion, Analyse, Synthese, Erg\u00e4nzung sowie intramodale und intermodale Integrationsprozesse (vgl. LAUER 2001, S. 15 - 19). Sie stellen die Basis des H\u00f6rens und der Kommunikation dar. Eine Zusammenstellung von auditory processing skills findet sich an anderer Stelle im Technical Assistance Paper des FLORIDA STATE DEPARTMENT OF EDUCATION. Aus der auditiven Weiterleitung ergeben sich demnach folgende F\u00e4higkeiten: sensation, discrimination, localization, auditory attention, auditory figure ground, auditory discrimination, auditory closure, auditory synthesis, auditory analysis, auditory association, auditory memory (FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 3, Online im Internet).\nSensation bezeichnet schlichtweg das Erkennen von Schallereignissen.\nMit Aufmerksamkeit und auditory attention ist die F\u00e4higkeit gemeint, sich bestimmten auditiven Stimuli zuzuwenden und sie bewusst wahrzunehmen. Damit ist sie die Basis anderer Teilfunktionen, da ohne Aufmerksamkeit keine komplexere Verarbeitung stattfinden kann (vgl. LAUER 2001, S. 15). LAUTH sieht einen weiteren Bestandteil der Aufmerksamkeit darin, dass Dinge, die sich ebenfalls aufdr\u00e4ngen nicht beachtet werden (vgl. LAUTH 2004, S. 240). STURM bezeichnet dies als selektive Aufmerksamkeit. Er unterteilt die Aufmerksamkeit in die Komponenten generelle Wachheit oder Aktivierung, selektive Aufmerksamkeit und Vigilanz, wobei die generelle Wachheit noch in tonische Wachheit (physiologischer Spannungszustand des Organismus) und phasische Wachheit (pl\u00f6tzlich verst\u00e4rkte Aufmerksamkeit) unterschieden wird. Vigilanz umschreibt das Ph\u00e4nomen, dass Aufmerksamkeit \u00fcber einen l\u00e4ngeren Zeitraum in Anspruch genommen wird, obwohl ein Stimulus nur in seltenen und unregelm\u00e4\u00dfigen Abschnitten auftritt (vgl. STURM 1989, S. 315).\nMit Speicherung und Sequenz sowie auditory memory ist die Speicherung auditiver Stimuli im Kurzzeit- oder Arbeitsged\u00e4chtnis, dem einzig bewussten Ged\u00e4chtnisanteil des Menschen, gemeint. Es dient sowohl der Elaboration als auch dem Abruf von Informationen. Wird ein auditiver Stimulus kurzfristig im Ged\u00e4chtnis behalten, so spricht man von Speicherung oder auditiver Merkspanne (vgl. LAUER 2001, S. 16). Auditory short-term memory als Teil des auditory memory meint dasselbe Ph\u00e4nomen, wohingegen\nim auditory sequential memory die Reihenfolge von Stimuli erinnert wird (vgl. FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 3, Online im Internet). Diese F\u00e4higkeit wird bei LAUER mit Sequenz bezeichnet (vgl. LAUER 2001, S. 16). Im Zusammenhang mit dem Ged\u00e4chtnis ist auch auditory association zu sehen. Hierbei handelt es sich um eine Basisf\u00e4higkeit zur Ausbildung des H\u00f6rged\u00e4chtnisses, die es erm\u00f6glicht, auditive Reize mit gespeicherten semantischen Inhalten zu verkn\u00fcpfen.\nDie Lokalisation/Localization bezeichnet die F\u00e4higkeit, die Position des Ger\u00e4usches vom eigenen Standpunkt aus zu bestimmen (vgl. FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 3, Online im Internet). Zur horizontalen Lokalisation werden binaural aufgenommene akustische Reize auf Zeit- und Intensit\u00e4tsunterschiede hin verglichen. Auch die Ohrmuscheln spielen bei der auditiven Lokalisation eine Rolle (vgl. GOLDSTEIN 1999, S. 368f.).\nUnter der Diskrimination/Discrimination versteht man die F\u00e4higkeit, zwischen auditiven Stimuli im Hinblick auf Frequenz, Dauer und Lautst\u00e4rke, also auf parasprachlicher Ebene zu unterscheiden. Auf suprasegmentaler Ebene werden die Stimuli nach Dauer, Akzent und Intonation unterschieden. Von auditory discrimination spricht man, wenn auf segmentaler Ebene zwischen phonetischen Merkmalen von Sprachlauten unterschieden wird (vgl. FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 3, Online im Internet).\nSelektion oder Figur-Hintergrund-Unterscheidung/auditory figure ground bezeichnet die F\u00e4higkeit zur Identifizierung des prim\u00e4ren sprachlichen oder nichtsprachlichen Schalls in einer Vielzahl von Umgebungsger\u00e4uschen, die wiederum unterdr\u00fcckt werden m\u00fcssen. Besonders schwierig ist dies in einem Umfeld mit vielen Nebenger\u00e4uschen, besonders dann, wenn diese sich akustisch sehr \u00e4hnlich sind.\nAnalyse/auditory analysis meint die F\u00e4higkeit Phoneme oder Morpheme in W\u00f6rtern zu identifizieren, sowie die Position von W\u00f6rtern in l\u00e4ngeren Sprach\u00e4u\u00dferungen zu bestimmen (vgl. FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 3, Online im Internet).\nSynthese/auditory synthesis beinhaltet aus lautsprachlicher Sicht die F\u00e4higkeit, aus einzelnen Lauten oder Morphemen ein Wort zusammenzusetzen (vgl. LAUER 2001, S. 18).\nErg\u00e4nzung/auditory closure bezeichnet die F\u00e4higkeit, auditive Gebilde zu sinnvollen Informationen zu vervollst\u00e4ndigen (vgl. B\u00d6HME 2006, S. 43).\nIntramodale und intermodale Integrationsprozesse schlie\u00dflich bezeichnen das Zusammenwirken einzelner Teilfunktionen untereinander und Verbindungen der AVW mit anderen Verarbeitungsbereichen (vgl. LAUER 2001, S. 19).\nBURRE (2006, S. 33) nennt als wichtigste auditive Teilfunktionen die dichotische Wahrnehmung, die St\u00f6r- und Nutzschallperzeption, die auditive Merkf\u00e4higkeit und die phonologische Bewusstheit.\n2.2 St\u00f6rungen der auditiven Verarbeitung und Wahrnehmung\n2.2.1 Definition der auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rung (AVWS)\nAllein in der deutschsprachigen Literatur finden sich unz\u00e4hlige Begriffe f\u00fcr das mehr oder weniger gleiche Ph\u00e4nomen. So ist von der auditiven Verarbeitungsst\u00f6rung die Rede, von der zentralen Schwerh\u00f6rigkeit, der auditiven Agnosie, einer zentralen Sprachverst\u00e4ndnisst\u00f6rung, einer H\u00f6rwahrnehmungsst\u00f6rung, einer auditiven Wahrnehmungsst\u00f6rung, der zentral-auditiven Informationsverarbeitungsst\u00f6rung, der auditiven Teilleistungsst\u00f6rung, der zentralen Fehlh\u00f6rigkeit oder der Lautagnosie (vgl. NICKISCH/HEBER/BURGER-GARTNER 2001, S. 11). In der internationalen Klassifikation von Krankheiten (ICD-10), die von der WHO (World Health Organization) herausgegeben wird, gibt es bisher keinen Schl\u00fcssel f\u00fcr AVWS. Die Existenz von AVWS als Grundlage f\u00fcr die ihr zugedachten Symptome war lange Zeit umstritten (vgl. FRIELPATTI 1999, S. 345f.).\nIn Anlehnung an den von der American Speech-Language-Hearing Association (ASHA) 2000 vorgeschlagenen Begriff auditory processing disorders (APD) einigten sich die Autoren des Konsensusstatement auf den Begriff auditive Verarbeitungs- und Wahrnehmungsst\u00f6rungen (AVWS) (vgl. B\u00d6HME 2006, S. 36).\nAPD werden von der ASHA schon 1996 definiert als:\nDeficits observed in one or more of the central auditory processes responsible for generating the auditory evoked potentials and the following behaviours: sound localisation and lateralisation; auditory discrimination; auditory pattern recognition; temporal aspects of audition including temporal resolution, temporal masking, temporal integration and temporal ordering; auditory performance with competing acoustic signals; auditory performance with degraded acoustic signals.\n(FLORIDA STATE DEPARTMENT OF EDUCATION 2001, S. 4, Online im Internet)\nM. PTOK, R. BERGER, Chr. von DEUSTER, M. GROSS, A. LAMPRECHT-DINNESEN,\nA. NICKISCH, H. J. RAD\u00dc und V. UTTENWEILER definieren auditive Verarbeitungs- und Wahrnehmungsst\u00f6rungen in ihrem Konsensus-Statement wie folgt:\nEine auditive Verarbeitungs- und/oder Wahrnehmungsst\u00f6rung (AVWS) liegt vor, wenn zentrale Prozesse des H\u00f6rens gest\u00f6rt sind. Zentrale Prozesse des H\u00f6rens erm\u00f6glichen u. a. die vorbewu\u00dfte und bewu\u00dfte Analyse von Zeit-, Frequenz- und Intensit\u00e4tsbeziehungen akustischer Signale, Prozesse der binauralen Interaktion (z. B. zur Ger\u00e4uschlokalisation und Lateralisation und St\u00f6rger\u00e4uschbefreiung).\n(PTOK / BERGER / VON DEUSTER / GROSS / LAMPRECHT-DINNESEN / NICKISCH / RAD\u00dc / UTTENWEILER 2005, S. 4, Online im Internet)\nLAUER (2001, S. 20) nennt in ihrer Definition von zentral-auditiven Verarbeitungs- und Wahrnehmungsst\u00f6rungen dar\u00fcber hinaus noch das Kriterium, dass das periphere H\u00f6ren intakt ist und im engeren Sinne immer auch \u201eaudiologisch me\u00dfbare St\u00f6rungen auf der die akustischen Stimuli vorverarbeitenden Ebene der zentralen H\u00f6rbahn\u201c vorliegen.\nManche Autoren unterscheiden auch nach auditiver Verarbeitungsst\u00f6rung und auditiver Wahrnehmungsst\u00f6rung, wobei ersteres auch als zentrale Fehlh\u00f6rigkeit bezeichnet wird und sich auf die zentrale Weiterleitung und Verschaltung von Nervenimpulsen auf der zentralen H\u00f6rbahn bezieht, w\u00e4hrend die auditive Wahrnehmungsst\u00f6rung die \u201eAufbereitung und Auswertung der Nervenimpulse in der Hirnrinde\u201c meint (ROSENK\u00d6TTER 2003, S. 79). Dieser Art der Gewichtung wird im Konsensus-Statement Rechnung getragen, indem nach AVWS \u201ebei \u00fcberwiegend gest\u00f6rter Verarbeitung\u201c und von AVWS \u201ebei \u00fcberwiegend gest\u00f6rter Wahrnehmung\u201c unterschieden wird (vgl. PTOK/BERGER/von DEUSTER/ GROSS/LAMPRECHT- DINNESEN/NICKISCH/RAD\u00dc/UTTENWEILER 2005, S. 6, Online im Internet).\nIm Technical Report der ASHA \u00fcber APD vom Januar 2005 werden diese ebenfalls als \u201edifficulties in the perceptual processing of auditory information\u201c, also der \u201eauditiven Wahrnehmungsverarbeitung\u201c definiert, die die F\u00e4higkeiten \u201esound localization and lateralization; auditory discrimination; auditory pattern recognition; temporal aspects of audition, including temporal integration, temporal discrimination (e.g., temporal gap detection), temporal ordering, and temporal masking; auditory performance in competing acoustic signals (including dichotic listening); and auditory performance with degraded acoustic signals\u201c umfasst, w\u00e4hrend F\u00e4higkeiten wie \u201ephonological awareness,attention to and memory for auditory information, auditory synthesis, comprehension and interpretation of auditorily presented information, and similar skills\u201c als kognitiv- kommunikative und/oder sprachbezogene Funktionen bezeichnet werden, die nicht in der auditiven Verarbeitung inbegriffen sind (vgl. ASHA 2005, S. 2, Online im Internet).\nNICKISCH (2001, S. 16) benennt diese als zentral-auditive Prozesse der Wahrnehmung und ordnet ihnen dar\u00fcber hinaus noch die auditive Sequenzierung und die auditive Erg\u00e4nzung zu.\n[...]", "language": null, "image": "https://cdn.openpublishing.com/thumbnail/products/61259/large.webp", "pagetype": null, "links": ["/de/", "#", "#", "#content", "/de/", "/de/catalog/", "/de/faq", "/submission/upload/#/sign-in?redirect_url=/login/&lng=de", "/shoppingcart", "/submission/upload/#/?lng=de", "/de/", "/de/catalog/", "/de/faq", "/submission/upload/#/sign-in?redirect_url=/login/&lng=de", "https://www.grin.com/", "https://www.grin.com/de/catalog/", "https://www.grin.com/de/catalog/subject/34/", "https://www.grin.com/de/catalog/subject/223/", "https://www.grin.com/user/34957", "http://www.grin.com/institution/10", "https://www.grin.com/user/34957", "https://www.grin.com/user/34957", "/submission/upload/#/sign-in?redirect_url=https://www.grin.com/document/61259&lng=de", "javascript:void(0);", "https://www.grin.com/document/312571", "https://www.grin.com/document/312571", "https://www.grin.com/document/1153842", "https://www.grin.com/document/1153842", "https://www.grin.com/document/491313", "https://www.grin.com/document/491313", "https://www.grin.com/document/142086", "https://www.grin.com/document/142086", "https://www.grin.com/document/323048", "https://www.grin.com/document/323048", "https://www.grin.com/document/63391", "https://www.grin.com/document/63391", "https://www.grin.com/document/149553", "https://www.grin.com/document/149553", "https://www.grin.com/document/286195", "https://www.grin.com/document/286195", "https://www.grin.com/document/37917", "https://www.grin.com/document/37917", "https://www.grin.com/document/71250", "https://www.grin.com/document/71250", "https://www.grin.com/document/142475", "https://www.grin.com/document/142475", "https://www.grin.com/document/312373", "https://www.grin.com/document/312373", "https://www.grin.com/document/6826", "https://www.grin.com/document/6826", "https://www.grin.com/document/186512", "https://www.grin.com/document/186512", "https://www.grin.com/document/202051", "https://www.grin.com/document/202051", "https://www.grin.com/document/1165430", "https://www.grin.com/document/1165430", "https://www.grin.com/document/1185996", "https://www.grin.com/document/1185996", "https://www.grin.com/document/48762", "https://www.grin.com/document/48762", "https://www.grin.com/document/90955", "https://www.grin.com/document/90955", "/upload-de/", "mailto:info@grin.com", "/de/faq/", "/de/ueber-grin/karriere/", "/de/impressum/", "/de/faq/kontaktaufnahme/", "/de/ueber-grin/", "/de/ueder-grin/", "/de/ueber-grin/partner/", "/submission/upload/#/?lng=de", "/de/agb/", "/de/datenschutz/", "#", "?lang=en", "?lang=es", "?lang=fr", "#0"]}